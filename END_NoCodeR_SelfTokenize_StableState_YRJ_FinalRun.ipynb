{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "END_NoCodeR_SelfTokenize_StableState_YRJ_FinalRun.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67b062dacd494e5aac89dc545fd5585b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_67178e86451a44dfa58db3661d60de4f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f275759ed0394cb7a587f957e67db844",
              "IPY_MODEL_9f45a06aa256402591fc5440747dbc01"
            ]
          }
        },
        "67178e86451a44dfa58db3661d60de4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f275759ed0394cb7a587f957e67db844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5cd1f1d6ed864527bec20dfb0dc51964",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898822,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898822,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba1faef5edba4cdd90e8bc978b225967"
          }
        },
        "9f45a06aa256402591fc5440747dbc01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f65b64b43bcb414081a52eb327223991",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:02&lt;00:00, 365kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aad1f9bbd2eb44068f10bae2612daea5"
          }
        },
        "5cd1f1d6ed864527bec20dfb0dc51964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba1faef5edba4cdd90e8bc978b225967": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f65b64b43bcb414081a52eb327223991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aad1f9bbd2eb44068f10bae2612daea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b7c0b6ac76c4f28b8b4a9348425bfd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e50c15a0065245c882f23c36f43459da",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8912db0e14fc495a9e4fbab67c5c0738",
              "IPY_MODEL_05646638bc75409ca6c66692cec9f948"
            ]
          }
        },
        "e50c15a0065245c882f23c36f43459da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8912db0e14fc495a9e4fbab67c5c0738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b89d77831d5f46559114beb2aa7d15aa",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_139c0072609a4fbe8c375157773c9276"
          }
        },
        "05646638bc75409ca6c66692cec9f948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7860cfa75b924f2f8b1d092e77da0415",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 485kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4f33da1aeb90425ab3ef38189064d2c1"
          }
        },
        "b89d77831d5f46559114beb2aa7d15aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "139c0072609a4fbe8c375157773c9276": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7860cfa75b924f2f8b1d092e77da0415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4f33da1aeb90425ab3ef38189064d2c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbd3d8f1d5f241e6bc462769681349f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9579d8666f924a10a65b618092645234",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_224710ffeb324fcaaee491a1dcce5372",
              "IPY_MODEL_8ab635d93716427f9bdc053bef715531"
            ]
          }
        },
        "9579d8666f924a10a65b618092645234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "224710ffeb324fcaaee491a1dcce5372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_58d2a8632466439eb1e57cdfdb6aa852",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 150,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 150,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5b05eb730f0e4866985487c22fb9db0e"
          }
        },
        "8ab635d93716427f9bdc053bef715531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_72c06506a1484b7cbd66154bf6b42326",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 150/150 [00:00&lt;00:00, 205B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b15e4609f8834c69bf94dc3f49b27068"
          }
        },
        "58d2a8632466439eb1e57cdfdb6aa852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5b05eb730f0e4866985487c22fb9db0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72c06506a1484b7cbd66154bf6b42326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b15e4609f8834c69bf94dc3f49b27068": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ba8ae8cdae04ff0852242ffd1a6482c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0370b1b5fc094b15b4decf918cdbc708",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_33d7fdc59f9a4d07af3d60ce998e2f02",
              "IPY_MODEL_a86fe9077e194225af659e5fd61f5825"
            ]
          }
        },
        "0370b1b5fc094b15b4decf918cdbc708": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33d7fdc59f9a4d07af3d60ce998e2f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4bbd913e8f5f4b1c8429bb07cfd00c0d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 25,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 25,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2187bf02da41489aa386c5a33adec6fe"
          }
        },
        "a86fe9077e194225af659e5fd61f5825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b7a7134377a04a659502369f38de952e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 25.0/25.0 [00:00&lt;00:00, 87.4B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_45ff0e31cf03475fa5065561dc4ca9aa"
          }
        },
        "4bbd913e8f5f4b1c8429bb07cfd00c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2187bf02da41489aa386c5a33adec6fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b7a7134377a04a659502369f38de952e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "45ff0e31cf03475fa5065561dc4ca9aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9f050fb379944db1865a680cec9e956e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_930df412f1e74260b58069aa386c4293",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6b88224a4393465abc698def8cf10a4b",
              "IPY_MODEL_26d8f53ca5d1494c8008ec7f896062f7"
            ]
          }
        },
        "930df412f1e74260b58069aa386c4293": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b88224a4393465abc698def8cf10a4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9554b28a0a7f4b5b9b9bbac945ee8a0d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 498,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 498,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3743bd136bf24526b64cd327f1beacbc"
          }
        },
        "26d8f53ca5d1494c8008ec7f896062f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4da493eb6d9149d8bd607ca5f28212b4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 498/498 [00:01&lt;00:00, 379B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db5dc5f3a52949c9855a1c34c2cc23b9"
          }
        },
        "9554b28a0a7f4b5b9b9bbac945ee8a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3743bd136bf24526b64cd327f1beacbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4da493eb6d9149d8bd607ca5f28212b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db5dc5f3a52949c9855a1c34c2cc23b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EVAP2/blob/master/END_NoCodeR_SelfTokenize_StableState_YRJ_FinalRun.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOdNYOq_V5C3",
        "outputId": "85d7c491-a35c-4b4c-bd65-c9588240d155"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Mar 13 14:56:13 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4uN4sSQlA7w"
      },
      "source": [
        "import torch\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import fileinput\r\n",
        "import re\r\n",
        "import itertools\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from IPython.core.display import display, HTML\r\n",
        "#import seaborn as sns\r\n",
        "import dateutil.parser\r\n",
        "import datetime\r\n",
        "#from ipyfilechooser import FileChooser\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import gzip\r\n",
        "import dateutil.parser\r\n",
        "from datetime import datetime\r\n",
        "import sys\r\n",
        "import glob\r\n",
        "import matplotlib.dates as mdates\r\n",
        "from datetime import timedelta\r\n",
        "import ipywidgets as widgets\r\n",
        "from IPython.display import display\r\n",
        "import torch\r\n",
        "import json\r\n",
        "import random\r\n",
        "import spacy\r\n",
        "from pprint import pprint\r\n",
        "import six\r\n",
        "import sys, token, tokenize\r\n",
        "import ast\r\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\r\n",
        "                              TensorDataset)\r\n",
        "import random\r\n",
        "import math\r\n",
        "import time\r\n",
        "from torch.utils.data.distributed import DistributedSampler\r\n",
        "#from tensorboardX import SummaryWriter\r\n",
        "from tqdm import tqdm, trange\r\n",
        "USE_CUDA = torch.cuda.is_available()\r\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\r\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzTyfBvwcT50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f24af10b-67c6-4051-9e14-cdc0d7901f6f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRNwGADTvOuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da28173f-1ebc-44ff-ddf3-51c4f3b2db9c"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 10.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 57.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 59.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=832b66d3c298e5f5ea1722ff45eeea9261c11c071c4069149fea8f39d0b0dfa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6XDZbeWK6u_"
      },
      "source": [
        "from torch.jit import script, trace\r\n",
        "import torch.nn as nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F\r\n",
        "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\r\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\r\n",
        "import tokenize"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_7b5amJJIQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1612131d-c134-48c7-b15d-41338961087e"
      },
      "source": [
        "%%bash\r\n",
        "python -m spacy download en\r\n",
        "python -m spacy download de"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.1)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py): started\n",
            "  Building wheel for de-core-news-sm (setup.py): finished with status 'done'\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp37-none-any.whl size=14907057 sha256=b320af4f9f05f0019d692eb45bb671b5704a3b71b524d27085cb283ab4a8c68d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0jdjn1lm/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqKpFyhplfvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c64aa79a-3461-4db7-ce38-88daaf8ebea5"
      },
      "source": [
        "!wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\r\n",
        "!wget -O gpt2_bpe_encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\r\n",
        "\r\n",
        "!unzip /content/python.zip\r\n",
        "!cp /content/python/final/jsonl/train/python_train_0.jsonl.gz .\r\n",
        "!gzip -d /content/python_train_0.jsonl.gz\r\n",
        "\r\n",
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/english_python_data.txt .\r\n",
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/english_python_cleaned.txt .\r\n",
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone.csv ."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-13 14:57:21--  https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.114.45\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.114.45|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 940909997 (897M) [application/zip]\n",
            "Saving to: ‘python.zip’\n",
            "\n",
            "python.zip          100%[===================>] 897.32M  46.6MB/s    in 20s     \n",
            "\n",
            "2021-03-13 14:57:41 (45.7 MB/s) - ‘python.zip’ saved [940909997/940909997]\n",
            "\n",
            "--2021-03-13 14:57:41--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [text/plain]\n",
            "Saving to: ‘gpt2_bpe_encoder.json’\n",
            "\n",
            "gpt2_bpe_encoder.js 100%[===================>]   1018K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-03-13 14:57:42 (21.5 MB/s) - ‘gpt2_bpe_encoder.json’ saved [1042301/1042301]\n",
            "\n",
            "Archive:  /content/python.zip\n",
            "   creating: python/\n",
            "   creating: python/final/\n",
            "   creating: python/final/jsonl/\n",
            "   creating: python/final/jsonl/train/\n",
            "  inflating: python/final/jsonl/train/python_train_9.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_12.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_10.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_0.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_6.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_2.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_4.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_8.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_11.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_5.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_13.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_3.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_1.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_7.jsonl.gz  \n",
            "   creating: python/final/jsonl/test/\n",
            "  inflating: python/final/jsonl/test/python_test_0.jsonl.gz  \n",
            "   creating: python/final/jsonl/valid/\n",
            "  inflating: python/final/jsonl/valid/python_valid_0.jsonl.gz  \n",
            "  inflating: python_dedupe_definitions_v2.pkl  \n",
            "  inflating: python_licenses.pkl     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGoF0UiKmLg-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262,
          "referenced_widgets": [
            "67b062dacd494e5aac89dc545fd5585b",
            "67178e86451a44dfa58db3661d60de4f",
            "f275759ed0394cb7a587f957e67db844",
            "9f45a06aa256402591fc5440747dbc01",
            "5cd1f1d6ed864527bec20dfb0dc51964",
            "ba1faef5edba4cdd90e8bc978b225967",
            "f65b64b43bcb414081a52eb327223991",
            "aad1f9bbd2eb44068f10bae2612daea5",
            "9b7c0b6ac76c4f28b8b4a9348425bfd9",
            "e50c15a0065245c882f23c36f43459da",
            "8912db0e14fc495a9e4fbab67c5c0738",
            "05646638bc75409ca6c66692cec9f948",
            "b89d77831d5f46559114beb2aa7d15aa",
            "139c0072609a4fbe8c375157773c9276",
            "7860cfa75b924f2f8b1d092e77da0415",
            "4f33da1aeb90425ab3ef38189064d2c1",
            "bbd3d8f1d5f241e6bc462769681349f8",
            "9579d8666f924a10a65b618092645234",
            "224710ffeb324fcaaee491a1dcce5372",
            "8ab635d93716427f9bdc053bef715531",
            "58d2a8632466439eb1e57cdfdb6aa852",
            "5b05eb730f0e4866985487c22fb9db0e",
            "72c06506a1484b7cbd66154bf6b42326",
            "b15e4609f8834c69bf94dc3f49b27068",
            "9ba8ae8cdae04ff0852242ffd1a6482c",
            "0370b1b5fc094b15b4decf918cdbc708",
            "33d7fdc59f9a4d07af3d60ce998e2f02",
            "a86fe9077e194225af659e5fd61f5825",
            "4bbd913e8f5f4b1c8429bb07cfd00c0d",
            "2187bf02da41489aa386c5a33adec6fe",
            "b7a7134377a04a659502369f38de952e",
            "45ff0e31cf03475fa5065561dc4ca9aa",
            "9f050fb379944db1865a680cec9e956e",
            "930df412f1e74260b58069aa386c4293",
            "6b88224a4393465abc698def8cf10a4b",
            "26d8f53ca5d1494c8008ec7f896062f7",
            "9554b28a0a7f4b5b9b9bbac945ee8a0d",
            "3743bd136bf24526b64cd327f1beacbc",
            "4da493eb6d9149d8bd607ca5f28212b4",
            "db5dc5f3a52949c9855a1c34c2cc23b9"
          ]
        },
        "outputId": "9fc3c616-361d-4035-f246-5ebb8f4e4b31"
      },
      "source": [
        "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\r\n",
        "from transformers import AutoTokenizer, AutoModel\r\n",
        "\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\r\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\r\n",
        "spacy_en = spacy.load('en')\r\n",
        "def tokenize_en(text):\r\n",
        "    \"\"\"\r\n",
        "    Tokenizes English text from a string into a list of strings\r\n",
        "    \"\"\"\r\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\r\n",
        "#model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\r\n",
        "#model.to(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67b062dacd494e5aac89dc545fd5585b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898822.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b7c0b6ac76c4f28b8b4a9348425bfd9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbd3d8f1d5f241e6bc462769681349f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=150.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ba8ae8cdae04ff0852242ffd1a6482c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=25.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f050fb379944db1865a680cec9e956e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=498.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NPC5Tjn-eHe"
      },
      "source": [
        "\"\"\"\r\n",
        "    Removes docstrings and comments\r\n",
        "\"\"\"\r\n",
        "def remove_docstrings_comments(src_string, doc_string=None, debug=False):\r\n",
        "    mod = []\r\n",
        "\r\n",
        "    prev_toktype = token.INDENT\r\n",
        "    first_line = None\r\n",
        "    last_lineno = -1\r\n",
        "    last_col = 0\r\n",
        "    try:\r\n",
        "        #tokgen = tokenize.generate_tokens(source.readline)\r\n",
        "        tokgen = tokenize.generate_tokens(six.StringIO(src_string.rstrip()).readline)\r\n",
        "        for toktype, ttext, (slineno, scol), (elineno, ecol), ltext in tokgen:\r\n",
        "            if 0:   # Change to if 1 to see the tokens fly by.\r\n",
        "                print(\"%10s %-14s %-20r %r\" % (\r\n",
        "                    tokenize.tok_name.get(toktype, toktype),\r\n",
        "                    \"%d.%d-%d.%d\" % (slineno, scol, elineno, ecol),\r\n",
        "                    ttext, ltext\r\n",
        "                    ))\r\n",
        "            if slineno > last_lineno:\r\n",
        "                last_col = 0\r\n",
        "            if scol > last_col:\r\n",
        "                mod.append(\" \" * (scol - last_col))\r\n",
        "            if toktype == token.STRING and prev_toktype == token.INDENT:\r\n",
        "                # Docstring\r\n",
        "                mod.append(\"#--\")\r\n",
        "            elif toktype == tokenize.COMMENT:\r\n",
        "                # Comment\r\n",
        "                mod.append(\"##\")\r\n",
        "            else:\r\n",
        "                mod.append(ttext)\r\n",
        "            prev_toktype = toktype\r\n",
        "            last_col = ecol\r\n",
        "            last_lineno = elineno\r\n",
        "        return \"\".join(mod)\r\n",
        "    except:\r\n",
        "        print(doc_string)\r\n",
        "        print(src_string )\r\n",
        "        print(sys.exc_info())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1px_H2FL3xEE"
      },
      "source": [
        "### CodeSearchNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0T-ZXnm-z3J"
      },
      "source": [
        "columns_long_list = ['repo', 'path', 'url', 'code', \r\n",
        "                     'code_tokens', 'docstring', 'docstring_tokens', \r\n",
        "                     'language', 'partition']\r\n",
        "\r\n",
        "with open(\"/content/python_train_0.jsonl\",'r') as f:\r\n",
        "    pd.concat([pd.read_json(f, \r\n",
        "                            orient='records', \r\n",
        "                            lines=True)[columns_long_list] \r\n",
        "                        ], sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4kcq2qc--AE"
      },
      "source": [
        "with open(\"/content/python_train_0.jsonl\",'r') as f:\r\n",
        "    my_df=pd.read_json(f, \r\n",
        "                orient='records', \r\n",
        "                lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhJZMD7y-bev"
      },
      "source": [
        "my_df_copy = my_df.loc[:20000,['docstring', 'code']].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMGDaIM-_L62"
      },
      "source": [
        "my_df_copy['docstring_len'] =my_df_copy['docstring'].apply(lambda x: len(x))\r\n",
        "my_df_copy['code_len'] = my_df_copy['code'].apply(lambda x: len(x))\r\n",
        "my_df_copy['cleaned_code'] = my_df_copy.apply(lambda x: remove_docstrings_comments(x.code, x.docstring), axis=1)\r\n",
        "my_df_copy['cleaned_code_len'] = my_df_copy['cleaned_code'].apply(lambda x: len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08lnHBCS_CFs",
        "outputId": "8114131b-94f5-4d73-8cc5-a8949c4617e5"
      },
      "source": [
        "my_df_copy[my_df_copy['cleaned_code_len'] <= 256].count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "docstring           5612\n",
              "code                5612\n",
              "docstring_len       5612\n",
              "code_len            5612\n",
              "cleaned_code        5612\n",
              "cleaned_code_len    5612\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltpMfQiZTFhA"
      },
      "source": [
        "with open(\"gpt2_bpe_encoder.json\") as f:\r\n",
        "    gpt2_bpe=json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JA2hXmMeGWl"
      },
      "source": [
        "my_df_copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kglj0SUbeIP7"
      },
      "source": [
        "nl_to_pl_df = nl_to_pl_df.append(my_df_copy,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c35HYbYxWBk7"
      },
      "source": [
        "### Create datasets and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nobKWg_DJ5Xp"
      },
      "source": [
        "nl_to_pl_df = pd.read_csv('/content/end_capstone.csv')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prv7EIAX5aOj"
      },
      "source": [
        "SEED = 1234\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR7nruWfNQNB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "91f7b90e-96f3-42c6-8c9a-6a01c26d964b"
      },
      "source": [
        "print(nl_to_pl_df['code_len'].max(),nl_to_pl_df['code_len'].min())\r\n",
        "print(nl_to_pl_df['docstring_len'].max(),nl_to_pl_df['docstring_len'].min())\r\n",
        "nl_to_pl_df[nl_to_pl_df['code_len'] ==0]\r\n",
        "nl_to_pl_df[(nl_to_pl_df['code_len'] > 256) & (nl_to_pl_df['code_len'] < 512)] "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2443 11\n",
            "313 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>docstring</th>\n",
              "      <th>code</th>\n",
              "      <th>docstring_len</th>\n",
              "      <th>code_len</th>\n",
              "      <th>cleaned_code</th>\n",
              "      <th>cleaned_code_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td># Write a program to check whether a number is...</td>\n",
              "      <td>num = 337\\n\\nif num &gt; 1:\\n    for i in range(2...</td>\n",
              "      <td>60</td>\n",
              "      <td>311</td>\n",
              "      <td>num = 337\\n\\nif num &gt; 1:\\n    for i in range(2...</td>\n",
              "      <td>308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td># Write a program to find the factorial of a n...</td>\n",
              "      <td>num = 13\\nfactorial = 1\\n\\nif num &lt; 0:\\n    pr...</td>\n",
              "      <td>52</td>\n",
              "      <td>265</td>\n",
              "      <td>num = 13\\nfactorial = 1\\n\\nif num &lt; 0:\\n    pr...</td>\n",
              "      <td>262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td># Write a function that takes in height(m) and...</td>\n",
              "      <td>\\ndef bmi(height: \"Meters\", weight: \"Kgs\"):\\n ...</td>\n",
              "      <td>98</td>\n",
              "      <td>450</td>\n",
              "      <td>\\ndef bmi(height: \"Meters\", weight: \"Kgs\"):\\n ...</td>\n",
              "      <td>447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td># write a python program to sort dictionary it...</td>\n",
              "      <td>dict1 = {'car': [7, 6, 3],  \\n        'bike': ...</td>\n",
              "      <td>50</td>\n",
              "      <td>262</td>\n",
              "      <td>dict1 = {'car': [7, 6, 3],  \\n        'bike': ...</td>\n",
              "      <td>260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td># write a python program to Get the maximum an...</td>\n",
              "      <td>\\nmy_dict = {'x':500, 'y':5874, 'z': 560}\\n\\nk...</td>\n",
              "      <td>78</td>\n",
              "      <td>276</td>\n",
              "      <td>\\nmy_dict = {'x':500, 'y':5874, 'z': 560}\\n\\nk...</td>\n",
              "      <td>274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4352</th>\n",
              "      <td># Define a class named Shape and its subclass ...</td>\n",
              "      <td>\\n\\nclass Shape(object):\\n    def __init__(sel...</td>\n",
              "      <td>234</td>\n",
              "      <td>314</td>\n",
              "      <td>\\n\\nclass Shape(object):\\n    def __init__(sel...</td>\n",
              "      <td>312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4354</th>\n",
              "      <td># Write a python program for a binary search f...</td>\n",
              "      <td>import math\\ndef bin_search(li, element):\\n   ...</td>\n",
              "      <td>171</td>\n",
              "      <td>390</td>\n",
              "      <td>import math\\ndef bin_search(li, element):\\n   ...</td>\n",
              "      <td>388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4359</th>\n",
              "      <td># Write a python program to check if a number ...</td>\n",
              "      <td>n = int(input(\"Enter any number: \"))\\nsum1 = 0...</td>\n",
              "      <td>67</td>\n",
              "      <td>261</td>\n",
              "      <td>n = int(input(\"Enter any number: \"))\\nsum1 = 0...</td>\n",
              "      <td>259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4360</th>\n",
              "      <td># Write a python program to Check if a Number ...</td>\n",
              "      <td>sum1 = 0\\nnum = int(input(\"Enter a number:\"))\\...</td>\n",
              "      <td>65</td>\n",
              "      <td>347</td>\n",
              "      <td>sum1 = 0\\nnum = int(input(\"Enter a number:\"))\\...</td>\n",
              "      <td>345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4362</th>\n",
              "      <td># Write python program to find whether-number-...</td>\n",
              "      <td>def is_power_of_two(n):\\n    \"\"\"Return True if...</td>\n",
              "      <td>56</td>\n",
              "      <td>310</td>\n",
              "      <td>def is_power_of_two(n):\\n    #--\\n    if n &lt;= ...</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>613 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              docstring  ... cleaned_code_len\n",
              "5     # Write a program to check whether a number is...  ...              308\n",
              "7     # Write a program to find the factorial of a n...  ...              262\n",
              "53    # Write a function that takes in height(m) and...  ...              447\n",
              "62    # write a python program to sort dictionary it...  ...              260\n",
              "74    # write a python program to Get the maximum an...  ...              274\n",
              "...                                                 ...  ...              ...\n",
              "4352  # Define a class named Shape and its subclass ...  ...              312\n",
              "4354  # Write a python program for a binary search f...  ...              388\n",
              "4359  # Write a python program to check if a number ...  ...              259\n",
              "4360  # Write a python program to Check if a Number ...  ...              345\n",
              "4362  # Write python program to find whether-number-...  ...              270\n",
              "\n",
              "[613 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJUyE3S39KWy"
      },
      "source": [
        "def set_seed(seed):\r\n",
        "    random.seed(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "    # if n_gpu > 0:\r\n",
        "    #     torch.cuda.manual_seed_all(seed)\r\n",
        "set_seed(0x1112233)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBDY67cXevhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c1f8827-af81-4771-c0cf-bef5a25dd096"
      },
      "source": [
        "!cp -rf /content/drive/MyDrive/EVA4/END_Capstone/cubert .\r\n",
        "!pip install -r /content/cubert/requirements.txt\r\n",
        "sys.path.append(\"/content/cubert/\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/16/0f9376af49c6adcfbaf2470a8f500105a74dd803aa54ac0110af445837b5/bert_tensorflow-1.0.4-py2.py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 20.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 25.2MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 20.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 51kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 61kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 6.5MB/s \n",
            "\u001b[?25hCollecting dopamine-rl==3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/a8/7cf1fe1b5b11a7165c8901bc50091e3fa68f97f48fb4035f31cf6fc675f2/dopamine_rl-3.0.1-py3-none-any.whl (84kB)\n",
            "\r\u001b[K     |███▉                            | 10kB 32.5MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 20kB 40.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 30kB 31.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 40kB 20.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 51kB 17.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 61kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 71kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 81kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from -r /content/cubert/requirements.txt (line 3)) (2019.12.20)\n",
            "Collecting tensor2tensor\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/7c/9e87d30cefad5cbc390bb7f626efb3ded9b19416b8160f1a1278da81b218/tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 14.7MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow->-r /content/cubert/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: gym>=0.10.5 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (0.17.3)\n",
            "Requirement already satisfied: Pillow>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (7.0.0)\n",
            "Requirement already satisfied: opencv-python>=3.4.1.15 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (4.1.2.30)\n",
            "Requirement already satisfied: gin-config>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (0.4.0)\n",
            "Collecting pypng\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.4.1)\n",
            "Collecting kfac\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/36/06fe2c757044bb51906fef231ac48cc5bf9a277fc9a8c7e1108d7e9e8cfd/kfac-0.2.3-py2.py3-none-any.whl (191kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (4.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.10.0)\n",
            "Collecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 51.3MB/s \n",
            "\u001b[?25hCollecting gevent\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/85/df3d1fd2b60a87455475f93012861b76a411d27ba4a0859939adbe2c9dc3/gevent-21.1.2-cp37-cp37m-manylinux2010_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 21.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.7.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.16.0)\n",
            "Collecting bz2file\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Collecting tensorflow-gan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/2e/62922111d7d50e1900e3030764743ea7735540ce103b3ab30fd5cd2d8a2b/tensorflow_gan-2.0.0-py2.py3-none-any.whl (365kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 53.1MB/s \n",
            "\u001b[?25hCollecting tf-slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 57.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (4.0.1)\n",
            "Collecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.23.0)\n",
            "Collecting mesh-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/20/23bbc94034e16bb1ace73e9e7922226e31d6d36b88dcfa257d2c59b3f465/mesh_tensorflow-0.1.18-py3-none-any.whl (361kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 55.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.12.8)\n",
            "Collecting tensorflow-probability==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/3a/c10b6c22320531c774402ac7186d1b673374e2a9d12502cbc8d811e4601c/tensorflow_probability-0.7.0-py2.py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (0.8.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (3.12.4)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 57.1MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 55.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (1.1.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.5->dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.5->dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.2.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.17.4)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.7.1)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/42/d8f11eaef844bee267821281fffe445e49cf31b486d72a81821a9d45cd0a/zope.interface-5.2.0-cp37-cp37m-manylinux2010_x86_64.whl (237kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 57.7MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Collecting greenlet<2.0,>=0.4.17; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/25/f52f0dde4135833c2f85eae30a749d260231065b46942534df8366d7e1ec/greenlet-1.0.0-cp37-cp37m-manylinux2010_x86_64.whl (160kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (54.0.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.2.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gan->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.28.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (20.3.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.1.5)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (5.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.27.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.26.1)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.0.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (3.3.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.53.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (4.2.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (20.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (3.7.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (3.7.4.3)\n",
            "Building wheels for collected packages: pypng, bz2file, gast\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-cp37-none-any.whl size=67163 sha256=49c9078a41c6165d730f778c7d5f7b3ffa5496d42ee75dde23ec5479df35f946\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-cp37-none-any.whl size=6884 sha256=907f3462f712d65c6ea798c74292d3d4fd5f89b546eb581b76a87a7fd7e0dd76\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=26030006b489be50c388c71237767ad96241b359219fb84667f986b4f0c6a045\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built pypng bz2file gast\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: bert-tensorflow, dopamine-rl, pypng, tensorflow-probability, kfac, tensorflow-addons, zope.interface, zope.event, greenlet, gevent, bz2file, tensorflow-gan, tf-slim, gunicorn, mesh-tensorflow, tensor2tensor, keras-applications, tensorflow-estimator, tensorboard, gast, tensorflow\n",
            "  Found existing installation: dopamine-rl 1.0.5\n",
            "    Uninstalling dopamine-rl-1.0.5:\n",
            "      Successfully uninstalled dopamine-rl-1.0.5\n",
            "  Found existing installation: tensorflow-probability 0.12.1\n",
            "    Uninstalling tensorflow-probability-0.12.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.12.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed bert-tensorflow-1.0.4 bz2file-0.98 dopamine-rl-3.0.1 gast-0.2.2 gevent-21.1.2 greenlet-1.0.0 gunicorn-20.0.4 keras-applications-1.0.8 kfac-0.2.3 mesh-tensorflow-0.1.18 pypng-0.0.20 tensor2tensor-1.15.7 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-addons-0.12.1 tensorflow-estimator-1.15.1 tensorflow-gan-2.0.0 tensorflow-probability-0.7.0 tf-slim-1.1.0 zope.event-4.5.0 zope.interface-5.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3k4UAjKgagA"
      },
      "source": [
        "\"\"\"A Python tokenizer subclass of CuBertTokenizer.\"\"\"\r\n",
        "import keyword\r\n",
        "import re\r\n",
        "import tokenize\r\n",
        "import typing\r\n",
        "from typing import Any\r\n",
        "from typing import List\r\n",
        "from typing import Sequence\r\n",
        "from typing import Tuple\r\n",
        "from absl import logging\r\n",
        "from cubert import cubert_tokenizer\r\n",
        "from cubert import unified_tokenizer\r\n",
        "\r\n",
        "\r\n",
        "class PythonTokenizer2(cubert_tokenizer.CuBertTokenizer):\r\n",
        "  \"\"\"Tokenizer that extracts Python's lexical elements preserving strings.\"\"\"\r\n",
        "  _TOKEN_TYPE_MAP = {\r\n",
        "      tokenize.COMMENT: unified_tokenizer.TokenKind.COMMENT,\r\n",
        "      tokenize.DEDENT: unified_tokenizer.TokenKind.KEYWORD,\r\n",
        "      tokenize.ENDMARKER: unified_tokenizer.TokenKind.EOS,\r\n",
        "      tokenize.ERRORTOKEN: unified_tokenizer.TokenKind.ERROR,\r\n",
        "      tokenize.INDENT: unified_tokenizer.TokenKind.KEYWORD,\r\n",
        "      tokenize.NEWLINE: unified_tokenizer.TokenKind.NEWLINE,\r\n",
        "      tokenize.NL: unified_tokenizer.TokenKind.PUNCTUATION,\r\n",
        "      tokenize.NUMBER: unified_tokenizer.TokenKind.NUMBER,\r\n",
        "      tokenize.OP: unified_tokenizer.TokenKind.PUNCTUATION,\r\n",
        "      tokenize.STRING: unified_tokenizer.TokenKind.STRING,\r\n",
        "  }\r\n",
        "  _REVERSE_TOKEN_MAP = {\r\n",
        "      cubert_tokenizer.token_from_token_type(tokenize.INDENT):\r\n",
        "          tokenize.INDENT,\r\n",
        "      cubert_tokenizer.token_from_token_type(tokenize.DEDENT):\r\n",
        "          tokenize.DEDENT,\r\n",
        "      unified_tokenizer.quote_special(unified_tokenizer.TokenKind.EOS.name):\r\n",
        "          tokenize.ENDMARKER,\r\n",
        "      unified_tokenizer.quote_special(unified_tokenizer.TokenKind.ERROR.name):\r\n",
        "          tokenize.ERRORTOKEN,\r\n",
        "      unified_tokenizer.quote_special(unified_tokenizer.TokenKind.NEWLINE.name):\r\n",
        "          tokenize.NEWLINE,\r\n",
        "      cubert_tokenizer.token_from_token_type(tokenize.NL):\r\n",
        "          tokenize.NL,\r\n",
        "  }\r\n",
        "  # Adding the end-of-string anchor \\Z below, since re.fullmatch wasn't\r\n",
        "  # available in Python2.\r\n",
        "  _NUMBERS = re.compile('(' + tokenize.Number + r')\\Z')  # pytype: disable=module-attr\r\n",
        "  _SINGLE_STRINGS = re.compile('(' + tokenize.String + r')\\Z')  # pytype: disable=module-attr\r\n",
        "  _TRIPLE_STRING_BEGINNINGS = re.compile(tokenize.Triple)  # pytype: disable=module-attr\r\n",
        "  _COMMENTS = re.compile('(' + tokenize.Comment + r')\\Z')  # pytype: disable=module-attr\r\n",
        "\r\n",
        "  _EXACT_TOKEN_TYPES = tokenize.EXACT_TOKEN_TYPES.keys()  # pytype: disable=module-attr\r\n",
        "\r\n",
        "  # Token types that CubertTokenizer will tokenize by their type and not\r\n",
        "  # content.\r\n",
        "  _TOKEN_TYPES_TO_TOKENIZE_BY_TYPE = [\r\n",
        "      tokenize.NEWLINE, tokenize.DEDENT, tokenize.NL\r\n",
        "  ]\r\n",
        "\r\n",
        "  def tokenize_and_abstract(\r\n",
        "      self,\r\n",
        "      source_code):\r\n",
        "    \"\"\"Produces a language-agnostic tokenization of the input code.\"\"\"\r\n",
        "    agnostic_tokens: List[unified_tokenizer.AbstractToken] = []\r\n",
        "\r\n",
        "    try:\r\n",
        "      token_tuples = unified_tokenizer.code_to_tokens(source_code)\r\n",
        "    except (tokenize.TokenError, IndentationError) as e:\r\n",
        "      logging.warning('The tokenizer raised exception `%s` while parsing %s', e,\r\n",
        "                      source_code)\r\n",
        "\r\n",
        "      # We don't try to do recovery from errors quite yet. Emit just an\r\n",
        "      # error and end-of-sequence and return.\r\n",
        "      agnostic_tokens.append(\r\n",
        "          unified_tokenizer.AbstractToken(\r\n",
        "              unified_tokenizer.quote_special(\r\n",
        "                  unified_tokenizer.TokenKind.ERROR.name),\r\n",
        "              unified_tokenizer.TokenKind.ERROR,\r\n",
        "              unified_tokenizer.TokenMetadata(\r\n",
        "                  start=unified_tokenizer.Position(\r\n",
        "                      line=0, column=0),\r\n",
        "                  end=unified_tokenizer.Position(\r\n",
        "                      line=0, column=0))))\r\n",
        "      agnostic_tokens.append(\r\n",
        "          unified_tokenizer.AbstractToken(\r\n",
        "              unified_tokenizer.quote_special(\r\n",
        "                  unified_tokenizer.TokenKind.EOS.name),\r\n",
        "              unified_tokenizer.TokenKind.EOS,\r\n",
        "              unified_tokenizer.TokenMetadata(\r\n",
        "                  start=unified_tokenizer.Position(\r\n",
        "                      line=0, column=0),\r\n",
        "                  end=unified_tokenizer.Position(\r\n",
        "                      line=0, column=0))))\r\n",
        "      return agnostic_tokens\r\n",
        "\r\n",
        "    for token_tuple in token_tuples:\r\n",
        "      spelling = token_tuple.string\r\n",
        "      kind = token_tuple.type\r\n",
        "\r\n",
        "      # We'll adjust the spelling of some tokens, e.g., those that we\r\n",
        "      # tokenize by their type rather than their original spelling. Indentation\r\n",
        "      # and dedentation tokens are like that.\r\n",
        "      adjusted_spelling = spelling\r\n",
        "      token_kind = unified_tokenizer.TokenKind.NONE\r\n",
        "      if kind == tokenize.NAME:\r\n",
        "        # Disambiguate identifiers from keywords.\r\n",
        "        if keyword.iskeyword(spelling):\r\n",
        "          token_kind = unified_tokenizer.TokenKind.KEYWORD\r\n",
        "        else:\r\n",
        "          token_kind = unified_tokenizer.TokenKind.IDENTIFIER\r\n",
        "      else:\r\n",
        "        if kind in PythonTokenizer2._TOKEN_TYPES_TO_TOKENIZE_BY_TYPE:\r\n",
        "          # Replace spelling with type.\r\n",
        "          adjusted_spelling = cubert_tokenizer.token_from_token_type(kind)\r\n",
        "        elif kind is tokenize.INDENT:\r\n",
        "          # For INDENT, in particular, we also record the actual spelling too.\r\n",
        "          adjusted_spelling = '{indent}{spelling}'.format(\r\n",
        "              indent=cubert_tokenizer.token_from_token_type(kind),\r\n",
        "              spelling=spelling)\r\n",
        "          #print(adjusted_spelling)\r\n",
        "        elif kind == tokenize.ENDMARKER:\r\n",
        "          adjusted_spelling = unified_tokenizer.quote_special(\r\n",
        "              unified_tokenizer.TokenKind.EOS.name)\r\n",
        "\r\n",
        "        # Map everything according to table.\r\n",
        "        try:\r\n",
        "          token_kind = PythonTokenizer2._TOKEN_TYPE_MAP[kind]\r\n",
        "        except KeyError as ke:\r\n",
        "          # It's possible we're here because of async/await. Those kept being\r\n",
        "          # turned into keywords and then removed from keywords, so we can't\r\n",
        "          # rely on knowing which they are. We'll check by spelling.\r\n",
        "          # See: https://bugs.python.org/issue30406\r\n",
        "          # and https://bugs.python.org/issue33260\r\n",
        "          # and https://bugs.python.org/issue35975\r\n",
        "          if spelling in ('async', 'await'):\r\n",
        "            token_kind = unified_tokenizer.TokenKind.KEYWORD\r\n",
        "          else:\r\n",
        "            raise ValueError('While trying to turn Python token %r into an '\r\n",
        "                             'agnostic one, raised %r.' %\r\n",
        "                             ((spelling, kind), ke))\r\n",
        "\r\n",
        "      start_line, start_column = token_tuple.start\r\n",
        "      end_line, end_column = token_tuple.end\r\n",
        "      # Unlike other languages, NEWLINE tokens are reported as ending on the\r\n",
        "      # same line as where they started. We adjust that here, to stick to the\r\n",
        "      # same convention as other tokenizers.\r\n",
        "      if ((token_kind == unified_tokenizer.TokenKind.NEWLINE) or\r\n",
        "          (kind == tokenize.NL)):\r\n",
        "        end_line = start_line + 1\r\n",
        "        end_column = 0\r\n",
        "\r\n",
        "      agnostic_tokens.append(\r\n",
        "          unified_tokenizer.AbstractToken(\r\n",
        "              spelling=adjusted_spelling, kind=token_kind,\r\n",
        "              metadata=unified_tokenizer.TokenMetadata(\r\n",
        "                  # Python's tokenizer counts lines starting from 1, so we\r\n",
        "                  # have to offset what we read from the `TokenInfo` tuple.\r\n",
        "                  start=unified_tokenizer.Position(\r\n",
        "                      line=start_line - 1, column=start_column),\r\n",
        "                  end=unified_tokenizer.Position(\r\n",
        "                      line=end_line - 1, column=end_column))))\r\n",
        "    #print(agnostic_tokens)\r\n",
        "    return agnostic_tokens\r\n",
        "\r\n",
        "  def untokenize_abstract(self, whole_tokens):\r\n",
        "    # Reconstruct Python tokenizer tuples, so that Python's untokenize can be\r\n",
        "    # invoked.\r\n",
        "    token_tuples: List[Tuple[int, str]] = []\r\n",
        "\r\n",
        "    for whole_token in whole_tokens:\r\n",
        "      if whole_token in PythonTokenizer2._EXACT_TOKEN_TYPES:\r\n",
        "        token_tuples.append((tokenize.OP, whole_token))\r\n",
        "      elif cubert_tokenizer.token_from_token_type(\r\n",
        "          tokenize.INDENT) in whole_token:\r\n",
        "        # We baked the type and spelling into one token. Break them up.\r\n",
        "        spelling = whole_token.replace(\r\n",
        "            cubert_tokenizer.token_from_token_type(tokenize.INDENT), '')\r\n",
        "        token_tuples.append((tokenize.INDENT, spelling))\r\n",
        "      elif whole_token in PythonTokenizer2._REVERSE_TOKEN_MAP:\r\n",
        "        python_kind = PythonTokenizer2._REVERSE_TOKEN_MAP[whole_token]\r\n",
        "        if python_kind in (tokenize.DEDENT, tokenize.ENDMARKER,\r\n",
        "                           tokenize.ERRORTOKEN):\r\n",
        "          spelling = ''\r\n",
        "        else:  # python_kind in (tokenize.NEWLINE, tokenize.NL)\r\n",
        "          spelling = '\\n'\r\n",
        "        token_tuples.append((python_kind, spelling))\r\n",
        "      elif keyword.iskeyword(whole_token):\r\n",
        "        token_tuples.append((tokenize.NAME, whole_token))\r\n",
        "      elif PythonTokenizer2._NUMBERS.match(whole_token):\r\n",
        "        token_tuples.append((tokenize.NUMBER, whole_token))\r\n",
        "      elif PythonTokenizer2._SINGLE_STRINGS.match(whole_token):\r\n",
        "        token_tuples.append((tokenize.STRING, whole_token))\r\n",
        "      elif PythonTokenizer2._TRIPLE_STRING_BEGINNINGS.match(whole_token):\r\n",
        "        token_tuples.append((tokenize.STRING, whole_token))\r\n",
        "      elif PythonTokenizer2._COMMENTS.match(whole_token):\r\n",
        "        token_tuples.append((tokenize.COMMENT, whole_token))\r\n",
        "      else:\r\n",
        "        # Everything else we map back to NAME.\r\n",
        "        token_tuples.append((tokenize.NAME, whole_token))\r\n",
        "\r\n",
        "    reconstructed = tokenize.untokenize(typing.cast(Any, token_tuples))\r\n",
        "    return reconstructed\r\n",
        "\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOS_mSj8hIOK"
      },
      "source": [
        "def get_lang_specific_tokens(init_tokenizer, code_snip):\r\n",
        "    #tokens_complete = init_tokenizer.tokenize(source_code=code_snip)\r\n",
        "    tokens = init_tokenizer.tokenize_and_abstract(source_code=code_snip )\r\n",
        "    conditioned = init_tokenizer.condition_full_tokens(tokens)\r\n",
        "    agnostic_token_lists = unified_tokenizer._agnostic_tokens_to_lists_of_token_lists(conditioned)\r\n",
        "    with_identifiers_heuristically_split = unified_tokenizer._subtokenize_identifiers_heuristically(\r\n",
        "        agnostic_token_lists)\r\n",
        "    with_string_tokens_heuristically_split = unified_tokenizer._subtokenize_strings_heuristically(\r\n",
        "        with_identifiers_heuristically_split)\r\n",
        "    shortened_subtokens = unified_tokenizer._shorten_subtokens(with_string_tokens_heuristically_split, 20)\r\n",
        "    sanitization_mapping = init_tokenizer.get_mappings()\r\n",
        "    subtoken_lists = unified_tokenizer.sanitize_subtoken_lists(shortened_subtokens,\r\n",
        "                                            sanitization_mapping,\r\n",
        "                                            unified_tokenizer.SENTINEL)\r\n",
        "    #flat_toks =unified_tokenizer.flatten_subtoken_lists(subtoken_lists)\r\n",
        "    test_spellings = []\r\n",
        "    test_tok_types = []\r\n",
        "    for t in subtoken_lists:\r\n",
        "        #if(len(t.spelling) == 1):\r\n",
        "        #print(len(t.spellings))\r\n",
        "        test_spellings.extend(t.spellings)\r\n",
        "        match=False\r\n",
        "        for cubert_token in set(init_tokenizer._REVERSE_TOKEN_MAP.keys()):\r\n",
        "            #print(\"Checking for:\",cubert_token)\r\n",
        "            if cubert_token in t.spellings[0]:\r\n",
        "                #print(t.spellings)\r\n",
        "                selected_token = tokenize.tok_name[init_tokenizer._REVERSE_TOKEN_MAP[cubert_token]]\r\n",
        "                test_tok_types.extend([selected_token]*len(t.spellings))\r\n",
        "                match=True          \r\n",
        "        if match == False:\r\n",
        "            test_tok_types.extend([t.kind.name]*len(t.spellings))\r\n",
        "    return test_spellings, test_tok_types\r\n",
        "\r\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h03QcbEIhLeg"
      },
      "source": [
        "class NewVectorizer():\r\n",
        "    def __init__(self, code_piece, tok_type_counter):\r\n",
        "        self.code_piece = code_piece\r\n",
        "        self.tok_type_counter = tok_type_counter\r\n",
        "        self.code_word2idx = {'<s>':0,'</s>':1,'<pad>':2, '<unk>':3}\r\n",
        "        self.code_idx2word = {v:k for k,v in self.code_word2idx.items()}\r\n",
        "        self.toktype_word2idx = {'<s>':0,'</s>':1,'<pad>':2, '<unk>':3}\r\n",
        "        self.toktype_idx2word = {v:k for k,v in self.toktype_word2idx.items()}\r\n",
        "        self.max_tok_length = len(self.toktype_word2idx)\r\n",
        "        self.max_code_length = len(self.code_word2idx)\r\n",
        "        self.UNK_FOR_TOKEN_TYPE = '<unk>'\r\n",
        "        self.UNK_FOR_CODEPIECE = '<unk>'\r\n",
        "        self.ID_UNK_TOKEN_TYPE = self.toktype_word2idx[self.UNK_FOR_TOKEN_TYPE]\r\n",
        "        self.ID_UNK_CODEPIECE = self.code_word2idx[self.UNK_FOR_CODEPIECE]\r\n",
        "\r\n",
        "        self.PAD_FOR_CODEPIECE = '<pad>'\r\n",
        "        self.PAD_FOR_TOKEN_TYPE = '<pad>'\r\n",
        "        self.ID_PAD_FOR_CODEPIECE = self.code_word2idx['<pad>']\r\n",
        "        self.ID_PAD_FOR_TOKEN_TYPE = self.toktype_word2idx['<pad>']\r\n",
        "        \r\n",
        "        self.SOS_FOR_CODEPIECE = '<s>'\r\n",
        "        self.SOS_FOR_TOKEN_TYPE = '<s>'\r\n",
        "        self.ID_SOS_FOR_CODEPIECE = self.code_word2idx['<s>']\r\n",
        "        self.ID_SOS_FOR_TOKEN_TYPE = self.toktype_word2idx['<s>']\r\n",
        "\r\n",
        "        self.EOS_FOR_CODEPIECE = '</s>'\r\n",
        "        self.EOS_FOR_TOKEN_TYPE = '</s>'\r\n",
        "        self.ID_EOS_FOR_CODEPIECE = self.code_word2idx['</s>']\r\n",
        "        self.ID_EOS_FOR_TOKEN_TYPE = self.toktype_word2idx['</s>']\r\n",
        "        self.build_vocab()\r\n",
        "\r\n",
        "    def build_vocab(self):\r\n",
        "        idx=len(self.code_word2idx.keys())\r\n",
        "        for k in self.code_piece.keys():\r\n",
        "            self.code_word2idx[k]=idx\r\n",
        "            self.code_idx2word[idx]=k\r\n",
        "            idx += 1\r\n",
        "        \r\n",
        "        idx=len(self.toktype_word2idx.keys())\r\n",
        "        for k in self.tok_type_counter.keys():\r\n",
        "            self.toktype_word2idx[k]=idx\r\n",
        "            self.toktype_idx2word[idx]=k\r\n",
        "            idx += 1\r\n",
        "\r\n",
        "        self.max_tok_length = len(self.toktype_word2idx.keys())\r\n",
        "        self.max_code_length = len(self.code_word2idx.keys())\r\n",
        "    ### Returns the code piece for a given ID\r\n",
        "    def convert_id_to_codepiece(self, id_for_code):\r\n",
        "        if(id_for_code not in list(self.code_idx2word.keys())):\r\n",
        "            return self.UNK_FOR_CODEPIECE\r\n",
        "        return self.code_idx2word[id_for_code]\r\n",
        "    ### Returns the ID for a given code piece\r\n",
        "    def convert_codepiece_to_id(self, code_piece):\r\n",
        "        if(code_piece not in list(self.code_word2idx.keys())):\r\n",
        "            return self.ID_UNK_CODEPIECE\r\n",
        "        return self.code_word2idx[code_piece]\r\n",
        "    ### Returns the TOKEN ID for a given TOKEN type\r\n",
        "    def convert_toktype_to_id(self, tok_piece):\r\n",
        "        if(tok_piece not in list(self.toktype_word2idx.keys())):\r\n",
        "            print(\"No match for\",tok_piece)\r\n",
        "            return self.ID_UNK_TOKEN_TYPE\r\n",
        "        return self.toktype_word2idx[tok_piece]\r\n",
        "    ### Returns the TOKEN type for a given TOKEN ID\r\n",
        "    def convert_id_to_toktype(self, id_for_toktype):\r\n",
        "        if(id_for_toktype not in list(self.toktype_idx2word.keys())):\r\n",
        "            return self.UNK_FOR_TOKEN_TYPE\r\n",
        "        return self.toktype_idx2word[id_for_toktype]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYzMGFIngltr"
      },
      "source": [
        "import cubert\r\n",
        "from absl import app\r\n",
        "from absl import flags\r\n",
        "from tensor2tensor.data_generators import text_encoder\r\n",
        "import enum\r\n",
        "import cubert_tokenizer\r\n",
        "from cubert import code_to_subtokenized_sentences\r\n",
        "#from cubert import tokenizer_registry\r\n",
        "from cubert import python_tokenizer\r\n",
        "import python_tokenizer\r\n",
        "from tensor2tensor.data_generators import text_encoder_build_subword\r\n",
        "from collections import Counter, defaultdict\r\n",
        "\r\n",
        "@enum.unique\r\n",
        "class TokenizerEnum(enum.Enum):\r\n",
        "  \"\"\"Enum for Tokenizers.\"\"\"\r\n",
        "  #PYTHON = python_tokenizer.PythonTokenizer\r\n",
        "  PYTHON = PythonTokenizer2"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7auIaOthNj6"
      },
      "source": [
        "#### Generate and create the Vectorizer instance\r\n",
        "word_counter=Counter()\r\n",
        "init_tokenizer=PythonTokenizer2()\r\n",
        "tok_type_counter = Counter()\r\n",
        "for code_snip in nl_to_pl_df['cleaned_code']:\r\n",
        "    toks, tok_types = get_lang_specific_tokens(init_tokenizer, code_snip)\r\n",
        "    word_counter.update(Counter(toks))\r\n",
        "    tok_type_counter.update(Counter(tok_types))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a-QqlOuhd3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22945672-39f8-4676-99df-0c2b72da4d4b"
      },
      "source": [
        "code_tok_vectorizer = NewVectorizer(word_counter, tok_type_counter)\r\n",
        "print(code_tok_vectorizer.max_code_length,code_tok_vectorizer.max_tok_length)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5813 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1HaJbq6BwtV"
      },
      "source": [
        "tok_type_ids = [ code_tok_vectorizer.convert_toktype_to_id(toktype) for toktype in tok_types]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfjy6DJUCCp_",
        "outputId": "56d834e2-8aa8-4556-d214-3a551618a1a4"
      },
      "source": [
        "[ code_tok_vectorizer.convert_id_to_toktype(toktype) for toktype in tok_type_ids] == tok_types"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5epf-rxhjY0"
      },
      "source": [
        "### Old NLP DS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k46GU0OzfGeE"
      },
      "source": [
        "tok_ids_list=[]\r\n",
        "class NLPLSingleEntry(object):\r\n",
        "    \"\"\"A single set of features of data.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, \r\n",
        "                 code_ids, \r\n",
        "                 code_mask, \r\n",
        "                 doc_ids,\r\n",
        "                 doc_mask,\r\n",
        "                 ):\r\n",
        "        self.code_ids = code_ids\r\n",
        "        self.code_mask = code_mask\r\n",
        "        self.doc_ids = doc_ids\r\n",
        "        self.doc_mask = doc_mask\r\n",
        "        #self.segment_ids = segment_ids\r\n",
        "        #self.label_id = label_id\r\n",
        "class NLPLDataSet():\r\n",
        "    def __init__(self, \r\n",
        "                 doc_tokenizer, \r\n",
        "                 code_tokenizer):\r\n",
        "        self.doc_tokenizer = doc_tokenizer\r\n",
        "        self.code_tokenizer = code_tokenizer\r\n",
        "\r\n",
        "    def prepare_tokens(self, \r\n",
        "                       samples, \r\n",
        "                       tokenizer, \r\n",
        "                       max_seq_length=0,\r\n",
        "                       data_type=None):\r\n",
        "        \"\"\"\r\n",
        "            Tokenizes an input sequence, adds padding and SOS+EOS \r\n",
        "        \"\"\"\r\n",
        "        toks = tokenizer.tokenize(samples)\r\n",
        "        # print(data_type)\r\n",
        "        # print(toks)\r\n",
        "        if max_seq_length > 2 and len(toks) > max_seq_length - 2:\r\n",
        "            toks = toks[:max_seq_length -2]\r\n",
        "        tok_ids =  tokenizer.convert_tokens_to_ids(toks)\r\n",
        "        ### We use pseudo-BERT process so we will add both CLS and SEP tokens for\r\n",
        "        ### src and target inputs\r\n",
        "        tok_ids = [tokenizer.cls_token_id] + tok_ids + [tokenizer.sep_token_id]\r\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\r\n",
        "        # tokens are attended to.\r\n",
        "        input_mask = [ 1 ] * len(tok_ids)\r\n",
        "\r\n",
        "        if len(tok_ids) < max_seq_length:\r\n",
        "            padding_length = max_seq_length - len(tok_ids)\r\n",
        "            tok_ids = tok_ids + ([tokenizer.pad_token_id] * padding_length)\r\n",
        "            input_mask = input_mask + ([ 0 ] * padding_length) ### Padded tokens are zero-masked\r\n",
        "        \r\n",
        "        # print(tok_ids)\r\n",
        "        return tok_ids, input_mask\r\n",
        "\r\n",
        "    def create_dataset(self,\r\n",
        "                    nl_to_pl_df,\r\n",
        "                    final_ds,\r\n",
        "                    sample_count=10000,\r\n",
        "                    max_doc_len=50,\r\n",
        "                    max_code_len=0):\r\n",
        "        \"\"\"\r\n",
        "            Reads from a dataframe, tokenizes and numericalizes both docstrings \r\n",
        "            and code. \r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        #final_ds = []\r\n",
        "        for idx in nl_to_pl_df.itertuples():\r\n",
        "            ## For SOS and EOS tokens 2 positions are left\r\n",
        "            if not idx.cleaned_code:\r\n",
        "                print(\"Invalid entry, No code found for:\", idx.docstring)\r\n",
        "            \r\n",
        "            doc_toks, doc_mask = self.prepare_tokens(idx.docstring,\r\n",
        "                                                      self.doc_tokenizer,\r\n",
        "                                                      max_doc_len,\r\n",
        "                                                      \"docs\")\r\n",
        "            code_toks, code_mask = self.prepare_tokens(idx.cleaned_code,\r\n",
        "                                                      self.code_tokenizer,\r\n",
        "                                                      max_code_len,\r\n",
        "                                                      \"code\")\r\n",
        "            #code_toks = None\r\n",
        "            ### Skip over current iteration if no valid code found\r\n",
        "\r\n",
        "            # print(code_toks)\r\n",
        "            # print(code_mask)\r\n",
        "            # print(doc_toks)\r\n",
        "            # print(doc_mask)\r\n",
        "            final_entry = NLPLSingleEntry(code_toks, \r\n",
        "                                            code_mask, \r\n",
        "                                            doc_toks, \r\n",
        "                                            doc_mask)\r\n",
        "            # print(final_entry.code_ids)\r\n",
        "            # print(final_entry.code_mask)\r\n",
        "            # print(final_entry.doc_ids)\r\n",
        "            # print(final_entry.doc_mask)\r\n",
        "            final_ds.append(final_entry)\r\n",
        "        #print(len(final_ds))\r\n",
        "        return final_ds\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTR0cQhHhmWC"
      },
      "source": [
        "### New NLP DS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVD-3TjxY1SD"
      },
      "source": [
        "tok_ids_list=[]\r\n",
        "class NLPLSingleEntry(object):\r\n",
        "    \"\"\"A single set of features of data.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, \r\n",
        "                 code_ids,\r\n",
        "                 tok_ids,\r\n",
        "                 code_mask, \r\n",
        "                 doc_ids,\r\n",
        "                 doc_mask,\r\n",
        "                 ):\r\n",
        "        self.code_ids = code_ids\r\n",
        "        self.code_mask = code_mask\r\n",
        "        self.tok_ids = tok_ids\r\n",
        "        self.doc_ids = doc_ids\r\n",
        "        self.doc_mask = doc_mask\r\n",
        "        #self.segment_ids = segment_ids\r\n",
        "        #self.label_id = label_id\r\n",
        "class NLPLDataSet():\r\n",
        "    def __init__(self, \r\n",
        "                 doc_tokenizer, \r\n",
        "                 code_tokenizer,\r\n",
        "                 code_tok_vectorizer):\r\n",
        "        self.doc_tokenizer = doc_tokenizer\r\n",
        "        self.code_tokenizer = code_tokenizer\r\n",
        "        self.code_tok_vectorizer = code_tok_vectorizer\r\n",
        "\r\n",
        "    def prepare_tokens(self, \r\n",
        "                       samples, \r\n",
        "                       tokenizer, \r\n",
        "                       max_seq_length=0,\r\n",
        "                       data_type=None):\r\n",
        "        \"\"\"\r\n",
        "            Tokenizes an input sequence, adds padding and SOS+EOS \r\n",
        "        \"\"\"\r\n",
        "        toks = tokenizer.tokenize(samples)\r\n",
        "        # print(data_type)\r\n",
        "        # print(toks)\r\n",
        "        if max_seq_length > 2 and len(toks) > max_seq_length - 2:\r\n",
        "            toks = toks[:max_seq_length -2]\r\n",
        "        tok_ids =  tokenizer.convert_tokens_to_ids(toks)\r\n",
        "        ### We use pseudo-BERT process so we will add both CLS and SEP tokens for\r\n",
        "        ### src and target inputs\r\n",
        "        tok_ids = [tokenizer.cls_token_id] + tok_ids + [tokenizer.sep_token_id]\r\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\r\n",
        "        # tokens are attended to.\r\n",
        "        input_mask = [ 1 ] * len(tok_ids)\r\n",
        "\r\n",
        "        if len(tok_ids) < max_seq_length:\r\n",
        "            padding_length = max_seq_length - len(tok_ids)\r\n",
        "            tok_ids = tok_ids + ([tokenizer.pad_token_id] * padding_length)\r\n",
        "            input_mask = input_mask + ([ 0 ] * padding_length) ### Padded tokens are zero-masked\r\n",
        "        \r\n",
        "        # print(tok_ids)\r\n",
        "        return tok_ids, input_mask\r\n",
        "    def prepare_code_tokens(self, \r\n",
        "                       samples, \r\n",
        "                       tokenizer, \r\n",
        "                       max_seq_length=0,\r\n",
        "                       data_type=None):\r\n",
        "        \"\"\"\r\n",
        "            Tokenizes an input sequence, adds padding and SOS+EOS \r\n",
        "        \"\"\"\r\n",
        "        _toks, _tok_types = get_lang_specific_tokens(self.code_tokenizer, samples)\r\n",
        "        #print(_tok_types)\r\n",
        "        # print(data_type)\r\n",
        "        # print(toks)\r\n",
        "        if max_seq_length > 2 and len(_toks) > max_seq_length - 2:\r\n",
        "            _toks = _toks[:max_seq_length -2]\r\n",
        "            _tok_types = _tok_types[:max_seq_length -2]\r\n",
        "        #tok_ids =  tokenizer.convert_tokens_to_ids(toks)\r\n",
        "        tok_ids = [ self.code_tok_vectorizer.convert_codepiece_to_id(code) for code in _toks]\r\n",
        "        tok_types = [ self.code_tok_vectorizer.convert_toktype_to_id(toktype) for toktype in _tok_types]\r\n",
        "        \r\n",
        "        ### We use pseudo-BERT process so we will add both CLS and SEP tokens for\r\n",
        "        ### src and target inputs\r\n",
        "        tok_ids = [self.code_tok_vectorizer.ID_SOS_FOR_CODEPIECE] + tok_ids + [self.code_tok_vectorizer.ID_EOS_FOR_CODEPIECE]\r\n",
        "        tok_types = [self.code_tok_vectorizer.ID_SOS_FOR_TOKEN_TYPE] + tok_types + [self.code_tok_vectorizer.ID_EOS_FOR_TOKEN_TYPE]\r\n",
        "        #print(len(tok_ids), len(tok_types))\r\n",
        "        assert(len(tok_ids) == len(tok_types))\r\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\r\n",
        "        # tokens are attended to.\r\n",
        "        input_mask = [ 1 ] * len(tok_ids)\r\n",
        "\r\n",
        "        if len(tok_ids) < max_seq_length:\r\n",
        "            padding_length = max_seq_length - len(tok_ids)\r\n",
        "            tok_ids = tok_ids + ([self.code_tok_vectorizer.ID_PAD_FOR_CODEPIECE] * padding_length)\r\n",
        "            tok_types = tok_types + ([self.code_tok_vectorizer.ID_PAD_FOR_TOKEN_TYPE] * padding_length)\r\n",
        "            input_mask = input_mask + ([ 0 ] * padding_length) ### Padded tokens are zero-masked\r\n",
        "        \r\n",
        "        # print(tok_ids)\r\n",
        "        return tok_ids, tok_types, input_mask\r\n",
        "\r\n",
        "    def create_dataset(self,\r\n",
        "                    nl_to_pl_df,\r\n",
        "                    final_ds,\r\n",
        "                    sample_count=10000,\r\n",
        "                    max_doc_len=50,\r\n",
        "                    max_code_len=0):\r\n",
        "        \"\"\"\r\n",
        "            Reads from a dataframe, tokenizes and numericalizes both docstrings \r\n",
        "            and code. \r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        #final_ds = []\r\n",
        "        for idx in nl_to_pl_df.itertuples():\r\n",
        "            ## For SOS and EOS tokens 2 positions are left\r\n",
        "            if not idx.cleaned_code:\r\n",
        "                print(\"Invalid entry, No code found for:\", idx.docstring)\r\n",
        "            \r\n",
        "            doc_toks, doc_mask = self.prepare_tokens(idx.docstring,\r\n",
        "                                                      self.doc_tokenizer,\r\n",
        "                                                      max_doc_len,\r\n",
        "                                                      \"docs\")\r\n",
        "            code_ids, tok_ids, code_mask = self.prepare_code_tokens(idx.cleaned_code,\r\n",
        "                                                      self.code_tokenizer,\r\n",
        "                                                      max_code_len,\r\n",
        "                                                      \"code\")\r\n",
        "            #code_toks = None\r\n",
        "            ### Skip over current iteration if no valid code found\r\n",
        "\r\n",
        "            # print(code_toks)\r\n",
        "            # print(code_mask)\r\n",
        "            # print(doc_toks)\r\n",
        "            # print(doc_mask)\r\n",
        "            final_entry = NLPLSingleEntry(code_ids,\r\n",
        "                                          tok_ids,\r\n",
        "                                          code_mask, \r\n",
        "                                          doc_toks, \r\n",
        "                                          doc_mask)\r\n",
        "            # print(final_entry.code_ids)\r\n",
        "            # print(final_entry.code_mask)\r\n",
        "            # print(final_entry.doc_ids)\r\n",
        "            # print(final_entry.doc_mask)\r\n",
        "            final_ds.append(final_entry)\r\n",
        "        #print(len(final_ds))\r\n",
        "        return final_ds\r\n",
        "        "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V71S9kbEyhv8"
      },
      "source": [
        "### We will use the same tokenizer for both docstrings and code\r\n",
        "final_ds = []\r\n",
        "MAX_LENGTH=512\r\n",
        "#selected_elems = nl_to_pl_df[nl_to_pl_df['cleaned_code_len'] <= MAX_LENGTH]\r\n",
        "#selected_elems = my_df_copy[my_df_copy['code_len'] <= MAX_LENGTH]\r\n",
        "MAX_VOCAB_LENGTH=512\r\n",
        "assert(MAX_VOCAB_LENGTH <= MAX_LENGTH)\r\n",
        "#selected_elems = nl_to_pl_df[nl_to_pl_df['cleaned_code_len'] <= MAX_LENGTH]\r\n",
        "#selected_elems = my_df_copy[my_df_copy['code_len'] <= MAX_LENGTH]\r\n",
        "init_tokenizer=PythonTokenizer2()\r\n",
        "#selected_elems = nl_to_pl_df[nl_to_pl_df['cleaned_code_len'] <= MAX_VOCAB_LENGTH]\r\n",
        "\r\n",
        "selected_elems = nl_to_pl_df[(nl_to_pl_df['cleaned_code_len'] <= MAX_VOCAB_LENGTH) & (nl_to_pl_df['docstring_len'] <= MAX_VOCAB_LENGTH*2)]\r\n",
        "my_nlpl_ds = NLPLDataSet(auto_tokenizer, init_tokenizer, code_tok_vectorizer).create_dataset(selected_elems, \r\n",
        "                                                                        final_ds, \r\n",
        "                                                                        max_doc_len=MAX_VOCAB_LENGTH, \r\n",
        "                                                                        max_code_len=MAX_VOCAB_LENGTH)\r\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxroVGJDIGq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c514f0-5dda-433f-c543-222cca41f4e8"
      },
      "source": [
        "nl_to_pl_df[(nl_to_pl_df['cleaned_code_len'] <= 512) & (nl_to_pl_df['docstring_len'] <= 512*2)].count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "docstring           4234\n",
              "code                4234\n",
              "docstring_len       4234\n",
              "code_len            4234\n",
              "cleaned_code        4234\n",
              "cleaned_code_len    4234\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShA8ITXD9wfe"
      },
      "source": [
        "all_code_ids = torch.tensor([f.code_ids for f in my_nlpl_ds], dtype=torch.long)\r\n",
        "all_code_mask = torch.tensor([f.code_mask for f in my_nlpl_ds], dtype=torch.long)\r\n",
        "all_doc_ids = torch.tensor([f.doc_ids for f in my_nlpl_ds], dtype=torch.long)\r\n",
        "all_doc_mask = torch.tensor([f.doc_mask for f in my_nlpl_ds], dtype=torch.long)\r\n",
        "all_tok_ids = torch.tensor([f.tok_ids for f in my_nlpl_ds], dtype=torch.long)\r\n",
        "# if output_mode == \"classification\":\r\n",
        "#     all_label_ids = torch.tensor([f.label_id for f in my_nlpl_ds], dtype=torch.long)\r\n",
        "\r\n",
        "train_dataset = TensorDataset(all_code_ids,\r\n",
        "                              all_code_mask, \r\n",
        "                              all_doc_ids, \r\n",
        "                              all_doc_mask,\r\n",
        "                              all_tok_ids)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKJG-nAJl3i9"
      },
      "source": [
        "dataset_size = len(train_dataset)\r\n",
        "dataset_indices = list(range(dataset_size))\r\n",
        "np.random.shuffle(dataset_indices)\r\n",
        "val_split_index = int(np.floor(0.2 * dataset_size))\r\n",
        "\r\n",
        "train_idx, val_idx = dataset_indices[val_split_index:], dataset_indices[:val_split_index]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grRIIkyMl4fY"
      },
      "source": [
        "BATCH_SIZE=8\r\n",
        "train_sampler = SubsetRandomSampler(train_idx)\r\n",
        "val_sampler = SubsetRandomSampler(val_idx)\r\n",
        "\r\n",
        "\r\n",
        "#train_sampler = RandomSampler(train_dataset,) #if args.local_rank == -1 else DistributedSampler(train_dataset)\r\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE, shuffle=False)\r\n",
        "val_dataloader = DataLoader(train_dataset, sampler=val_sampler, batch_size=BATCH_SIZE, shuffle=False)\r\n",
        "\r\n",
        "#train_sampler = RandomSampler(train_dataset) #if args.local_rank == -1 else DistributedSampler(train_dataset)\r\n",
        "\r\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSmbw44v__OU"
      },
      "source": [
        "iter_one = next(iter(train_dataloader))\r\n",
        "iter_one_val = next(iter(val_dataloader))\r\n",
        "\r\n",
        "#iter_one[0]=all_code_ids, \r\n",
        "#iter_one[1]=all_code_mask, \r\n",
        "#iter_one[2]=all_doc_ids, \r\n",
        "#iter_one[3]=all_doc_mask\r\n",
        "#mask_reshape.shape\r\n",
        "# trg = iter_one[0]\r\n",
        "# trg_len = trg.shape[1]\r\n",
        "# trg_pad_mask = iter_one[1].unsqueeze(1).unsqueeze(2) \r\n",
        "# trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len))).bool()\r\n",
        "\r\n",
        "# #trg_sub_mask = [trg len, trg len]\r\n",
        "    \r\n",
        "# trg_mask = trg_pad_mask & trg_sub_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbgAhdfWAIOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3da7f01-62d5-4c64-defd-a1b6d84711a6"
      },
      "source": [
        "iter_one[-1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scH0xbQHKvS3"
      },
      "source": [
        "### Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2wiHHFBuXxi"
      },
      "source": [
        "class TransEncoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 input_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,\r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 1000):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([TransEncoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim,\r\n",
        "                                                  dropout, \r\n",
        "                                                  device) \r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        batch_size = src.shape[0]\r\n",
        "        src_len = src.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        \r\n",
        "        #pos = [batch size, src len]\r\n",
        "        \r\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            src = layer(src, src_mask)\r\n",
        "            \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "            \r\n",
        "        return src"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_PH9wz_KzuM"
      },
      "source": [
        "class TransEncoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,  \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        #src_mask = [batch size, 1, 1, src len] \r\n",
        "                \r\n",
        "        #self attention\r\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _src = self.positionwise_feedforward(src)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        return src"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OodBNJ46LCQo"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        assert hid_dim % n_heads == 0\r\n",
        "        \r\n",
        "        self.hid_dim = hid_dim\r\n",
        "        self.n_heads = n_heads\r\n",
        "        self.head_dim = hid_dim // n_heads\r\n",
        "        \r\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, query, key, value, mask = None):\r\n",
        "        \r\n",
        "        batch_size = query.shape[0]\r\n",
        "        \r\n",
        "        #query = [batch size, query len, hid dim]\r\n",
        "        #key = [batch size, key len, hid dim]\r\n",
        "        #value = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = self.fc_q(query)\r\n",
        "        K = self.fc_k(key)\r\n",
        "        V = self.fc_v(value)\r\n",
        "        \r\n",
        "        #Q = [batch size, query len, hid dim]\r\n",
        "        #K = [batch size, key len, hid dim]\r\n",
        "        #V = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        \r\n",
        "        #Q = [batch size, n heads, query len, head dim]\r\n",
        "        #K = [batch size, n heads, key len, head dim]\r\n",
        "        #V = [batch size, n heads, value len, head dim]\r\n",
        "                \r\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\r\n",
        "        \r\n",
        "        #energy = [batch size, n heads, query len, key len]\r\n",
        "        \r\n",
        "        if mask is not None:\r\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\r\n",
        "        \r\n",
        "        attention = torch.softmax(energy, dim = -1)\r\n",
        "                \r\n",
        "        #attention = [batch size, n heads, query len, key len]\r\n",
        "                \r\n",
        "        x = torch.matmul(self.dropout(attention), V)\r\n",
        "        \r\n",
        "        #x = [batch size, n heads, query len, head dim]\r\n",
        "        \r\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\r\n",
        "        \r\n",
        "        #x = [batch size, query len, n heads, head dim]\r\n",
        "        \r\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        x = self.fc_o(x)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        return x, attention"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK2KNXZSLEAz"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\r\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, pf dim]\r\n",
        "        \r\n",
        "        x = self.fc_2(x)\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH18KwEnLFhF"
      },
      "source": [
        "class TransDecoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 output_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 1000,\r\n",
        "                 tok_type_dim=62):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\r\n",
        "        self.tok_type_embedding = nn.Embedding(tok_type_dim, hid_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([TransDecoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim, \r\n",
        "                                                  dropout, \r\n",
        "                                                  device)\r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\r\n",
        "        \r\n",
        "        self.fc_out_tok = nn.Linear(hid_dim, tok_type_dim)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask, src_tok_types):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "                \r\n",
        "        batch_size = trg.shape[0]\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "                            \r\n",
        "        #pos = [batch size, trg len]\r\n",
        "            \r\n",
        "        # trg = self.dropout((self.tok_embedding(trg) * self.scale) + \r\n",
        "        #                    self.pos_embedding(pos) + \r\n",
        "        #                    self.tok_type_embedding(src_tok_types))\r\n",
        "        trg = self.dropout(((self.tok_embedding(trg) + self.tok_type_embedding(src_tok_types)) * self.scale) + \r\n",
        "                           self.pos_embedding(pos))\r\n",
        "                           \r\n",
        "                \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        output = self.fc_out(trg)\r\n",
        "        output_tok = self.fc_out_tok(trg)\r\n",
        "        #output =F.softmax(output, dim=2)  \r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "            \r\n",
        "        return output, output_tok, attention"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IChtMK8QLHOP"
      },
      "source": [
        "class TransDecoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        #self attention\r\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "            \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "            \r\n",
        "        #encoder attention\r\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\r\n",
        "        # query, key, value\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "                    \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _trg = self.positionwise_feedforward(trg)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        return trg, attention"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swxv3Y0SLJmE"
      },
      "source": [
        "class TransSeq2Seq(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 encoder, \r\n",
        "                 decoder, \r\n",
        "                 src_pad_idx, \r\n",
        "                 trg_pad_idx, \r\n",
        "                 device,\r\n",
        "                 ):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.src_pad_idx = src_pad_idx\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "    def make_src_mask(self, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        \r\n",
        "        src_mask = src_mask.unsqueeze(1).unsqueeze(2)\r\n",
        "\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "\r\n",
        "        return src_mask\r\n",
        "    \r\n",
        "    def make_trg_mask(self, trg, trg_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        trg_pad_mask = trg_mask.unsqueeze(1).unsqueeze(2) \r\n",
        "        \"\"\"\r\n",
        "            A boolean tensor of shape [batch size, 1, 1, trg len]\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        \r\n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\r\n",
        "        \r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\r\n",
        "        \r\n",
        "        #trg_sub_mask = [trg len, trg len]\r\n",
        "            \r\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\r\n",
        "        \r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        return trg_mask\r\n",
        "\r\n",
        "    def forward(self, src, src_mask, trg, trg_mask, src_tok_types):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "                \r\n",
        "        src_mask = self.make_src_mask(src_mask)\r\n",
        "        trg_mask = self.make_trg_mask(trg, trg_mask)\r\n",
        "        \r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        enc_src = self.encoder(src, src_mask)\r\n",
        "        \r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "                \r\n",
        "        output, tok_output, attention = self.decoder(trg, \r\n",
        "                                         enc_src, \r\n",
        "                                         trg_mask, \r\n",
        "                                         src_mask, \r\n",
        "                                         src_tok_types)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        return output, tok_output, attention"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cJUUj5QLvbw"
      },
      "source": [
        "INPUT_DIM = auto_tokenizer.vocab_size\r\n",
        "OUTPUT_DIM = code_tok_vectorizer.max_code_length\r\n",
        "TOK_TYPE_OUTPUT_DIM = code_tok_vectorizer.max_tok_length\r\n",
        "HID_DIM = 256\r\n",
        "ENC_LAYERS = 3\r\n",
        "DEC_LAYERS = 3\r\n",
        "ENC_HEADS = 8\r\n",
        "DEC_HEADS = 8\r\n",
        "ENC_PF_DIM = 1024\r\n",
        "DEC_PF_DIM = 1024\r\n",
        "ENC_DROPOUT = 0.2\r\n",
        "DEC_DROPOUT = 0.2\r\n",
        "\r\n",
        "enc = TransEncoder(INPUT_DIM, \r\n",
        "              HID_DIM, \r\n",
        "              ENC_LAYERS, \r\n",
        "              ENC_HEADS, \r\n",
        "              ENC_PF_DIM, \r\n",
        "              ENC_DROPOUT, \r\n",
        "              device,\r\n",
        "              max_length=MAX_LENGTH)\r\n",
        "\r\n",
        "dec = TransDecoder(OUTPUT_DIM, \r\n",
        "              HID_DIM, \r\n",
        "              DEC_LAYERS, \r\n",
        "              DEC_HEADS, \r\n",
        "              DEC_PF_DIM, \r\n",
        "              DEC_DROPOUT, \r\n",
        "              device,\r\n",
        "              max_length=MAX_LENGTH,\r\n",
        "              tok_type_dim=TOK_TYPE_OUTPUT_DIM)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzSK7caBygSu",
        "outputId": "a0c13313-277b-4320-cc7a-6d0f1cfc0e5f"
      },
      "source": [
        "code_tok_vectorizer.max_tok_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK3PYSb6gT12"
      },
      "source": [
        "# INPUT_DIM = auto_tokenizer.vocab_size\r\n",
        "# OUTPUT_DIM = auto_tokenizer.vocab_size\r\n",
        "# HID_DIM = 256\r\n",
        "# ENC_LAYERS = 3\r\n",
        "# DEC_LAYERS = 3\r\n",
        "# ENC_HEADS = 8\r\n",
        "# DEC_HEADS = 8\r\n",
        "# ENC_PF_DIM = 1024\r\n",
        "# DEC_PF_DIM = 1024\r\n",
        "# ENC_DROPOUT = 0.2\r\n",
        "# DEC_DROPOUT = 0.2\r\n",
        "\r\n",
        "# enc = TransEncoder(INPUT_DIM, \r\n",
        "#               HID_DIM, \r\n",
        "#               ENC_LAYERS, \r\n",
        "#               ENC_HEADS, \r\n",
        "#               ENC_PF_DIM, \r\n",
        "#               ENC_DROPOUT, \r\n",
        "#               device)\r\n",
        "\r\n",
        "# dec = TransDecoder(OUTPUT_DIM, \r\n",
        "#               HID_DIM, \r\n",
        "#               DEC_LAYERS, \r\n",
        "#               DEC_HEADS, \r\n",
        "#               DEC_PF_DIM, \r\n",
        "#               DEC_DROPOUT, \r\n",
        "#               device,\r\n",
        "#               max_length=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mtBeOIl5uWg",
        "outputId": "4f072248-06a6-4469-b169-77afc99d49f5"
      },
      "source": [
        "MAX_LENGTH"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7yKcgxLPKiv"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olwlAq_B3boJ"
      },
      "source": [
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_baseline_128.pt ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lexhxG9rL8pQ"
      },
      "source": [
        "SRC_PAD_IDX = auto_tokenizer.pad_token_id #SRC.vocab.stoi[SRC.pad_token]\r\n",
        "TRG_PAD_IDX = code_tok_vectorizer.ID_PAD_FOR_TOKEN_TYPE #TRG.vocab.stoi[TRG.pad_token]\r\n",
        "\r\n",
        "model = TransSeq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQy4DgdKzAsB"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw9b7bQ-MIWM"
      },
      "source": [
        "def initialize_weights(m):\r\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\r\n",
        "        nn.init.xavier_uniform_(m.weight.data)\r\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-yLC9A_Mmuo",
        "outputId": "97b4a40a-60fa-47f9-cee8-79738aff6a87"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 21,649,861 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhrFXeouMQEb"
      },
      "source": [
        "model.apply(initialize_weights);\r\n",
        "LEARNING_RATE = 0.0005\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqHShEuoNQlw"
      },
      "source": [
        "iter_one = next(iter(train_dataloader))\r\n",
        "batch = tuple(t.to(device) for t in iter_one)\r\n",
        "# inputs = {'input_ids': batch[0],\r\n",
        "#             'attention_mask': batch[1],\r\n",
        "#             'token_type_ids': batch[2] ,\r\n",
        "#             # XLM don't use segment_ids\r\n",
        "#             'labels': batch[3]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cWIuwNUMQXs"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip, device,double_loss=False):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_crit_loss = 0\r\n",
        "    epoch_tok_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        trg = batch[0].to(device)\r\n",
        "        trg_mask = batch[1].to(device)\r\n",
        "        src = batch[2].to(device)\r\n",
        "        src_mask = batch[3].to(device)\r\n",
        "        src_tok_type = batch[4].to(device)\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output, tok_op, _ = model(src, src_mask, trg[:,:-1], trg_mask[:,:-1], src_tok_type[:,:-1])\r\n",
        "                \r\n",
        "        #output = [batch size, trg len - 1, output dim]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "                \r\n",
        "        #output = [batch size * trg len - 1, output dim]\r\n",
        "        #trg = [batch size * trg len - 1]\r\n",
        "        if(double_loss == True):            \r\n",
        "            tok_op_output_dim = tok_op.shape[-1]            \r\n",
        "            tok_op = tok_op.contiguous().view(-1, tok_op_output_dim)\r\n",
        "            src_tok_type = src_tok_type[:,1:].contiguous().view(-1)\r\n",
        "            #output = [batch size * trg len - 1, output dim]\r\n",
        "            #trg = [batch size * trg len - 1]            \r\n",
        "            loss,crit_loss, tok_loss = criterion(output, \r\n",
        "                                                 trg,\r\n",
        "                                                 tok_op,\r\n",
        "                                                 src_tok_type)\r\n",
        "            epoch_crit_loss += crit_loss.item()\r\n",
        "            epoch_tok_loss += tok_loss.item()\r\n",
        "        else:\r\n",
        "            loss = criterion(output, trg)\r\n",
        "            \r\n",
        "        loss.backward()        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        optimizer.step()        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "    \r\n",
        "    if(double_loss == True):  \r\n",
        "        print(f'Train\\tCrit Loss: {epoch_crit_loss/(len(iterator)):.3f} | Token Loss: {epoch_tok_loss/(len(iterator)):.3f}')\r\n",
        "\r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26yD4gOnMr9U"
      },
      "source": [
        "def evaluate(model, iterator, criterion, device,double_loss=False):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_crit_loss = 0\r\n",
        "    epoch_tok_loss = 0\r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            trg = batch[0].to(device)\r\n",
        "            trg_mask = batch[1].to(device)\r\n",
        "            src = batch[2].to(device)\r\n",
        "            src_mask = batch[3].to(device)\r\n",
        "            src_tok_type = batch[4].to(device)\r\n",
        "\r\n",
        "            #output, _ = model(src, src_mask, trg[:,:-1], trg_mask[:,:-1])\r\n",
        "            output, tok_op, _ = model(src, src_mask, trg[:,:-1], trg_mask[:,:-1], src_tok_type[:,:-1])\r\n",
        "            \r\n",
        "            #output = [batch size, trg len - 1, output dim]\r\n",
        "            #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "            \r\n",
        "            if(double_loss == True):            \r\n",
        "                tok_op_output_dim = tok_op.shape[-1]            \r\n",
        "                tok_op = tok_op.contiguous().view(-1, tok_op_output_dim)\r\n",
        "                src_tok_type = src_tok_type[:,1:].contiguous().view(-1)\r\n",
        "                #output = [batch size * trg len - 1, output dim]\r\n",
        "                #trg = [batch size * trg len - 1]            \r\n",
        "                loss,crit_loss, tok_loss = criterion(output, \r\n",
        "                                                     trg,\r\n",
        "                                                     tok_op,\r\n",
        "                                                     src_tok_type)\r\n",
        "                epoch_crit_loss += crit_loss.item()\r\n",
        "                epoch_tok_loss += tok_loss.item()\r\n",
        "            else:\r\n",
        "                loss = criterion(output, trg)\r\n",
        "            \r\n",
        "\r\n",
        "            #output = [batch size * trg len - 1, output dim]\r\n",
        "            #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "            # loss = criterion(output, \r\n",
        "            #                  trg,\r\n",
        "            #                  tok_op,\r\n",
        "            #                  src_tok_type)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "\r\n",
        "    if(double_loss == True):  \r\n",
        "        print(f'Val\\tCrit Loss: {epoch_crit_loss/(len(iterator)):.3f} | Token Loss: {epoch_tok_loss/(len(iterator)):.3f}')  \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGz-LB82MvFT"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHcNK9jmPC7U"
      },
      "source": [
        "# my_torch_weights = torch.ones(auto_tokenizer.vocab_size)\r\n",
        "# my_torch_weights[1437] = 2\r\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX, weight=my_torch_weights.to(device) )\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\r\n",
        "\r\n",
        "class WeightedCrossEntropy(nn.Module):\r\n",
        "    def __init__(self,\r\n",
        "                 code_weights=None,\r\n",
        "                 code_ignore_idx=None,\r\n",
        "                 tok_type_weights=None,\r\n",
        "                 tok_type_ignore_idx=None,\r\n",
        "                 mix_ratio=0.5):\r\n",
        "        \r\n",
        "        super(WeightedCrossEntropy, self).__init__()\r\n",
        "        self.code_weights=code_weights\r\n",
        "        self.code_ignore_idx=code_ignore_idx\r\n",
        "        self.tok_type_weights=tok_type_weights\r\n",
        "        self.tok_type_ignore_idx=tok_type_ignore_idx\r\n",
        "        self.mix_ratio = mix_ratio\r\n",
        "    \r\n",
        "    def forward(self, \r\n",
        "                code_output, \r\n",
        "                code_trg,\r\n",
        "                tok_type_output,\r\n",
        "                tok_type_trg):\r\n",
        "        \r\n",
        "        # code_criterion = nn.CrossEntropyLoss(ignore_index = self.code_ignore_idx,\r\n",
        "        #                                      weight=self.code_weights)\r\n",
        "        # toktype_criterion = nn.CrossEntropyLoss(ignore_index = self.tok_type_ignore_idx,\r\n",
        "        #                                 weight=self.tok_type_weights)\r\n",
        "\r\n",
        "        code_criterion = F.cross_entropy(code_output, \r\n",
        "                                         code_trg, \r\n",
        "                                         weight=self.code_weights,\r\n",
        "                                         ignore_index = self.code_ignore_idx)\r\n",
        "        toktype_criterion = F.cross_entropy(tok_type_output, \r\n",
        "                                         tok_type_trg, \r\n",
        "                                         weight=self.tok_type_weights,\r\n",
        "                                         ignore_index = self.tok_type_ignore_idx)\r\n",
        "        \r\n",
        "        #total_loss = self.mix_ratio * code_criterion + (1-self.mix_ratio )*toktype_criterion\r\n",
        "        total_loss = code_criterion + toktype_criterion\r\n",
        "        return total_loss, code_criterion, toktype_criterion\r\n",
        "\r\n",
        "criterion = WeightedCrossEntropy(code_ignore_idx=TRG_PAD_IDX, \r\n",
        "                                 tok_type_ignore_idx=TRG_PAD_IDX,\r\n",
        "                                 mix_ratio=0.9999)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HwsjJd3PiGf",
        "outputId": "d88b06d3-8eda-44d2-bb81-14301f3f3895"
      },
      "source": [
        "batch = next(iter(train_dataloader))\r\n",
        "trg = batch[0].to(device)\r\n",
        "trg_mask = batch[1].to(device)\r\n",
        "src = batch[2].to(device)\r\n",
        "src_mask = batch[3].to(device)\r\n",
        "src_tok_types = batch[4].to(device)\r\n",
        "print(trg.shape, trg_mask.shape, src.shape, src_mask.shape, src_tok_types.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 256]) torch.Size([8, 256]) torch.Size([8, 256]) torch.Size([8, 256]) torch.Size([8, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaK3e2iMP66R"
      },
      "source": [
        "model.train()\r\n",
        "#with torch.no_grad():\r\n",
        "output,tok_op,attend_val = model(src, src_mask, trg[:,:-1], trg_mask[:,:-1], src_tok_types[:,:-1])\r\n",
        "    # output_dim = output.shape[-1]\r\n",
        "    \r\n",
        "    # output = output.contiguous().view(-1, output_dim)\r\n",
        "    # trg = trg[:,1:].contiguous().view(-1)\r\n",
        "    \r\n",
        "    #output = [batch size * trg len - 1, output dim]\r\n",
        "    #trg = [batch size * trg len - 1]\r\n",
        "    \r\n",
        "    #loss = criterion(output, trg)\r\n",
        "    #print(loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY5d6VwOTrKc"
      },
      "source": [
        "nl_pl_to_df = pd.read_csv('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNgUyTyZ_k0m",
        "outputId": "742d1e3a-0e7f-4cc3-d498-38b5972869f9"
      },
      "source": [
        "output.shape, tok_op.shape, attend_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 255, 5809]),\n",
              " torch.Size([8, 255, 62]),\n",
              " torch.Size([8, 8, 255, 256]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxDsVPtGYvcd"
      },
      "source": [
        "output_dim = output.shape[-1]\r\n",
        "\r\n",
        "output = output.contiguous().view(-1, output_dim)\r\n",
        "trg = trg[:,1:].contiguous().view(-1)\r\n",
        "\r\n",
        "tok_output_dim = tok_op.shape[-1]\r\n",
        "tok_op = tok_op.contiguous().view(-1, tok_output_dim)\r\n",
        "src_tok_types = src_tok_types[:,1:].contiguous().view(-1)\r\n",
        "    \r\n",
        "#output = [batch size * trg len - 1, output dim]\r\n",
        "#trg = [batch size * trg len - 1]\r\n",
        "\r\n",
        "#loss = criterion(output, trg)\r\n",
        "#print(loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKqoz_GQBkWE"
      },
      "source": [
        "output.shape, trg.shape, tok_op.shape, src_tok_types.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBG82-UVaCab"
      },
      "source": [
        "loss = criterion(output, trg, tok_op, src_tok_types)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBiqn6yYvgs7"
      },
      "source": [
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hrlBV-kAM4h"
      },
      "source": [
        "import time\r\n",
        "import math\r\n",
        "\r\n",
        "def run_train_eval_loop(model, \r\n",
        "                        train_dataloader,\r\n",
        "                        val_dataloader,\r\n",
        "                        optimizer,\r\n",
        "                        criterion,\r\n",
        "                        device,\r\n",
        "                        epochs=20,\r\n",
        "                        clip=1,\r\n",
        "                        best_valid_loss=float('inf'),\r\n",
        "                        file_path='end_capstone_baseline_128.pt',\r\n",
        "                        double_loss=False,\r\n",
        "                        scheduler=None,\r\n",
        "                        mix_ratio=0.5):\r\n",
        "    \r\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\r\n",
        "\r\n",
        "    if double_loss == True:    \r\n",
        "        criterion = WeightedCrossEntropy(code_ignore_idx=TRG_PAD_IDX, \r\n",
        "                                        tok_type_ignore_idx=TRG_PAD_IDX,\r\n",
        "                                        mix_ratio=mix_ratio)            \r\n",
        "    \r\n",
        "    for epoch in range(epochs):\r\n",
        "    \r\n",
        "        start_time = time.time()\r\n",
        "        \r\n",
        "        train_loss = train(model, train_dataloader, optimizer, criterion, clip, device,double_loss=double_loss)\r\n",
        "        valid_loss = evaluate(model, val_dataloader, criterion, device,double_loss=double_loss)\r\n",
        "        \r\n",
        "        if(scheduler is not None):\r\n",
        "            scheduler.step(valid_loss)\r\n",
        "        end_time = time.time()\r\n",
        "        \r\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "        \r\n",
        "        if valid_loss < best_valid_loss:\r\n",
        "            best_valid_loss = valid_loss\r\n",
        "            torch.save({\"model\":model.state_dict(),\r\n",
        "                \"optimizer\":optimizer.state_dict(),\r\n",
        "                \"loss\":valid_loss,\r\n",
        "                },file_path)\r\n",
        "        \r\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE1Xa6EJwk2F"
      },
      "source": [
        "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\r\n",
        "    \r\n",
        "    assert n_rows * n_cols == n_heads\r\n",
        "    \r\n",
        "    fig = plt.figure(figsize=(15,25))\r\n",
        "    \r\n",
        "    for i in range(n_heads):\r\n",
        "        \r\n",
        "        ax = fig.add_subplot(n_rows, n_cols, i+1)\r\n",
        "        \r\n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\r\n",
        "\r\n",
        "        cax = ax.matshow(_attention, cmap='bone')\r\n",
        "\r\n",
        "        ax.tick_params(labelsize=12)\r\n",
        "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \r\n",
        "                           rotation=45)\r\n",
        "        ax.set_yticklabels(['']+translation)\r\n",
        "\r\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "    plt.show()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiwj5jwOA_Lf"
      },
      "source": [
        "file_path='end_capstone_self_encode_sizeCor_seq512.pt'\r\n",
        "LEARNING_RATE = 0.0005\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\r\n",
        "run_train_eval_loop(model,\r\n",
        "                    train_dataloader,\r\n",
        "                    val_dataloader,\r\n",
        "                    optimizer,\r\n",
        "                    criterion,\r\n",
        "                    device,\r\n",
        "                    epochs=30,\r\n",
        "                    clip=1,\r\n",
        "                    best_valid_loss=float('inf'),\r\n",
        "                    file_path=file_path,\r\n",
        "                    double_loss=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yra9NwY-vEL-"
      },
      "source": [
        "torch.save({\"model\":model.state_dict(),\r\n",
        "    \"optimizer\":optimizer.state_dict(),\r\n",
        "    \"loss\":1.037 ,\r\n",
        "    },'end_capstone_self_encode_sizeCor_stage2_256_wrn5.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP8TCASrKRB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac0dd1cb-ea24-4d77-ece8-d46ac00c88c2"
      },
      "source": [
        "file_path='/content/end_capstone_self_encode_sizeCor_seq128.pt'\r\n",
        "\r\n",
        "chkpt = torch.load(file_path)\r\n",
        "print(chkpt['loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2910448792907927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H35YKslnmSWT",
        "outputId": "27c63235-01f8-4d96-9653-94334ea02bc3"
      },
      "source": [
        "model.load_state_dict(chkpt['model'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-Ze2nKrvj1-"
      },
      "source": [
        "!cp /content/end_capstone_self_encode_sizeCor_stage2_256_wrn5.pt /content/drive/MyDrive/EVA4/END_Capstone/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xbt0QojKy6M"
      },
      "source": [
        "LEARNING_RATE = 0.0005\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\r\n",
        "optimizer.load_state_dict(chkpt['optimizer'])\r\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-8,verbose=True)\r\n",
        "run_train_eval_loop(model,\r\n",
        "                    train_dataloader,\r\n",
        "                    val_dataloader,\r\n",
        "                    optimizer,\r\n",
        "                    criterion,\r\n",
        "                    device,\r\n",
        "                    epochs=50,\r\n",
        "                    clip=1,\r\n",
        "                    best_valid_loss=chkpt['loss'],\r\n",
        "                    file_path=file_path,\r\n",
        "                    double_loss=True,\r\n",
        "                    scheduler=scheduler,\r\n",
        "                    mix_ratio=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBh5A2DmwfLI"
      },
      "source": [
        "!cp /content/end_capstone_self_encode_sizeCor.pt /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_self_encode_sizeCor_mixed2.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wv7Bi-17EsJ",
        "outputId": "28e351e4-0552-4abd-b821-44eea067b2a2"
      },
      "source": [
        "file_path='/content/drive/MyDrive/EVA4/END_Capstone/end_capstone_self_encode_sizeCor_stage2_256_wrn3.pt'\r\n",
        "\r\n",
        "chkpt = torch.load(file_path)\r\n",
        "print(chkpt['loss'])\r\n",
        "model.load_state_dict(chkpt['model'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.932535447990117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSKGPMshKnYF",
        "outputId": "de815772-946e-4cc2-d65e-eaee0f09babc"
      },
      "source": [
        "\r\n",
        "\r\n",
        "file_path='/content/end_capstone_self_encode_sizeCor_stage2_256_wrn5.pt'\r\n",
        "\r\n",
        "chkpt = torch.load(file_path)\r\n",
        "print(chkpt['loss'])\r\n",
        "model.load_state_dict(chkpt['model'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KrWgIHbvMVM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "936533ad-0ddb-4512-afb8-ee5dbf64a6e0"
      },
      "source": [
        "LEARNING_RATE = 0.00005\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\r\n",
        "file_path='/content/end_capstone_self_encode_sizeCor_stage2_256_wrn5.pt'\r\n",
        "\r\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-9,verbose=True)\r\n",
        "run_train_eval_loop(model,\r\n",
        "                    train_dataloader,\r\n",
        "                    val_dataloader,\r\n",
        "                    optimizer,\r\n",
        "                    criterion,\r\n",
        "                    device,\r\n",
        "                    epochs=100,\r\n",
        "                    clip=1.4,\r\n",
        "                    best_valid_loss=float('inf'),\r\n",
        "                    file_path=file_path,\r\n",
        "                    double_loss=True,\r\n",
        "                    scheduler=scheduler,\r\n",
        "                    mix_ratio=0.9) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\tCrit Loss: 0.247 | Token Loss: 0.140\n",
            "Val\tCrit Loss: 1.171 | Token Loss: 0.222\n",
            "Epoch: 01 | Time: 0m 27s\n",
            "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
            "\t Val. Loss: 1.076 |  Val. PPL:   2.933\n",
            "Train\tCrit Loss: 0.241 | Token Loss: 0.136\n",
            "Val\tCrit Loss: 1.137 | Token Loss: 0.220\n",
            "Epoch: 02 | Time: 0m 27s\n",
            "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
            "\t Val. Loss: 1.045 |  Val. PPL:   2.843\n",
            "Train\tCrit Loss: 0.236 | Token Loss: 0.136\n",
            "Val\tCrit Loss: 1.128 | Token Loss: 0.217\n",
            "Epoch: 03 | Time: 0m 27s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 1.037 |  Val. PPL:   2.821\n",
            "Train\tCrit Loss: 0.235 | Token Loss: 0.134\n",
            "Val\tCrit Loss: 1.183 | Token Loss: 0.225\n",
            "Epoch: 04 | Time: 0m 27s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
            "\t Val. Loss: 1.087 |  Val. PPL:   2.966\n",
            "Train\tCrit Loss: 0.230 | Token Loss: 0.133\n",
            "Val\tCrit Loss: 1.152 | Token Loss: 0.221\n",
            "Epoch: 05 | Time: 0m 27s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 1.059 |  Val. PPL:   2.882\n",
            "Train\tCrit Loss: 0.224 | Token Loss: 0.133\n",
            "Val\tCrit Loss: 1.159 | Token Loss: 0.220\n",
            "Epoch: 06 | Time: 0m 27s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 1.065 |  Val. PPL:   2.901\n",
            "Train\tCrit Loss: 0.219 | Token Loss: 0.130\n",
            "Val\tCrit Loss: 1.180 | Token Loss: 0.227\n",
            "Epoch: 07 | Time: 0m 27s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 1.084 |  Val. PPL:   2.957\n",
            "Train\tCrit Loss: 0.216 | Token Loss: 0.129\n",
            "Val\tCrit Loss: 1.197 | Token Loss: 0.226\n",
            "Epoch: 08 | Time: 0m 27s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
            "\t Val. Loss: 1.100 |  Val. PPL:   3.003\n",
            "Train\tCrit Loss: 0.214 | Token Loss: 0.127\n",
            "Val\tCrit Loss: 1.190 | Token Loss: 0.221\n",
            "Epoch     9: reducing learning rate of group 0 to 5.0000e-06.\n",
            "Epoch: 09 | Time: 0m 27s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
            "\t Val. Loss: 1.093 |  Val. PPL:   2.982\n",
            "Train\tCrit Loss: 0.206 | Token Loss: 0.124\n",
            "Val\tCrit Loss: 1.158 | Token Loss: 0.221\n",
            "Epoch: 10 | Time: 0m 27s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 1.064 |  Val. PPL:   2.898\n",
            "Train\tCrit Loss: 0.205 | Token Loss: 0.124\n",
            "Val\tCrit Loss: 1.168 | Token Loss: 0.223\n",
            "Epoch: 11 | Time: 0m 27s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
            "\t Val. Loss: 1.073 |  Val. PPL:   2.925\n",
            "Train\tCrit Loss: 0.200 | Token Loss: 0.124\n",
            "Val\tCrit Loss: 1.199 | Token Loss: 0.232\n",
            "Epoch: 12 | Time: 0m 27s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
            "\t Val. Loss: 1.102 |  Val. PPL:   3.010\n",
            "Train\tCrit Loss: 0.199 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.194 | Token Loss: 0.230\n",
            "Epoch: 13 | Time: 0m 27s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 1.098 |  Val. PPL:   2.998\n",
            "Train\tCrit Loss: 0.200 | Token Loss: 0.123\n",
            "Val\tCrit Loss: 1.157 | Token Loss: 0.221\n",
            "Epoch: 14 | Time: 0m 27s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
            "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
            "Train\tCrit Loss: 0.199 | Token Loss: 0.121\n",
            "Val\tCrit Loss: 1.186 | Token Loss: 0.225\n",
            "Epoch    15: reducing learning rate of group 0 to 5.0000e-07.\n",
            "Epoch: 15 | Time: 0m 27s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 1.089 |  Val. PPL:   2.973\n",
            "Train\tCrit Loss: 0.200 | Token Loss: 0.123\n",
            "Val\tCrit Loss: 1.154 | Token Loss: 0.223\n",
            "Epoch: 16 | Time: 0m 27s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
            "\t Val. Loss: 1.060 |  Val. PPL:   2.888\n",
            "Train\tCrit Loss: 0.197 | Token Loss: 0.120\n",
            "Val\tCrit Loss: 1.184 | Token Loss: 0.227\n",
            "Epoch: 17 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
            "Train\tCrit Loss: 0.198 | Token Loss: 0.121\n",
            "Val\tCrit Loss: 1.179 | Token Loss: 0.225\n",
            "Epoch: 18 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 1.084 |  Val. PPL:   2.956\n",
            "Train\tCrit Loss: 0.195 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.206 | Token Loss: 0.223\n",
            "Epoch: 19 | Time: 0m 27s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
            "\t Val. Loss: 1.108 |  Val. PPL:   3.027\n",
            "Train\tCrit Loss: 0.199 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.154 | Token Loss: 0.219\n",
            "Epoch: 20 | Time: 0m 27s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
            "\t Val. Loss: 1.061 |  Val. PPL:   2.888\n",
            "Train\tCrit Loss: 0.198 | Token Loss: 0.123\n",
            "Val\tCrit Loss: 1.164 | Token Loss: 0.220\n",
            "Epoch    21: reducing learning rate of group 0 to 5.0000e-08.\n",
            "Epoch: 21 | Time: 0m 27s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 1.069 |  Val. PPL:   2.913\n",
            "Train\tCrit Loss: 0.199 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.164 | Token Loss: 0.220\n",
            "Epoch: 22 | Time: 0m 27s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 1.069 |  Val. PPL:   2.913\n",
            "Train\tCrit Loss: 0.197 | Token Loss: 0.123\n",
            "Val\tCrit Loss: 1.177 | Token Loss: 0.220\n",
            "Epoch: 23 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 1.082 |  Val. PPL:   2.949\n",
            "Train\tCrit Loss: 0.197 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.180 | Token Loss: 0.224\n",
            "Epoch: 24 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 1.084 |  Val. PPL:   2.956\n",
            "Train\tCrit Loss: 0.197 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.170 | Token Loss: 0.225\n",
            "Epoch: 25 | Time: 0m 27s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 1.076 |  Val. PPL:   2.932\n",
            "Train\tCrit Loss: 0.196 | Token Loss: 0.121\n",
            "Val\tCrit Loss: 1.184 | Token Loss: 0.229\n",
            "Epoch: 26 | Time: 0m 27s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
            "Train\tCrit Loss: 0.197 | Token Loss: 0.121\n",
            "Val\tCrit Loss: 1.196 | Token Loss: 0.222\n",
            "Epoch    27: reducing learning rate of group 0 to 5.0000e-09.\n",
            "Epoch: 27 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 1.098 |  Val. PPL:   2.999\n",
            "Train\tCrit Loss: 0.198 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.185 | Token Loss: 0.228\n",
            "Epoch: 28 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 1.090 |  Val. PPL:   2.973\n",
            "Train\tCrit Loss: 0.196 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.170 | Token Loss: 0.224\n",
            "Epoch: 29 | Time: 0m 27s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 1.076 |  Val. PPL:   2.932\n",
            "Train\tCrit Loss: 0.196 | Token Loss: 0.121\n",
            "Val\tCrit Loss: 1.177 | Token Loss: 0.224\n",
            "Epoch: 30 | Time: 0m 27s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 1.082 |  Val. PPL:   2.950\n",
            "Train\tCrit Loss: 0.197 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.144 | Token Loss: 0.219\n",
            "Epoch: 31 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 1.051 |  Val. PPL:   2.861\n",
            "Train\tCrit Loss: 0.198 | Token Loss: 0.123\n",
            "Val\tCrit Loss: 1.173 | Token Loss: 0.222\n",
            "Epoch: 32 | Time: 0m 27s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 1.078 |  Val. PPL:   2.938\n",
            "Train\tCrit Loss: 0.200 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.182 | Token Loss: 0.226\n",
            "Epoch: 33 | Time: 0m 27s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 1.086 |  Val. PPL:   2.963\n",
            "Train\tCrit Loss: 0.198 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.166 | Token Loss: 0.226\n",
            "Epoch: 34 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 1.072 |  Val. PPL:   2.922\n",
            "Train\tCrit Loss: 0.198 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.173 | Token Loss: 0.222\n",
            "Epoch: 35 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 1.078 |  Val. PPL:   2.939\n",
            "Train\tCrit Loss: 0.198 | Token Loss: 0.121\n",
            "Val\tCrit Loss: 1.152 | Token Loss: 0.222\n",
            "Epoch: 36 | Time: 0m 27s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 1.059 |  Val. PPL:   2.884\n",
            "Train\tCrit Loss: 0.198 | Token Loss: 0.121\n",
            "Val\tCrit Loss: 1.192 | Token Loss: 0.226\n",
            "Epoch: 37 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 1.096 |  Val. PPL:   2.991\n",
            "Train\tCrit Loss: 0.196 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.184 | Token Loss: 0.228\n",
            "Epoch: 38 | Time: 0m 27s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 1.089 |  Val. PPL:   2.970\n",
            "Train\tCrit Loss: 0.198 | Token Loss: 0.121\n",
            "Val\tCrit Loss: 1.169 | Token Loss: 0.223\n",
            "Epoch: 39 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 1.075 |  Val. PPL:   2.929\n",
            "Train\tCrit Loss: 0.198 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.150 | Token Loss: 0.220\n",
            "Epoch: 40 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 1.057 |  Val. PPL:   2.878\n",
            "Train\tCrit Loss: 0.198 | Token Loss: 0.123\n",
            "Val\tCrit Loss: 1.172 | Token Loss: 0.224\n",
            "Epoch: 41 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 1.077 |  Val. PPL:   2.935\n",
            "Train\tCrit Loss: 0.197 | Token Loss: 0.121\n",
            "Val\tCrit Loss: 1.176 | Token Loss: 0.224\n",
            "Epoch: 42 | Time: 0m 27s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 1.081 |  Val. PPL:   2.947\n",
            "Train\tCrit Loss: 0.197 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.194 | Token Loss: 0.231\n",
            "Epoch: 43 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 1.098 |  Val. PPL:   2.997\n",
            "Train\tCrit Loss: 0.199 | Token Loss: 0.123\n",
            "Val\tCrit Loss: 1.190 | Token Loss: 0.227\n",
            "Epoch: 44 | Time: 0m 27s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 1.094 |  Val. PPL:   2.986\n",
            "Train\tCrit Loss: 0.197 | Token Loss: 0.121\n",
            "Val\tCrit Loss: 1.190 | Token Loss: 0.226\n",
            "Epoch: 45 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 1.093 |  Val. PPL:   2.984\n",
            "Train\tCrit Loss: 0.197 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.172 | Token Loss: 0.228\n",
            "Epoch: 46 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 1.077 |  Val. PPL:   2.937\n",
            "Train\tCrit Loss: 0.197 | Token Loss: 0.120\n",
            "Val\tCrit Loss: 1.155 | Token Loss: 0.220\n",
            "Epoch: 47 | Time: 0m 27s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 1.062 |  Val. PPL:   2.891\n",
            "Train\tCrit Loss: 0.200 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.181 | Token Loss: 0.226\n",
            "Epoch: 48 | Time: 0m 27s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 1.086 |  Val. PPL:   2.962\n",
            "Train\tCrit Loss: 0.196 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.163 | Token Loss: 0.223\n",
            "Epoch: 49 | Time: 0m 27s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 1.069 |  Val. PPL:   2.913\n",
            "Train\tCrit Loss: 0.197 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.177 | Token Loss: 0.224\n",
            "Epoch: 50 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 1.081 |  Val. PPL:   2.948\n",
            "Train\tCrit Loss: 0.198 | Token Loss: 0.122\n",
            "Val\tCrit Loss: 1.156 | Token Loss: 0.219\n",
            "Epoch: 51 | Time: 0m 27s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 1.063 |  Val. PPL:   2.894\n",
            "Train\tCrit Loss: 0.199 | Token Loss: 0.123\n",
            "Val\tCrit Loss: 1.182 | Token Loss: 0.224\n",
            "Epoch: 52 | Time: 0m 27s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 1.086 |  Val. PPL:   2.962\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-168-11a8bfa0e890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mdouble_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                     mix_ratio=0.9)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-e4ce168cb8c0>\u001b[0m in \u001b[0;36mrun_train_eval_loop\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, criterion, device, epochs, clip, best_valid_loss, file_path, double_loss, scheduler, mix_ratio)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdouble_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdouble_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdouble_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdouble_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-6a49ac91852e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip, device, double_loss)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_tok_type\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#output = [batch size, trg len - 1, output dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-0b31b460a3d2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, trg, trg_mask, src_tok_types)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                          \u001b[0mtrg_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                          \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                          src_tok_types)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m#output = [batch size, trg len, output dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-168090165279>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, trg, enc_src, trg_mask, src_mask, src_tok_types)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m#trg = [batch size, trg len, hid dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-5257014854e4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, trg, enc_src, trg_mask, src_mask)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#positionwise feedforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0m_trg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositionwise_feedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#dropout, residual and layer norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-279165f31071>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#x = [batch size, seq len, pf dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#x = [batch size, seq len, hid dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZO2dHwDSKQT",
        "outputId": "e2c2cafd-3830-4e89-f07a-1923c480c54e"
      },
      "source": [
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_self_encode_sizeCor_stage2_256_wrn5.pt .\r\n",
        "file_path='/content/end_capstone_self_encode_sizeCor_stage2_256_wrn5.pt'\r\n",
        "\r\n",
        "chkpt = torch.load(file_path)\r\n",
        "print(chkpt['loss'])\r\n",
        "model.load_state_dict(chkpt['model'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRBviYJnKOUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca35af9b-37b0-4f7a-d724-fd34982d442c"
      },
      "source": [
        "LEARNING_RATE = 0.00001\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\r\n",
        "file_path='/content/end_capstone_self_encode_sizeCor_stage2_256_wrn6.pt'\r\n",
        "\r\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-9,verbose=True)\r\n",
        "run_train_eval_loop(model,\r\n",
        "                    train_dataloader,\r\n",
        "                    val_dataloader,\r\n",
        "                    optimizer,\r\n",
        "                    criterion,\r\n",
        "                    device,\r\n",
        "                    epochs=100,\r\n",
        "                    clip=1.4,\r\n",
        "                    best_valid_loss=float('inf'),\r\n",
        "                    file_path=file_path,\r\n",
        "                    double_loss=True,\r\n",
        "                    scheduler=scheduler,\r\n",
        "                    mix_ratio=0.9) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\tCrit Loss: 1.313 | Token Loss: 0.306\n",
            "Val\tCrit Loss: 1.007 | Token Loss: 0.220\n",
            "Epoch: 01 | Time: 1m 21s\n",
            "\tTrain Loss: 1.212 | Train PPL:   3.361\n",
            "\t Val. Loss: 0.928 |  Val. PPL:   2.530\n",
            "Train\tCrit Loss: 1.229 | Token Loss: 0.296\n",
            "Val\tCrit Loss: 0.922 | Token Loss: 0.212\n",
            "Epoch: 02 | Time: 1m 21s\n",
            "\tTrain Loss: 1.136 | Train PPL:   3.114\n",
            "\t Val. Loss: 0.851 |  Val. PPL:   2.342\n",
            "Train\tCrit Loss: 1.148 | Token Loss: 0.298\n",
            "Val\tCrit Loss: 0.898 | Token Loss: 0.217\n",
            "Epoch: 03 | Time: 1m 22s\n",
            "\tTrain Loss: 1.063 | Train PPL:   2.896\n",
            "\t Val. Loss: 0.830 |  Val. PPL:   2.292\n",
            "Train\tCrit Loss: 1.089 | Token Loss: 0.296\n",
            "Val\tCrit Loss: 0.865 | Token Loss: 0.215\n",
            "Epoch: 04 | Time: 1m 22s\n",
            "\tTrain Loss: 1.010 | Train PPL:   2.746\n",
            "\t Val. Loss: 0.800 |  Val. PPL:   2.225\n",
            "Train\tCrit Loss: 1.055 | Token Loss: 0.292\n",
            "Val\tCrit Loss: 0.831 | Token Loss: 0.210\n",
            "Epoch: 05 | Time: 1m 22s\n",
            "\tTrain Loss: 0.978 | Train PPL:   2.660\n",
            "\t Val. Loss: 0.769 |  Val. PPL:   2.157\n",
            "Train\tCrit Loss: 1.023 | Token Loss: 0.283\n",
            "Val\tCrit Loss: 0.815 | Token Loss: 0.204\n",
            "Epoch: 06 | Time: 1m 22s\n",
            "\tTrain Loss: 0.949 | Train PPL:   2.582\n",
            "\t Val. Loss: 0.754 |  Val. PPL:   2.126\n",
            "Train\tCrit Loss: 0.993 | Token Loss: 0.276\n",
            "Val\tCrit Loss: 0.793 | Token Loss: 0.203\n",
            "Epoch: 07 | Time: 1m 21s\n",
            "\tTrain Loss: 0.921 | Train PPL:   2.513\n",
            "\t Val. Loss: 0.734 |  Val. PPL:   2.084\n",
            "Train\tCrit Loss: 0.968 | Token Loss: 0.265\n",
            "Val\tCrit Loss: 0.780 | Token Loss: 0.197\n",
            "Epoch: 08 | Time: 1m 22s\n",
            "\tTrain Loss: 0.897 | Train PPL:   2.453\n",
            "\t Val. Loss: 0.722 |  Val. PPL:   2.058\n",
            "Train\tCrit Loss: 0.946 | Token Loss: 0.260\n",
            "Val\tCrit Loss: 0.770 | Token Loss: 0.194\n",
            "Epoch: 09 | Time: 1m 21s\n",
            "\tTrain Loss: 0.878 | Train PPL:   2.406\n",
            "\t Val. Loss: 0.712 |  Val. PPL:   2.039\n",
            "Train\tCrit Loss: 0.931 | Token Loss: 0.259\n",
            "Val\tCrit Loss: 0.769 | Token Loss: 0.193\n",
            "Epoch: 10 | Time: 1m 21s\n",
            "\tTrain Loss: 0.864 | Train PPL:   2.373\n",
            "\t Val. Loss: 0.712 |  Val. PPL:   2.037\n",
            "Train\tCrit Loss: 0.913 | Token Loss: 0.254\n",
            "Val\tCrit Loss: 0.755 | Token Loss: 0.190\n",
            "Epoch: 11 | Time: 1m 21s\n",
            "\tTrain Loss: 0.848 | Train PPL:   2.334\n",
            "\t Val. Loss: 0.699 |  Val. PPL:   2.011\n",
            "Train\tCrit Loss: 0.900 | Token Loss: 0.250\n",
            "Val\tCrit Loss: 0.737 | Token Loss: 0.184\n",
            "Epoch: 12 | Time: 1m 21s\n",
            "\tTrain Loss: 0.835 | Train PPL:   2.305\n",
            "\t Val. Loss: 0.682 |  Val. PPL:   1.977\n",
            "Train\tCrit Loss: 0.895 | Token Loss: 0.250\n",
            "Val\tCrit Loss: 0.756 | Token Loss: 0.187\n",
            "Epoch: 13 | Time: 1m 21s\n",
            "\tTrain Loss: 0.830 | Train PPL:   2.294\n",
            "\t Val. Loss: 0.699 |  Val. PPL:   2.012\n",
            "Train\tCrit Loss: 0.866 | Token Loss: 0.244\n",
            "Val\tCrit Loss: 0.759 | Token Loss: 0.188\n",
            "Epoch: 14 | Time: 1m 22s\n",
            "\tTrain Loss: 0.804 | Train PPL:   2.235\n",
            "\t Val. Loss: 0.702 |  Val. PPL:   2.018\n",
            "Train\tCrit Loss: 0.846 | Token Loss: 0.241\n",
            "Val\tCrit Loss: 0.732 | Token Loss: 0.183\n",
            "Epoch: 15 | Time: 1m 22s\n",
            "\tTrain Loss: 0.786 | Train PPL:   2.194\n",
            "\t Val. Loss: 0.677 |  Val. PPL:   1.968\n",
            "Train\tCrit Loss: 0.848 | Token Loss: 0.241\n",
            "Val\tCrit Loss: 0.704 | Token Loss: 0.176\n",
            "Epoch: 16 | Time: 1m 22s\n",
            "\tTrain Loss: 0.788 | Train PPL:   2.198\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.918\n",
            "Train\tCrit Loss: 0.826 | Token Loss: 0.236\n",
            "Val\tCrit Loss: 0.737 | Token Loss: 0.182\n",
            "Epoch: 17 | Time: 1m 21s\n",
            "\tTrain Loss: 0.767 | Train PPL:   2.153\n",
            "\t Val. Loss: 0.682 |  Val. PPL:   1.978\n",
            "Train\tCrit Loss: 0.824 | Token Loss: 0.235\n",
            "Val\tCrit Loss: 0.719 | Token Loss: 0.178\n",
            "Epoch: 18 | Time: 1m 21s\n",
            "\tTrain Loss: 0.765 | Train PPL:   2.149\n",
            "\t Val. Loss: 0.665 |  Val. PPL:   1.944\n",
            "Train\tCrit Loss: 0.811 | Token Loss: 0.232\n",
            "Val\tCrit Loss: 0.735 | Token Loss: 0.181\n",
            "Epoch: 19 | Time: 1m 22s\n",
            "\tTrain Loss: 0.753 | Train PPL:   2.124\n",
            "\t Val. Loss: 0.680 |  Val. PPL:   1.974\n",
            "Train\tCrit Loss: 0.797 | Token Loss: 0.230\n",
            "Val\tCrit Loss: 0.716 | Token Loss: 0.175\n",
            "Epoch: 20 | Time: 1m 21s\n",
            "\tTrain Loss: 0.740 | Train PPL:   2.096\n",
            "\t Val. Loss: 0.662 |  Val. PPL:   1.939\n",
            "Train\tCrit Loss: 0.793 | Token Loss: 0.228\n",
            "Val\tCrit Loss: 0.722 | Token Loss: 0.178\n",
            "Epoch: 21 | Time: 1m 21s\n",
            "\tTrain Loss: 0.737 | Train PPL:   2.089\n",
            "\t Val. Loss: 0.667 |  Val. PPL:   1.949\n",
            "Train\tCrit Loss: 0.787 | Token Loss: 0.227\n",
            "Val\tCrit Loss: 0.719 | Token Loss: 0.176\n",
            "Epoch    22: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch: 22 | Time: 1m 21s\n",
            "\tTrain Loss: 0.731 | Train PPL:   2.077\n",
            "\t Val. Loss: 0.665 |  Val. PPL:   1.945\n",
            "Train\tCrit Loss: 0.774 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.699 | Token Loss: 0.172\n",
            "Epoch: 23 | Time: 1m 21s\n",
            "\tTrain Loss: 0.719 | Train PPL:   2.052\n",
            "\t Val. Loss: 0.647 |  Val. PPL:   1.909\n",
            "Train\tCrit Loss: 0.771 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.725 | Token Loss: 0.178\n",
            "Epoch: 24 | Time: 1m 21s\n",
            "\tTrain Loss: 0.717 | Train PPL:   2.048\n",
            "\t Val. Loss: 0.670 |  Val. PPL:   1.954\n",
            "Train\tCrit Loss: 0.776 | Token Loss: 0.227\n",
            "Val\tCrit Loss: 0.710 | Token Loss: 0.174\n",
            "Epoch: 25 | Time: 1m 21s\n",
            "\tTrain Loss: 0.721 | Train PPL:   2.057\n",
            "\t Val. Loss: 0.657 |  Val. PPL:   1.928\n",
            "Train\tCrit Loss: 0.777 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.735 | Token Loss: 0.180\n",
            "Epoch: 26 | Time: 1m 21s\n",
            "\tTrain Loss: 0.722 | Train PPL:   2.058\n",
            "\t Val. Loss: 0.679 |  Val. PPL:   1.972\n",
            "Train\tCrit Loss: 0.763 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.704 | Token Loss: 0.173\n",
            "Epoch: 27 | Time: 1m 21s\n",
            "\tTrain Loss: 0.709 | Train PPL:   2.032\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.917\n",
            "Train\tCrit Loss: 0.773 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.719 | Token Loss: 0.174\n",
            "Epoch: 28 | Time: 1m 21s\n",
            "\tTrain Loss: 0.719 | Train PPL:   2.051\n",
            "\t Val. Loss: 0.664 |  Val. PPL:   1.943\n",
            "Train\tCrit Loss: 0.772 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.703 | Token Loss: 0.172\n",
            "Epoch    29: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch: 29 | Time: 1m 21s\n",
            "\tTrain Loss: 0.717 | Train PPL:   2.049\n",
            "\t Val. Loss: 0.649 |  Val. PPL:   1.915\n",
            "Train\tCrit Loss: 0.768 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.704 | Token Loss: 0.174\n",
            "Epoch: 30 | Time: 1m 21s\n",
            "\tTrain Loss: 0.713 | Train PPL:   2.041\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.917\n",
            "Train\tCrit Loss: 0.766 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.726 | Token Loss: 0.177\n",
            "Epoch: 31 | Time: 1m 21s\n",
            "\tTrain Loss: 0.712 | Train PPL:   2.038\n",
            "\t Val. Loss: 0.671 |  Val. PPL:   1.956\n",
            "Train\tCrit Loss: 0.766 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.710 | Token Loss: 0.174\n",
            "Epoch: 32 | Time: 1m 21s\n",
            "\tTrain Loss: 0.712 | Train PPL:   2.037\n",
            "\t Val. Loss: 0.656 |  Val. PPL:   1.927\n",
            "Train\tCrit Loss: 0.772 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.710 | Token Loss: 0.175\n",
            "Epoch: 33 | Time: 1m 21s\n",
            "\tTrain Loss: 0.718 | Train PPL:   2.050\n",
            "\t Val. Loss: 0.657 |  Val. PPL:   1.928\n",
            "Train\tCrit Loss: 0.764 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.724 | Token Loss: 0.177\n",
            "Epoch: 34 | Time: 1m 21s\n",
            "\tTrain Loss: 0.710 | Train PPL:   2.035\n",
            "\t Val. Loss: 0.669 |  Val. PPL:   1.952\n",
            "Train\tCrit Loss: 0.772 | Token Loss: 0.227\n",
            "Val\tCrit Loss: 0.707 | Token Loss: 0.174\n",
            "Epoch    35: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Epoch: 35 | Time: 1m 21s\n",
            "\tTrain Loss: 0.717 | Train PPL:   2.049\n",
            "\t Val. Loss: 0.653 |  Val. PPL:   1.922\n",
            "Train\tCrit Loss: 0.760 | Token Loss: 0.223\n",
            "Val\tCrit Loss: 0.707 | Token Loss: 0.174\n",
            "Epoch: 36 | Time: 1m 21s\n",
            "\tTrain Loss: 0.706 | Train PPL:   2.026\n",
            "\t Val. Loss: 0.654 |  Val. PPL:   1.923\n",
            "Train\tCrit Loss: 0.762 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.725 | Token Loss: 0.177\n",
            "Epoch: 37 | Time: 1m 21s\n",
            "\tTrain Loss: 0.708 | Train PPL:   2.030\n",
            "\t Val. Loss: 0.670 |  Val. PPL:   1.954\n",
            "Train\tCrit Loss: 0.775 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.695 | Token Loss: 0.170\n",
            "Epoch: 38 | Time: 1m 21s\n",
            "\tTrain Loss: 0.720 | Train PPL:   2.055\n",
            "\t Val. Loss: 0.643 |  Val. PPL:   1.901\n",
            "Train\tCrit Loss: 0.756 | Token Loss: 0.223\n",
            "Val\tCrit Loss: 0.718 | Token Loss: 0.175\n",
            "Epoch: 39 | Time: 1m 21s\n",
            "\tTrain Loss: 0.702 | Train PPL:   2.018\n",
            "\t Val. Loss: 0.664 |  Val. PPL:   1.942\n",
            "Train\tCrit Loss: 0.765 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.711 | Token Loss: 0.174\n",
            "Epoch: 40 | Time: 1m 21s\n",
            "\tTrain Loss: 0.711 | Train PPL:   2.036\n",
            "\t Val. Loss: 0.657 |  Val. PPL:   1.930\n",
            "Train\tCrit Loss: 0.767 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.729 | Token Loss: 0.177\n",
            "Epoch: 41 | Time: 1m 21s\n",
            "\tTrain Loss: 0.712 | Train PPL:   2.039\n",
            "\t Val. Loss: 0.674 |  Val. PPL:   1.962\n",
            "Train\tCrit Loss: 0.768 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.706 | Token Loss: 0.173\n",
            "Epoch: 42 | Time: 1m 21s\n",
            "\tTrain Loss: 0.714 | Train PPL:   2.042\n",
            "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
            "Train\tCrit Loss: 0.761 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.714 | Token Loss: 0.175\n",
            "Epoch: 43 | Time: 1m 21s\n",
            "\tTrain Loss: 0.708 | Train PPL:   2.029\n",
            "\t Val. Loss: 0.661 |  Val. PPL:   1.936\n",
            "Train\tCrit Loss: 0.765 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.702 | Token Loss: 0.174\n",
            "Epoch: 44 | Time: 1m 21s\n",
            "\tTrain Loss: 0.711 | Train PPL:   2.035\n",
            "\t Val. Loss: 0.649 |  Val. PPL:   1.915\n",
            "Train\tCrit Loss: 0.761 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.726 | Token Loss: 0.178\n",
            "Epoch: 45 | Time: 1m 21s\n",
            "\tTrain Loss: 0.707 | Train PPL:   2.028\n",
            "\t Val. Loss: 0.672 |  Val. PPL:   1.957\n",
            "Train\tCrit Loss: 0.768 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.728 | Token Loss: 0.177\n",
            "Epoch: 46 | Time: 1m 21s\n",
            "\tTrain Loss: 0.714 | Train PPL:   2.041\n",
            "\t Val. Loss: 0.673 |  Val. PPL:   1.960\n",
            "Train\tCrit Loss: 0.759 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.699 | Token Loss: 0.171\n",
            "Epoch: 47 | Time: 1m 21s\n",
            "\tTrain Loss: 0.705 | Train PPL:   2.024\n",
            "\t Val. Loss: 0.646 |  Val. PPL:   1.907\n",
            "Train\tCrit Loss: 0.765 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.714 | Token Loss: 0.174\n",
            "Epoch: 48 | Time: 1m 21s\n",
            "\tTrain Loss: 0.711 | Train PPL:   2.035\n",
            "\t Val. Loss: 0.660 |  Val. PPL:   1.936\n",
            "Train\tCrit Loss: 0.769 | Token Loss: 0.223\n",
            "Val\tCrit Loss: 0.711 | Token Loss: 0.175\n",
            "Epoch: 49 | Time: 1m 21s\n",
            "\tTrain Loss: 0.715 | Train PPL:   2.044\n",
            "\t Val. Loss: 0.657 |  Val. PPL:   1.930\n",
            "Train\tCrit Loss: 0.773 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.710 | Token Loss: 0.174\n",
            "Epoch: 50 | Time: 1m 21s\n",
            "\tTrain Loss: 0.719 | Train PPL:   2.052\n",
            "\t Val. Loss: 0.657 |  Val. PPL:   1.929\n",
            "Train\tCrit Loss: 0.755 | Token Loss: 0.221\n",
            "Val\tCrit Loss: 0.710 | Token Loss: 0.174\n",
            "Epoch: 51 | Time: 1m 21s\n",
            "\tTrain Loss: 0.701 | Train PPL:   2.016\n",
            "\t Val. Loss: 0.656 |  Val. PPL:   1.928\n",
            "Train\tCrit Loss: 0.762 | Token Loss: 0.223\n",
            "Val\tCrit Loss: 0.724 | Token Loss: 0.175\n",
            "Epoch: 52 | Time: 1m 21s\n",
            "\tTrain Loss: 0.708 | Train PPL:   2.030\n",
            "\t Val. Loss: 0.669 |  Val. PPL:   1.953\n",
            "Train\tCrit Loss: 0.763 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.721 | Token Loss: 0.175\n",
            "Epoch: 53 | Time: 1m 21s\n",
            "\tTrain Loss: 0.709 | Train PPL:   2.033\n",
            "\t Val. Loss: 0.666 |  Val. PPL:   1.947\n",
            "Train\tCrit Loss: 0.769 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.710 | Token Loss: 0.175\n",
            "Epoch: 54 | Time: 1m 21s\n",
            "\tTrain Loss: 0.715 | Train PPL:   2.044\n",
            "\t Val. Loss: 0.657 |  Val. PPL:   1.929\n",
            "Train\tCrit Loss: 0.764 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.733 | Token Loss: 0.178\n",
            "Epoch: 55 | Time: 1m 21s\n",
            "\tTrain Loss: 0.710 | Train PPL:   2.035\n",
            "\t Val. Loss: 0.677 |  Val. PPL:   1.969\n",
            "Train\tCrit Loss: 0.761 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.708 | Token Loss: 0.173\n",
            "Epoch: 56 | Time: 1m 21s\n",
            "\tTrain Loss: 0.707 | Train PPL:   2.028\n",
            "\t Val. Loss: 0.655 |  Val. PPL:   1.924\n",
            "Train\tCrit Loss: 0.764 | Token Loss: 0.223\n",
            "Val\tCrit Loss: 0.698 | Token Loss: 0.170\n",
            "Epoch: 57 | Time: 1m 21s\n",
            "\tTrain Loss: 0.710 | Train PPL:   2.034\n",
            "\t Val. Loss: 0.645 |  Val. PPL:   1.906\n",
            "Train\tCrit Loss: 0.769 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.714 | Token Loss: 0.175\n",
            "Epoch: 58 | Time: 1m 21s\n",
            "\tTrain Loss: 0.715 | Train PPL:   2.044\n",
            "\t Val. Loss: 0.660 |  Val. PPL:   1.935\n",
            "Train\tCrit Loss: 0.769 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.718 | Token Loss: 0.175\n",
            "Epoch: 59 | Time: 1m 21s\n",
            "\tTrain Loss: 0.714 | Train PPL:   2.043\n",
            "\t Val. Loss: 0.664 |  Val. PPL:   1.942\n",
            "Train\tCrit Loss: 0.763 | Token Loss: 0.223\n",
            "Val\tCrit Loss: 0.716 | Token Loss: 0.175\n",
            "Epoch: 60 | Time: 1m 21s\n",
            "\tTrain Loss: 0.709 | Train PPL:   2.033\n",
            "\t Val. Loss: 0.662 |  Val. PPL:   1.939\n",
            "Train\tCrit Loss: 0.771 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.698 | Token Loss: 0.172\n",
            "Epoch: 61 | Time: 1m 21s\n",
            "\tTrain Loss: 0.716 | Train PPL:   2.047\n",
            "\t Val. Loss: 0.646 |  Val. PPL:   1.907\n",
            "Train\tCrit Loss: 0.768 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.709 | Token Loss: 0.174\n",
            "Epoch: 62 | Time: 1m 21s\n",
            "\tTrain Loss: 0.714 | Train PPL:   2.041\n",
            "\t Val. Loss: 0.656 |  Val. PPL:   1.927\n",
            "Train\tCrit Loss: 0.770 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.719 | Token Loss: 0.175\n",
            "Epoch: 63 | Time: 1m 21s\n",
            "\tTrain Loss: 0.716 | Train PPL:   2.046\n",
            "\t Val. Loss: 0.665 |  Val. PPL:   1.944\n",
            "Train\tCrit Loss: 0.764 | Token Loss: 0.223\n",
            "Val\tCrit Loss: 0.711 | Token Loss: 0.175\n",
            "Epoch: 64 | Time: 1m 21s\n",
            "\tTrain Loss: 0.710 | Train PPL:   2.033\n",
            "\t Val. Loss: 0.658 |  Val. PPL:   1.930\n",
            "Train\tCrit Loss: 0.768 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.722 | Token Loss: 0.176\n",
            "Epoch: 65 | Time: 1m 21s\n",
            "\tTrain Loss: 0.714 | Train PPL:   2.042\n",
            "\t Val. Loss: 0.667 |  Val. PPL:   1.949\n",
            "Train\tCrit Loss: 0.769 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.723 | Token Loss: 0.177\n",
            "Epoch: 66 | Time: 1m 21s\n",
            "\tTrain Loss: 0.715 | Train PPL:   2.044\n",
            "\t Val. Loss: 0.669 |  Val. PPL:   1.951\n",
            "Train\tCrit Loss: 0.766 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.698 | Token Loss: 0.171\n",
            "Epoch: 67 | Time: 1m 21s\n",
            "\tTrain Loss: 0.712 | Train PPL:   2.038\n",
            "\t Val. Loss: 0.646 |  Val. PPL:   1.907\n",
            "Train\tCrit Loss: 0.763 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.734 | Token Loss: 0.178\n",
            "Epoch: 68 | Time: 1m 21s\n",
            "\tTrain Loss: 0.709 | Train PPL:   2.032\n",
            "\t Val. Loss: 0.678 |  Val. PPL:   1.970\n",
            "Train\tCrit Loss: 0.763 | Token Loss: 0.227\n",
            "Val\tCrit Loss: 0.718 | Token Loss: 0.176\n",
            "Epoch: 69 | Time: 1m 21s\n",
            "\tTrain Loss: 0.710 | Train PPL:   2.033\n",
            "\t Val. Loss: 0.663 |  Val. PPL:   1.942\n",
            "Train\tCrit Loss: 0.765 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.715 | Token Loss: 0.174\n",
            "Epoch: 70 | Time: 1m 21s\n",
            "\tTrain Loss: 0.711 | Train PPL:   2.037\n",
            "\t Val. Loss: 0.661 |  Val. PPL:   1.936\n",
            "Train\tCrit Loss: 0.765 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.723 | Token Loss: 0.175\n",
            "Epoch: 71 | Time: 1m 21s\n",
            "\tTrain Loss: 0.711 | Train PPL:   2.036\n",
            "\t Val. Loss: 0.669 |  Val. PPL:   1.952\n",
            "Train\tCrit Loss: 0.761 | Token Loss: 0.223\n",
            "Val\tCrit Loss: 0.726 | Token Loss: 0.176\n",
            "Epoch: 72 | Time: 1m 21s\n",
            "\tTrain Loss: 0.707 | Train PPL:   2.028\n",
            "\t Val. Loss: 0.671 |  Val. PPL:   1.956\n",
            "Train\tCrit Loss: 0.769 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.720 | Token Loss: 0.177\n",
            "Epoch: 73 | Time: 1m 21s\n",
            "\tTrain Loss: 0.715 | Train PPL:   2.044\n",
            "\t Val. Loss: 0.666 |  Val. PPL:   1.946\n",
            "Train\tCrit Loss: 0.767 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.698 | Token Loss: 0.170\n",
            "Epoch: 74 | Time: 1m 21s\n",
            "\tTrain Loss: 0.713 | Train PPL:   2.040\n",
            "\t Val. Loss: 0.645 |  Val. PPL:   1.906\n",
            "Train\tCrit Loss: 0.761 | Token Loss: 0.221\n",
            "Val\tCrit Loss: 0.716 | Token Loss: 0.176\n",
            "Epoch: 75 | Time: 1m 21s\n",
            "\tTrain Loss: 0.707 | Train PPL:   2.028\n",
            "\t Val. Loss: 0.662 |  Val. PPL:   1.939\n",
            "Train\tCrit Loss: 0.758 | Token Loss: 0.221\n",
            "Val\tCrit Loss: 0.702 | Token Loss: 0.174\n",
            "Epoch: 76 | Time: 1m 21s\n",
            "\tTrain Loss: 0.704 | Train PPL:   2.023\n",
            "\t Val. Loss: 0.649 |  Val. PPL:   1.914\n",
            "Train\tCrit Loss: 0.766 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.722 | Token Loss: 0.178\n",
            "Epoch: 77 | Time: 1m 21s\n",
            "\tTrain Loss: 0.712 | Train PPL:   2.038\n",
            "\t Val. Loss: 0.668 |  Val. PPL:   1.950\n",
            "Train\tCrit Loss: 0.766 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.716 | Token Loss: 0.176\n",
            "Epoch: 78 | Time: 1m 21s\n",
            "\tTrain Loss: 0.712 | Train PPL:   2.038\n",
            "\t Val. Loss: 0.662 |  Val. PPL:   1.939\n",
            "Train\tCrit Loss: 0.770 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.711 | Token Loss: 0.173\n",
            "Epoch: 79 | Time: 1m 21s\n",
            "\tTrain Loss: 0.715 | Train PPL:   2.045\n",
            "\t Val. Loss: 0.657 |  Val. PPL:   1.929\n",
            "Train\tCrit Loss: 0.761 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.718 | Token Loss: 0.175\n",
            "Epoch: 80 | Time: 1m 21s\n",
            "\tTrain Loss: 0.708 | Train PPL:   2.029\n",
            "\t Val. Loss: 0.664 |  Val. PPL:   1.942\n",
            "Train\tCrit Loss: 0.767 | Token Loss: 0.223\n",
            "Val\tCrit Loss: 0.721 | Token Loss: 0.176\n",
            "Epoch: 81 | Time: 1m 21s\n",
            "\tTrain Loss: 0.713 | Train PPL:   2.040\n",
            "\t Val. Loss: 0.667 |  Val. PPL:   1.948\n",
            "Train\tCrit Loss: 0.761 | Token Loss: 0.223\n",
            "Val\tCrit Loss: 0.716 | Token Loss: 0.175\n",
            "Epoch: 82 | Time: 1m 21s\n",
            "\tTrain Loss: 0.708 | Train PPL:   2.029\n",
            "\t Val. Loss: 0.662 |  Val. PPL:   1.938\n",
            "Train\tCrit Loss: 0.771 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.697 | Token Loss: 0.172\n",
            "Epoch: 83 | Time: 1m 21s\n",
            "\tTrain Loss: 0.716 | Train PPL:   2.046\n",
            "\t Val. Loss: 0.644 |  Val. PPL:   1.904\n",
            "Train\tCrit Loss: 0.761 | Token Loss: 0.223\n",
            "Val\tCrit Loss: 0.709 | Token Loss: 0.174\n",
            "Epoch: 84 | Time: 1m 21s\n",
            "\tTrain Loss: 0.708 | Train PPL:   2.029\n",
            "\t Val. Loss: 0.656 |  Val. PPL:   1.927\n",
            "Train\tCrit Loss: 0.767 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.709 | Token Loss: 0.173\n",
            "Epoch: 85 | Time: 1m 21s\n",
            "\tTrain Loss: 0.713 | Train PPL:   2.040\n",
            "\t Val. Loss: 0.655 |  Val. PPL:   1.926\n",
            "Train\tCrit Loss: 0.767 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.699 | Token Loss: 0.171\n",
            "Epoch: 86 | Time: 1m 21s\n",
            "\tTrain Loss: 0.713 | Train PPL:   2.041\n",
            "\t Val. Loss: 0.646 |  Val. PPL:   1.908\n",
            "Train\tCrit Loss: 0.760 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.721 | Token Loss: 0.173\n",
            "Epoch: 87 | Time: 1m 21s\n",
            "\tTrain Loss: 0.706 | Train PPL:   2.026\n",
            "\t Val. Loss: 0.667 |  Val. PPL:   1.947\n",
            "Train\tCrit Loss: 0.765 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.720 | Token Loss: 0.175\n",
            "Epoch: 88 | Time: 1m 21s\n",
            "\tTrain Loss: 0.711 | Train PPL:   2.036\n",
            "\t Val. Loss: 0.666 |  Val. PPL:   1.946\n",
            "Train\tCrit Loss: 0.770 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.732 | Token Loss: 0.178\n",
            "Epoch: 89 | Time: 1m 21s\n",
            "\tTrain Loss: 0.716 | Train PPL:   2.045\n",
            "\t Val. Loss: 0.677 |  Val. PPL:   1.968\n",
            "Train\tCrit Loss: 0.771 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.709 | Token Loss: 0.175\n",
            "Epoch: 90 | Time: 1m 21s\n",
            "\tTrain Loss: 0.716 | Train PPL:   2.047\n",
            "\t Val. Loss: 0.656 |  Val. PPL:   1.926\n",
            "Train\tCrit Loss: 0.764 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.718 | Token Loss: 0.176\n",
            "Epoch: 91 | Time: 1m 21s\n",
            "\tTrain Loss: 0.710 | Train PPL:   2.033\n",
            "\t Val. Loss: 0.664 |  Val. PPL:   1.942\n",
            "Train\tCrit Loss: 0.768 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.704 | Token Loss: 0.172\n",
            "Epoch: 92 | Time: 1m 21s\n",
            "\tTrain Loss: 0.714 | Train PPL:   2.041\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.918\n",
            "Train\tCrit Loss: 0.770 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.719 | Token Loss: 0.176\n",
            "Epoch: 93 | Time: 1m 21s\n",
            "\tTrain Loss: 0.716 | Train PPL:   2.046\n",
            "\t Val. Loss: 0.664 |  Val. PPL:   1.943\n",
            "Train\tCrit Loss: 0.767 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.704 | Token Loss: 0.173\n",
            "Epoch: 94 | Time: 1m 21s\n",
            "\tTrain Loss: 0.713 | Train PPL:   2.040\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.917\n",
            "Train\tCrit Loss: 0.767 | Token Loss: 0.226\n",
            "Val\tCrit Loss: 0.705 | Token Loss: 0.173\n",
            "Epoch: 95 | Time: 1m 21s\n",
            "\tTrain Loss: 0.713 | Train PPL:   2.039\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.918\n",
            "Train\tCrit Loss: 0.771 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.703 | Token Loss: 0.172\n",
            "Epoch: 96 | Time: 1m 21s\n",
            "\tTrain Loss: 0.717 | Train PPL:   2.047\n",
            "\t Val. Loss: 0.650 |  Val. PPL:   1.916\n",
            "Train\tCrit Loss: 0.763 | Token Loss: 0.225\n",
            "Val\tCrit Loss: 0.732 | Token Loss: 0.177\n",
            "Epoch: 97 | Time: 1m 21s\n",
            "\tTrain Loss: 0.710 | Train PPL:   2.033\n",
            "\t Val. Loss: 0.677 |  Val. PPL:   1.967\n",
            "Train\tCrit Loss: 0.758 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.719 | Token Loss: 0.176\n",
            "Epoch: 98 | Time: 1m 21s\n",
            "\tTrain Loss: 0.705 | Train PPL:   2.024\n",
            "\t Val. Loss: 0.664 |  Val. PPL:   1.943\n",
            "Train\tCrit Loss: 0.761 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.728 | Token Loss: 0.177\n",
            "Epoch: 99 | Time: 1m 21s\n",
            "\tTrain Loss: 0.707 | Train PPL:   2.029\n",
            "\t Val. Loss: 0.673 |  Val. PPL:   1.961\n",
            "Train\tCrit Loss: 0.764 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.723 | Token Loss: 0.177\n",
            "Epoch: 100 | Time: 1m 21s\n",
            "\tTrain Loss: 0.710 | Train PPL:   2.034\n",
            "\t Val. Loss: 0.668 |  Val. PPL:   1.951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x0qbQ0a_B9D"
      },
      "source": [
        "!cp /content/end_capstone_self_encode_sizeCor_stage2_256_wrn6.pt /content/drive/MyDrive/EVA4/END_Capstone/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzcGjSHHyBPk"
      },
      "source": [
        "!cp /content/end_capstone_self_encode_sizeCor_stage2_256_wrn7.pt /content/drive/MyDrive/EVA4/END_Capstone/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COeuVi0edn7c"
      },
      "source": [
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_self_encode_sizeCor_stage2_256_wrn7.pt ."
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqsSdlhKdzr1",
        "outputId": "c16f8956-e7b7-4967-d946-0c71c53a7721"
      },
      "source": [
        "#!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_self_encode_sizeCor_stage2_256_wrn5.pt .\r\n",
        "file_path='/content/end_capstone_self_encode_sizeCor_stage2_256_wrn7.pt'\r\n",
        "\r\n",
        "chkpt = torch.load(file_path)\r\n",
        "print(chkpt['loss'])\r\n",
        "model.load_state_dict(chkpt['model'])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6274991343325039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2husK3UdJv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d96c8a-57ac-400c-834d-e74fd3a1067c"
      },
      "source": [
        "LEARNING_RATE = 0.00001\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\r\n",
        "file_path='/content/end_capstone_self_encode_sizeCor_stage2_256_wrn7.pt'\r\n",
        "\r\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-9,verbose=True)\r\n",
        "run_train_eval_loop(model,\r\n",
        "                    train_dataloader,\r\n",
        "                    val_dataloader,\r\n",
        "                    optimizer,\r\n",
        "                    criterion,\r\n",
        "                    device,\r\n",
        "                    epochs=100,\r\n",
        "                    clip=1.4,\r\n",
        "                    best_valid_loss=float('inf'),\r\n",
        "                    file_path=file_path,\r\n",
        "                    double_loss=True,\r\n",
        "                    scheduler=scheduler,\r\n",
        "                    mix_ratio=0.95) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\tCrit Loss: 0.534 | Token Loss: 0.148\n",
            "Val\tCrit Loss: 0.691 | Token Loss: 0.151\n",
            "Epoch: 01 | Time: 1m 21s\n",
            "\tTrain Loss: 0.515 | Train PPL:   1.673\n",
            "\t Val. Loss: 0.664 |  Val. PPL:   1.943\n",
            "Train\tCrit Loss: 0.529 | Token Loss: 0.148\n",
            "Val\tCrit Loss: 0.673 | Token Loss: 0.149\n",
            "Epoch: 02 | Time: 1m 21s\n",
            "\tTrain Loss: 0.510 | Train PPL:   1.665\n",
            "\t Val. Loss: 0.647 |  Val. PPL:   1.910\n",
            "Train\tCrit Loss: 0.522 | Token Loss: 0.148\n",
            "Val\tCrit Loss: 0.689 | Token Loss: 0.153\n",
            "Epoch: 03 | Time: 1m 21s\n",
            "\tTrain Loss: 0.503 | Train PPL:   1.654\n",
            "\t Val. Loss: 0.662 |  Val. PPL:   1.939\n",
            "Train\tCrit Loss: 0.515 | Token Loss: 0.148\n",
            "Val\tCrit Loss: 0.684 | Token Loss: 0.153\n",
            "Epoch: 04 | Time: 1m 21s\n",
            "\tTrain Loss: 0.497 | Train PPL:   1.644\n",
            "\t Val. Loss: 0.658 |  Val. PPL:   1.930\n",
            "Train\tCrit Loss: 0.514 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.673 | Token Loss: 0.150\n",
            "Epoch: 05 | Time: 1m 21s\n",
            "\tTrain Loss: 0.496 | Train PPL:   1.641\n",
            "\t Val. Loss: 0.647 |  Val. PPL:   1.909\n",
            "Train\tCrit Loss: 0.511 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.675 | Token Loss: 0.150\n",
            "Epoch: 06 | Time: 1m 21s\n",
            "\tTrain Loss: 0.493 | Train PPL:   1.637\n",
            "\t Val. Loss: 0.649 |  Val. PPL:   1.914\n",
            "Train\tCrit Loss: 0.503 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.674 | Token Loss: 0.151\n",
            "Epoch: 07 | Time: 1m 21s\n",
            "\tTrain Loss: 0.485 | Train PPL:   1.624\n",
            "\t Val. Loss: 0.648 |  Val. PPL:   1.911\n",
            "Train\tCrit Loss: 0.502 | Token Loss: 0.148\n",
            "Val\tCrit Loss: 0.670 | Token Loss: 0.149\n",
            "Epoch: 08 | Time: 1m 21s\n",
            "\tTrain Loss: 0.485 | Train PPL:   1.624\n",
            "\t Val. Loss: 0.644 |  Val. PPL:   1.904\n",
            "Train\tCrit Loss: 0.496 | Token Loss: 0.148\n",
            "Val\tCrit Loss: 0.672 | Token Loss: 0.150\n",
            "Epoch: 09 | Time: 1m 21s\n",
            "\tTrain Loss: 0.478 | Train PPL:   1.613\n",
            "\t Val. Loss: 0.646 |  Val. PPL:   1.908\n",
            "Train\tCrit Loss: 0.493 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.684 | Token Loss: 0.153\n",
            "Epoch: 10 | Time: 1m 21s\n",
            "\tTrain Loss: 0.476 | Train PPL:   1.609\n",
            "\t Val. Loss: 0.658 |  Val. PPL:   1.930\n",
            "Train\tCrit Loss: 0.488 | Token Loss: 0.148\n",
            "Val\tCrit Loss: 0.669 | Token Loss: 0.151\n",
            "Epoch: 11 | Time: 1m 21s\n",
            "\tTrain Loss: 0.471 | Train PPL:   1.601\n",
            "\t Val. Loss: 0.643 |  Val. PPL:   1.902\n",
            "Train\tCrit Loss: 0.487 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.666 | Token Loss: 0.150\n",
            "Epoch: 12 | Time: 1m 21s\n",
            "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
            "\t Val. Loss: 0.640 |  Val. PPL:   1.897\n",
            "Train\tCrit Loss: 0.489 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.674 | Token Loss: 0.151\n",
            "Epoch: 13 | Time: 1m 21s\n",
            "\tTrain Loss: 0.472 | Train PPL:   1.603\n",
            "\t Val. Loss: 0.648 |  Val. PPL:   1.911\n",
            "Train\tCrit Loss: 0.476 | Token Loss: 0.148\n",
            "Val\tCrit Loss: 0.687 | Token Loss: 0.154\n",
            "Epoch: 14 | Time: 1m 21s\n",
            "\tTrain Loss: 0.459 | Train PPL:   1.583\n",
            "\t Val. Loss: 0.661 |  Val. PPL:   1.936\n",
            "Train\tCrit Loss: 0.471 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.666 | Token Loss: 0.151\n",
            "Epoch: 15 | Time: 1m 21s\n",
            "\tTrain Loss: 0.455 | Train PPL:   1.577\n",
            "\t Val. Loss: 0.641 |  Val. PPL:   1.898\n",
            "Train\tCrit Loss: 0.475 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.653 | Token Loss: 0.147\n",
            "Epoch: 16 | Time: 1m 21s\n",
            "\tTrain Loss: 0.459 | Train PPL:   1.582\n",
            "\t Val. Loss: 0.628 |  Val. PPL:   1.874\n",
            "Train\tCrit Loss: 0.466 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.677 | Token Loss: 0.153\n",
            "Epoch: 17 | Time: 1m 21s\n",
            "\tTrain Loss: 0.450 | Train PPL:   1.569\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.918\n",
            "Train\tCrit Loss: 0.468 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.668 | Token Loss: 0.152\n",
            "Epoch: 18 | Time: 1m 21s\n",
            "\tTrain Loss: 0.452 | Train PPL:   1.571\n",
            "\t Val. Loss: 0.642 |  Val. PPL:   1.900\n",
            "Train\tCrit Loss: 0.463 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.686 | Token Loss: 0.155\n",
            "Epoch: 19 | Time: 1m 21s\n",
            "\tTrain Loss: 0.447 | Train PPL:   1.564\n",
            "\t Val. Loss: 0.659 |  Val. PPL:   1.933\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.664 | Token Loss: 0.150\n",
            "Epoch: 20 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
            "\t Val. Loss: 0.638 |  Val. PPL:   1.893\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.679 | Token Loss: 0.153\n",
            "Epoch: 21 | Time: 1m 21s\n",
            "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
            "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
            "Train\tCrit Loss: 0.456 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.678 | Token Loss: 0.153\n",
            "Epoch    22: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch: 22 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.918\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.656 | Token Loss: 0.149\n",
            "Epoch: 23 | Time: 1m 21s\n",
            "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
            "\t Val. Loss: 0.631 |  Val. PPL:   1.879\n",
            "Train\tCrit Loss: 0.460 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.678 | Token Loss: 0.154\n",
            "Epoch: 24 | Time: 1m 21s\n",
            "\tTrain Loss: 0.444 | Train PPL:   1.559\n",
            "\t Val. Loss: 0.652 |  Val. PPL:   1.919\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.670 | Token Loss: 0.152\n",
            "Epoch: 25 | Time: 1m 21s\n",
            "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
            "\t Val. Loss: 0.644 |  Val. PPL:   1.904\n",
            "Train\tCrit Loss: 0.460 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.696 | Token Loss: 0.158\n",
            "Epoch: 26 | Time: 1m 21s\n",
            "\tTrain Loss: 0.445 | Train PPL:   1.560\n",
            "\t Val. Loss: 0.670 |  Val. PPL:   1.953\n",
            "Train\tCrit Loss: 0.453 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.665 | Token Loss: 0.150\n",
            "Epoch: 27 | Time: 1m 21s\n",
            "\tTrain Loss: 0.438 | Train PPL:   1.550\n",
            "\t Val. Loss: 0.639 |  Val. PPL:   1.894\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.681 | Token Loss: 0.152\n",
            "Epoch    28: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch: 28 | Time: 1m 21s\n",
            "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
            "\t Val. Loss: 0.654 |  Val. PPL:   1.924\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.662 | Token Loss: 0.150\n",
            "Epoch: 29 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
            "\t Val. Loss: 0.637 |  Val. PPL:   1.890\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.664 | Token Loss: 0.152\n",
            "Epoch: 30 | Time: 1m 21s\n",
            "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
            "\t Val. Loss: 0.638 |  Val. PPL:   1.893\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.678 | Token Loss: 0.154\n",
            "Epoch: 31 | Time: 1m 21s\n",
            "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
            "\t Val. Loss: 0.652 |  Val. PPL:   1.919\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.667 | Token Loss: 0.151\n",
            "Epoch: 32 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.552\n",
            "\t Val. Loss: 0.641 |  Val. PPL:   1.899\n",
            "Train\tCrit Loss: 0.460 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.674 | Token Loss: 0.154\n",
            "Epoch: 33 | Time: 1m 21s\n",
            "\tTrain Loss: 0.445 | Train PPL:   1.560\n",
            "\t Val. Loss: 0.648 |  Val. PPL:   1.911\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.681 | Token Loss: 0.155\n",
            "Epoch    34: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Epoch: 34 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
            "\t Val. Loss: 0.655 |  Val. PPL:   1.925\n",
            "Train\tCrit Loss: 0.459 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.668 | Token Loss: 0.152\n",
            "Epoch: 35 | Time: 1m 21s\n",
            "\tTrain Loss: 0.444 | Train PPL:   1.558\n",
            "\t Val. Loss: 0.642 |  Val. PPL:   1.900\n",
            "Train\tCrit Loss: 0.454 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.674 | Token Loss: 0.153\n",
            "Epoch: 36 | Time: 1m 21s\n",
            "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
            "\t Val. Loss: 0.647 |  Val. PPL:   1.911\n",
            "Train\tCrit Loss: 0.454 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.682 | Token Loss: 0.154\n",
            "Epoch: 37 | Time: 1m 21s\n",
            "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
            "\t Val. Loss: 0.656 |  Val. PPL:   1.926\n",
            "Train\tCrit Loss: 0.462 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.660 | Token Loss: 0.149\n",
            "Epoch: 38 | Time: 1m 21s\n",
            "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
            "\t Val. Loss: 0.635 |  Val. PPL:   1.887\n",
            "Train\tCrit Loss: 0.451 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.681 | Token Loss: 0.153\n",
            "Epoch: 39 | Time: 1m 21s\n",
            "\tTrain Loss: 0.436 | Train PPL:   1.546\n",
            "\t Val. Loss: 0.654 |  Val. PPL:   1.924\n",
            "Train\tCrit Loss: 0.456 | Token Loss: 0.154\n",
            "Val\tCrit Loss: 0.676 | Token Loss: 0.153\n",
            "Epoch: 40 | Time: 1m 21s\n",
            "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
            "\t Val. Loss: 0.649 |  Val. PPL:   1.914\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.688 | Token Loss: 0.155\n",
            "Epoch: 41 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.555\n",
            "\t Val. Loss: 0.661 |  Val. PPL:   1.937\n",
            "Train\tCrit Loss: 0.456 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.667 | Token Loss: 0.151\n",
            "Epoch: 42 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
            "\t Val. Loss: 0.641 |  Val. PPL:   1.899\n",
            "Train\tCrit Loss: 0.454 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.677 | Token Loss: 0.153\n",
            "Epoch: 43 | Time: 1m 21s\n",
            "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.917\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.664 | Token Loss: 0.152\n",
            "Epoch: 44 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
            "\t Val. Loss: 0.639 |  Val. PPL:   1.894\n",
            "Train\tCrit Loss: 0.454 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.685 | Token Loss: 0.155\n",
            "Epoch: 45 | Time: 1m 21s\n",
            "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
            "\t Val. Loss: 0.658 |  Val. PPL:   1.931\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.687 | Token Loss: 0.155\n",
            "Epoch: 46 | Time: 1m 21s\n",
            "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
            "\t Val. Loss: 0.660 |  Val. PPL:   1.935\n",
            "Train\tCrit Loss: 0.451 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.661 | Token Loss: 0.149\n",
            "Epoch: 47 | Time: 1m 21s\n",
            "\tTrain Loss: 0.436 | Train PPL:   1.546\n",
            "\t Val. Loss: 0.635 |  Val. PPL:   1.887\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.678 | Token Loss: 0.153\n",
            "Epoch: 48 | Time: 1m 21s\n",
            "\tTrain Loss: 0.439 | Train PPL:   1.552\n",
            "\t Val. Loss: 0.652 |  Val. PPL:   1.919\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.671 | Token Loss: 0.153\n",
            "Epoch: 49 | Time: 1m 21s\n",
            "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
            "\t Val. Loss: 0.645 |  Val. PPL:   1.905\n",
            "Train\tCrit Loss: 0.459 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.671 | Token Loss: 0.153\n",
            "Epoch: 50 | Time: 1m 21s\n",
            "\tTrain Loss: 0.444 | Train PPL:   1.559\n",
            "\t Val. Loss: 0.646 |  Val. PPL:   1.907\n",
            "Train\tCrit Loss: 0.450 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.671 | Token Loss: 0.152\n",
            "Epoch: 51 | Time: 1m 21s\n",
            "\tTrain Loss: 0.435 | Train PPL:   1.545\n",
            "\t Val. Loss: 0.645 |  Val. PPL:   1.906\n",
            "Train\tCrit Loss: 0.453 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.682 | Token Loss: 0.153\n",
            "Epoch: 52 | Time: 1m 21s\n",
            "\tTrain Loss: 0.438 | Train PPL:   1.549\n",
            "\t Val. Loss: 0.655 |  Val. PPL:   1.926\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.679 | Token Loss: 0.152\n",
            "Epoch: 53 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
            "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
            "Train\tCrit Loss: 0.459 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.672 | Token Loss: 0.153\n",
            "Epoch: 54 | Time: 1m 21s\n",
            "\tTrain Loss: 0.444 | Train PPL:   1.558\n",
            "\t Val. Loss: 0.646 |  Val. PPL:   1.909\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.687 | Token Loss: 0.155\n",
            "Epoch: 55 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
            "\t Val. Loss: 0.661 |  Val. PPL:   1.936\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.674 | Token Loss: 0.152\n",
            "Epoch: 56 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.552\n",
            "\t Val. Loss: 0.648 |  Val. PPL:   1.911\n",
            "Train\tCrit Loss: 0.456 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.656 | Token Loss: 0.149\n",
            "Epoch: 57 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
            "\t Val. Loss: 0.630 |  Val. PPL:   1.878\n",
            "Train\tCrit Loss: 0.460 | Token Loss: 0.153\n",
            "Val\tCrit Loss: 0.675 | Token Loss: 0.153\n",
            "Epoch: 58 | Time: 1m 21s\n",
            "\tTrain Loss: 0.445 | Train PPL:   1.560\n",
            "\t Val. Loss: 0.649 |  Val. PPL:   1.914\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.677 | Token Loss: 0.153\n",
            "Epoch: 59 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.918\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.677 | Token Loss: 0.153\n",
            "Epoch: 60 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.917\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.662 | Token Loss: 0.151\n",
            "Epoch: 61 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.557\n",
            "\t Val. Loss: 0.636 |  Val. PPL:   1.890\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.665 | Token Loss: 0.151\n",
            "Epoch: 62 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
            "\t Val. Loss: 0.639 |  Val. PPL:   1.895\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.679 | Token Loss: 0.152\n",
            "Epoch: 63 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
            "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
            "Train\tCrit Loss: 0.454 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.674 | Token Loss: 0.154\n",
            "Epoch: 64 | Time: 1m 21s\n",
            "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
            "\t Val. Loss: 0.648 |  Val. PPL:   1.912\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.684 | Token Loss: 0.155\n",
            "Epoch: 65 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
            "\t Val. Loss: 0.657 |  Val. PPL:   1.930\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.684 | Token Loss: 0.156\n",
            "Epoch: 66 | Time: 1m 21s\n",
            "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
            "\t Val. Loss: 0.658 |  Val. PPL:   1.930\n",
            "Train\tCrit Loss: 0.456 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.653 | Token Loss: 0.148\n",
            "Epoch: 67 | Time: 1m 21s\n",
            "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
            "\t Val. Loss: 0.627 |  Val. PPL:   1.873\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.691 | Token Loss: 0.155\n",
            "Epoch: 68 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.552\n",
            "\t Val. Loss: 0.664 |  Val. PPL:   1.943\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.153\n",
            "Val\tCrit Loss: 0.678 | Token Loss: 0.154\n",
            "Epoch: 69 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.555\n",
            "\t Val. Loss: 0.652 |  Val. PPL:   1.919\n",
            "Train\tCrit Loss: 0.456 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.677 | Token Loss: 0.152\n",
            "Epoch: 70 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.918\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.685 | Token Loss: 0.154\n",
            "Epoch: 71 | Time: 1m 21s\n",
            "\tTrain Loss: 0.439 | Train PPL:   1.552\n",
            "\t Val. Loss: 0.658 |  Val. PPL:   1.931\n",
            "Train\tCrit Loss: 0.454 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.682 | Token Loss: 0.154\n",
            "Epoch: 72 | Time: 1m 21s\n",
            "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
            "\t Val. Loss: 0.656 |  Val. PPL:   1.926\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.678 | Token Loss: 0.155\n",
            "Epoch: 73 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.555\n",
            "\t Val. Loss: 0.652 |  Val. PPL:   1.918\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.660 | Token Loss: 0.148\n",
            "Epoch: 74 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
            "\t Val. Loss: 0.635 |  Val. PPL:   1.886\n",
            "Train\tCrit Loss: 0.454 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.677 | Token Loss: 0.153\n",
            "Epoch: 75 | Time: 1m 21s\n",
            "\tTrain Loss: 0.438 | Train PPL:   1.550\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.917\n",
            "Train\tCrit Loss: 0.453 | Token Loss: 0.149\n",
            "Val\tCrit Loss: 0.659 | Token Loss: 0.151\n",
            "Epoch: 76 | Time: 1m 21s\n",
            "\tTrain Loss: 0.437 | Train PPL:   1.549\n",
            "\t Val. Loss: 0.634 |  Val. PPL:   1.885\n",
            "Train\tCrit Loss: 0.456 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.673 | Token Loss: 0.155\n",
            "Epoch: 77 | Time: 1m 21s\n",
            "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
            "\t Val. Loss: 0.647 |  Val. PPL:   1.909\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.676 | Token Loss: 0.154\n",
            "Epoch: 78 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.552\n",
            "\t Val. Loss: 0.650 |  Val. PPL:   1.916\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.676 | Token Loss: 0.153\n",
            "Epoch: 79 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
            "\t Val. Loss: 0.649 |  Val. PPL:   1.914\n",
            "Train\tCrit Loss: 0.454 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.680 | Token Loss: 0.154\n",
            "Epoch: 80 | Time: 1m 21s\n",
            "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
            "\t Val. Loss: 0.654 |  Val. PPL:   1.923\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.684 | Token Loss: 0.155\n",
            "Epoch: 81 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
            "\t Val. Loss: 0.657 |  Val. PPL:   1.930\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.677 | Token Loss: 0.153\n",
            "Epoch: 82 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
            "\t Val. Loss: 0.650 |  Val. PPL:   1.916\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.659 | Token Loss: 0.150\n",
            "Epoch: 83 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
            "\t Val. Loss: 0.634 |  Val. PPL:   1.884\n",
            "Train\tCrit Loss: 0.454 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.668 | Token Loss: 0.151\n",
            "Epoch: 84 | Time: 1m 21s\n",
            "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
            "\t Val. Loss: 0.642 |  Val. PPL:   1.900\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.672 | Token Loss: 0.152\n",
            "Epoch: 85 | Time: 1m 21s\n",
            "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
            "\t Val. Loss: 0.646 |  Val. PPL:   1.909\n",
            "Train\tCrit Loss: 0.456 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.657 | Token Loss: 0.149\n",
            "Epoch: 86 | Time: 1m 21s\n",
            "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
            "\t Val. Loss: 0.632 |  Val. PPL:   1.881\n",
            "Train\tCrit Loss: 0.452 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.680 | Token Loss: 0.151\n",
            "Epoch: 87 | Time: 1m 21s\n",
            "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
            "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.679 | Token Loss: 0.152\n",
            "Epoch: 88 | Time: 1m 21s\n",
            "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
            "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
            "Train\tCrit Loss: 0.459 | Token Loss: 0.153\n",
            "Val\tCrit Loss: 0.689 | Token Loss: 0.155\n",
            "Epoch: 89 | Time: 1m 21s\n",
            "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
            "\t Val. Loss: 0.662 |  Val. PPL:   1.939\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.669 | Token Loss: 0.153\n",
            "Epoch: 90 | Time: 1m 21s\n",
            "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
            "\t Val. Loss: 0.644 |  Val. PPL:   1.903\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.678 | Token Loss: 0.153\n",
            "Epoch: 91 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
            "\t Val. Loss: 0.652 |  Val. PPL:   1.919\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.668 | Token Loss: 0.150\n",
            "Epoch: 92 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.555\n",
            "\t Val. Loss: 0.642 |  Val. PPL:   1.900\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.680 | Token Loss: 0.154\n",
            "Epoch: 93 | Time: 1m 21s\n",
            "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
            "\t Val. Loss: 0.653 |  Val. PPL:   1.922\n",
            "Train\tCrit Loss: 0.458 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.658 | Token Loss: 0.150\n",
            "Epoch: 94 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
            "\t Val. Loss: 0.633 |  Val. PPL:   1.883\n",
            "Train\tCrit Loss: 0.456 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.664 | Token Loss: 0.151\n",
            "Epoch: 95 | Time: 1m 21s\n",
            "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
            "\t Val. Loss: 0.639 |  Val. PPL:   1.894\n",
            "Train\tCrit Loss: 0.457 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.668 | Token Loss: 0.151\n",
            "Epoch: 96 | Time: 1m 21s\n",
            "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
            "\t Val. Loss: 0.642 |  Val. PPL:   1.901\n",
            "Train\tCrit Loss: 0.454 | Token Loss: 0.151\n",
            "Val\tCrit Loss: 0.690 | Token Loss: 0.155\n",
            "Epoch: 97 | Time: 1m 21s\n",
            "\tTrain Loss: 0.439 | Train PPL:   1.552\n",
            "\t Val. Loss: 0.664 |  Val. PPL:   1.942\n",
            "Train\tCrit Loss: 0.452 | Token Loss: 0.150\n",
            "Val\tCrit Loss: 0.683 | Token Loss: 0.154\n",
            "Epoch: 98 | Time: 1m 21s\n",
            "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
            "\t Val. Loss: 0.656 |  Val. PPL:   1.928\n",
            "Train\tCrit Loss: 0.455 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.687 | Token Loss: 0.155\n",
            "Epoch: 99 | Time: 1m 21s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
            "\t Val. Loss: 0.661 |  Val. PPL:   1.936\n",
            "Train\tCrit Loss: 0.456 | Token Loss: 0.152\n",
            "Val\tCrit Loss: 0.683 | Token Loss: 0.155\n",
            "Epoch: 100 | Time: 1m 21s\n",
            "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
            "\t Val. Loss: 0.656 |  Val. PPL:   1.927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K70eii8C1Fg-",
        "outputId": "1cd7cfce-4710-4bc0-a4bf-53cd859977fa"
      },
      "source": [
        "LEARNING_RATE = 0.00001\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\r\n",
        "file_path='/content/end_capstone_self_encode_sizeCor_stage2_256_wrn8.pt'\r\n",
        "\r\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-9,verbose=True)\r\n",
        "run_train_eval_loop(model,\r\n",
        "                    train_dataloader,\r\n",
        "                    val_dataloader,\r\n",
        "                    optimizer,\r\n",
        "                    criterion,\r\n",
        "                    device,\r\n",
        "                    epochs=100,\r\n",
        "                    clip=1.4,\r\n",
        "                    best_valid_loss=float('inf'),\r\n",
        "                    file_path=file_path,\r\n",
        "                    double_loss=True,\r\n",
        "                    scheduler=scheduler,\r\n",
        "                    mix_ratio=0.95) "
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\tCrit Loss: 3.168 | Token Loss: 1.146\n",
            "Val\tCrit Loss: 2.248 | Token Loss: 0.656\n",
            "Epoch: 01 | Time: 1m 21s\n",
            "\tTrain Loss: 3.067 | Train PPL:  21.473\n",
            "\t Val. Loss: 2.168 |  Val. PPL:   8.744\n",
            "Train\tCrit Loss: 2.344 | Token Loss: 0.790\n",
            "Val\tCrit Loss: 1.859 | Token Loss: 0.491\n",
            "Epoch: 02 | Time: 1m 23s\n",
            "\tTrain Loss: 2.266 | Train PPL:   9.645\n",
            "\t Val. Loss: 1.791 |  Val. PPL:   5.994\n",
            "Train\tCrit Loss: 2.027 | Token Loss: 0.649\n",
            "Val\tCrit Loss: 1.649 | Token Loss: 0.416\n",
            "Epoch: 03 | Time: 1m 23s\n",
            "\tTrain Loss: 1.958 | Train PPL:   7.084\n",
            "\t Val. Loss: 1.587 |  Val. PPL:   4.890\n",
            "Train\tCrit Loss: 1.817 | Token Loss: 0.570\n",
            "Val\tCrit Loss: 1.498 | Token Loss: 0.372\n",
            "Epoch: 04 | Time: 1m 23s\n",
            "\tTrain Loss: 1.755 | Train PPL:   5.784\n",
            "\t Val. Loss: 1.442 |  Val. PPL:   4.229\n",
            "Train\tCrit Loss: 1.677 | Token Loss: 0.520\n",
            "Val\tCrit Loss: 1.397 | Token Loss: 0.345\n",
            "Epoch: 05 | Time: 1m 23s\n",
            "\tTrain Loss: 1.619 | Train PPL:   5.048\n",
            "\t Val. Loss: 1.344 |  Val. PPL:   3.834\n",
            "Train\tCrit Loss: 1.563 | Token Loss: 0.483\n",
            "Val\tCrit Loss: 1.329 | Token Loss: 0.324\n",
            "Epoch: 06 | Time: 1m 23s\n",
            "\tTrain Loss: 1.509 | Train PPL:   4.523\n",
            "\t Val. Loss: 1.279 |  Val. PPL:   3.592\n",
            "Train\tCrit Loss: 1.472 | Token Loss: 0.454\n",
            "Val\tCrit Loss: 1.283 | Token Loss: 0.310\n",
            "Epoch: 07 | Time: 1m 23s\n",
            "\tTrain Loss: 1.421 | Train PPL:   4.142\n",
            "\t Val. Loss: 1.235 |  Val. PPL:   3.437\n",
            "Train\tCrit Loss: 1.407 | Token Loss: 0.432\n",
            "Val\tCrit Loss: 1.232 | Token Loss: 0.299\n",
            "Epoch: 08 | Time: 1m 23s\n",
            "\tTrain Loss: 1.358 | Train PPL:   3.889\n",
            "\t Val. Loss: 1.186 |  Val. PPL:   3.273\n",
            "Train\tCrit Loss: 1.345 | Token Loss: 0.412\n",
            "Val\tCrit Loss: 1.208 | Token Loss: 0.292\n",
            "Epoch: 09 | Time: 1m 23s\n",
            "\tTrain Loss: 1.298 | Train PPL:   3.662\n",
            "\t Val. Loss: 1.162 |  Val. PPL:   3.197\n",
            "Train\tCrit Loss: 1.292 | Token Loss: 0.398\n",
            "Val\tCrit Loss: 1.187 | Token Loss: 0.288\n",
            "Epoch: 10 | Time: 1m 23s\n",
            "\tTrain Loss: 1.248 | Train PPL:   3.482\n",
            "\t Val. Loss: 1.142 |  Val. PPL:   3.134\n",
            "Train\tCrit Loss: 1.239 | Token Loss: 0.382\n",
            "Val\tCrit Loss: 1.150 | Token Loss: 0.279\n",
            "Epoch: 11 | Time: 1m 23s\n",
            "\tTrain Loss: 1.196 | Train PPL:   3.307\n",
            "\t Val. Loss: 1.107 |  Val. PPL:   3.025\n",
            "Train\tCrit Loss: 1.208 | Token Loss: 0.375\n",
            "Val\tCrit Loss: 1.117 | Token Loss: 0.270\n",
            "Epoch: 12 | Time: 1m 23s\n",
            "\tTrain Loss: 1.166 | Train PPL:   3.209\n",
            "\t Val. Loss: 1.075 |  Val. PPL:   2.929\n",
            "Train\tCrit Loss: 1.173 | Token Loss: 0.364\n",
            "Val\tCrit Loss: 1.108 | Token Loss: 0.266\n",
            "Epoch: 13 | Time: 1m 23s\n",
            "\tTrain Loss: 1.132 | Train PPL:   3.102\n",
            "\t Val. Loss: 1.066 |  Val. PPL:   2.905\n",
            "Train\tCrit Loss: 1.136 | Token Loss: 0.353\n",
            "Val\tCrit Loss: 1.104 | Token Loss: 0.265\n",
            "Epoch: 14 | Time: 1m 23s\n",
            "\tTrain Loss: 1.097 | Train PPL:   2.995\n",
            "\t Val. Loss: 1.062 |  Val. PPL:   2.892\n",
            "Train\tCrit Loss: 1.103 | Token Loss: 0.345\n",
            "Val\tCrit Loss: 1.062 | Token Loss: 0.255\n",
            "Epoch: 15 | Time: 1m 23s\n",
            "\tTrain Loss: 1.065 | Train PPL:   2.902\n",
            "\t Val. Loss: 1.022 |  Val. PPL:   2.778\n",
            "Train\tCrit Loss: 1.077 | Token Loss: 0.339\n",
            "Val\tCrit Loss: 1.040 | Token Loss: 0.248\n",
            "Epoch: 16 | Time: 1m 23s\n",
            "\tTrain Loss: 1.040 | Train PPL:   2.828\n",
            "\t Val. Loss: 1.001 |  Val. PPL:   2.720\n",
            "Train\tCrit Loss: 1.051 | Token Loss: 0.331\n",
            "Val\tCrit Loss: 1.048 | Token Loss: 0.251\n",
            "Epoch: 17 | Time: 1m 23s\n",
            "\tTrain Loss: 1.015 | Train PPL:   2.758\n",
            "\t Val. Loss: 1.008 |  Val. PPL:   2.741\n",
            "Train\tCrit Loss: 1.032 | Token Loss: 0.326\n",
            "Val\tCrit Loss: 1.028 | Token Loss: 0.245\n",
            "Epoch: 18 | Time: 1m 23s\n",
            "\tTrain Loss: 0.997 | Train PPL:   2.709\n",
            "\t Val. Loss: 0.989 |  Val. PPL:   2.689\n",
            "Train\tCrit Loss: 1.006 | Token Loss: 0.317\n",
            "Val\tCrit Loss: 1.028 | Token Loss: 0.244\n",
            "Epoch: 19 | Time: 1m 23s\n",
            "\tTrain Loss: 0.971 | Train PPL:   2.641\n",
            "\t Val. Loss: 0.989 |  Val. PPL:   2.688\n",
            "Train\tCrit Loss: 0.982 | Token Loss: 0.313\n",
            "Val\tCrit Loss: 0.998 | Token Loss: 0.236\n",
            "Epoch: 20 | Time: 1m 23s\n",
            "\tTrain Loss: 0.949 | Train PPL:   2.583\n",
            "\t Val. Loss: 0.960 |  Val. PPL:   2.613\n",
            "Train\tCrit Loss: 0.965 | Token Loss: 0.309\n",
            "Val\tCrit Loss: 1.005 | Token Loss: 0.238\n",
            "Epoch: 21 | Time: 1m 23s\n",
            "\tTrain Loss: 0.933 | Train PPL:   2.541\n",
            "\t Val. Loss: 0.967 |  Val. PPL:   2.630\n",
            "Train\tCrit Loss: 0.953 | Token Loss: 0.304\n",
            "Val\tCrit Loss: 0.990 | Token Loss: 0.234\n",
            "Epoch: 22 | Time: 1m 23s\n",
            "\tTrain Loss: 0.921 | Train PPL:   2.512\n",
            "\t Val. Loss: 0.952 |  Val. PPL:   2.591\n",
            "Train\tCrit Loss: 0.940 | Token Loss: 0.302\n",
            "Val\tCrit Loss: 0.964 | Token Loss: 0.228\n",
            "Epoch: 23 | Time: 1m 23s\n",
            "\tTrain Loss: 0.908 | Train PPL:   2.480\n",
            "\t Val. Loss: 0.927 |  Val. PPL:   2.527\n",
            "Train\tCrit Loss: 0.923 | Token Loss: 0.296\n",
            "Val\tCrit Loss: 0.976 | Token Loss: 0.230\n",
            "Epoch: 24 | Time: 1m 23s\n",
            "\tTrain Loss: 0.892 | Train PPL:   2.440\n",
            "\t Val. Loss: 0.939 |  Val. PPL:   2.557\n",
            "Train\tCrit Loss: 0.912 | Token Loss: 0.293\n",
            "Val\tCrit Loss: 0.960 | Token Loss: 0.225\n",
            "Epoch: 25 | Time: 1m 23s\n",
            "\tTrain Loss: 0.881 | Train PPL:   2.413\n",
            "\t Val. Loss: 0.924 |  Val. PPL:   2.518\n",
            "Train\tCrit Loss: 0.897 | Token Loss: 0.290\n",
            "Val\tCrit Loss: 0.980 | Token Loss: 0.228\n",
            "Epoch: 26 | Time: 1m 23s\n",
            "\tTrain Loss: 0.867 | Train PPL:   2.380\n",
            "\t Val. Loss: 0.942 |  Val. PPL:   2.566\n",
            "Train\tCrit Loss: 0.879 | Token Loss: 0.286\n",
            "Val\tCrit Loss: 0.943 | Token Loss: 0.220\n",
            "Epoch: 27 | Time: 1m 23s\n",
            "\tTrain Loss: 0.849 | Train PPL:   2.338\n",
            "\t Val. Loss: 0.907 |  Val. PPL:   2.476\n",
            "Train\tCrit Loss: 0.868 | Token Loss: 0.284\n",
            "Val\tCrit Loss: 0.951 | Token Loss: 0.221\n",
            "Epoch: 28 | Time: 1m 23s\n",
            "\tTrain Loss: 0.839 | Train PPL:   2.314\n",
            "\t Val. Loss: 0.914 |  Val. PPL:   2.495\n",
            "Train\tCrit Loss: 0.855 | Token Loss: 0.279\n",
            "Val\tCrit Loss: 0.925 | Token Loss: 0.216\n",
            "Epoch: 29 | Time: 1m 23s\n",
            "\tTrain Loss: 0.826 | Train PPL:   2.285\n",
            "\t Val. Loss: 0.889 |  Val. PPL:   2.434\n",
            "Train\tCrit Loss: 0.838 | Token Loss: 0.275\n",
            "Val\tCrit Loss: 0.920 | Token Loss: 0.217\n",
            "Epoch: 30 | Time: 1m 23s\n",
            "\tTrain Loss: 0.810 | Train PPL:   2.247\n",
            "\t Val. Loss: 0.885 |  Val. PPL:   2.423\n",
            "Train\tCrit Loss: 0.825 | Token Loss: 0.271\n",
            "Val\tCrit Loss: 0.924 | Token Loss: 0.216\n",
            "Epoch: 31 | Time: 1m 23s\n",
            "\tTrain Loss: 0.797 | Train PPL:   2.220\n",
            "\t Val. Loss: 0.888 |  Val. PPL:   2.431\n",
            "Train\tCrit Loss: 0.813 | Token Loss: 0.269\n",
            "Val\tCrit Loss: 0.912 | Token Loss: 0.213\n",
            "Epoch: 32 | Time: 1m 23s\n",
            "\tTrain Loss: 0.786 | Train PPL:   2.195\n",
            "\t Val. Loss: 0.877 |  Val. PPL:   2.404\n",
            "Train\tCrit Loss: 0.808 | Token Loss: 0.269\n",
            "Val\tCrit Loss: 0.909 | Token Loss: 0.213\n",
            "Epoch: 33 | Time: 1m 23s\n",
            "\tTrain Loss: 0.781 | Train PPL:   2.183\n",
            "\t Val. Loss: 0.874 |  Val. PPL:   2.396\n",
            "Train\tCrit Loss: 0.790 | Token Loss: 0.262\n",
            "Val\tCrit Loss: 0.912 | Token Loss: 0.213\n",
            "Epoch: 34 | Time: 1m 23s\n",
            "\tTrain Loss: 0.764 | Train PPL:   2.147\n",
            "\t Val. Loss: 0.877 |  Val. PPL:   2.404\n",
            "Train\tCrit Loss: 0.783 | Token Loss: 0.261\n",
            "Val\tCrit Loss: 0.893 | Token Loss: 0.209\n",
            "Epoch: 35 | Time: 1m 23s\n",
            "\tTrain Loss: 0.757 | Train PPL:   2.132\n",
            "\t Val. Loss: 0.859 |  Val. PPL:   2.360\n",
            "Train\tCrit Loss: 0.767 | Token Loss: 0.257\n",
            "Val\tCrit Loss: 0.895 | Token Loss: 0.208\n",
            "Epoch: 36 | Time: 1m 23s\n",
            "\tTrain Loss: 0.742 | Train PPL:   2.100\n",
            "\t Val. Loss: 0.861 |  Val. PPL:   2.365\n",
            "Train\tCrit Loss: 0.756 | Token Loss: 0.256\n",
            "Val\tCrit Loss: 0.898 | Token Loss: 0.209\n",
            "Epoch: 37 | Time: 1m 23s\n",
            "\tTrain Loss: 0.731 | Train PPL:   2.077\n",
            "\t Val. Loss: 0.863 |  Val. PPL:   2.371\n",
            "Train\tCrit Loss: 0.753 | Token Loss: 0.254\n",
            "Val\tCrit Loss: 0.876 | Token Loss: 0.203\n",
            "Epoch: 38 | Time: 1m 23s\n",
            "\tTrain Loss: 0.728 | Train PPL:   2.072\n",
            "\t Val. Loss: 0.842 |  Val. PPL:   2.321\n",
            "Train\tCrit Loss: 0.734 | Token Loss: 0.250\n",
            "Val\tCrit Loss: 0.889 | Token Loss: 0.205\n",
            "Epoch: 39 | Time: 1m 23s\n",
            "\tTrain Loss: 0.710 | Train PPL:   2.034\n",
            "\t Val. Loss: 0.855 |  Val. PPL:   2.352\n",
            "Train\tCrit Loss: 0.730 | Token Loss: 0.250\n",
            "Val\tCrit Loss: 0.875 | Token Loss: 0.202\n",
            "Epoch: 40 | Time: 1m 23s\n",
            "\tTrain Loss: 0.706 | Train PPL:   2.025\n",
            "\t Val. Loss: 0.842 |  Val. PPL:   2.320\n",
            "Train\tCrit Loss: 0.719 | Token Loss: 0.247\n",
            "Val\tCrit Loss: 0.885 | Token Loss: 0.205\n",
            "Epoch: 41 | Time: 1m 23s\n",
            "\tTrain Loss: 0.696 | Train PPL:   2.005\n",
            "\t Val. Loss: 0.851 |  Val. PPL:   2.341\n",
            "Train\tCrit Loss: 0.711 | Token Loss: 0.244\n",
            "Val\tCrit Loss: 0.864 | Token Loss: 0.200\n",
            "Epoch: 42 | Time: 1m 23s\n",
            "\tTrain Loss: 0.688 | Train PPL:   1.990\n",
            "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
            "Train\tCrit Loss: 0.700 | Token Loss: 0.241\n",
            "Val\tCrit Loss: 0.865 | Token Loss: 0.200\n",
            "Epoch: 43 | Time: 1m 23s\n",
            "\tTrain Loss: 0.677 | Train PPL:   1.967\n",
            "\t Val. Loss: 0.832 |  Val. PPL:   2.297\n",
            "Train\tCrit Loss: 0.691 | Token Loss: 0.239\n",
            "Val\tCrit Loss: 0.852 | Token Loss: 0.200\n",
            "Epoch: 44 | Time: 1m 23s\n",
            "\tTrain Loss: 0.669 | Train PPL:   1.951\n",
            "\t Val. Loss: 0.819 |  Val. PPL:   2.269\n",
            "Train\tCrit Loss: 0.684 | Token Loss: 0.238\n",
            "Val\tCrit Loss: 0.868 | Token Loss: 0.202\n",
            "Epoch: 45 | Time: 1m 23s\n",
            "\tTrain Loss: 0.662 | Train PPL:   1.938\n",
            "\t Val. Loss: 0.834 |  Val. PPL:   2.303\n",
            "Train\tCrit Loss: 0.679 | Token Loss: 0.236\n",
            "Val\tCrit Loss: 0.863 | Token Loss: 0.200\n",
            "Epoch: 46 | Time: 1m 23s\n",
            "\tTrain Loss: 0.657 | Train PPL:   1.928\n",
            "\t Val. Loss: 0.830 |  Val. PPL:   2.293\n",
            "Train\tCrit Loss: 0.666 | Token Loss: 0.233\n",
            "Val\tCrit Loss: 0.839 | Token Loss: 0.195\n",
            "Epoch: 47 | Time: 1m 23s\n",
            "\tTrain Loss: 0.644 | Train PPL:   1.905\n",
            "\t Val. Loss: 0.807 |  Val. PPL:   2.241\n",
            "Train\tCrit Loss: 0.662 | Token Loss: 0.233\n",
            "Val\tCrit Loss: 0.851 | Token Loss: 0.197\n",
            "Epoch: 48 | Time: 1m 23s\n",
            "\tTrain Loss: 0.640 | Train PPL:   1.897\n",
            "\t Val. Loss: 0.818 |  Val. PPL:   2.267\n",
            "Train\tCrit Loss: 0.658 | Token Loss: 0.230\n",
            "Val\tCrit Loss: 0.839 | Token Loss: 0.196\n",
            "Epoch: 49 | Time: 1m 23s\n",
            "\tTrain Loss: 0.637 | Train PPL:   1.890\n",
            "\t Val. Loss: 0.807 |  Val. PPL:   2.242\n",
            "Train\tCrit Loss: 0.652 | Token Loss: 0.229\n",
            "Val\tCrit Loss: 0.837 | Token Loss: 0.193\n",
            "Epoch: 50 | Time: 1m 23s\n",
            "\tTrain Loss: 0.631 | Train PPL:   1.879\n",
            "\t Val. Loss: 0.805 |  Val. PPL:   2.236\n",
            "Train\tCrit Loss: 0.634 | Token Loss: 0.224\n",
            "Val\tCrit Loss: 0.834 | Token Loss: 0.193\n",
            "Epoch: 51 | Time: 1m 23s\n",
            "\tTrain Loss: 0.613 | Train PPL:   1.847\n",
            "\t Val. Loss: 0.802 |  Val. PPL:   2.231\n",
            "Train\tCrit Loss: 0.629 | Token Loss: 0.223\n",
            "Val\tCrit Loss: 0.837 | Token Loss: 0.193\n",
            "Epoch: 52 | Time: 1m 23s\n",
            "\tTrain Loss: 0.608 | Train PPL:   1.838\n",
            "\t Val. Loss: 0.805 |  Val. PPL:   2.236\n",
            "Train\tCrit Loss: 0.627 | Token Loss: 0.223\n",
            "Val\tCrit Loss: 0.840 | Token Loss: 0.192\n",
            "Epoch: 53 | Time: 1m 23s\n",
            "\tTrain Loss: 0.607 | Train PPL:   1.835\n",
            "\t Val. Loss: 0.808 |  Val. PPL:   2.243\n",
            "Train\tCrit Loss: 0.621 | Token Loss: 0.220\n",
            "Val\tCrit Loss: 0.826 | Token Loss: 0.192\n",
            "Epoch: 54 | Time: 1m 23s\n",
            "\tTrain Loss: 0.601 | Train PPL:   1.823\n",
            "\t Val. Loss: 0.794 |  Val. PPL:   2.213\n",
            "Train\tCrit Loss: 0.613 | Token Loss: 0.221\n",
            "Val\tCrit Loss: 0.836 | Token Loss: 0.192\n",
            "Epoch: 55 | Time: 1m 23s\n",
            "\tTrain Loss: 0.593 | Train PPL:   1.810\n",
            "\t Val. Loss: 0.804 |  Val. PPL:   2.234\n",
            "Train\tCrit Loss: 0.604 | Token Loss: 0.220\n",
            "Val\tCrit Loss: 0.824 | Token Loss: 0.190\n",
            "Epoch: 56 | Time: 1m 23s\n",
            "\tTrain Loss: 0.585 | Train PPL:   1.795\n",
            "\t Val. Loss: 0.792 |  Val. PPL:   2.208\n",
            "Train\tCrit Loss: 0.602 | Token Loss: 0.218\n",
            "Val\tCrit Loss: 0.802 | Token Loss: 0.187\n",
            "Epoch: 57 | Time: 1m 23s\n",
            "\tTrain Loss: 0.583 | Train PPL:   1.791\n",
            "\t Val. Loss: 0.772 |  Val. PPL:   2.163\n",
            "Train\tCrit Loss: 0.598 | Token Loss: 0.219\n",
            "Val\tCrit Loss: 0.823 | Token Loss: 0.189\n",
            "Epoch: 58 | Time: 1m 23s\n",
            "\tTrain Loss: 0.579 | Train PPL:   1.784\n",
            "\t Val. Loss: 0.791 |  Val. PPL:   2.206\n",
            "Train\tCrit Loss: 0.588 | Token Loss: 0.215\n",
            "Val\tCrit Loss: 0.819 | Token Loss: 0.189\n",
            "Epoch: 59 | Time: 1m 23s\n",
            "\tTrain Loss: 0.569 | Train PPL:   1.767\n",
            "\t Val. Loss: 0.787 |  Val. PPL:   2.198\n",
            "Train\tCrit Loss: 0.584 | Token Loss: 0.213\n",
            "Val\tCrit Loss: 0.815 | Token Loss: 0.188\n",
            "Epoch: 60 | Time: 1m 23s\n",
            "\tTrain Loss: 0.566 | Train PPL:   1.761\n",
            "\t Val. Loss: 0.784 |  Val. PPL:   2.189\n",
            "Train\tCrit Loss: 0.580 | Token Loss: 0.214\n",
            "Val\tCrit Loss: 0.801 | Token Loss: 0.185\n",
            "Epoch: 61 | Time: 1m 23s\n",
            "\tTrain Loss: 0.562 | Train PPL:   1.754\n",
            "\t Val. Loss: 0.770 |  Val. PPL:   2.159\n",
            "Train\tCrit Loss: 0.572 | Token Loss: 0.210\n",
            "Val\tCrit Loss: 0.799 | Token Loss: 0.185\n",
            "Epoch: 62 | Time: 1m 23s\n",
            "\tTrain Loss: 0.554 | Train PPL:   1.740\n",
            "\t Val. Loss: 0.769 |  Val. PPL:   2.157\n",
            "Train\tCrit Loss: 0.567 | Token Loss: 0.210\n",
            "Val\tCrit Loss: 0.815 | Token Loss: 0.187\n",
            "Epoch: 63 | Time: 1m 23s\n",
            "\tTrain Loss: 0.549 | Train PPL:   1.731\n",
            "\t Val. Loss: 0.783 |  Val. PPL:   2.189\n",
            "Train\tCrit Loss: 0.560 | Token Loss: 0.208\n",
            "Val\tCrit Loss: 0.806 | Token Loss: 0.186\n",
            "Epoch: 64 | Time: 1m 23s\n",
            "\tTrain Loss: 0.542 | Train PPL:   1.719\n",
            "\t Val. Loss: 0.775 |  Val. PPL:   2.170\n",
            "Train\tCrit Loss: 0.556 | Token Loss: 0.208\n",
            "Val\tCrit Loss: 0.816 | Token Loss: 0.188\n",
            "Epoch: 65 | Time: 1m 23s\n",
            "\tTrain Loss: 0.538 | Train PPL:   1.713\n",
            "\t Val. Loss: 0.784 |  Val. PPL:   2.191\n",
            "Train\tCrit Loss: 0.552 | Token Loss: 0.206\n",
            "Val\tCrit Loss: 0.814 | Token Loss: 0.187\n",
            "Epoch: 66 | Time: 1m 23s\n",
            "\tTrain Loss: 0.534 | Train PPL:   1.707\n",
            "\t Val. Loss: 0.782 |  Val. PPL:   2.187\n",
            "Train\tCrit Loss: 0.544 | Token Loss: 0.205\n",
            "Val\tCrit Loss: 0.778 | Token Loss: 0.179\n",
            "Epoch: 67 | Time: 1m 23s\n",
            "\tTrain Loss: 0.527 | Train PPL:   1.695\n",
            "\t Val. Loss: 0.749 |  Val. PPL:   2.114\n",
            "Train\tCrit Loss: 0.539 | Token Loss: 0.203\n",
            "Val\tCrit Loss: 0.812 | Token Loss: 0.186\n",
            "Epoch: 68 | Time: 1m 23s\n",
            "\tTrain Loss: 0.522 | Train PPL:   1.685\n",
            "\t Val. Loss: 0.780 |  Val. PPL:   2.182\n",
            "Train\tCrit Loss: 0.536 | Token Loss: 0.205\n",
            "Val\tCrit Loss: 0.801 | Token Loss: 0.185\n",
            "Epoch: 69 | Time: 1m 23s\n",
            "\tTrain Loss: 0.519 | Train PPL:   1.680\n",
            "\t Val. Loss: 0.770 |  Val. PPL:   2.160\n",
            "Train\tCrit Loss: 0.529 | Token Loss: 0.202\n",
            "Val\tCrit Loss: 0.796 | Token Loss: 0.180\n",
            "Epoch: 70 | Time: 1m 23s\n",
            "\tTrain Loss: 0.512 | Train PPL:   1.669\n",
            "\t Val. Loss: 0.765 |  Val. PPL:   2.149\n",
            "Train\tCrit Loss: 0.524 | Token Loss: 0.198\n",
            "Val\tCrit Loss: 0.803 | Token Loss: 0.183\n",
            "Epoch: 71 | Time: 1m 23s\n",
            "\tTrain Loss: 0.507 | Train PPL:   1.661\n",
            "\t Val. Loss: 0.772 |  Val. PPL:   2.164\n",
            "Train\tCrit Loss: 0.518 | Token Loss: 0.198\n",
            "Val\tCrit Loss: 0.797 | Token Loss: 0.183\n",
            "Epoch: 72 | Time: 1m 23s\n",
            "\tTrain Loss: 0.502 | Train PPL:   1.652\n",
            "\t Val. Loss: 0.766 |  Val. PPL:   2.151\n",
            "Train\tCrit Loss: 0.516 | Token Loss: 0.198\n",
            "Val\tCrit Loss: 0.793 | Token Loss: 0.184\n",
            "Epoch    73: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch: 73 | Time: 1m 23s\n",
            "\tTrain Loss: 0.500 | Train PPL:   1.649\n",
            "\t Val. Loss: 0.762 |  Val. PPL:   2.144\n",
            "Train\tCrit Loss: 0.512 | Token Loss: 0.199\n",
            "Val\tCrit Loss: 0.779 | Token Loss: 0.177\n",
            "Epoch: 74 | Time: 1m 23s\n",
            "\tTrain Loss: 0.496 | Train PPL:   1.642\n",
            "\t Val. Loss: 0.748 |  Val. PPL:   2.114\n",
            "Train\tCrit Loss: 0.510 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.791 | Token Loss: 0.182\n",
            "Epoch: 75 | Time: 1m 23s\n",
            "\tTrain Loss: 0.495 | Train PPL:   1.640\n",
            "\t Val. Loss: 0.761 |  Val. PPL:   2.140\n",
            "Train\tCrit Loss: 0.507 | Token Loss: 0.194\n",
            "Val\tCrit Loss: 0.773 | Token Loss: 0.180\n",
            "Epoch: 76 | Time: 1m 23s\n",
            "\tTrain Loss: 0.491 | Train PPL:   1.634\n",
            "\t Val. Loss: 0.743 |  Val. PPL:   2.102\n",
            "Train\tCrit Loss: 0.508 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.783 | Token Loss: 0.183\n",
            "Epoch: 77 | Time: 1m 23s\n",
            "\tTrain Loss: 0.492 | Train PPL:   1.636\n",
            "\t Val. Loss: 0.753 |  Val. PPL:   2.124\n",
            "Train\tCrit Loss: 0.506 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.790 | Token Loss: 0.182\n",
            "Epoch: 78 | Time: 1m 23s\n",
            "\tTrain Loss: 0.491 | Train PPL:   1.633\n",
            "\t Val. Loss: 0.760 |  Val. PPL:   2.138\n",
            "Train\tCrit Loss: 0.509 | Token Loss: 0.195\n",
            "Val\tCrit Loss: 0.790 | Token Loss: 0.181\n",
            "Epoch: 79 | Time: 1m 23s\n",
            "\tTrain Loss: 0.493 | Train PPL:   1.638\n",
            "\t Val. Loss: 0.760 |  Val. PPL:   2.138\n",
            "Train\tCrit Loss: 0.504 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.795 | Token Loss: 0.182\n",
            "Epoch: 80 | Time: 1m 23s\n",
            "\tTrain Loss: 0.489 | Train PPL:   1.631\n",
            "\t Val. Loss: 0.765 |  Val. PPL:   2.148\n",
            "Train\tCrit Loss: 0.508 | Token Loss: 0.195\n",
            "Val\tCrit Loss: 0.794 | Token Loss: 0.182\n",
            "Epoch: 81 | Time: 1m 23s\n",
            "\tTrain Loss: 0.492 | Train PPL:   1.635\n",
            "\t Val. Loss: 0.764 |  Val. PPL:   2.146\n",
            "Train\tCrit Loss: 0.507 | Token Loss: 0.195\n",
            "Val\tCrit Loss: 0.792 | Token Loss: 0.181\n",
            "Epoch    82: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch: 82 | Time: 1m 23s\n",
            "\tTrain Loss: 0.491 | Train PPL:   1.634\n",
            "\t Val. Loss: 0.761 |  Val. PPL:   2.140\n",
            "Train\tCrit Loss: 0.511 | Token Loss: 0.197\n",
            "Val\tCrit Loss: 0.775 | Token Loss: 0.179\n",
            "Epoch: 83 | Time: 1m 23s\n",
            "\tTrain Loss: 0.495 | Train PPL:   1.640\n",
            "\t Val. Loss: 0.745 |  Val. PPL:   2.106\n",
            "Train\tCrit Loss: 0.503 | Token Loss: 0.194\n",
            "Val\tCrit Loss: 0.783 | Token Loss: 0.179\n",
            "Epoch: 84 | Time: 1m 23s\n",
            "\tTrain Loss: 0.487 | Train PPL:   1.628\n",
            "\t Val. Loss: 0.753 |  Val. PPL:   2.122\n",
            "Train\tCrit Loss: 0.505 | Token Loss: 0.195\n",
            "Val\tCrit Loss: 0.787 | Token Loss: 0.180\n",
            "Epoch: 85 | Time: 1m 23s\n",
            "\tTrain Loss: 0.489 | Train PPL:   1.631\n",
            "\t Val. Loss: 0.757 |  Val. PPL:   2.131\n",
            "Train\tCrit Loss: 0.506 | Token Loss: 0.197\n",
            "Val\tCrit Loss: 0.770 | Token Loss: 0.177\n",
            "Epoch: 86 | Time: 1m 23s\n",
            "\tTrain Loss: 0.491 | Train PPL:   1.634\n",
            "\t Val. Loss: 0.740 |  Val. PPL:   2.096\n",
            "Train\tCrit Loss: 0.502 | Token Loss: 0.195\n",
            "Val\tCrit Loss: 0.791 | Token Loss: 0.179\n",
            "Epoch: 87 | Time: 1m 23s\n",
            "\tTrain Loss: 0.487 | Train PPL:   1.627\n",
            "\t Val. Loss: 0.760 |  Val. PPL:   2.139\n",
            "Train\tCrit Loss: 0.506 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.791 | Token Loss: 0.180\n",
            "Epoch: 88 | Time: 1m 23s\n",
            "\tTrain Loss: 0.490 | Train PPL:   1.633\n",
            "\t Val. Loss: 0.760 |  Val. PPL:   2.139\n",
            "Train\tCrit Loss: 0.507 | Token Loss: 0.198\n",
            "Val\tCrit Loss: 0.802 | Token Loss: 0.183\n",
            "Epoch: 89 | Time: 1m 23s\n",
            "\tTrain Loss: 0.492 | Train PPL:   1.636\n",
            "\t Val. Loss: 0.771 |  Val. PPL:   2.161\n",
            "Train\tCrit Loss: 0.507 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.781 | Token Loss: 0.181\n",
            "Epoch: 90 | Time: 1m 23s\n",
            "\tTrain Loss: 0.492 | Train PPL:   1.635\n",
            "\t Val. Loss: 0.751 |  Val. PPL:   2.120\n",
            "Train\tCrit Loss: 0.505 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.792 | Token Loss: 0.181\n",
            "Epoch: 91 | Time: 1m 23s\n",
            "\tTrain Loss: 0.489 | Train PPL:   1.631\n",
            "\t Val. Loss: 0.761 |  Val. PPL:   2.141\n",
            "Train\tCrit Loss: 0.506 | Token Loss: 0.195\n",
            "Val\tCrit Loss: 0.782 | Token Loss: 0.179\n",
            "Epoch    92: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Epoch: 92 | Time: 1m 23s\n",
            "\tTrain Loss: 0.491 | Train PPL:   1.634\n",
            "\t Val. Loss: 0.752 |  Val. PPL:   2.121\n",
            "Train\tCrit Loss: 0.505 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.794 | Token Loss: 0.182\n",
            "Epoch: 93 | Time: 1m 23s\n",
            "\tTrain Loss: 0.490 | Train PPL:   1.632\n",
            "\t Val. Loss: 0.763 |  Val. PPL:   2.145\n",
            "Train\tCrit Loss: 0.507 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.770 | Token Loss: 0.178\n",
            "Epoch: 94 | Time: 1m 23s\n",
            "\tTrain Loss: 0.491 | Train PPL:   1.635\n",
            "\t Val. Loss: 0.740 |  Val. PPL:   2.096\n",
            "Train\tCrit Loss: 0.506 | Token Loss: 0.197\n",
            "Val\tCrit Loss: 0.778 | Token Loss: 0.180\n",
            "Epoch: 95 | Time: 1m 23s\n",
            "\tTrain Loss: 0.491 | Train PPL:   1.634\n",
            "\t Val. Loss: 0.748 |  Val. PPL:   2.113\n",
            "Train\tCrit Loss: 0.507 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.784 | Token Loss: 0.180\n",
            "Epoch: 96 | Time: 1m 23s\n",
            "\tTrain Loss: 0.492 | Train PPL:   1.635\n",
            "\t Val. Loss: 0.753 |  Val. PPL:   2.124\n",
            "Train\tCrit Loss: 0.504 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.802 | Token Loss: 0.183\n",
            "Epoch: 97 | Time: 1m 23s\n",
            "\tTrain Loss: 0.489 | Train PPL:   1.630\n",
            "\t Val. Loss: 0.771 |  Val. PPL:   2.163\n",
            "Train\tCrit Loss: 0.501 | Token Loss: 0.194\n",
            "Val\tCrit Loss: 0.799 | Token Loss: 0.183\n",
            "Epoch: 98 | Time: 1m 23s\n",
            "\tTrain Loss: 0.486 | Train PPL:   1.625\n",
            "\t Val. Loss: 0.768 |  Val. PPL:   2.155\n",
            "Train\tCrit Loss: 0.505 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.799 | Token Loss: 0.183\n",
            "Epoch: 99 | Time: 1m 23s\n",
            "\tTrain Loss: 0.490 | Train PPL:   1.632\n",
            "\t Val. Loss: 0.768 |  Val. PPL:   2.156\n",
            "Train\tCrit Loss: 0.507 | Token Loss: 0.197\n",
            "Val\tCrit Loss: 0.793 | Token Loss: 0.183\n",
            "Epoch: 100 | Time: 1m 23s\n",
            "\tTrain Loss: 0.491 | Train PPL:   1.634\n",
            "\t Val. Loss: 0.763 |  Val. PPL:   2.144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLbIfgGln4yQ"
      },
      "source": [
        "!cp /content/end_capstone_self_encode_sizeCor_stage2_256_wrn8.pt /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_self_encode_sizeCor_stage2_256_wrn8_2norm.pt"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oaf6olWIoTYb",
        "outputId": "0290b11f-6d6e-421b-ff8c-9f68d4f5ea41"
      },
      "source": [
        "file_path='/content/end_capstone_self_encode_sizeCor_stage2_256_wrn8.pt'\r\n",
        "\r\n",
        "chkpt = torch.load(file_path)\r\n",
        "print(chkpt['loss'])\r\n",
        "model.load_state_dict(chkpt['model'])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7399555317495229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml8NgPNyoOjT",
        "outputId": "9548a539-f9e8-40b0-e214-a2394b0bbc6f"
      },
      "source": [
        "LEARNING_RATE = 0.00001\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\r\n",
        "file_path='/content/end_capstone_self_encode_sizeCor_stage2_256_wrn9.pt'\r\n",
        "optimizer.load_state_dict(chkpt['optimizer'])\r\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-9,verbose=True)\r\n",
        "run_train_eval_loop(model,\r\n",
        "                    train_dataloader,\r\n",
        "                    val_dataloader,\r\n",
        "                    optimizer,\r\n",
        "                    criterion,\r\n",
        "                    device,\r\n",
        "                    epochs=100,\r\n",
        "                    clip=1.4,\r\n",
        "                    best_valid_loss=float('inf'),\r\n",
        "                    file_path=file_path,\r\n",
        "                    double_loss=True,\r\n",
        "                    scheduler=scheduler,\r\n",
        "                    mix_ratio=0.95) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\tCrit Loss: 0.509 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.784 | Token Loss: 0.182\n",
            "Epoch: 01 | Time: 1m 19s\n",
            "\tTrain Loss: 0.705 | Train PPL:   2.023\n",
            "\t Val. Loss: 0.966 |  Val. PPL:   2.626\n",
            "Train\tCrit Loss: 0.503 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.790 | Token Loss: 0.182\n",
            "Epoch: 02 | Time: 1m 21s\n",
            "\tTrain Loss: 0.699 | Train PPL:   2.012\n",
            "\t Val. Loss: 0.972 |  Val. PPL:   2.643\n",
            "Train\tCrit Loss: 0.504 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.792 | Token Loss: 0.183\n",
            "Epoch: 03 | Time: 1m 22s\n",
            "\tTrain Loss: 0.701 | Train PPL:   2.015\n",
            "\t Val. Loss: 0.974 |  Val. PPL:   2.649\n",
            "Train\tCrit Loss: 0.510 | Token Loss: 0.197\n",
            "Val\tCrit Loss: 0.798 | Token Loss: 0.182\n",
            "Epoch: 04 | Time: 1m 23s\n",
            "\tTrain Loss: 0.707 | Train PPL:   2.028\n",
            "\t Val. Loss: 0.980 |  Val. PPL:   2.664\n",
            "Train\tCrit Loss: 0.500 | Token Loss: 0.194\n",
            "Val\tCrit Loss: 0.773 | Token Loss: 0.179\n",
            "Epoch: 05 | Time: 1m 23s\n",
            "\tTrain Loss: 0.695 | Train PPL:   2.003\n",
            "\t Val. Loss: 0.951 |  Val. PPL:   2.589\n",
            "Train\tCrit Loss: 0.508 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.793 | Token Loss: 0.182\n",
            "Epoch: 06 | Time: 1m 23s\n",
            "\tTrain Loss: 0.704 | Train PPL:   2.022\n",
            "\t Val. Loss: 0.975 |  Val. PPL:   2.650\n",
            "Train\tCrit Loss: 0.513 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.779 | Token Loss: 0.178\n",
            "Epoch: 07 | Time: 1m 23s\n",
            "\tTrain Loss: 0.708 | Train PPL:   2.030\n",
            "\t Val. Loss: 0.957 |  Val. PPL:   2.603\n",
            "Train\tCrit Loss: 0.506 | Token Loss: 0.195\n",
            "Val\tCrit Loss: 0.779 | Token Loss: 0.180\n",
            "Epoch: 08 | Time: 1m 23s\n",
            "\tTrain Loss: 0.702 | Train PPL:   2.017\n",
            "\t Val. Loss: 0.959 |  Val. PPL:   2.609\n",
            "Train\tCrit Loss: 0.504 | Token Loss: 0.194\n",
            "Val\tCrit Loss: 0.782 | Token Loss: 0.181\n",
            "Epoch: 09 | Time: 1m 23s\n",
            "\tTrain Loss: 0.698 | Train PPL:   2.010\n",
            "\t Val. Loss: 0.963 |  Val. PPL:   2.619\n",
            "Train\tCrit Loss: 0.504 | Token Loss: 0.195\n",
            "Val\tCrit Loss: 0.798 | Token Loss: 0.182\n",
            "Epoch: 10 | Time: 1m 23s\n",
            "\tTrain Loss: 0.699 | Train PPL:   2.013\n",
            "\t Val. Loss: 0.980 |  Val. PPL:   2.664\n",
            "Train\tCrit Loss: 0.504 | Token Loss: 0.194\n",
            "Val\tCrit Loss: 0.781 | Token Loss: 0.180\n",
            "Epoch    11: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Epoch: 11 | Time: 1m 23s\n",
            "\tTrain Loss: 0.698 | Train PPL:   2.010\n",
            "\t Val. Loss: 0.961 |  Val. PPL:   2.614\n",
            "Train\tCrit Loss: 0.505 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.797 | Token Loss: 0.181\n",
            "Epoch: 12 | Time: 1m 23s\n",
            "\tTrain Loss: 0.701 | Train PPL:   2.016\n",
            "\t Val. Loss: 0.978 |  Val. PPL:   2.660\n",
            "Train\tCrit Loss: 0.505 | Token Loss: 0.195\n",
            "Val\tCrit Loss: 0.785 | Token Loss: 0.182\n",
            "Epoch: 13 | Time: 1m 23s\n",
            "\tTrain Loss: 0.700 | Train PPL:   2.014\n",
            "\t Val. Loss: 0.967 |  Val. PPL:   2.631\n",
            "Train\tCrit Loss: 0.508 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.789 | Token Loss: 0.181\n",
            "Epoch: 14 | Time: 1m 23s\n",
            "\tTrain Loss: 0.704 | Train PPL:   2.022\n",
            "\t Val. Loss: 0.971 |  Val. PPL:   2.640\n",
            "Train\tCrit Loss: 0.505 | Token Loss: 0.194\n",
            "Val\tCrit Loss: 0.799 | Token Loss: 0.182\n",
            "Epoch: 15 | Time: 1m 23s\n",
            "\tTrain Loss: 0.699 | Train PPL:   2.012\n",
            "\t Val. Loss: 0.981 |  Val. PPL:   2.666\n",
            "Train\tCrit Loss: 0.506 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.798 | Token Loss: 0.182\n",
            "Epoch: 16 | Time: 1m 23s\n",
            "\tTrain Loss: 0.701 | Train PPL:   2.016\n",
            "\t Val. Loss: 0.981 |  Val. PPL:   2.666\n",
            "Train\tCrit Loss: 0.505 | Token Loss: 0.194\n",
            "Val\tCrit Loss: 0.774 | Token Loss: 0.178\n",
            "Epoch: 17 | Time: 1m 23s\n",
            "\tTrain Loss: 0.700 | Train PPL:   2.013\n",
            "\t Val. Loss: 0.952 |  Val. PPL:   2.590\n",
            "Train\tCrit Loss: 0.505 | Token Loss: 0.194\n",
            "Val\tCrit Loss: 0.800 | Token Loss: 0.182\n",
            "Epoch: 18 | Time: 1m 23s\n",
            "\tTrain Loss: 0.699 | Train PPL:   2.012\n",
            "\t Val. Loss: 0.982 |  Val. PPL:   2.670\n",
            "Train\tCrit Loss: 0.505 | Token Loss: 0.195\n",
            "Val\tCrit Loss: 0.782 | Token Loss: 0.179\n",
            "Epoch: 19 | Time: 1m 23s\n",
            "\tTrain Loss: 0.700 | Train PPL:   2.014\n",
            "\t Val. Loss: 0.961 |  Val. PPL:   2.615\n",
            "Train\tCrit Loss: 0.507 | Token Loss: 0.195\n",
            "Val\tCrit Loss: 0.783 | Token Loss: 0.180\n",
            "Epoch: 20 | Time: 1m 23s\n",
            "\tTrain Loss: 0.702 | Train PPL:   2.017\n",
            "\t Val. Loss: 0.963 |  Val. PPL:   2.619\n",
            "Train\tCrit Loss: 0.503 | Token Loss: 0.194\n",
            "Val\tCrit Loss: 0.785 | Token Loss: 0.181\n",
            "Epoch: 21 | Time: 1m 23s\n",
            "\tTrain Loss: 0.698 | Train PPL:   2.009\n",
            "\t Val. Loss: 0.966 |  Val. PPL:   2.627\n",
            "Train\tCrit Loss: 0.503 | Token Loss: 0.193\n",
            "Val\tCrit Loss: 0.788 | Token Loss: 0.182\n",
            "Epoch: 22 | Time: 1m 23s\n",
            "\tTrain Loss: 0.697 | Train PPL:   2.007\n",
            "\t Val. Loss: 0.970 |  Val. PPL:   2.638\n",
            "Train\tCrit Loss: 0.504 | Token Loss: 0.196\n",
            "Val\tCrit Loss: 0.781 | Token Loss: 0.179\n",
            "Epoch: 23 | Time: 1m 23s\n",
            "\tTrain Loss: 0.700 | Train PPL:   2.014\n",
            "\t Val. Loss: 0.960 |  Val. PPL:   2.613\n",
            "Train\tCrit Loss: 0.499 | Token Loss: 0.195\n",
            "Val\tCrit Loss: 0.793 | Token Loss: 0.183\n",
            "Epoch: 24 | Time: 1m 23s\n",
            "\tTrain Loss: 0.694 | Train PPL:   2.001\n",
            "\t Val. Loss: 0.976 |  Val. PPL:   2.654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDo3e4HVKMxI"
      },
      "source": [
        "def get_code(sentence,\r\n",
        "             doc_tokenizer,\r\n",
        "             code_tokenizer,\r\n",
        "             code_tok_vectorizer,\r\n",
        "             model, \r\n",
        "             device, \r\n",
        "             max_len = 100):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    dataset_handler = NLPLDataSet(doc_tokenizer, code_tokenizer, code_tok_vectorizer)\r\n",
        "    src_indexes, src_mask =  dataset_handler.prepare_tokens(sentence, dataset_handler.doc_tokenizer)\r\n",
        "\r\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\r\n",
        "    src_mask = torch.LongTensor(src_mask).unsqueeze(0).to(device)\r\n",
        "    \r\n",
        "    src_mask = model.make_src_mask(src_mask)\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\r\n",
        "\r\n",
        "    trg_indexes = [code_tok_vectorizer.ID_SOS_FOR_CODEPIECE]\r\n",
        "    trg_tok_indexes = [code_tok_vectorizer.ID_SOS_FOR_TOKEN_TYPE]\r\n",
        "    #trg_mask = [1]\r\n",
        "\r\n",
        "    for i in range(max_len):\r\n",
        "\r\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\r\n",
        "        trg_tok_tensor = torch.LongTensor(trg_tok_indexes).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "        base_mask = torch.LongTensor([1]*(i+1)).unsqueeze(0).to(device)\r\n",
        "        trg_mask = model.make_trg_mask(trg_tensor, base_mask)\r\n",
        "        \r\n",
        "        with torch.no_grad():\r\n",
        "            output, tok_output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask, trg_tok_tensor)\r\n",
        "        \r\n",
        "        pred_token = output.argmax(2)[:,-1].item()\r\n",
        "        trg_indexes.append(pred_token)\r\n",
        "\r\n",
        "        pred_tok_type = tok_output.argmax(2)[:,-1].item()\r\n",
        "        trg_tok_indexes.append(pred_tok_type)\r\n",
        "\r\n",
        "        if (pred_token == code_tok_vectorizer.ID_EOS_FOR_CODEPIECE or \r\n",
        "            pred_token == code_tok_vectorizer.code_word2idx['___EOS___']):\r\n",
        "            break\r\n",
        "\r\n",
        "    trg_tokens = [code_tok_vectorizer.convert_id_to_codepiece(i) for i in trg_indexes]\r\n",
        "    trg_token_types = [code_tok_vectorizer.convert_id_to_toktype(i) for i in trg_tok_indexes]\r\n",
        "    \r\n",
        "    return trg_tokens[1:], trg_token_types[1:], attention"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ammem8M7qAL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6be36aaa-2739-484b-9109-1a18fb2e1e8c"
      },
      "source": [
        "input_text = \"write a python program that would swap variable values\"\r\n",
        "splitted_text = auto_tokenizer.tokenize(input_text)\r\n",
        "mycode, mytoks, attention_val = get_code(input_text,\r\n",
        "                                 auto_tokenizer, \r\n",
        "                                 init_tokenizer,\r\n",
        "                                 code_tok_vectorizer,\r\n",
        "                                 model, \r\n",
        "                                 device,\r\n",
        "                                 max_len=512)\r\n",
        "\r\n",
        "print(init_tokenizer.untokenize(mycode))\r\n",
        "#output = auto_tokenizer.convert_tokens_to_string(mycode[:-1])\r\n",
        "#print(output)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a =10 \n",
            "b =15 \n",
            "a ,b =b ,a \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1jk-tl8LFYX"
      },
      "source": [
        "a =10 \r\n",
        "b =15 \r\n",
        "a ,b =b ,a "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hGdF_mLk164",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56f4937-685b-452a-b22e-d2a1f853fdda"
      },
      "source": [
        "cal_eq_triangle_area(20)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "455.9014113909555"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayl995l96d6L"
      },
      "source": [
        "import matplotlib.ticker as ticker\r\n",
        "\r\n",
        "#display_attention(splitted_text, mycode, attention_val[:,2,:,:].unsqueeze(1), n_heads=1, n_rows=1, n_cols=1)\r\n",
        "n_heads=4\r\n",
        "n_rows=2\r\n",
        "n_cols=n_heads/n_rows\r\n",
        "display_attention(splitted_text, \r\n",
        "                  mycode, \r\n",
        "                  attention_val[:,:8,:,:], \r\n",
        "                  n_heads=n_heads, \r\n",
        "                  n_rows=n_rows, \r\n",
        "                  n_cols=n_cols)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66sIoARXeD2l",
        "outputId": "b8282a9b-a512-4a1e-be7b-4517380290d9"
      },
      "source": [
        "all_strings = nl_to_pl_df[(nl_to_pl_df['cleaned_code_len'] <= 256) ]['docstring'].values\r\n",
        "\r\n",
        "gen_code_arr = []\r\n",
        "for one_string in all_strings[100:200]:\r\n",
        "    #splitted_text = auto_tokenizer.tokenize(input_text)\r\n",
        "    gen_code = \"NoCode\"\r\n",
        "    cleaned_string = one_string.rstrip('\\n').lstrip('#')\r\n",
        "    print(cleaned_string)\r\n",
        "    try:\r\n",
        "        mycode, my_tok, attention_val = get_code(cleaned_string,\r\n",
        "                                        auto_tokenizer, \r\n",
        "                                        init_tokenizer,\r\n",
        "                                        code_tok_vectorizer,\r\n",
        "                                        model, \r\n",
        "                                        device,\r\n",
        "                                        max_len=200)\r\n",
        "        gen_code = init_tokenizer.untokenize(mycode)\r\n",
        "        print(gen_code) \r\n",
        "        #gen_code_arr.append(gen_code)\r\n",
        "        #print()\r\n",
        "    except:\r\n",
        "        print(\"Error\")        \r\n",
        "        #gen_code_arr.append(gen_code)\r\n",
        "        continue"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " write a python program to convert all list elements to upper case\n",
            "s =[\"pune\",\"mumbai\",\"delhi\"]\n",
            "print ([(.upper ()for w in s ])\n",
            " write a python program to expalin python zip method\n",
            "l1 =[2 ,4 ,6 ]\n",
            "l2 =[-2 ,-4 ,-]\n",
            "for =6 ,-64 -l2 \n",
            "l1 ]\n",
            "for i in zip (l1 ,l2 ):\n",
            "    print (i )\n",
            " write a python program to add two list using python zip method\n",
            "\n",
            "l1 =[10 ,20 ,30 ]\n",
            "l2 =[-20 ,-30 ]\n",
            "\n",
            "=[-30 ,-20 ,-30 ]\n",
            "print ('val:',x )\n",
            " write a list comprehension for number and its cube \n",
            "l =[1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ]\n",
            "print ([x **3 for x in l ])\n",
            " write a list comprehension for printing rows into columns and vv\n",
            "Error\n",
            " write a python program to use python lambda function\n",
            "\n",
            "data =[(\"Apples\",\"20\"),(print (\"Pears\",\"5\",\"Oranges\",\"10\")\n",
            " write a python program to multiply a string n times\n",
            "a ='python'\n",
            "print (a *5 )\n",
            " write a python to check two numbers are greater than or equal or less than\n",
            "\n",
            "def hanoi (x ,y ):\n",
            "    if x >y :\n",
            "        return x \n",
            "    elif x ==y :\n",
            "        return x \n",
            "    else :\n",
            "        return x \n",
            "\n",
            "print (\"Total numbers from the numbers numbers: \",x ,y ,z )\n",
            "print (\"Minimum\") \n",
            " write a python to dict to zip and print as dictionary elements in original form\n",
            "a a ={\"a\":1 ,\"b\":2 ,\"c\":3 }\n",
            "b =dict (zip (a ,a .values ()))\n",
            "print (b )\n",
            " write a python program to delete an dictionary element\n",
            "a ={1 :1 ,2 :3 ,4 :5 }\n",
            "b =3 \n",
            "a .pop ())\n",
            "print (a )\n",
            " write a python program to check two dictionary are equal or not\n",
            "a ={'a':1 ,'b':2 ,'c':3 }\n",
            "b ={={(a ,b )' )\n",
            "\n",
            "if (==b :\n",
            "==dict [5 ]:\n",
            "    print (f\"Dictionary:{c}\")\n",
            "else :\n",
            "    print (f\"ckeys )\n",
            " write a python program to print only dictionary keys as list\n",
            "\n",
            "d ={\"john\":40 ,\"year\":1964 }\n",
            "print (dict )\n",
            "write a python program to check two lists are equal or not\n",
            "\n",
            "a =[1 ,2 ,3 ,4 ]\n",
            "b =[5 ,6 ,7 ,8 ]\n",
            "\n",
            "print (a ==b )\n",
            "\n",
            "print (a )\n",
            "b =set (b )\n",
            "print (a )\n",
            "print (b )\n",
            "write a python program to sum the set of unqiue elements\n",
            "\n",
            "a ={5 ,6 ,7 }\n",
            "print (sum (a ,5 ))\n",
            "write a python program to implement try catch code\n",
            "\n",
            "try :\n",
            "    s =s *3 \n",
            "except Exception as e :\n",
            "    print (e )\n",
            "write a python program to count the len of unique elements\n",
            "\n",
            "={'apple':1 ,'orange':2 ,'banana':3 }\n",
            "print (len ' )\n",
            "write a python program to split in python\n",
            "\n",
            "={'the':1000 ,'cd':-}\n",
            "for ==int (i .split ())\n",
            "for i in i :\n",
            "    print (i )\n",
            " write a python program to add title to string\n",
            "print ('ab cd'.title ())\n",
            " write a python program to print equal lenght of string\n",
            "print (f'ab cd'.format (8 8 ))\n",
            " write a python program to use string replace\n",
            "s ='the quick brown fox'\n",
            "print (string .replace (' ',' '))\n",
            "  write a python program to check string istitle\n",
            "str1 ='Hello! It is a good day'\n",
            "print (\"The string is : \"+str (str1 ))\n",
            "  write a python program to do lstrip on string\n",
            "\n",
            "print ('ab'.lstrip ())\n",
            "  write a python program to check identifier/keyword\n",
            "print ('ab'.timeit ())\n",
            "  write a python program to check is an num/int\n",
            "print ('ab'.isalnum ())\n",
            "  write a python program to check is an variable is printable\n",
            "print ('ab'.isprintable ())\n",
            "  write a python program to check it contains any space\n",
            "\n",
            "test_str ='It is a glorious day'\n",
            "\n",
            "print (\"The original string is : \"+str (test_str ))\n",
            "\n",
            "res =len (test_str )\n",
            "for idx in range (len (test_str )):\n",
            "    if s [idx ]==s [idx ]:\n",
            "        res .append (s )\n",
            "print (\"The string is not palindrome palindrome palindrome\"+str (res )\n",
            "  write a python program to check is an title\n",
            "print ('ab cd. ,' )\n",
            "  write a python program to check is all are num/int\n",
            "print ('ab'.isalnum ())\n",
            "  write a python program to check is all are alphanumeric\n",
            "print ('ab'.isalpha ())\n",
            "  write a python program to check is all are digit\n",
            "print (f'11'.isnumeric ())\n",
            "  write a python program to use f string\n",
            "var1 ='python language'\n",
            "print (f'f'f' )\n",
            "  write a python program to iterate an dict and concatenate\n",
            "\n",
            "D =dict (p ='san',q ='foundry')\n",
            "print ('{dict .month (height ))\n",
            " write a python program to replace blank space to 1\n",
            "Error\n",
            " write a python program to explain the generator\n",
            "def f11 (x ):\n",
            "    yield x +1 \n",
            "g =f11 (8 )\n",
            "print (next (g ))\n",
            " write a python program to replace blank space to 1\n",
            "Error\n",
            " write a python program to print current working directory\n",
            "import os \n",
            "print (os .getcwd ())\n",
            " write a python program to print the ascii value of a string\n",
            "print ([ord (ch )for ch in 'abc'])\n",
            " write a python program to use extend in list/ append to a list\n",
            "a =[2 ,3 ,8 ,6 ]\n",
            "b =[],[,18 ]\n",
            "a a &b =a \n",
            "b \n",
            "print (a .extend (b )\n",
            " write a python program to replace blank space to 1\n",
            "Error\n",
            " write a python program to add and square a range of number\n",
            "x =[i **+1 for i in range (3 )];print (x )\n",
            " write a python program to replace blank space to 1\n",
            "Error\n",
            " write a python program to multiply two list with list comprehensive\n",
            "l1 =[1 ,2 ,3 ]\n",
            "l2 =[4 ,5 ,6 ]\n",
            "print ([x *y for x in l1 for y in l2 ])\n",
            " write a python program to print only digit or only apha charac in a given list\n",
            "l =[\"good\",\"oh!\",\"excellent!\",\"#450\"]\n",
            "print ([.lower ())\n",
            " write a python program to print todays date\n",
            "print ('%cd. /doc__ )\n",
            " write a python program to check tuple are immutable\n",
            "a =(1 ,2 ,3 )\n",
            "try =a +1 \n",
            "print (\"The dimension in the first : \"+e )\n",
            " write a python program to calculate factorial sum using list comprehensive\n",
            "import functools \n",
            "n =5 \n",
            "print (functools  for given array {x}' )\n",
            " write a python program to print len of each characters\n",
            "words =['cat','window','defenestrate']\n",
            "for w in words :\n",
            "    print (w ,len (w ))\n",
            " write a python program to make increment on each call of method using lambda function\n",
            "def make_incrementor (n ):\n",
            "    return lambda x :x +1 \n",
            "f =make_incrementor (42 )\n",
            "print (f (1 ))\n",
            " write a python program to sort using list comprehensive\n",
            "pairs =[(1 ,'one'),(2 ,'two'),(3 ,'three'),(4 ,'four')]\n",
            "pairs .sort (key =lambda pair :pair [1 ])\n",
            "print (pairs )\n",
            " write a python program to del the first element of the array/list\n",
            "a =[1 1 ,2 ,3 ,4 ,5 ,6 ]\n",
            "for a in a [0 ]=a [-,a ]\n",
            "while a <a [-1 ]:\n",
            "    print (a [-1 ])\n",
            " write a python program to replace the first character of a given word\n",
            "word =\"Hello World\"\n",
            "x =word .replace (\"Bye\")\n",
            "print (f\"Integer String ater case:{letter}\")\n",
            " write a python program to find a string in a given phrase\n",
            "phrase =\"the surprise is in here somewhere\"\n",
            "print (phrase .find (\"surprise\"))\n",
            " write a python program to expalin the use of f-string\n",
            "n =3 \n",
            "print (f\"{n} is {m} n*m} n*m}\")\n",
            " write a python program to explain the use of assert\n",
            "x =[\n",
            "y =1 \n",
            "assert x >y ,'X too small'\n",
            " write a python program to multiply three numbers\n",
            "num1 =1.5 \n",
            "num2 =6.3 \n",
            "num3 =-2.3 \n",
            "product =num1 *num2 *print (f'Product: {product}')\n",
            " write a python function that when given two numbers, would divide the first number by second number and return the quotient and remainder\n",
            "Error\n",
            " write a python function to return the largest and smallest numbers in the given list and return None if the list is empty\n",
            "Error\n",
            " write a recursive python function to print the nth fibonacci number, where n is provided as the argument\n",
            "def fibonacci_recursive (n ):\n",
            "    if n <=1 :\n",
            "        return n \n",
            "    else :\n",
            "        return (fibonacci_recursive (n -1 )+fibonacci_recursive (n -2 ))\n",
            " write a python function that would read the given input file path and print its contents\n",
            "def read_and_print_file (filepath ):\n",
            "    with open (filepath ,\"r\")as infile :\n",
            "        print (infile .read ())\n",
            " write a python program that would print the first n positive integers using a for loop\n",
            "n =62 \n",
            "for num in range (n ):\n",
            "    print (num )\n",
            " write a python function that returns the input list sorted in ascending order\n",
            "def sort_ascending (list_to_be_sorted ):\n",
            "    return sorted (list_to_be_sorted )\n",
            " write a python function that returns the input list sorted in descending order\n",
            "def sort_descending (list_to_be_sorted ):\n",
            "    return sorted (list_to_be_sorted ,reverse =True )\n",
            " write a python function that would return the sum of first n natural numbers, where n is the input\n",
            "def sum_first_n (n ):\n",
            "    return (n *(n +1 ))//2 \n",
            " write a recursive python function that would return the sum of first n natural numbers, where n is the input\n",
            "def sum_first_n_recursive (n ):\n",
            "    if n ==0 :\n",
            "        return 0 \n",
            "    return sum____NEWLINE___ n_recursive (n -1 )+n \n",
            " write a python function that would filter a list of dictionaries where a specified key equals given value, list_of_dictionaries, key and value are inputs to this function.\n",
            "def filter_with_key_value (list_of_dicts ,key ,value ):\n",
            "    return list (filter (lambda x :x .get (key )==value ,list_of_dicts ))\n",
            " write a recursive python function that takes either a list or tuple as input and reverses the order of its elements\n",
            "def reverse (seq ):\n",
            "    SeqType =type (seq )\n",
            "    emptySeq =SeqType (seq )\n",
            "    if seq ==emptySeq :\n",
            "        return emptySeq \n",
            "    restrev seq ==emptySeq :\n",
            "        return seq \n",
            " write a python function that returns the square of a given input number\n",
            "def square (x ):\n",
            "    return x **2 \n",
            " write a python program that asks for user input and prints the given input\n",
            "a =input (\"User Input\")\n",
            "print (a )\n",
            " write a python function shifts and scales all numbers in the given list by the given mean and standard deviation\n",
            "def shift_and_scale (list_of_nums ,mean ,std ):\n",
            "    return [(x -mean )/std for x in list_of_nums ]\n",
            " write a python function that takes in a list of sequences and zips each corresponding element from the list into a tuple and returns the list of such tuples\n",
            "def zip_ (list_of_seq ):\n",
            "    return list (zip (*list_of_seq ))\n",
            " write python program that would merge two dictionaries by adding the second one into the first\n",
            "a ={\"a\":1 ,\"b\":3 }\n",
            "b ={\"c\":1 ,\"d\":3 }\n",
            "a .update (b )\n",
            " write a python function that would reverse the given string\n",
            "def reverse_string (str_to_be_reversed ):\n",
            "    return str_to_be_reversed [::-1 ]\n",
            " write a python program that would print \"Hello World\"\n",
            "print (\"Hello World\")\n",
            " write a python program that would swap variable values\n",
            "a =10 \n",
            "b =15 \n",
            "a ,b =b ,a \n",
            " write a python program that iterates over a dictionary and prints its keys and values\n",
            "a ={\"a\":1 ,\"b\":2 ,\"c\":3 ,\"d\":4 }\n",
            "for k ,v in a .items ():\n",
            "    print (k ,v )\n",
            " write a python function that would print the ASCII value of a given character\n",
            "def print_ascii (char ):\n",
            "    print (ord (char ))\n",
            " write a python function that takes in two numbers and returns their HCF\n",
            "def hcf (num1 ,num2 ):\n",
            "    smaller =num1 if num1 <num2 else num2 \n",
            "    for i in range (1 ,smaller +1 ):\n",
            "        if (num1 %i ==0 )and (num2 %i ==0 ):\n",
            "            hcf =i \n",
            "    return hcf \n",
            " write a python function that takes in two numbers and returns their LCM\n",
            "def lcm (num1 ,num2 ):\n",
            "    bigger =num1 if num1 >num2 else num2 \n",
            "    while True :\n",
            "        if (bigger %num1 ==0 )and (bigger %num2 ==0 ):\n",
            "            break \n",
            "        bigger bigger +=1 \n",
            "    return bigger \n",
            " write a recursive python function to calculate sum of natural numbers upto n, where n is an argument\n",
            "def recursive_sum (n ):\n",
            "    if n <=1 :\n",
            "        return n \n",
            "    else :\n",
            "        return n +recursive_sum (n -1 )\n",
            " write a python function that deletes the last element of a list and returns the list and the deleted element\n",
            "def delete_last_element (list_to_be_processed ):\n",
            "    deleted_element =list_to_be_processed .pop ()\n",
            "    return list_to_be_processed ,deleted_element \n",
            " write a python function that takes in a list and returns a list containing the squares of the elements of the input list\n",
            "def square_list_elements (list_to_be_squared ):\n",
            "    return list (map (lambda x :x **2 ,list_to_be_squared ))\n",
            " write a python function that finds square roots of a given number, if the square root is an integer, else returns the message \"Error - the square root is not an integer\"\n",
            "def find_integer_square_roots (num ):\n",
            "    found =False \n",
            "    for k in range (1 ,(num //2 )+1 ):\n",
            "        if (((k **2 )==num ):\n",
            "            found =True \n",
            "            break \n",
            "    found :\n",
            "        return \"Error the square root is not an integer\"\n",
            "    return -k \n",
            " write a python program that prints out natural numbers less than or equal to the given number using a while loop\n",
            "input_num =27 \n",
            "while input_num :\n",
            "    print (input_num )\n",
            "    input_num -=1 \n",
            " write a python function that takes two numbers. The function divides the first number by the second and returns the answer. The function returns None, if the second number is 0\n",
            "def divide (num1 ,num2 ):\n",
            "    if num2 ==0 :\n",
            "        return \n",
            "    else :\n",
            "        return num1 /num2 \n",
            " write a python program uses else with for loop\n",
            "seq =\"abcde\"\n",
            "for k in seq :\n",
            "    if k ==\"f\":\n",
            "        break \n",
            "else :\n",
            "    print (\"f Not Found!\")\n",
            " Write a function to return the mean of numbers in a list\n",
            "def cal_mean (num_list :list )->float :\n",
            "    if num_list :\n",
            "        return sum (num_list )/len (num_list )\n",
            "    else :\n",
            "        return None \n",
            " Write a function to return the area of a equilateral triangle\n",
            "def cal_eq_triangle_area (a :float )->float :\n",
            "    if a :\n",
            "        return (3 **(3 /4 ))*(a **2 )/2 \n",
            "    else :\n",
            "        return None \n",
            " Write a function to return the area of a right angle triangle\n",
            "\n",
            "def cal_rt_triangle_area (base :float ,height :float )->float :\n",
            "    if base and height :\n",
            "        return (base *height )/2 \n",
            "    else :\n",
            "        return None \n",
            " Write a function to return the cartisian distance of a point from origin\n",
            "def cal_dist_from_orign (x :float ,y :float )->float :\n",
            "    return (x **2 +y **2 )**(1 /2 )\n",
            " Write a function to return the cartisian distance between two points\n",
            "def cal_cart_distance (x1 :float ,y1 :float ,x2 :float ,y2 :float )->float :\n",
            "    return ((x1 -x2 )**2 +(y1 -y2 )**2 )**(1 /2 )\n",
            " Write a function to return the type roots of a quadratic equation ax**2 + bx + c = 0\n",
            "def prod_of_roots (a :float ,b :float ):\n",
            "    if a :\n",
            "        return -b /a \n",
            "    else :\n",
            "        return None \n",
            " Write a function to return the sum of the roots of a quadratic equation ax**2 + bx + c = 0\n",
            "def sum_of_roots (a :float ,c :float ):\n",
            "    if a :\n",
            "        return c /a \n",
            "    else :\n",
            "        return None \n",
            " Write a function to return the product of the roots of a quadratic equation ax**2 + bx + c = 0\n",
            "def prod_of_roots (a :float ,b :float ):\n",
            "    if a :\n",
            "        return -b /a \n",
            "    else :\n",
            "        return None \n",
            " Write a function to return the real of the roots of a quadratic equation else return None ax**2 + bx + c = 0\n",
            "def roots_of_qad_eq (a :float ,b :float ,c :float ):\n",
            "    d =b **2 -4 *a *c \n",
            "    if d >=0 :\n",
            "        return (-b +(d )/2 )/2 *a ,(d )/2 *a ,(-)/2 )/2 *a \n",
            "        d -b -b -(d )**(1 /2 *a ,(-))/2 *a \n",
            "        d )/2 \n",
            "    else :\n",
            "        return None \n",
            " Write a function to return the profit or loss based on cost price and selling price\n",
            "def find_profit_or_loss (cp ,sp ):\n",
            "    if cp >sp :\n",
            "        return 'loss',cp -sp \n",
            "    elif cp <sp :\n",
            "        return 'profit',sp -cp \n",
            "    else :\n",
            "        return 'no profit or loss',0 \n",
            " Write a function to return the area of a rectangle\n",
            "def cal_area_rect (length ,breadth ):\n",
            "    return length *breadth \n",
            " Write a function to return the area of a square\n",
            "def cal_area_square (side ):\n",
            "    return side **2 \n",
            " Write a function to return the area of a rhombus with diagonals q1 and q2\n",
            "def cal_area_rhombus (q1 ,q2 ):\n",
            "    return (q1 *q2 )/2 \n",
            " Write a function to return the area of a trapezium with base a base b and height h between parallel sides\n",
            "def cal_area_trapezium (a ,b ,h ):\n",
            "    return h *(a +b )/2 \n",
            " Write a function to return the area of a circle of raidus r\n",
            "def cal_area_circle (r ):\n",
            "    pi =3.14 \n",
            "    return pi *r **\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_Arp5Z30Hds"
      },
      "source": [
        "#all_strings = nl_to_pl_df[(nl_to_pl_df['cleaned_code_len'] <= 128) & (nl_to_pl_df['docstring_len'] <= 256)]['docstring'].values\r\n",
        "all_strings = nl_to_pl_df[(nl_to_pl_df['cleaned_code_len'] <= 256) ]['docstring'].values\r\n",
        "\r\n",
        "gen_code_arr = []\r\n",
        "for one_string in all_strings:\r\n",
        "    #splitted_text = auto_tokenizer.tokenize(input_text)\r\n",
        "    gen_code = \"NoCode\"\r\n",
        "    cleaned_string = one_string.rstrip('\\n').lstrip('#')\r\n",
        "    try:\r\n",
        "        mycode, my_tok, attention_val = get_code(cleaned_string,\r\n",
        "                                        auto_tokenizer, \r\n",
        "                                        init_tokenizer,\r\n",
        "                                        code_tok_vectorizer,\r\n",
        "                                        model, \r\n",
        "                                        device,\r\n",
        "                                        max_len=200)\r\n",
        "        gen_code = init_tokenizer.untokenize(mycode)\r\n",
        "        gen_code_arr.append(gen_code)\r\n",
        "    except:\r\n",
        "        print(\"Error\")        \r\n",
        "        print(cleaned_string)\r\n",
        "        gen_code_arr.append(gen_code)\r\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL6OAWoma4Tj"
      },
      "source": [
        "nl_to_pl_df.loc[(nl_to_pl_df['cleaned_code_len'] <= 256), 'gen_code'] = gen_code_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLXQhfXohAnM"
      },
      "source": [
        "nl_to_pl_df.to_csv(\"end_capstone_gen_code.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbsDAfE8hN4V"
      },
      "source": [
        "all_strings = nl_to_pl_df[(nl_to_pl_df['cleaned_code_len'] <= 256) ][['docstring','cleaned_code', 'gen_code']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afz7IwWVhZ-U",
        "outputId": "723c5b7c-e5b0-44b4-fba5-9eecfab9ef55"
      },
      "source": [
        "for row in all_strings[-50:].itertuples():\r\n",
        "    print(\"*****************START****************************\")\r\n",
        "    print(row.docstring)\r\n",
        "    print(row.cleaned_code)\r\n",
        "    print(\"*********************************************\")\r\n",
        "    print(row.gen_code)\r\n",
        "    print(\"*******************END**************************\")\r\n",
        "    #print(row.gen_code)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*****************START****************************\n",
            "# By using list comprehension, write a program to print the list after removing the 0th, 2nd, 4th,6th numbers in [12,24,35,70,88,120,155].\n",
            "\n",
            "li = [12,24,35,70,88,120,155]\n",
            "li = [x for (i,x) in enumerate(li) if i%2!=0]\n",
            "print li\n",
            "*********************************************\n",
            "li =[12 ,24 ,35 ,70 ,88 ,120 ,155 ]\n",
            "li =[x for (i ,x )in enumerate (li )if i %2 !=0 ]\n",
            "print li \n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# By using list comprehension, write a program generate a 3*5*8 3D array whose each element is 0.\n",
            "\n",
            "array = [[ [0 for col in range(8)] for col in range(5)] for row in range(3)]\n",
            "print array\n",
            "*********************************************\n",
            "array =[[[0 for col in range (8 )]for col in range (8 )]for row in range (3 )]\n",
            "print array \n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# By using list comprehension, write a program to print the list after removing the 0th,4th,5th numbers in [12,24,35,70,88,120,155].\n",
            "\n",
            "li = [12,24,35,70,88,120,155]\n",
            "li = [x for (i,x) in enumerate(li) if i not in (0,4,5)]\n",
            "print li\n",
            "*********************************************\n",
            "li =[12 ,24 ,35 ,70 ,88 ,120 ,155 ]\n",
            "li =[x for (i ,x )in enumerate (li )if i not in (0 ,4 ,5 )]\n",
            "print (li )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# By using list comprehension, write a program to print the list after removing the value 24 in [12,24,35,24,88,120,155].\n",
            "\n",
            "li = [12,24,35,24,88,120,155]\n",
            "li = [x for x in li if x!=24]\n",
            "print li\n",
            "*********************************************\n",
            "li =[12 ,24 ,35 ,24 ,88 ,120 ,155 ]\n",
            "li =[x for x in li if x !=24 ]\n",
            "print li \n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# With two given lists [1,3,6,78,35,55] and [12,24,35,24,88,120,155], write a program to make a list whose elements are intersection of the above given lists.\n",
            "\n",
            "set1=set([1,3,6,78,35,55])\n",
            "set2=set([12,24,35,24,88,120,155])\n",
            "set1 &= set2\n",
            "li=list(set1)\n",
            "print li\n",
            "*********************************************\n",
            "set1 =set ([1 ,3 ,6 ,78 ,35 ,55 ])\n",
            "set2 =set ([12 ,24 ,35 ,24 ,88 ,120 ,155 ])\n",
            "set1 &=set2 \n",
            "li =list (set1 )\n",
            "print li \n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# With a given list [12,24,35,24,88,120,155,88,120,155], write a program to print this list after removing all duplicate values with original order reserved.\n",
            "\n",
            "def removeDuplicate( li ):\n",
            "    newli=[]\n",
            "    seen = set()\n",
            "    for item in li:\n",
            "        if item not in seen:\n",
            "            seen.add( item )\n",
            "            newli.append(item)\n",
            "    return newli\n",
            "li=[12,24,35,24,88,120,155,88,120,155]\n",
            "print removeDuplicate(li)\n",
            "*********************************************\n",
            "def removeDuplicate (li ):\n",
            "    newli =[]\n",
            "    seen =set ()\n",
            "    for item in li :\n",
            "        if item not in seen :\n",
            "            seen .add (item )\n",
            "            newli .append (item )\n",
            "            newli .append (item )\n",
            "    return newli \n",
            "li =[12 ,24 ,35 ,24 ,88 ,120 ,155 ]\n",
            "print removeDuplicate (li )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program which count and print the numbers of each character in a string input by console.\n",
            "\n",
            "dic = {}\n",
            "s=raw_input()\n",
            "for s in s:\n",
            "    dic[s] = dic.get(s,0)+1\n",
            "print '\\n'.join(['%s,%s' % (k, v) for k, v in dic.items()])\n",
            "*********************************************\n",
            "dic ={}\n",
            "s =raw_input ()\n",
            "for s in s :\n",
            "    dic [s ]=dic .get (s ,0 )+1 \n",
            "print '\\n'.join (['%s,%s'%(k ,v )for k ,v in dic .items ()])\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program which accepts a string from console and print it in reverse order.\n",
            "\n",
            "s=raw_input()\n",
            "s = s[::-1]\n",
            "print s\n",
            "*********************************************\n",
            "s =input ()\n",
            "s =s [::-1 ]\n",
            "print (s )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program which accepts a string from console and print the characters that have even indexes.\n",
            "\n",
            "s=raw_input()\n",
            "s = s[::2]\n",
            "print s\n",
            "*********************************************\n",
            "s =input ()\n",
            "s =s [::2 ]\n",
            "print (s )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program which prints all permutations of [1,2,3]\n",
            "\n",
            "import itertools\n",
            "print list(itertools.permutations([1,2,3]))\n",
            "*********************************************\n",
            "import itertools \n",
            "print list (itertools .permutations ([1 ,2 ,3 ]))\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a program to solve a classic ancient Chinese puzzle:  We count 35 heads and 94 legs among the chickens and rabbits in a farm. How many rabbits and how many chickens do we have?\n",
            "\n",
            "def solve(numheads,numlegs):\n",
            "    ns='No solutions!'\n",
            "    for i in range(numheads+1):\n",
            "        j=numheads-i\n",
            "        if 2*i+4*j==numlegs:\n",
            "            return i,j\n",
            "    return ns,ns\n",
            "*********************************************\n",
            "def solve (numheads ,numlegs ):\n",
            "    ns ='No solutions!'\n",
            "    for i in range (numheads +1 ):\n",
            "        j =numheads -i \n",
            "        if 2 *i +4 *j ==numlegs :\n",
            "            return i ,j \n",
            "    return ns ,ns \n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to count characters in a string\n",
            "\n",
            "st = \"AmmarAdil\"\n",
            "count = {}\n",
            "for a in st:\n",
            "    if a in count:\n",
            "        count[a]+=1\n",
            "    else:\n",
            "        count[a] = 1\n",
            "print('Count', count)\n",
            "*********************************************\n",
            "st =\"AmmarAdil\"\n",
            "count ={}\n",
            "for a in st :\n",
            "    if a in count :\n",
            "        count [a ]+=1 \n",
            "    else :\n",
            "        count [a ]=1 \n",
            "print ('Count',count )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to print count of vowels in a string\n",
            "\n",
            "st = \"ammaradil\"\n",
            "vowle = ['a', 'e', 'i', 'o', 'u']\n",
            "count = 0\n",
            "\n",
            "for s in st:\n",
            "    if s in vowle:\n",
            "        count = count+1\n",
            "\n",
            "print(\"Count\", count)\n",
            "*********************************************\n",
            "\n",
            "str1 =\"ammaradil\"\n",
            "vowle =['a','e','i','o','u']\n",
            "count =0 \n",
            "\n",
            "for s in str1 :\n",
            "    if s in count :\n",
            "        count =count +1 \n",
            "print (\"Count\",count )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write program to convert string to upper case\n",
            "\n",
            "st = \"ammar adil\"\n",
            "\n",
            "upper_st = st.upper()\n",
            "print(\"Upper Case\", upper_st)\n",
            "*********************************************\n",
            "word =\"Hello World\"\n",
            "print (f\"String contains upper case?:{check}\")\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write program to convert string to lower case\n",
            "\n",
            "st = \"AMMAR ADIL\"\n",
            "\n",
            "lower_st = st.lower()\n",
            "print(\"Lower Case\", lower_st)\n",
            "*********************************************\n",
            "\n",
            "string =\"AMMAR ADIL\"\n",
            "\n",
            "lower_st =st .lower ()\n",
            "print (\"Lower Case\",lower_st )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to find union of 2 arrays\n",
            "\n",
            "a = {1, 2, 3, 4}\n",
            "b = {3, 4, 5, 6}\n",
            "\n",
            "union_both = a.union(b)\n",
            "print(\"Union\", union_both)\n",
            "*********************************************\n",
            "a ={\"a\",\"b\",\"c\"}\n",
            "b ={\"d\",\"d\"}\n",
            "a .update (b )\n",
            "print (b )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to find intersection\n",
            "\n",
            "a = {1, 2, 3, 4}\n",
            "b = {3, 4, 5, 6}\n",
            "\n",
            "intersection_both = a.intersection(b)\n",
            "print(\"Intersection\", intersection_both)\n",
            "*********************************************\n",
            "\n",
            "intersection =lambda a ,b :list (set (a )&set (b ))\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to create print array in beautiful format\n",
            "\n",
            "a = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]\n",
            "\n",
            "for i in a:\n",
            "    row = '|'\n",
            "    for b in i:\n",
            "        row = row + ' ' + str(b)\n",
            "    print(row + ' ' + '|')\n",
            "*********************************************\n",
            "a =[[1 ,2 ,3 ,4 ,5 ,6 ],[7 ,8 ,9 ],[9 ,12 ,13 ,14 ,15 ]\n",
            "\n",
            "for i in range (0 ,11 ):\n",
            "    row =row +' '+'___NEWLINE___ for row in range (row +' '+' '+'|'):\n",
            "        row +str (row +' '+str (row +' '+str (row +' '))\n",
            "print (row +'|')\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to join all items in a tuple into a string, using a hash character as separator\n",
            "\n",
            "myTuple = (\"John\", \"Peter\", \"Vicky\")\n",
            "x = \"#\".join(myTuple)\n",
            "print(x)\n",
            "*********************************************\n",
            "myTuple =(\"John\",\"Peter\",\"Vicky\")\n",
            "x =\"#\".join (myTuple )\n",
            "print (x )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to remove spaces at the beginning and at the end of the string\n",
            "\n",
            "txt = \"     banana     \"\n",
            "x = txt.strip()\n",
            "print(\"of all fruits\", x, \"is my favorite\")\n",
            "*********************************************\n",
            "txt =\"     banana     \"\n",
            "x =txt .strip ()\n",
            "print (\"of all fruits\",x ,\"is my favorite\")\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to remove the leading and trailing characters\n",
            "\n",
            "txt = \",,,,,rrttgg.....banana....rrr\"\n",
            "x = txt.strip(\",.grt\")\n",
            "print(x)\n",
            "*********************************************\n",
            "txt =\",,,,,rrttgg.....banana....rrr\"\n",
            "x =txt .strip (\",.grt\")\n",
            "print (x )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to split a string into a list where each line is a list item\n",
            "\n",
            "txt = \"Thank you for the music\\nWelcome to the jungle\"\n",
            "x = txt.splitlines()\n",
            "print(x)\n",
            "*********************************************\n",
            "txt =\"Thank you for the music\\nWelcome to the jungle\"\n",
            "x =txt .splitlines ()\n",
            "print (x )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to find index of a word in given string\n",
            "\n",
            "txt = \"Hello, welcome to my world.\"\n",
            "x = txt.index(\"welcome\")\n",
            "print(x)\n",
            "*********************************************\n",
            "txt =\"Hello, welcome to my world.\"\n",
            "x =txt .index (\"welcome\")\n",
            "print (x )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to find ceil of a number\n",
            "\n",
            "import math\n",
            "\n",
            "number = 34.564\n",
            "ce = math.ceil(number)\n",
            "print('Ceil', ce)\n",
            "*********************************************\n",
            "import math \n",
            "\n",
            "number =34.564 \n",
            "fa =math .fabs (number )\n",
            "print ('Fabs',fa )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to find absoluute number of a given number\n",
            "\n",
            "import math\n",
            "\n",
            "number = 34.564\n",
            "fa = math.fabs(number)\n",
            "print('Fabs', fa)\n",
            "*********************************************\n",
            "import math \n",
            "\n",
            "number =34.564 \n",
            "fa =math .fabs (number )\n",
            "print ('Fabs',fa )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to find factorinal of a number\n",
            "\n",
            "import math\n",
            "\n",
            "number = 8\n",
            "fa = math.factorial(number)\n",
            "print('Factorial', fa)\n",
            "*********************************************\n",
            "import math \n",
            "\n",
            "number =8 \n",
            "fa =math .factorial (number )\n",
            "print ('Factorial',fa )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to find exponential of a number\n",
            "\n",
            "import math\n",
            "\n",
            "number = 3\n",
            "\n",
            "print('Exponential', math.exp(number))\n",
            "*********************************************\n",
            "import math \n",
            "\n",
            "number =3 \n",
            "\n",
            "print ('Exponential',math .exp (number ))\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to find log of a number\n",
            "\n",
            "import math\n",
            "\n",
            "num = 5\n",
            "base = 7\n",
            "\n",
            "print(\"Log_x_b\", math.log(num, base))\n",
            "*********************************************\n",
            "\n",
            "import math \n",
            "\n",
            "num =5 \n",
            "\n",
            "print (\"Log_x_x_x_math .log (num ,base ))\n",
            "print (\"y =\",math .log (num ))\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to find cosine of a number\n",
            "\n",
            "import math\n",
            "\n",
            "num = 45\n",
            "print(\"Cosine\", math.cos(num))\n",
            "*********************************************\n",
            "import math \n",
            "\n",
            "num =45 \n",
            "print (\"Cosine\",math .cos (num ))\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to find sin of a number\n",
            "\n",
            "import math\n",
            "\n",
            "num = 45\n",
            "print(\"Sin\", math.sin(num))\n",
            "*********************************************\n",
            "import math \n",
            "\n",
            "num =45 \n",
            "print (\"Cosine\",math .cos (num ))\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to find tangent of a number\n",
            "\n",
            "import math\n",
            "\n",
            "num = 45\n",
            "print(\"Tangent\", math.tan(num))\n",
            "*********************************************\n",
            "def randomto_type (a :float ,b :float )->float :\n",
            "\n",
            "    return math .cos (a )**2 )\n",
            "\n",
            "a =5 \n",
            "\n",
            "print (\"Input the power of is \",x )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a program to print bit wise AND of two numbers\n",
            "\n",
            "a = 60            ##\n",
            "b = 13            ##\n",
            "\n",
            "c = a & b        ##\n",
            "print(\"AND\", c)\n",
            "*********************************************\n",
            "a =60 \n",
            "b =13 \n",
            "\n",
            "c =a ^b \n",
            "print (\"XOR\",c )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a program to print bit wise OR of two numbers\n",
            "\n",
            "a = 60\n",
            "b = 13\n",
            "\n",
            "c = a | b\n",
            "print(\"OR\", c)\n",
            "*********************************************\n",
            "a =60 \n",
            "b =13 \n",
            "\n",
            "c =a |b \n",
            "print (\"OR\",c )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a program to print bit wise XOR of two numbers\n",
            "\n",
            "a = 60\n",
            "b = 13\n",
            "\n",
            "c = a ^ b\n",
            "print(\"XOR\", c)\n",
            "*********************************************\n",
            "a =60 \n",
            "b =13 \n",
            "\n",
            "c =a ^b \n",
            "print (\"XOR\",c )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a program to calculate Binary Ones Complement of a number\n",
            "\n",
            "a = 60\n",
            "\n",
            "c = ~a\n",
            "print(\"Binary Ones Complement\", c)\n",
            "*********************************************\n",
            "a =60 \n",
            "\n",
            "c =~a \n",
            "print (\"Binary Ones Complement\",c )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to Binary Left Shift a number\n",
            "\n",
            "c = a << 2\n",
            "print(\"Binary Left Shift\", c)\n",
            "*********************************************\n",
            "c =a <<2 \n",
            "print (\"Binary Left Shift\",c )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# write a program to Binary Right Shift a number\n",
            "\n",
            "c = a >> 2\n",
            "print(\"Binary Right Shift\", c)\n",
            "*********************************************\n",
            "c =a >>2 \n",
            "print (\"Binary Right Shift\",c )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a python program, which will find all such numbers between 1000 and 3000 (both included) such that each digit of the number is an even number.The numbers obtained should be printed in a comma-separated sequence on a single line.\n",
            "\n",
            "values = []\n",
            "for i in range(1000, 3001):\n",
            "    s = str(i)\n",
            "    if (int(s[0])%2==0) and (int(s[1])%2==0) and (int(s[2])%2==0) and (int(s[3])%2==0):\n",
            "        values.append(s)\n",
            "        print (\",\".join(values))\n",
            "*********************************************\n",
            "values =[]\n",
            "for i in range (1000 ,3001 ):\n",
            "    s =str (i )\n",
            "    if (int (s [0 ])%2 ==0 )and (int (s [1 ])%2 ==0 )and (int (s [2 ])%2 ==0 )and (int (s [3 ])%2 ==0 )and (int (s [2 ==0 )%2 ==0 )and (s [3 ])%2 ==0 )and (int (s [3 ])%2 ==0 ):\n",
            "        values .append (s )\n",
            "        print (\",\".append (s )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a python program that accepts a sentence and calculate the number of letters and digits.\n",
            "\n",
            "s = raw_input()\n",
            "d={\"DIGITS\":0, \"LETTERS\":0}\n",
            "for c in s:\n",
            "    if c.isdigit():\n",
            "        d[\"DIGITS\"]+=1\n",
            "    elif c.isalpha():\n",
            "        d[\"LETTERS\"]+=1\n",
            "    else:\n",
            "        pass\n",
            "print (\"LETTERS\", d[\"LETTERS\"])\n",
            "print (\"DIGITS\", d[\"DIGITS\"])\n",
            "*********************************************\n",
            "s =input ()\n",
            "d ={\"DIGITS\":0 ,\"LETTERS\":0 }\n",
            "for c in s :\n",
            "    if c .isdigit ():\n",
            "        d [\"DIGITS\"]+=1 \n",
            "    elif c .isalpha ():\n",
            "        d [\"LETTERS\"]+=1 \n",
            "    else :\n",
            "        pass \n",
            "print (\"LETTERS\",d [\"DIGITS\"])\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a python program using a list comprehension to square each odd number in a list. The list is input by a sequence of comma-separated numbers.\n",
            "\n",
            "values = raw_input()\n",
            "numbers = [x for x in values.split(\",\") if int(x)%2!=0]\n",
            "print (\",\".join(numbers))\n",
            "*********************************************\n",
            "values =raw_input ()\n",
            "numbers =[x for x in values .split (\",\")if int (x )%2 !=0 ]\n",
            "print \",\".join (numbers )\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Define a class with a generator which can iterate the numbers, which are divisible by 7, between a given range 0 and n.\n",
            "\n",
            "def putNumbers(n):\n",
            "    i = 0\n",
            "    while i<n:\n",
            "        j=i\n",
            "        i=i+1\n",
            "        if j%7==0:\n",
            "            yield j\n",
            "\n",
            "    for i in reverse(100):\n",
            "        print (i)\n",
            "*********************************************\n",
            "def putNumbers (n ):\n",
            "    i =0 \n",
            "    while i <n :\n",
            "        j =i \n",
            "        i =i +1 \n",
            "        if j %7 ==0 :\n",
            "            yield j \n",
            "\n",
            "    for i in reverse (100 ):\n",
            "        print i \n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a python program using a function which can print a dictionary where the keys are numbers between 1 and 3 (both included) and the values are square of keys.\n",
            "\n",
            "def printDict():\n",
            "    d=dict()\n",
            "    d[1]=1\n",
            "    d[2]=2**2\n",
            "    d[3]=3**2\n",
            "    print (d)\n",
            "printDict()\n",
            "*********************************************\n",
            "def printDict ():\n",
            "    d =dict ()\n",
            "    d [1 ]=1 \n",
            "    d [2 ]=2 **2 \n",
            "    d [3 ]=3 **2 \n",
            "    print d \n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Define a class named American which has a static method called printNationality.\n",
            "\n",
            "class American(object):\n",
            "    @staticmethod\n",
            "    def printNationality():\n",
            "        print(\"America\")\n",
            "\n",
            "anAmerican = American()\n",
            "anAmerican.printNationality()\n",
            "American.printNationality()\n",
            "*********************************************\n",
            "class American (object ):\n",
            "    @staticmethod \n",
            "    def printNationality ():\n",
            "        print \"America\"\n",
            "anAmerican =American ()\n",
            "anAmerican .printNationality ()\n",
            "American .printNationality ()\n",
            "American .printNationality ()\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a function to compute 5/0 and use try/except to catch the exceptions.\n",
            "\n",
            "\n",
            "\n",
            "def throws():\n",
            "    return 5/0\n",
            "\n",
            "try:\n",
            "    throws()\n",
            "except ZeroDivisionError:\n",
            "    print (\"division by zero!\")\n",
            "except Exception, err:\n",
            "    print ('Caught an exception')\n",
            "finally:\n",
            "    print ('In finally block for cleanup')\n",
            "*********************************************\n",
            "def throws ():\n",
            "    return 5 /0 \n",
            "\n",
            "try :\n",
            "    throws ()\n",
            "except ZeroDivisionError :\n",
            "    print \"division by zero!\"\n",
            "except Exception :\n",
            "    print ('Caught an exception')\n",
            "finally :\n",
            "    print ('In finally block for cleanup')\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a python program to print the list after removing the 0th, 2nd, 4th,6th numbers in [12,24,35,70,88,120,155].\n",
            "\n",
            "li = [12,24,35,70,88,120,155]\n",
            "li = [x for (i,x) in enumerate(li) if i%2!=0]\n",
            "print(li)\n",
            "*********************************************\n",
            "li =[12 ,24 ,35 ,70 ,88 ,120 ,155 ]\n",
            "li =[x for (i ,x )in enumerate (li )if i %2 !=0 ]\n",
            "print li \n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a python program for selection sort\n",
            "\n",
            "for i in range(len(A)):\n",
            "    min_idx = i\n",
            "    for j in range(i+1, len(A)):\n",
            "        if A[min_idx] > A[j]:\n",
            "            min_idx = j\n",
            "\n",
            "A[i], A[min_idx] = A[min_idx], A[i]\n",
            "*********************************************\n",
            "\n",
            "test_list =[4 ,5 ,6 ,7 ,8 ]\n",
            "\n",
            "\n",
            "count =0 \n",
            "\n",
            "for i in range (len (X )):\n",
            "    for j in range (len (X )):\n",
            "        if j %2 ==0 :\n",
            "            count +=1 \n",
            "\n",
            "print (f'count of elements {count}')\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a python program for implementation of Bubble Sort\n",
            "\n",
            "def bubbleSort(arr):\n",
            "    n = len(arr)\n",
            "    for i in range(n-1):\n",
            "        for j in range(0, n-i-1):\n",
            "            if arr[j] > arr[j+1] :\n",
            "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
            "\n",
            "arr = [64, 34, 25, 12, 22, 11, 90]\n",
            "bubbleSort(arr)\n",
            "*********************************************\n",
            "\n",
            "def bubbleSort (arr ):\n",
            "    n =len (arr )\n",
            "    for i in range (0 ,n -1 ):\n",
            "        for j in range (0 ,n -i -i -i -i -1 ):\n",
            "            if arr [j ]>arr [j +1 ]:\n",
            "                arr [j +1 ],arr [j ],arr [j ]=arr [j +1 ]\n",
            "\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a python program to check if a number is an Armstrong number.\n",
            "\n",
            "n=int(input(\"Enter any number: \"))\n",
            "a=list(map(int,str(n)))\n",
            "b=list(map(lambda x:x**3,a))\n",
            "\n",
            "if(sum(b)==n):\n",
            "    print(\"The number is an armstrong number. \")\n",
            "else:\n",
            "    print(\"The number isn't an arsmtrong number. \")\n",
            "*********************************************\n",
            "NoCode\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "# Write a python to find LCM of two numbers\n",
            "\n",
            "a=int(input(\"Enter the first number:\"))\n",
            "b=int(input(\"Enter the second number:\"))\n",
            "if(a>b):\n",
            "    min1=a\n",
            "else:\n",
            "    min1=b\n",
            "while(1):\n",
            "    if(min1%a==0 and min1%b==0):\n",
            "        print(\"LCM is:\",min1)\n",
            "        break\n",
            "        min1=min1+1\n",
            "*********************************************\n",
            "\n",
            "limit =0 \n",
            "\n",
            "while (c >0 ):\n",
            "    a =b \n",
            "    c =b =b \n",
            "    c =a +c \n",
            "    if (b >a else b %c ==0 ):\n",
            "        if (a ==0 ):\n",
            "            break \n",
            "    if (a ==0 ):\n",
            "        break \n",
            "    else :\n",
            "        print (\"LCM is:\",min1 )\n",
            "\n",
            "*******************END**************************\n",
            "*****************START****************************\n",
            "#Write a python program to find length of list using recursion\n",
            "\n",
            "def length(lst):\n",
            "    if not lst:\n",
            "        return 0\n",
            "    return 1 + length(lst[1::2]) + length(lst[2::2])\n",
            "a=[1,2,3]\n",
            "print(\"Length of the string is: \")\n",
            "print(a)\n",
            "*********************************************\n",
            "def add_two_numbers (num1 ,num2 ):\n",
            "    sum =num1 +num2 \n",
            "    return sum \n",
            "*******************END**************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEeqtSb8OGIr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "844cf3fe-d368-4527-84ac-97a8d14a118a"
      },
      "source": [
        "limit =0 \r\n",
        "\r\n",
        "while (c >0 ):\r\n",
        "    a =b \r\n",
        "    c =b =b \r\n",
        "    c =a +c \r\n",
        "    if (b >a else b %c ==0 ):\r\n",
        "        if (a ==0 ):\r\n",
        "            break \r\n",
        "    if (a ==0 ):\r\n",
        "        break \r\n",
        "    else :\r\n",
        "        print (\"LCM is:\",min1 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-225-d7923ff36823>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    if (b >a else b %c ==0 ):\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "aww8H4mEj7Zb",
        "outputId": "5e6b9fb5-81ab-4fa7-b17d-e906b569b05e"
      },
      "source": [
        "mean_key_val_diff({1:2, 3:4})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-bc08ed9439ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_key_val_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-96-2dfdc366ffca>\u001b[0m in \u001b[0;36mmean_key_val_diff\u001b[0;34m(input_dict)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msum_diff\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_dict\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0msum_diff\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0mabs\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum_diff\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAVE6pwqdNxD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "a325eff9-e285-495f-82c1-5f19b25d337f"
      },
      "source": [
        "all_strings = nl_to_pl_df['docstring'].values\r\n",
        "for one_string in all_strings[100:149]:\r\n",
        "    cleaned_string = one_string.rstrip('\\n').lstrip('#')\r\n",
        "    mycode, attention_val = get_code(cleaned_string,auto_tokenizer, model, device, max_len=512)\r\n",
        "    print(cleaned_string)\r\n",
        "    print(auto_tokenizer.convert_tokens_to_string(mycode[:-1]))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-172-6951cadf7289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mone_string\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_strings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m149\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcleaned_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmycode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mauto_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmycode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: get_code() missing 2 required positional arguments: 'model' and 'device'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVWgi6c5dyPC"
      },
      "source": [
        "def even_odd_num(num):\r\n",
        "    max = 0\r\n",
        "    for num in num:\r\n",
        "         if num % 10 == 0:\r\n",
        "                 maxnum = num\r\n",
        "         return maxnum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G7TU847WEVV"
      },
      "source": [
        "even_odd_num(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzNkBY3S7owj"
      },
      "source": [
        "torch.save({\"model\":model.state_dict(),\r\n",
        "            \"optimizer\":optimizer.state_dict(),\r\n",
        "            \"loss\":1.373\r\n",
        "            },'end_capstone_baseline_128.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLGgHyMv8CTy"
      },
      "source": [
        "n = int(input(\"How many terms? \"))\r\n",
        "\r\n",
        "n1 = 0\r\n",
        "\r\n",
        "for i in range(n+1):\r\n",
        "    result = 0\r\n",
        "    for i in range(n2, n+1):\r\n",
        "        result = result + result*n2\r\n",
        "    print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT1a37CF7tD4"
      },
      "source": [
        "!cp /content/end_capstone_baseline_128.pt /content/drive/MyDrive/EVA4/END_Capstone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fGMACkcqByC"
      },
      "source": [
        "a = [1, 2, 3, 4, 5]\r\n",
        "b = [5, 6, 7, 8]\r\n",
        "a.update(b)\r\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTPUPBz4qJki"
      },
      "source": [
        "a = {1, 2, 3}\r\n",
        "b = {3, 4, 5, 6}\r\n",
        "a = {1, 2, 2, 3, 4}\r\n",
        "x = a[i]*b for (a, b) in zip(a, b) )\r\n",
        "print(f\"{a}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUiEo5VBx1K6"
      },
      "source": [
        "def printSubArrays(arr, start, end):\r\n",
        "    if end == len(arr):\r\n",
        "        return\r\n",
        "    elif start > end:\r\n",
        "        return printSubArrays(arr, 0, end + 1)\r\n",
        "    else:\r\n",
        "            print(arr[start:end + 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6swZArv2CvL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
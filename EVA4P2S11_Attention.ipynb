{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "EVA4P2S11_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8r_nhmV0jHVj"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EVAP2/blob/master/EVA4P2S11_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw_v8iOVjgCV",
        "outputId": "bac970fa-f4a1-4921-af42-18792d0bf16f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Nov  1 10:40:52 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH0k3Jl2jHUv"
      },
      "source": [
        "#!pip install torch numpy matplotlib sacrebleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfVSfjQhjScG",
        "outputId": "f526ba7e-41d7-4006-ffa0-23c64634708e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install sacrebleu\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.2MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.0.0 sacrebleu-1.4.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuBro1etmj1K",
        "outputId": "f1f47272-7b40-46b6-cf29-e3f32f437bf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install torch==1.5.0+cu92 torchvision==0.6.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.0+cu92\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torch-1.5.0%2Bcu92-cp36-cp36m-linux_x86_64.whl (603.7MB)\n",
            "\u001b[K     |████████████████████████████████| 603.7MB 31kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.0+cu92\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.6.0%2Bcu92-cp36-cp36m-linux_x86_64.whl (6.5MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5MB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0+cu92) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0+cu92) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.0+cu92) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.0+cu92 torchvision-0.6.0+cu92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Z9AThkKp79",
        "outputId": "beda79fc-5aff-4ded-c876-dbf80987f4c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install boto3\n",
        "!pip install git+git://github.com/pytorch/text spacy \n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/99/3979b617c0cbb3d7260cd3357b4a06edaa91073dd252687b7502f6678bb8/boto3-1.16.9-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/c7/ffd9653ac448eb4d1034b3422b4699b6e0a1d0550a33587f876efb14569b/botocore-1.19.9-py2.py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 8.3MB/s \n",
            "\u001b[?25hCollecting urllib3<1.26,>=1.25.4; python_version != \"3.4\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.9->boto3) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.9->boto3) (1.15.0)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.16.9 botocore-1.19.9 jmespath-0.10.0 s3transfer-0.3.3 urllib3-1.25.11\n",
            "Collecting git+git://github.com/pytorch/text\n",
            "  Cloning git://github.com/pytorch/text to /tmp/pip-req-build-meqqtqkv\n",
            "  Running command git clone -q git://github.com/pytorch/text /tmp/pip-req-build-meqqtqkv\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+97e6d1d) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+97e6d1d) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+97e6d1d) (1.5.0+cu92)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+97e6d1d) (1.18.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+97e6d1d) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+97e6d1d) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+97e6d1d) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+97e6d1d) (2020.6.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.9.0a0+97e6d1d) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.3.1)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Building wheel for torchtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchtext: filename=torchtext-0.9.0a0+97e6d1d-cp36-cp36m-linux_x86_64.whl size=7204252 sha256=5d2f17c79beb6e8e232509b2eeb4102ef101e04b9f18276abe96e6093b1190d8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9apwmnuf/wheels/39/42/ff/82f5ccbb0f30b25e14610376f5d0c67913fc05017dab59f8eb\n",
            "Successfully built torchtext\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.9.0a0+97e6d1d\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.11)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.1)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907056 sha256=7befae435a1012c99e75c51d99ead4988f64a34ffe85ad726502c77bba83b619\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7937s5gv/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx80SHMCjHUy",
        "outputId": "1f61c108-c49c-4d6a-c4ac-5370dd7f949c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from IPython.core.debugger import set_trace\n",
        "import sacrebleu\n",
        "import os\n",
        "import sys\n",
        "import io\n",
        "import glob\n",
        "import boto3\n",
        "# we will use CUDA if it is available\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE=torch.device('cuda:0') # or set to 'cpu'\n",
        "print(\"CUDA:\", USE_CUDA)\n",
        "print(DEVICE)\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA: True\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8F5ocXbE46t",
        "outputId": "b807970d-ea1e-40e4-89ca-bdc56a6cad7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv1_1k-KMPDo"
      },
      "source": [
        "sys.path.append(\"/content/drive/My Drive/EVA4/RekogNizer\")\n",
        "sys.path.append(\"/content/drive/My Drive/EVA4/\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guldkGJSjHVJ"
      },
      "source": [
        "## Full Model\n",
        "\n",
        "Here we define a function from hyperparameters to a full model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWEDpLncjHU3"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.trg_embed = trg_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
        "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
        "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
        "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask, src_lengths):\n",
        "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
        "    \n",
        "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
        "               decoder_hidden=None):\n",
        "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
        "                            src_mask, trg_mask, hidden=decoder_hidden)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbkORXbfjHU6"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpLHDTCCjHU-"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
        "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        \n",
        "    def forward(self, x, mask, lengths):\n",
        "        \"\"\"\n",
        "        Applies a bidirectional GRU to sequence of embeddings x.\n",
        "        The input mini-batch x needs to be sorted by length.\n",
        "        x should have dimensions [batch, time, dim].\n",
        "        \"\"\"\n",
        "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        output, final = self.rnn(packed)\n",
        "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
        "\n",
        "        # we need to manually concatenate the final states for both directions\n",
        "        fwd_final = final[0:final.size(0):2]\n",
        "        bwd_final = final[1:final.size(0):2]\n",
        "        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
        "\n",
        "        return output, final"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdcetW_SjHVC"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
        "    \n",
        "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
        "                 bridge=True):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.attention = attention\n",
        "        self.dropout = dropout\n",
        "                 \n",
        "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
        "                          batch_first=True, dropout=dropout)\n",
        "                 \n",
        "        # to initialize from the final encoder state\n",
        "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
        "\n",
        "        self.dropout_layer = nn.Dropout(p=dropout)\n",
        "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
        "                                          hidden_size, bias=False)\n",
        "        \n",
        "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
        "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
        "\n",
        "        # compute context vector using attention mechanism\n",
        "        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n",
        "        context, attn_probs = self.attention(\n",
        "            query=query, proj_key=proj_key,\n",
        "            value=encoder_hidden, mask=src_mask)\n",
        "\n",
        "        # update rnn hidden state\n",
        "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        \n",
        "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
        "        pre_output = self.dropout_layer(pre_output)\n",
        "        pre_output = self.pre_output_layer(pre_output)\n",
        "\n",
        "        return output, hidden, pre_output\n",
        "    \n",
        "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
        "                src_mask, trg_mask, hidden=None, max_len=None):\n",
        "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
        "                                         \n",
        "        # the maximum number of steps to unroll the RNN\n",
        "        if max_len is None:\n",
        "            max_len = trg_mask.size(-1)\n",
        "\n",
        "        # initialize decoder hidden state\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(encoder_final)\n",
        "        \n",
        "        # pre-compute projected encoder hidden states\n",
        "        # (the \"keys\" for the attention mechanism)\n",
        "        # this is only done for efficiency\n",
        "        proj_key = self.attention.key_layer(encoder_hidden)\n",
        "        \n",
        "        # here we store all intermediate hidden states and pre-output vectors\n",
        "        decoder_states = []\n",
        "        pre_output_vectors = []\n",
        "        \n",
        "        # unroll the decoder RNN for max_len steps\n",
        "        for i in range(max_len):\n",
        "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
        "            output, hidden, pre_output = self.forward_step(\n",
        "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
        "            decoder_states.append(output)\n",
        "            pre_output_vectors.append(pre_output)\n",
        "\n",
        "        decoder_states = torch.cat(decoder_states, dim=1)\n",
        "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
        "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
        "\n",
        "    def init_hidden(self, encoder_final):\n",
        "        \"\"\"Returns the initial decoder state,\n",
        "        conditioned on the final encoder state.\"\"\"\n",
        "\n",
        "        if encoder_final is None:\n",
        "            return None  # start with zeros\n",
        "\n",
        "        return torch.tanh(self.bridge(encoder_final))            \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV9MaBlLjHVG"
      },
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        \n",
        "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
        "        key_size = 2 * hidden_size if key_size is None else key_size\n",
        "        query_size = hidden_size if query_size is None else query_size\n",
        "\n",
        "\n",
        "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
        "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
        "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
        "        \n",
        "        # to store attention scores\n",
        "        self.alphas = None\n",
        "        \n",
        "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
        "        assert mask is not None, \"mask is required\"\n",
        "\n",
        "        # We first project the query (the decoder state).\n",
        "        # The projected keys (the encoder states) were already pre-computated.\n",
        "        query = self.query_layer(query)\n",
        "        \n",
        "        # Calculate scores.\n",
        "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "        \n",
        "        # Mask out invalid positions.\n",
        "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
        "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
        "        \n",
        "        # Turn scores to probabilities.\n",
        "        alphas = F.softmax(scores, dim=-1)\n",
        "        self.alphas = alphas        \n",
        "        \n",
        "        # The context vector is the weighted sum of the values.\n",
        "        context = torch.bmm(alphas, value)\n",
        "        \n",
        "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
        "        return context, alphas"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms8cRw6tjHVK"
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "\n",
        "    attention = BahdanauAttention(hidden_size)\n",
        "\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
        "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
        "        nn.Embedding(src_vocab, emb_size),\n",
        "        nn.Embedding(tgt_vocab, emb_size),\n",
        "        Generator(hidden_size, tgt_vocab))\n",
        "\n",
        "    return model.cuda() if USE_CUDA else model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps4LhN1ejHVN"
      },
      "source": [
        "# Training\n",
        "\n",
        "This section describes the training regime for our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRyTmUrsjHVN"
      },
      "source": [
        "We stop for a quick interlude to introduce some of the tools \n",
        "needed to train a standard encoder decoder model. First we define a batch object that holds the src and target sentences for training, as well as their lengths and masks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzT7L2aFjHVO"
      },
      "source": [
        "## Batches and Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6iHu4GPjHVO"
      },
      "source": [
        "class Batch:\n",
        "    \"\"\"Object for holding a batch of data with mask during training.\n",
        "    Input is a batch from a torch text iterator.\n",
        "    \"\"\"\n",
        "    def __init__(self, src, trg, pad_index=0):\n",
        "        \n",
        "        src, src_lengths = src\n",
        "        \n",
        "        self.src = src\n",
        "        self.src_lengths = src_lengths\n",
        "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
        "        self.nseqs = src.size(0)\n",
        "        \n",
        "        self.trg = None\n",
        "        self.trg_y = None\n",
        "        self.trg_mask = None\n",
        "        self.trg_lengths = None\n",
        "        self.ntokens = None\n",
        "\n",
        "        if trg is not None:\n",
        "            trg, trg_lengths = trg\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_lengths = trg_lengths\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = (self.trg_y != pad_index)\n",
        "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
        "        \n",
        "        if USE_CUDA:\n",
        "            self.src = self.src.cuda()\n",
        "            self.src_mask = self.src_mask.cuda()\n",
        "\n",
        "            if trg is not None:\n",
        "                self.trg = self.trg.cuda()\n",
        "                self.trg_y = self.trg_y.cuda()\n",
        "                self.trg_mask = self.trg_mask.cuda()\n",
        "                "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOpl9bROjHVR"
      },
      "source": [
        "## Training Loop\n",
        "The code below trains the model for 1 epoch (=1 pass through the training data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TFmLT2XjHVS"
      },
      "source": [
        "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
        "    \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    print_tokens = 0\n",
        "\n",
        "    for i, batch in enumerate(data_iter, 1):\n",
        "        \n",
        "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
        "                                           batch.src_mask, batch.trg_mask,\n",
        "                                           batch.src_lengths, batch.trg_lengths)\n",
        "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        print_tokens += batch.ntokens\n",
        "        \n",
        "        if model.training and i % print_every == 0:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
        "            start = time.time()\n",
        "            print_tokens = 0\n",
        "\n",
        "    return math.exp(total_loss / float(total_tokens))\n",
        "def data_gen(num_words=11, batch_size=16, num_batches=100, length=10, pad_index=0, sos_index=1):\n",
        "    \"\"\"Generate random data for a src-tgt copy task.\"\"\"\n",
        "    for i in range(num_batches):\n",
        "        data = torch.from_numpy(\n",
        "          np.random.randint(1, num_words, size=(batch_size, length)))\n",
        "        data[:, 0] = sos_index\n",
        "        data = data.cuda() if USE_CUDA else data\n",
        "        src = data[:, 1:]\n",
        "        trg = data\n",
        "        src_lengths = [length-1] * batch_size\n",
        "        trg_lengths = [length] * batch_size\n",
        "        yield Batch((src, src_lengths), (trg, trg_lengths), pad_index=pad_index)\n",
        "class SimpleLossCompute:\n",
        "    \"\"\"A simple loss compute and train function.\"\"\"\n",
        "\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "\n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
        "                              y.contiguous().view(-1))\n",
        "        loss = loss / norm\n",
        "\n",
        "        if self.opt is not None:\n",
        "            loss.backward()          \n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "\n",
        "        return loss.data.item() * norm"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmtHygJrjHVW"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "We will use the [Adam optimizer](https://arxiv.org/abs/1412.6980) with default settings ($\\beta_1=0.9$, $\\beta_2=0.999$ and $\\epsilon=10^{-8}$).\n",
        "\n",
        "We will use $0.0003$ as the learning rate here, but for different problems another learning rate may be more appropriate. You will have to tune that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT80h71_6IDb"
      },
      "source": [
        "torch.load(\"/content/attention_s11_new.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CITjE2MMjHVe"
      },
      "source": [
        "def plot_perplexity(perplexities):\n",
        "    \"\"\"plot perplexities\"\"\"\n",
        "    plt.title(\"Perplexity per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Perplexity\")\n",
        "    plt.plot(perplexities)\n",
        "\n",
        "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
        "    \"\"\"Greedily decode a sentence.\"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
        "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
        "        trg_mask = torch.ones_like(prev_y)\n",
        "\n",
        "    output = []\n",
        "    attention_scores = []\n",
        "    hidden = None\n",
        "\n",
        "    for i in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            out, hidden, pre_output = model.decode(\n",
        "              encoder_hidden, encoder_final, src_mask,\n",
        "              prev_y, trg_mask, hidden)\n",
        "\n",
        "            # we predict from the pre-output layer, which is\n",
        "            # a combination of Decoder state, prev emb, and context\n",
        "            prob = model.generator(pre_output[:, -1])\n",
        "\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.data.item()\n",
        "        output.append(next_word)\n",
        "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
        "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
        "    \n",
        "    output = np.array(output)\n",
        "        \n",
        "    # cut off everything starting from </s> \n",
        "    # (only when eos_index provided)\n",
        "    if eos_index is not None:\n",
        "        first_eos = np.where(output==eos_index)[0]\n",
        "        if len(first_eos) > 0:\n",
        "            output = output[:first_eos[0]]      \n",
        "    \n",
        "    return output, np.concatenate(attention_scores, axis=1)\n",
        "  \n",
        "\n",
        "def lookup_words(x, vocab=None):\n",
        "    if vocab is not None:\n",
        "        x = [vocab.itos[i] for i in x]\n",
        "\n",
        "    return [str(t) for t in x]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM7l6HFjjHVh"
      },
      "source": [
        "def print_examples(example_iter, model, n=2, max_len=100, \n",
        "                   sos_index=1, \n",
        "                   src_eos_index=None, \n",
        "                   trg_eos_index=None, \n",
        "                   src_vocab=None, trg_vocab=None):\n",
        "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    print()\n",
        "    \n",
        "    if src_vocab is not None and trg_vocab is not None:\n",
        "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
        "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
        "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
        "    else:\n",
        "        src_eos_index = None\n",
        "        trg_sos_index = 1\n",
        "        trg_eos_index = None\n",
        "        \n",
        "    for i, batch in enumerate(example_iter):\n",
        "      \n",
        "        src = batch.src.cpu().numpy()[0, :]\n",
        "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
        "\n",
        "        # remove </s> (if it is there)\n",
        "        src = src[:-1] if src[-1] == src_eos_index else src\n",
        "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
        "      \n",
        "        result, _ = greedy_decode(\n",
        "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
        "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
        "        print(\"Example #%d\" % (i+1))\n",
        "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
        "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
        "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
        "        print()\n",
        "        \n",
        "        count += 1\n",
        "        if count == n:\n",
        "            break"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r_nhmV0jHVj"
      },
      "source": [
        "## Training the copy task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ZzC732RTjHVk"
      },
      "source": [
        "def train_copy_task():\n",
        "    \"\"\"Train the simple copy task.\"\"\"\n",
        "    num_words = 11\n",
        "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=0)\n",
        "    model = make_model(num_words, num_words, emb_size=32, hidden_size=64)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
        "    eval_data = list(data_gen(num_words=num_words, batch_size=1, num_batches=100))\n",
        " \n",
        "    dev_perplexities = []\n",
        "    \n",
        "    if USE_CUDA:\n",
        "        model.cuda()\n",
        "\n",
        "    for epoch in range(10):\n",
        "        \n",
        "        print(\"Epoch %d\" % epoch)\n",
        "\n",
        "        # train\n",
        "        model.train()\n",
        "        data = data_gen(num_words=num_words, batch_size=32, num_batches=100)\n",
        "        run_epoch(data, model,\n",
        "                  SimpleLossCompute(model.generator, criterion, optim))\n",
        "\n",
        "        # evaluate\n",
        "        model.eval()\n",
        "        with torch.no_grad(): \n",
        "            perplexity = run_epoch(eval_data, model,\n",
        "                                   SimpleLossCompute(model.generator, criterion, None))\n",
        "            print(\"Evaluation perplexity: %f\" % perplexity)\n",
        "            dev_perplexities.append(perplexity)\n",
        "            print_examples(eval_data, model, n=2, max_len=9)\n",
        "        \n",
        "    return dev_perplexities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "9q9u7ThljHVm"
      },
      "source": [
        "# train the copy task\n",
        "dev_perplexities = train_copy_task()\n",
        "\n",
        "\n",
        "    \n",
        "plot_perplexity(dev_perplexities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFSGgQ2gjHVp"
      },
      "source": [
        "You can see that the model managed to correctly 'translate' the two examples in the end.\n",
        "\n",
        "Moreover, the perplexity of the development data nicely went down towards 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzHbgHcsjHVq"
      },
      "source": [
        "# A Real World Example\n",
        "\n",
        "Now we consider a real-world example using the IWSLT German-English Translation task. \n",
        "This task is much smaller than usual, but it illustrates the whole system. \n",
        "\n",
        "The cell below installs torch text and spacy. This might take a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyPF0e2ijHVt",
        "outputId": "50b7daaf-4550-48f5-f32c-acbc63a05ce2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# For data loading.\n",
        "from torchtext import data, datasets\n",
        "\n",
        "if True:\n",
        "    import spacy\n",
        "    spacy_de = spacy.load('de')\n",
        "    spacy_en = spacy.load('en')\n",
        "\n",
        "    def tokenize_de(text):\n",
        "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "    def tokenize_en(text):\n",
        "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "    UNK_TOKEN = \"<unk>\"\n",
        "    PAD_TOKEN = \"<pad>\"    \n",
        "    SOS_TOKEN = \"<s>\"\n",
        "    EOS_TOKEN = \"</s>\"\n",
        "    LOWER = True\n",
        "    \n",
        "    # we include lengths to provide to the RNNs\n",
        "    SRC = data.Field(tokenize=tokenize_de, \n",
        "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
        "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
        "    TRG = data.Field(tokenize=tokenize_en, \n",
        "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
        "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
        "\n",
        "    MAX_LEN = 25  # NOTE: we filter out a lot of sentences for speed\n",
        "    train_data, valid_data, test_data = datasets.IWSLT.splits(\n",
        "        exts=('.de', '.en'), fields=(SRC, TRG), \n",
        "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "            len(vars(x)['trg']) <= MAX_LEN)\n",
        "    MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
        "    SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
        "    TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
        "    \n",
        "    PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading de-en.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "de-en.tgz: 100%|██████████| 24.2M/24.2M [00:04<00:00, 5.43MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/train.tags.de-en.en\n",
            ".data/iwslt/de-en/train.tags.de-en.de\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUeBoHtzjHVw",
        "outputId": "fe549dcf-c079-49be-955a-20d78977aead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\n",
        "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
        "\n",
        "    print(\"Data set sizes (number of sentence pairs):\")\n",
        "    print('train', len(train_data))\n",
        "    print('valid', len(valid_data))\n",
        "    print('test', len(test_data), \"\\n\")\n",
        "\n",
        "    print(\"First training example:\")\n",
        "    print(\"src:\", \" \".join(vars(train_data[0])['src']))\n",
        "    print(\"trg:\", \" \".join(vars(train_data[0])['trg']), \"\\n\")\n",
        "\n",
        "    print(\"Most common words (src):\")\n",
        "    print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
        "    print(\"Most common words (trg):\")\n",
        "    print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
        "\n",
        "    print(\"First 10 words (src):\")\n",
        "    print(\"\\n\".join(\n",
        "        '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
        "    print(\"First 10 words (trg):\")\n",
        "    print(\"\\n\".join(\n",
        "        '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
        "\n",
        "    print(\"Number of German words (types):\", len(src_field.vocab))\n",
        "    print(\"Number of English words (types):\", len(trg_field.vocab), \"\\n\")\n",
        "    \n",
        "    \n",
        "print_data_info(train_data, valid_data, test_data, SRC, TRG)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data set sizes (number of sentence pairs):\n",
            "train 143115\n",
            "valid 690\n",
            "test 963 \n",
            "\n",
            "First training example:\n",
            "src: david gallo : das ist bill lange . ich bin dave gallo .\n",
            "trg: david gallo : this is bill lange . i 'm dave gallo . \n",
            "\n",
            "Most common words (src):\n",
            "         .     138329\n",
            "         ,     105944\n",
            "       und      41843\n",
            "       die      40808\n",
            "       das      33324\n",
            "       sie      33034\n",
            "       ich      31150\n",
            "       ist      31037\n",
            "        es      27449\n",
            "       wir      25817 \n",
            "\n",
            "Most common words (trg):\n",
            "         .     137259\n",
            "         ,      91615\n",
            "       the      73343\n",
            "       and      50276\n",
            "        to      42799\n",
            "         a      39572\n",
            "        of      39496\n",
            "         i      33521\n",
            "        it      32920\n",
            "      that      32640 \n",
            "\n",
            "First 10 words (src):\n",
            "00 <unk>\n",
            "01 <pad>\n",
            "02 </s>\n",
            "03 .\n",
            "04 ,\n",
            "05 und\n",
            "06 die\n",
            "07 das\n",
            "08 sie\n",
            "09 ich \n",
            "\n",
            "First 10 words (trg):\n",
            "00 <unk>\n",
            "01 <pad>\n",
            "02 <s>\n",
            "03 </s>\n",
            "04 .\n",
            "05 ,\n",
            "06 the\n",
            "07 and\n",
            "08 to\n",
            "09 a \n",
            "\n",
            "Number of German words (types): 15765\n",
            "Number of English words (types): 13002 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrTOR8UrjHV0",
        "outputId": "a0a64f54-d0da-48ca-be82-cadf7a01c2be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_iter = data.BucketIterator(train_data, batch_size=64, train=True, \n",
        "                                 sort_within_batch=True, \n",
        "                                 sort_key=lambda x: (len(x.src), len(x.trg)), repeat=False,\n",
        "                                 device=DEVICE)\n",
        "valid_iter = data.Iterator(valid_data, batch_size=1, train=False, sort=False, repeat=False, \n",
        "                           device=DEVICE)\n",
        "\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"\"\"Wrap torchtext batch into our own Batch class for pre-processing\"\"\"\n",
        "    return Batch(batch.src, batch.trg, pad_idx)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y5jizPvjHV2"
      },
      "source": [
        "## Training the System\n",
        "\n",
        "Now we train the model. \n",
        "\n",
        "On a Titan X GPU, this runs at ~18,000 tokens per second with a batch size of 64."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAUbXRmFjHV3"
      },
      "source": [
        "def train(model, num_epochs=10, lr=0.0003, print_every=100):\n",
        "    \"\"\"Train a model on IWSLT\"\"\"\n",
        "    \n",
        "    if USE_CUDA:\n",
        "        model.cuda()\n",
        "\n",
        "    # optionally add label smoothing; see the Annotated Transformer\n",
        "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    dev_perplexities = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      \n",
        "        print(\"Epoch\", epoch)\n",
        "        model.train()\n",
        "        train_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in train_iter), \n",
        "                                     model,\n",
        "                                     SimpleLossCompute(model.generator, criterion, optim),\n",
        "                                     print_every=print_every)\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            print_examples((rebatch(PAD_INDEX, x) for x in valid_iter), \n",
        "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)        \n",
        "\n",
        "            dev_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in valid_iter), \n",
        "                                       model, \n",
        "                                       SimpleLossCompute(model.generator, criterion, None))\n",
        "            print(\"Validation perplexity: %f\" % dev_perplexity)\n",
        "            dev_perplexities.append(dev_perplexity)\n",
        "        \n",
        "    return dev_perplexities\n",
        "        "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0CZ3yprZgTl",
        "outputId": "6afba6cd-b98a-481a-9c56-148cdfba48ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
        "                   emb_size=256, hidden_size=256,\n",
        "                   num_layers=1, dropout=0.2)\n",
        "dev_perplexities = train(model, print_every=100,num_epochs=40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 100 Loss: 52.758408 Tokens per Sec: 26225.081144\n",
            "Epoch Step: 200 Loss: 73.788116 Tokens per Sec: 28199.162658\n",
            "Epoch Step: 300 Loss: 26.435041 Tokens per Sec: 28276.119648\n",
            "Epoch Step: 400 Loss: 42.796860 Tokens per Sec: 28118.784847\n",
            "Epoch Step: 500 Loss: 33.503792 Tokens per Sec: 28368.577241\n",
            "Epoch Step: 600 Loss: 62.243206 Tokens per Sec: 27600.858441\n",
            "Epoch Step: 700 Loss: 33.888512 Tokens per Sec: 28136.415754\n",
            "Epoch Step: 800 Loss: 13.238980 Tokens per Sec: 27817.317703\n",
            "Epoch Step: 900 Loss: 31.219894 Tokens per Sec: 27897.772714\n",
            "Epoch Step: 1000 Loss: 91.953209 Tokens per Sec: 27076.136394\n",
            "Epoch Step: 1100 Loss: 58.344067 Tokens per Sec: 27549.053975\n",
            "Epoch Step: 1200 Loss: 41.832642 Tokens per Sec: 27028.820668\n",
            "Epoch Step: 1300 Loss: 104.536095 Tokens per Sec: 27225.955834\n",
            "Epoch Step: 1400 Loss: 98.792259 Tokens per Sec: 27837.841108\n",
            "Epoch Step: 1500 Loss: 55.546982 Tokens per Sec: 28013.454281\n",
            "Epoch Step: 1600 Loss: 38.666183 Tokens per Sec: 27801.779761\n",
            "Epoch Step: 1700 Loss: 29.173876 Tokens per Sec: 27838.045363\n",
            "Epoch Step: 1800 Loss: 78.605072 Tokens per Sec: 27696.113024\n",
            "Epoch Step: 1900 Loss: 18.532665 Tokens per Sec: 27755.538497\n",
            "Epoch Step: 2000 Loss: 93.569626 Tokens per Sec: 28316.561630\n",
            "Epoch Step: 2100 Loss: 20.792868 Tokens per Sec: 27995.135107\n",
            "Epoch Step: 2200 Loss: 69.604691 Tokens per Sec: 28173.349015\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was years old , i was a <unk> of the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father had to be on a little bit of the <unk> , the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he was very much , what was pretty much more than the <unk> , it was the <unk> .\n",
            "\n",
            "Validation perplexity: 32.166477\n",
            "Epoch 1\n",
            "Epoch Step: 100 Loss: 51.876453 Tokens per Sec: 26297.321449\n",
            "Epoch Step: 200 Loss: 25.874340 Tokens per Sec: 28186.448556\n",
            "Epoch Step: 300 Loss: 55.469940 Tokens per Sec: 28220.732734\n",
            "Epoch Step: 400 Loss: 54.866840 Tokens per Sec: 27782.004047\n",
            "Epoch Step: 500 Loss: 87.835976 Tokens per Sec: 27986.645711\n",
            "Epoch Step: 600 Loss: 49.571495 Tokens per Sec: 27519.795518\n",
            "Epoch Step: 700 Loss: 58.503372 Tokens per Sec: 27914.534798\n",
            "Epoch Step: 800 Loss: 54.617542 Tokens per Sec: 27989.872646\n",
            "Epoch Step: 900 Loss: 64.276314 Tokens per Sec: 27454.747484\n",
            "Epoch Step: 1000 Loss: 26.332340 Tokens per Sec: 27442.841499\n",
            "Epoch Step: 1100 Loss: 23.029425 Tokens per Sec: 27652.823813\n",
            "Epoch Step: 1200 Loss: 83.728210 Tokens per Sec: 27827.068814\n",
            "Epoch Step: 1300 Loss: 9.946512 Tokens per Sec: 27912.014503\n",
            "Epoch Step: 1400 Loss: 34.845322 Tokens per Sec: 27356.539449\n",
            "Epoch Step: 1500 Loss: 74.865417 Tokens per Sec: 27877.161143\n",
            "Epoch Step: 1600 Loss: 61.827148 Tokens per Sec: 27252.650290\n",
            "Epoch Step: 1700 Loss: 18.545334 Tokens per Sec: 27333.537119\n",
            "Epoch Step: 1800 Loss: 62.639534 Tokens per Sec: 28000.389343\n",
            "Epoch Step: 1900 Loss: 88.101212 Tokens per Sec: 27442.386897\n",
            "Epoch Step: 2000 Loss: 10.831727 Tokens per Sec: 28268.924632\n",
            "Epoch Step: 2100 Loss: 17.706079 Tokens per Sec: 27750.544133\n",
            "Epoch Step: 2200 Loss: 41.914387 Tokens per Sec: 27331.009764\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father had to go on his little , <unk> , the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , what was pretty much of the <unk> , because it 's the news that 's the <unk> .\n",
            "\n",
            "Validation perplexity: 19.708223\n",
            "Epoch 2\n",
            "Epoch Step: 100 Loss: 41.051884 Tokens per Sec: 26654.388309\n",
            "Epoch Step: 200 Loss: 50.427345 Tokens per Sec: 28065.646984\n",
            "Epoch Step: 300 Loss: 51.466274 Tokens per Sec: 27878.237020\n",
            "Epoch Step: 400 Loss: 38.070751 Tokens per Sec: 28021.525686\n",
            "Epoch Step: 500 Loss: 60.590508 Tokens per Sec: 27644.249944\n",
            "Epoch Step: 600 Loss: 49.453262 Tokens per Sec: 27662.111339\n",
            "Epoch Step: 700 Loss: 17.118881 Tokens per Sec: 27854.373087\n",
            "Epoch Step: 800 Loss: 43.249874 Tokens per Sec: 27598.575003\n",
            "Epoch Step: 900 Loss: 5.196908 Tokens per Sec: 28032.030193\n",
            "Epoch Step: 1000 Loss: 36.210857 Tokens per Sec: 27838.711608\n",
            "Epoch Step: 1100 Loss: 27.625404 Tokens per Sec: 27528.736200\n",
            "Epoch Step: 1200 Loss: 13.047630 Tokens per Sec: 27928.811065\n",
            "Epoch Step: 1300 Loss: 54.445892 Tokens per Sec: 28204.923293\n",
            "Epoch Step: 1400 Loss: 22.704256 Tokens per Sec: 28301.745055\n",
            "Epoch Step: 1500 Loss: 71.689178 Tokens per Sec: 27806.065996\n",
            "Epoch Step: 1600 Loss: 40.749855 Tokens per Sec: 27959.395166\n",
            "Epoch Step: 1700 Loss: 37.117687 Tokens per Sec: 28123.099559\n",
            "Epoch Step: 1800 Loss: 55.881332 Tokens per Sec: 28287.084595\n",
            "Epoch Step: 1900 Loss: 43.175144 Tokens per Sec: 27601.117378\n",
            "Epoch Step: 2000 Loss: 46.842331 Tokens per Sec: 28043.009665\n",
            "Epoch Step: 2100 Loss: 37.356777 Tokens per Sec: 28049.941293\n",
            "Epoch Step: 2200 Loss: 36.677841 Tokens per Sec: 28518.548029\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my dad heard on his little , <unk> , the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty much of the very unusual , because it 's the news of the <unk> .\n",
            "\n",
            "Validation perplexity: 15.072062\n",
            "Epoch 3\n",
            "Epoch Step: 100 Loss: 28.902145 Tokens per Sec: 26582.997340\n",
            "Epoch Step: 200 Loss: 7.616714 Tokens per Sec: 27787.446964\n",
            "Epoch Step: 300 Loss: 40.384979 Tokens per Sec: 28005.839440\n",
            "Epoch Step: 400 Loss: 53.031818 Tokens per Sec: 27873.738753\n",
            "Epoch Step: 500 Loss: 32.178886 Tokens per Sec: 28080.125231\n",
            "Epoch Step: 600 Loss: 28.598713 Tokens per Sec: 28304.225030\n",
            "Epoch Step: 700 Loss: 27.887161 Tokens per Sec: 28325.036786\n",
            "Epoch Step: 800 Loss: 30.577408 Tokens per Sec: 27970.595448\n",
            "Epoch Step: 900 Loss: 70.488716 Tokens per Sec: 27954.345812\n",
            "Epoch Step: 1000 Loss: 16.499945 Tokens per Sec: 27673.439511\n",
            "Epoch Step: 1100 Loss: 21.162668 Tokens per Sec: 28113.805759\n",
            "Epoch Step: 1200 Loss: 25.000948 Tokens per Sec: 28272.564008\n",
            "Epoch Step: 1300 Loss: 58.569557 Tokens per Sec: 28075.336476\n",
            "Epoch Step: 1400 Loss: 68.234116 Tokens per Sec: 27737.353154\n",
            "Epoch Step: 1500 Loss: 54.764046 Tokens per Sec: 28024.474523\n",
            "Epoch Step: 1600 Loss: 60.288853 Tokens per Sec: 27938.448110\n",
            "Epoch Step: 1700 Loss: 40.619999 Tokens per Sec: 27882.148337\n",
            "Epoch Step: 1800 Loss: 7.960891 Tokens per Sec: 27970.775933\n",
            "Epoch Step: 1900 Loss: 43.804016 Tokens per Sec: 27822.062769\n",
            "Epoch Step: 2000 Loss: 33.677036 Tokens per Sec: 27612.726633\n",
            "Epoch Step: 2100 Loss: 32.277405 Tokens per Sec: 27687.230516\n",
            "Epoch Step: 2200 Loss: 25.044931 Tokens per Sec: 28160.676044\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> of the <unk> of joy .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father listened to his little , the radio - <unk> , the radio - <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very happy , which was the very unusual , because the news is <unk> .\n",
            "\n",
            "Validation perplexity: 13.286194\n",
            "Epoch 4\n",
            "Epoch Step: 100 Loss: 45.044338 Tokens per Sec: 26748.632555\n",
            "Epoch Step: 200 Loss: 42.850483 Tokens per Sec: 27817.826325\n",
            "Epoch Step: 300 Loss: 8.235190 Tokens per Sec: 27952.780966\n",
            "Epoch Step: 400 Loss: 44.219658 Tokens per Sec: 27378.464146\n",
            "Epoch Step: 500 Loss: 64.018677 Tokens per Sec: 26993.715967\n",
            "Epoch Step: 600 Loss: 9.979264 Tokens per Sec: 27128.501720\n",
            "Epoch Step: 700 Loss: 50.958012 Tokens per Sec: 27087.710560\n",
            "Epoch Step: 800 Loss: 42.585133 Tokens per Sec: 27916.328963\n",
            "Epoch Step: 900 Loss: 23.110092 Tokens per Sec: 27906.831807\n",
            "Epoch Step: 1000 Loss: 22.902403 Tokens per Sec: 28378.401121\n",
            "Epoch Step: 1100 Loss: 30.892487 Tokens per Sec: 28479.581637\n",
            "Epoch Step: 1200 Loss: 26.389181 Tokens per Sec: 28378.737729\n",
            "Epoch Step: 1300 Loss: 34.722225 Tokens per Sec: 27817.240244\n",
            "Epoch Step: 1400 Loss: 47.419228 Tokens per Sec: 28096.342619\n",
            "Epoch Step: 1500 Loss: 33.225098 Tokens per Sec: 28087.027260\n",
            "Epoch Step: 1600 Loss: 21.379074 Tokens per Sec: 28251.940804\n",
            "Epoch Step: 1700 Loss: 24.610294 Tokens per Sec: 28241.610626\n",
            "Epoch Step: 1800 Loss: 29.803442 Tokens per Sec: 28071.119612\n",
            "Epoch Step: 1900 Loss: 25.555071 Tokens per Sec: 28330.416331\n",
            "Epoch Step: 2000 Loss: 35.917675 Tokens per Sec: 28311.818554\n",
            "Epoch Step: 2100 Loss: 23.567841 Tokens per Sec: 28056.274837\n",
            "Epoch Step: 2200 Loss: 59.127552 Tokens per Sec: 27916.047927\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> of the <unk> of joy .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard on his little , radio radio shack , the radio waves of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was very early , because the news was <unk> .\n",
            "\n",
            "Validation perplexity: 12.701202\n",
            "Epoch 5\n",
            "Epoch Step: 100 Loss: 24.219587 Tokens per Sec: 26866.682982\n",
            "Epoch Step: 200 Loss: 13.002197 Tokens per Sec: 28340.938379\n",
            "Epoch Step: 300 Loss: 18.604992 Tokens per Sec: 27795.959258\n",
            "Epoch Step: 400 Loss: 26.313707 Tokens per Sec: 27904.560012\n",
            "Epoch Step: 500 Loss: 55.615353 Tokens per Sec: 28212.382405\n",
            "Epoch Step: 600 Loss: 55.521847 Tokens per Sec: 28190.274976\n",
            "Epoch Step: 700 Loss: 9.048133 Tokens per Sec: 27546.081525\n",
            "Epoch Step: 800 Loss: 27.678268 Tokens per Sec: 27169.573724\n",
            "Epoch Step: 900 Loss: 12.144869 Tokens per Sec: 27307.275336\n",
            "Epoch Step: 1000 Loss: 38.305576 Tokens per Sec: 27713.161073\n",
            "Epoch Step: 1100 Loss: 40.992435 Tokens per Sec: 27563.615184\n",
            "Epoch Step: 1200 Loss: 34.124535 Tokens per Sec: 28097.038139\n",
            "Epoch Step: 1300 Loss: 49.977016 Tokens per Sec: 27293.167727\n",
            "Epoch Step: 1400 Loss: 11.202545 Tokens per Sec: 28068.211454\n",
            "Epoch Step: 1500 Loss: 50.708332 Tokens per Sec: 28365.003285\n",
            "Epoch Step: 1600 Loss: 6.826338 Tokens per Sec: 28295.087444\n",
            "Epoch Step: 1700 Loss: 28.750349 Tokens per Sec: 28099.500100\n",
            "Epoch Step: 1800 Loss: 55.970913 Tokens per Sec: 28294.173020\n",
            "Epoch Step: 1900 Loss: 61.625130 Tokens per Sec: 28245.733897\n",
            "Epoch Step: 2000 Loss: 32.353268 Tokens per Sec: 28612.956839\n",
            "Epoch Step: 2100 Loss: 7.970485 Tokens per Sec: 28755.348613\n",
            "Epoch Step: 2200 Loss: 47.063488 Tokens per Sec: 28587.166342\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard on his little , the radio shack was the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty much , and the <unk> was the most expensive disease .\n",
            "\n",
            "Validation perplexity: 11.969292\n",
            "Epoch 6\n",
            "Epoch Step: 100 Loss: 51.729111 Tokens per Sec: 26859.931408\n",
            "Epoch Step: 200 Loss: 5.565504 Tokens per Sec: 28458.485617\n",
            "Epoch Step: 300 Loss: 44.330040 Tokens per Sec: 27767.040000\n",
            "Epoch Step: 400 Loss: 49.478436 Tokens per Sec: 28360.044479\n",
            "Epoch Step: 500 Loss: 23.627007 Tokens per Sec: 28518.042492\n",
            "Epoch Step: 600 Loss: 57.603710 Tokens per Sec: 28239.311557\n",
            "Epoch Step: 700 Loss: 28.549862 Tokens per Sec: 28173.302719\n",
            "Epoch Step: 800 Loss: 54.186073 Tokens per Sec: 28309.290851\n",
            "Epoch Step: 900 Loss: 52.141682 Tokens per Sec: 28191.925248\n",
            "Epoch Step: 1000 Loss: 56.970387 Tokens per Sec: 28221.630195\n",
            "Epoch Step: 1100 Loss: 35.553162 Tokens per Sec: 28715.184586\n",
            "Epoch Step: 1200 Loss: 53.183170 Tokens per Sec: 28258.716216\n",
            "Epoch Step: 1300 Loss: 15.666752 Tokens per Sec: 28527.544843\n",
            "Epoch Step: 1400 Loss: 34.552917 Tokens per Sec: 28636.371955\n",
            "Epoch Step: 1500 Loss: 16.982391 Tokens per Sec: 28767.799103\n",
            "Epoch Step: 1600 Loss: 34.018864 Tokens per Sec: 28659.795488\n",
            "Epoch Step: 1700 Loss: 28.538139 Tokens per Sec: 28847.482268\n",
            "Epoch Step: 1800 Loss: 22.731863 Tokens per Sec: 28773.956529\n",
            "Epoch Step: 1900 Loss: 42.615608 Tokens per Sec: 28701.671895\n",
            "Epoch Step: 2000 Loss: 29.118237 Tokens per Sec: 28559.436011\n",
            "Epoch Step: 2100 Loss: 50.953022 Tokens per Sec: 28876.596416\n",
            "Epoch Step: 2200 Loss: 18.628326 Tokens per Sec: 28713.209044\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> <unk> of joy .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard on his little , radio shack , the radio waves .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw a happy about what was pretty much at the time , because it 's the news - <unk> .\n",
            "\n",
            "Validation perplexity: 11.649135\n",
            "Epoch 7\n",
            "Epoch Step: 100 Loss: 52.394753 Tokens per Sec: 27328.071006\n",
            "Epoch Step: 200 Loss: 39.049881 Tokens per Sec: 28532.241064\n",
            "Epoch Step: 300 Loss: 16.806885 Tokens per Sec: 28332.019560\n",
            "Epoch Step: 400 Loss: 35.453796 Tokens per Sec: 29158.486526\n",
            "Epoch Step: 500 Loss: 23.252001 Tokens per Sec: 28670.256915\n",
            "Epoch Step: 600 Loss: 7.889039 Tokens per Sec: 28894.272572\n",
            "Epoch Step: 700 Loss: 41.740917 Tokens per Sec: 28407.998057\n",
            "Epoch Step: 800 Loss: 33.014465 Tokens per Sec: 28031.161639\n",
            "Epoch Step: 900 Loss: 29.691679 Tokens per Sec: 28547.633670\n",
            "Epoch Step: 1000 Loss: 47.338085 Tokens per Sec: 28874.613427\n",
            "Epoch Step: 1100 Loss: 7.139932 Tokens per Sec: 28846.555109\n",
            "Epoch Step: 1200 Loss: 50.792271 Tokens per Sec: 29094.432414\n",
            "Epoch Step: 1300 Loss: 55.194435 Tokens per Sec: 28738.641491\n",
            "Epoch Step: 1400 Loss: 13.040042 Tokens per Sec: 28661.958231\n",
            "Epoch Step: 1500 Loss: 47.542213 Tokens per Sec: 28968.820844\n",
            "Epoch Step: 1600 Loss: 57.895416 Tokens per Sec: 29247.452736\n",
            "Epoch Step: 1700 Loss: 32.440983 Tokens per Sec: 28731.952741\n",
            "Epoch Step: 1800 Loss: 16.254347 Tokens per Sec: 29059.257424\n",
            "Epoch Step: 1900 Loss: 22.513477 Tokens per Sec: 28902.126282\n",
            "Epoch Step: 2000 Loss: 12.260516 Tokens per Sec: 29253.247973\n",
            "Epoch Step: 2100 Loss: 35.916428 Tokens per Sec: 29034.571831\n",
            "Epoch Step: 2200 Loss: 56.597118 Tokens per Sec: 29369.936529\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard on his little , gray radio shack of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he was very happy , which was pretty staggering , and there was the news mostly <unk> .\n",
            "\n",
            "Validation perplexity: 11.766433\n",
            "Epoch 8\n",
            "Epoch Step: 100 Loss: 15.802040 Tokens per Sec: 26428.868134\n",
            "Epoch Step: 200 Loss: 26.309013 Tokens per Sec: 28367.419098\n",
            "Epoch Step: 300 Loss: 10.257086 Tokens per Sec: 28247.776786\n",
            "Epoch Step: 400 Loss: 17.093157 Tokens per Sec: 28875.381311\n",
            "Epoch Step: 500 Loss: 49.149776 Tokens per Sec: 29182.054394\n",
            "Epoch Step: 600 Loss: 12.997840 Tokens per Sec: 28721.856977\n",
            "Epoch Step: 700 Loss: 31.770882 Tokens per Sec: 28542.852051\n",
            "Epoch Step: 800 Loss: 23.737764 Tokens per Sec: 28480.267855\n",
            "Epoch Step: 900 Loss: 26.133385 Tokens per Sec: 28224.798984\n",
            "Epoch Step: 1000 Loss: 44.879776 Tokens per Sec: 28881.210349\n",
            "Epoch Step: 1100 Loss: 40.851578 Tokens per Sec: 28737.321454\n",
            "Epoch Step: 1200 Loss: 28.873964 Tokens per Sec: 28761.450624\n",
            "Epoch Step: 1300 Loss: 30.238098 Tokens per Sec: 28717.118837\n",
            "Epoch Step: 1400 Loss: 33.139519 Tokens per Sec: 28659.332081\n",
            "Epoch Step: 1500 Loss: 11.417410 Tokens per Sec: 28640.173156\n",
            "Epoch Step: 1600 Loss: 13.903357 Tokens per Sec: 28847.027485\n",
            "Epoch Step: 1700 Loss: 5.199735 Tokens per Sec: 29048.227194\n",
            "Epoch Step: 1800 Loss: 58.356564 Tokens per Sec: 28554.374795\n",
            "Epoch Step: 1900 Loss: 51.664749 Tokens per Sec: 28857.766127\n",
            "Epoch Step: 2000 Loss: 19.983635 Tokens per Sec: 28781.038930\n",
            "Epoch Step: 2100 Loss: 47.913956 Tokens per Sec: 29077.984921\n",
            "Epoch Step: 2200 Loss: 1.908404 Tokens per Sec: 28738.147304\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> <unk> , i 'm going to be a <unk> in the morning .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard on his little , <unk> radio shack .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very happy about what was pretty unusual , because the news was mostly the news .\n",
            "\n",
            "Validation perplexity: 11.692098\n",
            "Epoch 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "h69msy3AjHV5",
        "outputId": "2b611af4-30df-468d-f0cd-407992182a37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
        "                   emb_size=256, hidden_size=256,\n",
        "                   num_layers=1, dropout=0.2)\n",
        "dev_perplexities = train(model, print_every=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 100 Loss: 130.834930 Tokens per Sec: 27548.226392\n",
            "Epoch Step: 200 Loss: 89.320900 Tokens per Sec: 29081.990985\n",
            "Epoch Step: 300 Loss: 12.831619 Tokens per Sec: 29717.777041\n",
            "Epoch Step: 400 Loss: 49.309677 Tokens per Sec: 29005.374845\n",
            "Epoch Step: 500 Loss: 51.600765 Tokens per Sec: 29171.152785\n",
            "Epoch Step: 600 Loss: 28.300226 Tokens per Sec: 29289.414841\n",
            "Epoch Step: 700 Loss: 83.725327 Tokens per Sec: 28761.485103\n",
            "Epoch Step: 800 Loss: 101.949692 Tokens per Sec: 28399.194288\n",
            "Epoch Step: 900 Loss: 100.176270 Tokens per Sec: 28842.431339\n",
            "Epoch Step: 1000 Loss: 109.360718 Tokens per Sec: 28900.006790\n",
            "Epoch Step: 1100 Loss: 29.438190 Tokens per Sec: 28994.195425\n",
            "Epoch Step: 1200 Loss: 70.065575 Tokens per Sec: 28895.440701\n",
            "Epoch Step: 1300 Loss: 27.028048 Tokens per Sec: 29035.224079\n",
            "Epoch Step: 1400 Loss: 37.210262 Tokens per Sec: 28928.172939\n",
            "Epoch Step: 1500 Loss: 54.257084 Tokens per Sec: 28972.660540\n",
            "Epoch Step: 1600 Loss: 100.972443 Tokens per Sec: 29374.648998\n",
            "Epoch Step: 1700 Loss: 96.968513 Tokens per Sec: 29003.264793\n",
            "Epoch Step: 1800 Loss: 97.380737 Tokens per Sec: 29004.204050\n",
            "Epoch Step: 1900 Loss: 20.421162 Tokens per Sec: 29135.515534\n",
            "Epoch Step: 2000 Loss: 63.934834 Tokens per Sec: 28624.925214\n",
            "Epoch Step: 2100 Loss: 42.685169 Tokens per Sec: 29262.344769\n",
            "Epoch Step: 2200 Loss: 57.756718 Tokens per Sec: 29268.634378\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i went to years old , i was a <unk> of the <unk> of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was a little bit of the <unk> , the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he 's very interesting , what was pretty much , which was n't really the most of the most important .\n",
            "\n",
            "Validation perplexity: 31.568197\n",
            "Epoch 1\n",
            "Epoch Step: 100 Loss: 81.039932 Tokens per Sec: 27803.141672\n",
            "Epoch Step: 200 Loss: 34.011436 Tokens per Sec: 29275.235065\n",
            "Epoch Step: 300 Loss: 84.508476 Tokens per Sec: 29151.716593\n",
            "Epoch Step: 400 Loss: 90.400902 Tokens per Sec: 29348.867763\n",
            "Epoch Step: 500 Loss: 23.072495 Tokens per Sec: 28950.460677\n",
            "Epoch Step: 600 Loss: 45.271053 Tokens per Sec: 29022.970407\n",
            "Epoch Step: 700 Loss: 23.936327 Tokens per Sec: 28794.833669\n",
            "Epoch Step: 800 Loss: 48.653858 Tokens per Sec: 28819.570931\n",
            "Epoch Step: 900 Loss: 44.730659 Tokens per Sec: 29179.245355\n",
            "Epoch Step: 1000 Loss: 87.711395 Tokens per Sec: 29312.305360\n",
            "Epoch Step: 1100 Loss: 88.490601 Tokens per Sec: 29430.453308\n",
            "Epoch Step: 1200 Loss: 31.719570 Tokens per Sec: 28838.102744\n",
            "Epoch Step: 1300 Loss: 22.390854 Tokens per Sec: 28909.887412\n",
            "Epoch Step: 1400 Loss: 27.137877 Tokens per Sec: 29235.557803\n",
            "Epoch Step: 1500 Loss: 51.444481 Tokens per Sec: 28873.770988\n",
            "Epoch Step: 1600 Loss: 63.179691 Tokens per Sec: 28991.076350\n",
            "Epoch Step: 1700 Loss: 61.332970 Tokens per Sec: 28880.741531\n",
            "Epoch Step: 1800 Loss: 83.198654 Tokens per Sec: 29029.426446\n",
            "Epoch Step: 1900 Loss: 49.988155 Tokens per Sec: 28659.231638\n",
            "Epoch Step: 2000 Loss: 10.409410 Tokens per Sec: 29182.284132\n",
            "Epoch Step: 2100 Loss: 53.123791 Tokens per Sec: 28882.506432\n",
            "Epoch Step: 2200 Loss: 27.112932 Tokens per Sec: 28695.844300\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was one of the <unk> of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little , the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very happy from what was really pretty much , because it was the <unk> of the <unk> .\n",
            "\n",
            "Validation perplexity: 19.680761\n",
            "Epoch 2\n",
            "Epoch Step: 100 Loss: 49.917435 Tokens per Sec: 27648.322187\n",
            "Epoch Step: 200 Loss: 82.050797 Tokens per Sec: 28714.249867\n",
            "Epoch Step: 300 Loss: 76.888496 Tokens per Sec: 29433.127338\n",
            "Epoch Step: 400 Loss: 22.368706 Tokens per Sec: 28718.850185\n",
            "Epoch Step: 500 Loss: 59.879726 Tokens per Sec: 29023.497979\n",
            "Epoch Step: 600 Loss: 65.277306 Tokens per Sec: 28978.849693\n",
            "Epoch Step: 700 Loss: 23.857601 Tokens per Sec: 29130.192232\n",
            "Epoch Step: 800 Loss: 61.142895 Tokens per Sec: 29062.124308\n",
            "Epoch Step: 900 Loss: 57.187092 Tokens per Sec: 28776.398783\n",
            "Epoch Step: 1000 Loss: 75.192001 Tokens per Sec: 29249.892239\n",
            "Epoch Step: 1100 Loss: 53.979927 Tokens per Sec: 28917.811118\n",
            "Epoch Step: 1200 Loss: 34.266808 Tokens per Sec: 28942.416997\n",
            "Epoch Step: 1300 Loss: 34.602875 Tokens per Sec: 29227.154178\n",
            "Epoch Step: 1400 Loss: 16.447834 Tokens per Sec: 29059.115597\n",
            "Epoch Step: 1500 Loss: 16.658573 Tokens per Sec: 29406.909171\n",
            "Epoch Step: 1600 Loss: 50.670712 Tokens per Sec: 29092.148338\n",
            "Epoch Step: 1700 Loss: 19.827473 Tokens per Sec: 29298.539595\n",
            "Epoch Step: 1800 Loss: 70.591362 Tokens per Sec: 28979.459521\n",
            "Epoch Step: 1900 Loss: 40.514297 Tokens per Sec: 28939.922867\n",
            "Epoch Step: 2000 Loss: 64.188049 Tokens per Sec: 29051.614889\n",
            "Epoch Step: 2100 Loss: 36.543110 Tokens per Sec: 28782.539398\n",
            "Epoch Step: 2200 Loss: 36.545444 Tokens per Sec: 29184.910525\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> from the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father stopped on his little , my <unk> <unk> the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy from what was really unusual , because it was the <unk> of the <unk> .\n",
            "\n",
            "Validation perplexity: 15.192839\n",
            "Epoch 3\n",
            "Epoch Step: 100 Loss: 59.959290 Tokens per Sec: 27526.635054\n",
            "Epoch Step: 200 Loss: 16.845873 Tokens per Sec: 29225.811709\n",
            "Epoch Step: 300 Loss: 18.204269 Tokens per Sec: 29224.955556\n",
            "Epoch Step: 400 Loss: 36.765743 Tokens per Sec: 29192.266435\n",
            "Epoch Step: 500 Loss: 17.121929 Tokens per Sec: 29121.762623\n",
            "Epoch Step: 600 Loss: 34.920578 Tokens per Sec: 29049.866863\n",
            "Epoch Step: 700 Loss: 54.098759 Tokens per Sec: 29147.146654\n",
            "Epoch Step: 800 Loss: 38.194176 Tokens per Sec: 28498.399663\n",
            "Epoch Step: 900 Loss: 12.713120 Tokens per Sec: 28565.224386\n",
            "Epoch Step: 1000 Loss: 41.075680 Tokens per Sec: 28243.162676\n",
            "Epoch Step: 1100 Loss: 49.752132 Tokens per Sec: 28205.800169\n",
            "Epoch Step: 1200 Loss: 25.035955 Tokens per Sec: 28310.086471\n",
            "Epoch Step: 1300 Loss: 48.902443 Tokens per Sec: 28930.509761\n",
            "Epoch Step: 1400 Loss: 50.566528 Tokens per Sec: 29009.975355\n",
            "Epoch Step: 1500 Loss: 11.488853 Tokens per Sec: 28951.004242\n",
            "Epoch Step: 1600 Loss: 42.559059 Tokens per Sec: 29387.456110\n",
            "Epoch Step: 1700 Loss: 15.837053 Tokens per Sec: 29201.580573\n",
            "Epoch Step: 1800 Loss: 55.358662 Tokens per Sec: 28634.285032\n",
            "Epoch Step: 1900 Loss: 46.570595 Tokens per Sec: 28519.679389\n",
            "Epoch Step: 2000 Loss: 15.354738 Tokens per Sec: 28358.271973\n",
            "Epoch Step: 2100 Loss: 35.570103 Tokens per Sec: 28637.192441\n",
            "Epoch Step: 2200 Loss: 18.375294 Tokens per Sec: 29215.084182\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> of happiness .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard on his little , my radio <unk> the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very carefully , which was really unusual , because it was the <unk> of the <unk> .\n",
            "\n",
            "Validation perplexity: 13.547883\n",
            "Epoch 4\n",
            "Epoch Step: 100 Loss: 28.695009 Tokens per Sec: 27742.193678\n",
            "Epoch Step: 200 Loss: 33.912121 Tokens per Sec: 28624.965957\n",
            "Epoch Step: 300 Loss: 30.031982 Tokens per Sec: 29082.895302\n",
            "Epoch Step: 400 Loss: 54.361637 Tokens per Sec: 28374.595124\n",
            "Epoch Step: 500 Loss: 58.241276 Tokens per Sec: 28734.448793\n",
            "Epoch Step: 600 Loss: 31.601265 Tokens per Sec: 28760.806530\n",
            "Epoch Step: 700 Loss: 31.098164 Tokens per Sec: 28930.592329\n",
            "Epoch Step: 800 Loss: 38.043594 Tokens per Sec: 28808.559972\n",
            "Epoch Step: 900 Loss: 32.440044 Tokens per Sec: 28662.493358\n",
            "Epoch Step: 1000 Loss: 47.995056 Tokens per Sec: 29278.205871\n",
            "Epoch Step: 1100 Loss: 40.889374 Tokens per Sec: 28635.557959\n",
            "Epoch Step: 1200 Loss: 49.481934 Tokens per Sec: 28636.001516\n",
            "Epoch Step: 1300 Loss: 30.986576 Tokens per Sec: 29238.389971\n",
            "Epoch Step: 1400 Loss: 47.095505 Tokens per Sec: 28774.447574\n",
            "Epoch Step: 1500 Loss: 36.726379 Tokens per Sec: 28708.442909\n",
            "Epoch Step: 1600 Loss: 12.665362 Tokens per Sec: 28700.389946\n",
            "Epoch Step: 1700 Loss: 41.767323 Tokens per Sec: 29155.166790\n",
            "Epoch Step: 1800 Loss: 18.434191 Tokens per Sec: 29214.198240\n",
            "Epoch Step: 1900 Loss: 19.614084 Tokens per Sec: 29031.677885\n",
            "Epoch Step: 2000 Loss: 31.366083 Tokens per Sec: 28932.428216\n",
            "Epoch Step: 2100 Loss: 50.381660 Tokens per Sec: 28768.783748\n",
            "Epoch Step: 2200 Loss: 34.857784 Tokens per Sec: 29193.486639\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> of the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father stopped on his little , my radio dots the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very , from what was really unusual , because it 's the <unk> of the <unk> of the <unk> .\n",
            "\n",
            "Validation perplexity: 12.600398\n",
            "Epoch 5\n",
            "Epoch Step: 100 Loss: 40.822987 Tokens per Sec: 27545.303079\n",
            "Epoch Step: 200 Loss: 49.574287 Tokens per Sec: 29633.761905\n",
            "Epoch Step: 300 Loss: 57.219482 Tokens per Sec: 29066.773258\n",
            "Epoch Step: 400 Loss: 50.783096 Tokens per Sec: 29214.780167\n",
            "Epoch Step: 500 Loss: 18.288828 Tokens per Sec: 29235.431991\n",
            "Epoch Step: 600 Loss: 17.111889 Tokens per Sec: 29031.760204\n",
            "Epoch Step: 700 Loss: 6.253371 Tokens per Sec: 29322.793624\n",
            "Epoch Step: 800 Loss: 39.375900 Tokens per Sec: 29447.485723\n",
            "Epoch Step: 900 Loss: 51.061844 Tokens per Sec: 29001.757326\n",
            "Epoch Step: 1000 Loss: 40.181156 Tokens per Sec: 29235.361951\n",
            "Epoch Step: 1100 Loss: 22.049023 Tokens per Sec: 28746.174544\n",
            "Epoch Step: 1200 Loss: 21.445948 Tokens per Sec: 28936.372235\n",
            "Epoch Step: 1300 Loss: 7.413289 Tokens per Sec: 28783.949518\n",
            "Epoch Step: 1400 Loss: 53.527161 Tokens per Sec: 29069.905361\n",
            "Epoch Step: 1500 Loss: 44.789108 Tokens per Sec: 29427.919890\n",
            "Epoch Step: 1600 Loss: 59.256256 Tokens per Sec: 29062.232407\n",
            "Epoch Step: 1700 Loss: 63.078266 Tokens per Sec: 29137.502870\n",
            "Epoch Step: 1800 Loss: 36.653309 Tokens per Sec: 29346.948975\n",
            "Epoch Step: 1900 Loss: 42.502903 Tokens per Sec: 29172.731611\n",
            "Epoch Step: 2000 Loss: 61.216164 Tokens per Sec: 28810.996632\n",
            "Epoch Step: 2100 Loss: 30.354204 Tokens per Sec: 29022.134088\n",
            "Epoch Step: 2200 Loss: 14.650349 Tokens per Sec: 28830.520516\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard on his little gray , my radio <unk> the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy from what was really unusual , because it was the news <unk> .\n",
            "\n",
            "Validation perplexity: 12.060902\n",
            "Epoch 6\n",
            "Epoch Step: 100 Loss: 27.840481 Tokens per Sec: 27342.204663\n",
            "Epoch Step: 200 Loss: 13.657578 Tokens per Sec: 28989.513746\n",
            "Epoch Step: 300 Loss: 44.118584 Tokens per Sec: 29285.042875\n",
            "Epoch Step: 400 Loss: 45.230209 Tokens per Sec: 28966.592774\n",
            "Epoch Step: 500 Loss: 17.408461 Tokens per Sec: 29178.428750\n",
            "Epoch Step: 600 Loss: 22.802994 Tokens per Sec: 29266.907595\n",
            "Epoch Step: 700 Loss: 37.701790 Tokens per Sec: 29142.764432\n",
            "Epoch Step: 800 Loss: 22.524405 Tokens per Sec: 29240.158403\n",
            "Epoch Step: 900 Loss: 40.791168 Tokens per Sec: 29025.892855\n",
            "Epoch Step: 1000 Loss: 24.494846 Tokens per Sec: 28758.029855\n",
            "Epoch Step: 1100 Loss: 55.676224 Tokens per Sec: 29059.925081\n",
            "Epoch Step: 1200 Loss: 42.464375 Tokens per Sec: 29138.133765\n",
            "Epoch Step: 1300 Loss: 28.302307 Tokens per Sec: 29170.874565\n",
            "Epoch Step: 1400 Loss: 22.383537 Tokens per Sec: 29454.435593\n",
            "Epoch Step: 1500 Loss: 16.842199 Tokens per Sec: 29327.629757\n",
            "Epoch Step: 1600 Loss: 29.079235 Tokens per Sec: 28802.084952\n",
            "Epoch Step: 1700 Loss: 26.282661 Tokens per Sec: 29192.558887\n",
            "Epoch Step: 1800 Loss: 30.351963 Tokens per Sec: 28989.133787\n",
            "Epoch Step: 1900 Loss: 44.326225 Tokens per Sec: 29189.332594\n",
            "Epoch Step: 2000 Loss: 54.164230 Tokens per Sec: 28452.541024\n",
            "Epoch Step: 2100 Loss: 35.238007 Tokens per Sec: 28928.579953\n",
            "Epoch Step: 2200 Loss: 26.872246 Tokens per Sec: 28607.769115\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 , i was a <unk> of the morning , i was <unk> by the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father listened on his little gray , my radio shack <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very , very , very , very , very unusual , because it 's the <unk> of the <unk> .\n",
            "\n",
            "Validation perplexity: 11.905903\n",
            "Epoch 7\n",
            "Epoch Step: 100 Loss: 31.370100 Tokens per Sec: 27273.176583\n",
            "Epoch Step: 200 Loss: 17.208212 Tokens per Sec: 29125.876604\n",
            "Epoch Step: 300 Loss: 32.976021 Tokens per Sec: 29112.747692\n",
            "Epoch Step: 400 Loss: 17.925777 Tokens per Sec: 29242.857060\n",
            "Epoch Step: 500 Loss: 31.083645 Tokens per Sec: 28956.363502\n",
            "Epoch Step: 600 Loss: 24.965416 Tokens per Sec: 28613.008461\n",
            "Epoch Step: 700 Loss: 55.889778 Tokens per Sec: 28241.234379\n",
            "Epoch Step: 800 Loss: 16.136583 Tokens per Sec: 28712.395493\n",
            "Epoch Step: 900 Loss: 38.979958 Tokens per Sec: 28833.486356\n",
            "Epoch Step: 1000 Loss: 17.549976 Tokens per Sec: 28562.722276\n",
            "Epoch Step: 1100 Loss: 7.237950 Tokens per Sec: 28984.714520\n",
            "Epoch Step: 1200 Loss: 40.000294 Tokens per Sec: 29180.351580\n",
            "Epoch Step: 1300 Loss: 29.311810 Tokens per Sec: 29221.356876\n",
            "Epoch Step: 1400 Loss: 32.662121 Tokens per Sec: 28812.328510\n",
            "Epoch Step: 1500 Loss: 48.174706 Tokens per Sec: 29122.521937\n",
            "Epoch Step: 1600 Loss: 42.316772 Tokens per Sec: 29213.164764\n",
            "Epoch Step: 1700 Loss: 35.514687 Tokens per Sec: 28810.882741\n",
            "Epoch Step: 1800 Loss: 35.401131 Tokens per Sec: 29170.161183\n",
            "Epoch Step: 1900 Loss: 37.184811 Tokens per Sec: 28993.266615\n",
            "Epoch Step: 2000 Loss: 9.460119 Tokens per Sec: 29206.845875\n",
            "Epoch Step: 2100 Loss: 52.206375 Tokens per Sec: 28539.406072\n",
            "Epoch Step: 2200 Loss: 24.853544 Tokens per Sec: 28444.837515\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 , i was a <unk> of the dawn of <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my dad stopped on his little gray gray gray , <unk> <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very happy , what was pretty unusual , because it was the <unk> of the <unk> <unk> .\n",
            "\n",
            "Validation perplexity: 11.612149\n",
            "Epoch 8\n",
            "Epoch Step: 100 Loss: 35.571095 Tokens per Sec: 27512.237069\n",
            "Epoch Step: 200 Loss: 23.965170 Tokens per Sec: 29004.626289\n",
            "Epoch Step: 300 Loss: 44.167015 Tokens per Sec: 29264.379826\n",
            "Epoch Step: 400 Loss: 1.687232 Tokens per Sec: 28588.265642\n",
            "Epoch Step: 500 Loss: 16.283075 Tokens per Sec: 28876.164950\n",
            "Epoch Step: 600 Loss: 36.435619 Tokens per Sec: 29075.960997\n",
            "Epoch Step: 700 Loss: 37.197735 Tokens per Sec: 29011.777620\n",
            "Epoch Step: 800 Loss: 15.185457 Tokens per Sec: 28901.704697\n",
            "Epoch Step: 900 Loss: 45.186115 Tokens per Sec: 29103.020918\n",
            "Epoch Step: 1000 Loss: 5.773870 Tokens per Sec: 28892.173728\n",
            "Epoch Step: 1100 Loss: 19.793694 Tokens per Sec: 29243.161620\n",
            "Epoch Step: 1200 Loss: 49.502163 Tokens per Sec: 28874.088942\n",
            "Epoch Step: 1300 Loss: 23.477718 Tokens per Sec: 28925.578408\n",
            "Epoch Step: 1400 Loss: 10.937868 Tokens per Sec: 29474.742663\n",
            "Epoch Step: 1500 Loss: 46.332554 Tokens per Sec: 29292.121301\n",
            "Epoch Step: 1600 Loss: 34.781494 Tokens per Sec: 29281.727403\n",
            "Epoch Step: 1700 Loss: 15.318990 Tokens per Sec: 29260.831129\n",
            "Epoch Step: 1800 Loss: 10.568456 Tokens per Sec: 28999.453181\n",
            "Epoch Step: 1900 Loss: 39.440567 Tokens per Sec: 29540.918902\n",
            "Epoch Step: 2000 Loss: 53.110287 Tokens per Sec: 29025.172782\n",
            "Epoch Step: 2100 Loss: 20.393204 Tokens per Sec: 28446.446722\n",
            "Epoch Step: 2200 Loss: 33.137966 Tokens per Sec: 28804.418174\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a ph.d. in the morning of the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my dad listened on his little gray gray gray , the <unk> <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very happy from what was pretty unusual , because it was the <unk> of the <unk> <unk> .\n",
            "\n",
            "Validation perplexity: 11.641655\n",
            "Epoch 9\n",
            "Epoch Step: 100 Loss: 27.108768 Tokens per Sec: 27270.928308\n",
            "Epoch Step: 200 Loss: 16.608078 Tokens per Sec: 28659.478610\n",
            "Epoch Step: 300 Loss: 48.140705 Tokens per Sec: 29107.307543\n",
            "Epoch Step: 400 Loss: 13.345468 Tokens per Sec: 29131.865386\n",
            "Epoch Step: 500 Loss: 14.647388 Tokens per Sec: 28934.581109\n",
            "Epoch Step: 600 Loss: 14.314142 Tokens per Sec: 28923.463620\n",
            "Epoch Step: 700 Loss: 44.472015 Tokens per Sec: 28777.609663\n",
            "Epoch Step: 800 Loss: 48.570206 Tokens per Sec: 29661.032236\n",
            "Epoch Step: 900 Loss: 4.929482 Tokens per Sec: 29045.164435\n",
            "Epoch Step: 1000 Loss: 17.713764 Tokens per Sec: 28949.899794\n",
            "Epoch Step: 1100 Loss: 38.769722 Tokens per Sec: 29215.586135\n",
            "Epoch Step: 1200 Loss: 10.158217 Tokens per Sec: 28881.498705\n",
            "Epoch Step: 1300 Loss: 42.284805 Tokens per Sec: 28769.907938\n",
            "Epoch Step: 1400 Loss: 11.048635 Tokens per Sec: 28814.439561\n",
            "Epoch Step: 1500 Loss: 33.117142 Tokens per Sec: 28969.611984\n",
            "Epoch Step: 1600 Loss: 17.913532 Tokens per Sec: 28919.820483\n",
            "Epoch Step: 1700 Loss: 37.104961 Tokens per Sec: 28973.540249\n",
            "Epoch Step: 1800 Loss: 26.449266 Tokens per Sec: 29524.899086\n",
            "Epoch Step: 1900 Loss: 39.147018 Tokens per Sec: 29271.330839\n",
            "Epoch Step: 2000 Loss: 14.998662 Tokens per Sec: 29278.334016\n",
            "Epoch Step: 2100 Loss: 49.289413 Tokens per Sec: 28813.018516\n",
            "Epoch Step: 2200 Loss: 32.175297 Tokens per Sec: 28962.810628\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the dawn of happiness .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father listened on his little gray gray gray , the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very happy from what was pretty unusual , because the news was <unk> the <unk> .\n",
            "\n",
            "Validation perplexity: 11.892751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li5zHZN9stk4",
        "outputId": "e2d4504f-9016-4a45-ba98-4ece92ffa8a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
        "                   emb_size=256, hidden_size=256,\n",
        "                   num_layers=1, dropout=0.24)\n",
        "dev_perplexities = train(model, print_every=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.24 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 100 Loss: 79.398621 Tokens per Sec: 26168.597590\n",
            "Epoch Step: 200 Loss: 65.201523 Tokens per Sec: 27970.748824\n",
            "Epoch Step: 300 Loss: 52.471935 Tokens per Sec: 28140.624236\n",
            "Epoch Step: 400 Loss: 26.248688 Tokens per Sec: 27872.780900\n",
            "Epoch Step: 500 Loss: 36.311172 Tokens per Sec: 27551.072097\n",
            "Epoch Step: 600 Loss: 40.871048 Tokens per Sec: 28000.416757\n",
            "Epoch Step: 700 Loss: 11.260392 Tokens per Sec: 28089.551954\n",
            "Epoch Step: 800 Loss: 92.174011 Tokens per Sec: 27861.456530\n",
            "Epoch Step: 900 Loss: 61.917427 Tokens per Sec: 28167.960648\n",
            "Epoch Step: 1000 Loss: 36.739239 Tokens per Sec: 28036.690226\n",
            "Epoch Step: 1100 Loss: 82.654099 Tokens per Sec: 28024.414241\n",
            "Epoch Step: 1200 Loss: 76.082390 Tokens per Sec: 27801.779125\n",
            "Epoch Step: 1300 Loss: 59.996544 Tokens per Sec: 27956.074040\n",
            "Epoch Step: 1400 Loss: 85.761475 Tokens per Sec: 28181.281864\n",
            "Epoch Step: 1500 Loss: 5.540013 Tokens per Sec: 28419.632025\n",
            "Epoch Step: 1600 Loss: 88.067154 Tokens per Sec: 28426.225835\n",
            "Epoch Step: 1700 Loss: 37.706150 Tokens per Sec: 28227.225519\n",
            "Epoch Step: 1800 Loss: 90.854347 Tokens per Sec: 28215.226666\n",
            "Epoch Step: 1900 Loss: 14.883327 Tokens per Sec: 28051.512701\n",
            "Epoch Step: 2000 Loss: 87.283989 Tokens per Sec: 28222.798300\n",
            "Epoch Step: 2100 Loss: 44.354500 Tokens per Sec: 27971.747473\n",
            "Epoch Step: 2200 Loss: 44.827522 Tokens per Sec: 27818.046230\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was years old , i was a <unk> of my <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on the <unk> on the <unk> , the <unk> of the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he 's very happy , what happened about the time , it was very <unk> , it was the <unk> .\n",
            "\n",
            "Validation perplexity: 31.601161\n",
            "Epoch 1\n",
            "Epoch Step: 100 Loss: 55.941849 Tokens per Sec: 26757.429787\n",
            "Epoch Step: 200 Loss: 39.329311 Tokens per Sec: 27893.071320\n",
            "Epoch Step: 300 Loss: 46.285683 Tokens per Sec: 27641.267743\n",
            "Epoch Step: 400 Loss: 44.124023 Tokens per Sec: 27841.459965\n",
            "Epoch Step: 500 Loss: 22.608906 Tokens per Sec: 27763.900050\n",
            "Epoch Step: 600 Loss: 78.449097 Tokens per Sec: 28427.756440\n",
            "Epoch Step: 700 Loss: 25.326401 Tokens per Sec: 27913.207539\n",
            "Epoch Step: 800 Loss: 19.252617 Tokens per Sec: 27886.784245\n",
            "Epoch Step: 900 Loss: 50.209850 Tokens per Sec: 28494.552003\n",
            "Epoch Step: 1000 Loss: 35.513599 Tokens per Sec: 27954.575309\n",
            "Epoch Step: 1100 Loss: 45.396133 Tokens per Sec: 27965.299171\n",
            "Epoch Step: 1200 Loss: 68.333168 Tokens per Sec: 28162.010288\n",
            "Epoch Step: 1300 Loss: 11.565917 Tokens per Sec: 28272.280371\n",
            "Epoch Step: 1400 Loss: 18.586817 Tokens per Sec: 27922.904662\n",
            "Epoch Step: 1500 Loss: 88.866440 Tokens per Sec: 28006.640350\n",
            "Epoch Step: 1600 Loss: 48.137722 Tokens per Sec: 27652.080058\n",
            "Epoch Step: 1700 Loss: 81.488205 Tokens per Sec: 28516.152418\n",
            "Epoch Step: 1800 Loss: 69.054260 Tokens per Sec: 28123.504910\n",
            "Epoch Step: 1900 Loss: 39.440205 Tokens per Sec: 27562.367648\n",
            "Epoch Step: 2000 Loss: 20.090355 Tokens per Sec: 27694.059898\n",
            "Epoch Step: 2100 Loss: 64.708023 Tokens per Sec: 27527.364118\n",
            "Epoch Step: 2200 Loss: 34.669380 Tokens per Sec: 27834.354587\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my dad was invited on his little , <unk> , the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , what was pretty much , because it was the first thing .\n",
            "\n",
            "Validation perplexity: 19.988499\n",
            "Epoch 2\n",
            "Epoch Step: 100 Loss: 78.348160 Tokens per Sec: 26508.286514\n",
            "Epoch Step: 200 Loss: 18.878263 Tokens per Sec: 27786.283957\n",
            "Epoch Step: 300 Loss: 46.314404 Tokens per Sec: 28039.479917\n",
            "Epoch Step: 400 Loss: 48.080883 Tokens per Sec: 27853.074227\n",
            "Epoch Step: 500 Loss: 5.000438 Tokens per Sec: 28068.628529\n",
            "Epoch Step: 600 Loss: 43.059322 Tokens per Sec: 28030.278546\n",
            "Epoch Step: 700 Loss: 24.233318 Tokens per Sec: 28113.120010\n",
            "Epoch Step: 800 Loss: 48.007812 Tokens per Sec: 28079.147490\n",
            "Epoch Step: 900 Loss: 43.694332 Tokens per Sec: 27777.787446\n",
            "Epoch Step: 1000 Loss: 41.649593 Tokens per Sec: 27695.525677\n",
            "Epoch Step: 1100 Loss: 40.583878 Tokens per Sec: 27488.487092\n",
            "Epoch Step: 1200 Loss: 19.112732 Tokens per Sec: 27787.053127\n",
            "Epoch Step: 1300 Loss: 25.952806 Tokens per Sec: 28100.902759\n",
            "Epoch Step: 1400 Loss: 45.917297 Tokens per Sec: 27921.961208\n",
            "Epoch Step: 1500 Loss: 21.818092 Tokens per Sec: 27933.199164\n",
            "Epoch Step: 1600 Loss: 32.889839 Tokens per Sec: 28269.733977\n",
            "Epoch Step: 1700 Loss: 41.748508 Tokens per Sec: 28057.791603\n",
            "Epoch Step: 1800 Loss: 27.052839 Tokens per Sec: 27423.447389\n",
            "Epoch Step: 1900 Loss: 39.918873 Tokens per Sec: 27746.492262\n",
            "Epoch Step: 2000 Loss: 9.228935 Tokens per Sec: 27743.496877\n",
            "Epoch Step: 2100 Loss: 69.226074 Tokens per Sec: 27547.213113\n",
            "Epoch Step: 2200 Loss: 71.505768 Tokens per Sec: 28011.630720\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a letter from the <unk> of the most successful <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little boy , the <unk> of the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was quite a very unusual , because it was the first <unk> .\n",
            "\n",
            "Validation perplexity: 15.644882\n",
            "Epoch 3\n",
            "Epoch Step: 100 Loss: 30.847586 Tokens per Sec: 26557.359654\n",
            "Epoch Step: 200 Loss: 67.124168 Tokens per Sec: 27599.730251\n",
            "Epoch Step: 300 Loss: 64.869171 Tokens per Sec: 27321.260697\n",
            "Epoch Step: 400 Loss: 34.104317 Tokens per Sec: 28096.251780\n",
            "Epoch Step: 500 Loss: 29.833239 Tokens per Sec: 27890.287087\n",
            "Epoch Step: 600 Loss: 70.586357 Tokens per Sec: 27156.067222\n",
            "Epoch Step: 700 Loss: 40.821537 Tokens per Sec: 27208.977702\n",
            "Epoch Step: 800 Loss: 55.084110 Tokens per Sec: 27313.781820\n",
            "Epoch Step: 900 Loss: 52.055779 Tokens per Sec: 27465.205561\n",
            "Epoch Step: 1000 Loss: 16.993938 Tokens per Sec: 27426.373060\n",
            "Epoch Step: 1100 Loss: 29.338795 Tokens per Sec: 28022.531252\n",
            "Epoch Step: 1200 Loss: 47.780468 Tokens per Sec: 27408.110536\n",
            "Epoch Step: 1300 Loss: 39.570698 Tokens per Sec: 28222.948234\n",
            "Epoch Step: 1400 Loss: 10.925343 Tokens per Sec: 28141.062574\n",
            "Epoch Step: 1500 Loss: 35.753212 Tokens per Sec: 28171.270718\n",
            "Epoch Step: 1600 Loss: 33.150192 Tokens per Sec: 27888.021684\n",
            "Epoch Step: 1700 Loss: 16.659107 Tokens per Sec: 27879.221142\n",
            "Epoch Step: 1800 Loss: 32.890400 Tokens per Sec: 27820.596366\n",
            "Epoch Step: 1900 Loss: 69.288101 Tokens per Sec: 27670.891351\n",
            "Epoch Step: 2000 Loss: 59.980446 Tokens per Sec: 27470.748268\n",
            "Epoch Step: 2100 Loss: 20.271576 Tokens per Sec: 27718.447256\n",
            "Epoch Step: 2200 Loss: 36.369938 Tokens per Sec: 27997.459787\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 11 years old , i was a <unk> of the most successful <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little <unk> , the radio <unk> , the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , what was pretty unusual was , it was the news of the <unk> .\n",
            "\n",
            "Validation perplexity: 13.467558\n",
            "Epoch 4\n",
            "Epoch Step: 100 Loss: 6.710524 Tokens per Sec: 26684.249200\n",
            "Epoch Step: 200 Loss: 15.786372 Tokens per Sec: 27456.095045\n",
            "Epoch Step: 300 Loss: 54.540756 Tokens per Sec: 28040.298389\n",
            "Epoch Step: 400 Loss: 32.607231 Tokens per Sec: 28023.828410\n",
            "Epoch Step: 500 Loss: 12.428413 Tokens per Sec: 27031.645973\n",
            "Epoch Step: 600 Loss: 45.496933 Tokens per Sec: 27604.349535\n",
            "Epoch Step: 700 Loss: 53.610744 Tokens per Sec: 27534.598797\n",
            "Epoch Step: 800 Loss: 61.646839 Tokens per Sec: 27785.191851\n",
            "Epoch Step: 900 Loss: 24.969746 Tokens per Sec: 27851.268767\n",
            "Epoch Step: 1000 Loss: 64.589447 Tokens per Sec: 28018.181538\n",
            "Epoch Step: 1100 Loss: 57.571022 Tokens per Sec: 27899.131314\n",
            "Epoch Step: 1200 Loss: 14.966666 Tokens per Sec: 27907.030436\n",
            "Epoch Step: 1300 Loss: 52.061852 Tokens per Sec: 28213.145981\n",
            "Epoch Step: 1400 Loss: 21.915266 Tokens per Sec: 27663.265139\n",
            "Epoch Step: 1500 Loss: 54.362045 Tokens per Sec: 27540.481291\n",
            "Epoch Step: 1600 Loss: 38.532562 Tokens per Sec: 27873.454061\n",
            "Epoch Step: 1700 Loss: 59.059162 Tokens per Sec: 27722.990004\n",
            "Epoch Step: 1800 Loss: 17.899002 Tokens per Sec: 27911.772473\n",
            "Epoch Step: 1900 Loss: 48.428848 Tokens per Sec: 28029.257295\n",
            "Epoch Step: 2000 Loss: 38.550991 Tokens per Sec: 28112.881601\n",
            "Epoch Step: 2100 Loss: 29.682302 Tokens per Sec: 28239.963973\n",
            "Epoch Step: 2200 Loss: 32.864964 Tokens per Sec: 28148.224571\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little boy , his radio <unk> <unk> the bbc of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , what was pretty unusual , it was the <unk> of the <unk> .\n",
            "\n",
            "Validation perplexity: 12.492093\n",
            "Epoch 5\n",
            "Epoch Step: 100 Loss: 51.116554 Tokens per Sec: 27154.047630\n",
            "Epoch Step: 200 Loss: 23.656433 Tokens per Sec: 27951.490908\n",
            "Epoch Step: 300 Loss: 23.531666 Tokens per Sec: 28285.702750\n",
            "Epoch Step: 400 Loss: 27.082075 Tokens per Sec: 27847.893452\n",
            "Epoch Step: 500 Loss: 59.462677 Tokens per Sec: 28203.400711\n",
            "Epoch Step: 600 Loss: 45.715019 Tokens per Sec: 27893.761755\n",
            "Epoch Step: 700 Loss: 15.275893 Tokens per Sec: 27830.606754\n",
            "Epoch Step: 800 Loss: 37.490139 Tokens per Sec: 27638.826052\n",
            "Epoch Step: 900 Loss: 17.476929 Tokens per Sec: 27868.681077\n",
            "Epoch Step: 1000 Loss: 50.834995 Tokens per Sec: 27374.711962\n",
            "Epoch Step: 1100 Loss: 30.093092 Tokens per Sec: 27865.474054\n",
            "Epoch Step: 1200 Loss: 61.334072 Tokens per Sec: 27356.006350\n",
            "Epoch Step: 1300 Loss: 15.041637 Tokens per Sec: 28276.803253\n",
            "Epoch Step: 1400 Loss: 30.774847 Tokens per Sec: 27766.698713\n",
            "Epoch Step: 1500 Loss: 26.155788 Tokens per Sec: 28142.755253\n",
            "Epoch Step: 1600 Loss: 2.133941 Tokens per Sec: 27259.602423\n",
            "Epoch Step: 1700 Loss: 27.304564 Tokens per Sec: 27708.982985\n",
            "Epoch Step: 1800 Loss: 41.793377 Tokens per Sec: 27957.366509\n",
            "Epoch Step: 1900 Loss: 40.007214 Tokens per Sec: 28017.957241\n",
            "Epoch Step: 2000 Loss: 20.669127 Tokens per Sec: 27929.148056\n",
            "Epoch Step: 2100 Loss: 49.000473 Tokens per Sec: 28043.243568\n",
            "Epoch Step: 2200 Loss: 37.841801 Tokens per Sec: 27608.224225\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> <unk> pleasure .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father stopped on his little , gray radio <unk> the bbc of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was quite unusual , and it was the <unk> of the <unk> .\n",
            "\n",
            "Validation perplexity: 11.975512\n",
            "Epoch 6\n",
            "Epoch Step: 100 Loss: 50.905518 Tokens per Sec: 27095.811113\n",
            "Epoch Step: 200 Loss: 44.171627 Tokens per Sec: 28128.772979\n",
            "Epoch Step: 300 Loss: 4.739514 Tokens per Sec: 27843.089914\n",
            "Epoch Step: 400 Loss: 37.983540 Tokens per Sec: 27520.930573\n",
            "Epoch Step: 500 Loss: 47.788727 Tokens per Sec: 27486.052064\n",
            "Epoch Step: 600 Loss: 34.178947 Tokens per Sec: 28143.547793\n",
            "Epoch Step: 700 Loss: 33.932003 Tokens per Sec: 28192.863614\n",
            "Epoch Step: 800 Loss: 19.384535 Tokens per Sec: 27803.780188\n",
            "Epoch Step: 900 Loss: 27.928577 Tokens per Sec: 28030.871358\n",
            "Epoch Step: 1000 Loss: 15.057267 Tokens per Sec: 27453.291390\n",
            "Epoch Step: 1100 Loss: 26.680754 Tokens per Sec: 27757.073030\n",
            "Epoch Step: 1200 Loss: 46.025394 Tokens per Sec: 27657.967368\n",
            "Epoch Step: 1300 Loss: 16.544731 Tokens per Sec: 27756.183506\n",
            "Epoch Step: 1400 Loss: 50.675789 Tokens per Sec: 28171.464130\n",
            "Epoch Step: 1500 Loss: 29.877808 Tokens per Sec: 27866.497453\n",
            "Epoch Step: 1600 Loss: 58.810638 Tokens per Sec: 27548.057544\n",
            "Epoch Step: 1700 Loss: 26.291830 Tokens per Sec: 28413.708857\n",
            "Epoch Step: 1800 Loss: 22.796452 Tokens per Sec: 27724.487660\n",
            "Epoch Step: 1900 Loss: 48.270069 Tokens per Sec: 28095.467553\n",
            "Epoch Step: 2000 Loss: 17.834471 Tokens per Sec: 27765.778148\n",
            "Epoch Step: 2100 Loss: 24.329945 Tokens per Sec: 27404.457401\n",
            "Epoch Step: 2200 Loss: 34.376022 Tokens per Sec: 27734.647707\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was always a morning at the morning of the <unk> of pleasure .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father stopped on his little , gray radio <unk> the bbc of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy from what was very unusual , because it was the news of the <unk> .\n",
            "\n",
            "Validation perplexity: 11.794506\n",
            "Epoch 7\n",
            "Epoch Step: 100 Loss: 38.388214 Tokens per Sec: 26145.225328\n",
            "Epoch Step: 200 Loss: 16.395697 Tokens per Sec: 26768.718784\n",
            "Epoch Step: 300 Loss: 51.352520 Tokens per Sec: 27825.867221\n",
            "Epoch Step: 400 Loss: 12.908516 Tokens per Sec: 27710.665609\n",
            "Epoch Step: 500 Loss: 28.769632 Tokens per Sec: 27393.686097\n",
            "Epoch Step: 600 Loss: 34.374260 Tokens per Sec: 27893.272262\n",
            "Epoch Step: 700 Loss: 37.480175 Tokens per Sec: 28122.119032\n",
            "Epoch Step: 800 Loss: 24.355082 Tokens per Sec: 28042.844235\n",
            "Epoch Step: 900 Loss: 49.986095 Tokens per Sec: 28233.285766\n",
            "Epoch Step: 1000 Loss: 53.461784 Tokens per Sec: 28116.800288\n",
            "Epoch Step: 1100 Loss: 10.872833 Tokens per Sec: 28058.268739\n",
            "Epoch Step: 1200 Loss: 24.825878 Tokens per Sec: 28028.351014\n",
            "Epoch Step: 1300 Loss: 36.729721 Tokens per Sec: 27903.869851\n",
            "Epoch Step: 1400 Loss: 44.210686 Tokens per Sec: 28133.723835\n",
            "Epoch Step: 1500 Loss: 30.275179 Tokens per Sec: 28039.934062\n",
            "Epoch Step: 1600 Loss: 19.577538 Tokens per Sec: 27886.757798\n",
            "Epoch Step: 1700 Loss: 17.869192 Tokens per Sec: 27610.040972\n",
            "Epoch Step: 1800 Loss: 37.075615 Tokens per Sec: 27975.599252\n",
            "Epoch Step: 1900 Loss: 21.178347 Tokens per Sec: 27683.003437\n",
            "Epoch Step: 2000 Loss: 2.224754 Tokens per Sec: 27815.645111\n",
            "Epoch Step: 2100 Loss: 30.117271 Tokens per Sec: 27329.663180\n",
            "Epoch Step: 2200 Loss: 18.756598 Tokens per Sec: 27310.816697\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the morning i was going to be <unk> in the morning .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father listened to his little , gray <unk> the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty unusual , because it was the news <unk> for the news .\n",
            "\n",
            "Validation perplexity: 11.618956\n",
            "Epoch 8\n",
            "Epoch Step: 100 Loss: 45.613613 Tokens per Sec: 26851.270384\n",
            "Epoch Step: 200 Loss: 50.922421 Tokens per Sec: 27596.149015\n",
            "Epoch Step: 300 Loss: 21.967241 Tokens per Sec: 27836.964410\n",
            "Epoch Step: 400 Loss: 44.378395 Tokens per Sec: 28020.505886\n",
            "Epoch Step: 500 Loss: 36.282536 Tokens per Sec: 27770.781913\n",
            "Epoch Step: 600 Loss: 10.875543 Tokens per Sec: 28227.168845\n",
            "Epoch Step: 700 Loss: 29.966558 Tokens per Sec: 27771.738283\n",
            "Epoch Step: 800 Loss: 41.007740 Tokens per Sec: 27894.368248\n",
            "Epoch Step: 900 Loss: 48.328045 Tokens per Sec: 28234.929614\n",
            "Epoch Step: 1000 Loss: 22.163847 Tokens per Sec: 28191.636031\n",
            "Epoch Step: 1100 Loss: 25.023169 Tokens per Sec: 28055.226082\n",
            "Epoch Step: 1200 Loss: 6.921091 Tokens per Sec: 28399.146597\n",
            "Epoch Step: 1300 Loss: 7.186458 Tokens per Sec: 27833.550677\n",
            "Epoch Step: 1400 Loss: 19.428154 Tokens per Sec: 28012.756967\n",
            "Epoch Step: 1500 Loss: 16.630894 Tokens per Sec: 27695.499697\n",
            "Epoch Step: 1600 Loss: 14.307495 Tokens per Sec: 27732.719898\n",
            "Epoch Step: 1700 Loss: 30.167589 Tokens per Sec: 27954.320538\n",
            "Epoch Step: 1800 Loss: 20.674988 Tokens per Sec: 27526.651306\n",
            "Epoch Step: 1900 Loss: 53.147156 Tokens per Sec: 27632.525706\n",
            "Epoch Step: 2000 Loss: 14.681211 Tokens per Sec: 27425.759191\n",
            "Epoch Step: 2100 Loss: 39.049953 Tokens per Sec: 27300.049058\n",
            "Epoch Step: 2200 Loss: 24.708204 Tokens per Sec: 27276.443608\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father stopped on his little gray gray , radio <unk> the bbc of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty unusual , because it was the news <unk> .\n",
            "\n",
            "Validation perplexity: 11.701978\n",
            "Epoch 9\n",
            "Epoch Step: 100 Loss: 33.011341 Tokens per Sec: 26755.442942\n",
            "Epoch Step: 200 Loss: 29.948671 Tokens per Sec: 27714.935550\n",
            "Epoch Step: 300 Loss: 25.134539 Tokens per Sec: 27632.089855\n",
            "Epoch Step: 400 Loss: 30.676149 Tokens per Sec: 27703.105145\n",
            "Epoch Step: 500 Loss: 25.851318 Tokens per Sec: 27954.287234\n",
            "Epoch Step: 600 Loss: 39.958202 Tokens per Sec: 27749.595882\n",
            "Epoch Step: 700 Loss: 49.739292 Tokens per Sec: 27377.375154\n",
            "Epoch Step: 800 Loss: 23.223625 Tokens per Sec: 28041.692429\n",
            "Epoch Step: 900 Loss: 28.411011 Tokens per Sec: 27741.327880\n",
            "Epoch Step: 1000 Loss: 19.183277 Tokens per Sec: 27692.329831\n",
            "Epoch Step: 1100 Loss: 19.387817 Tokens per Sec: 27941.652701\n",
            "Epoch Step: 1200 Loss: 44.894619 Tokens per Sec: 27910.114281\n",
            "Epoch Step: 1300 Loss: 11.683228 Tokens per Sec: 27785.631350\n",
            "Epoch Step: 1400 Loss: 25.196964 Tokens per Sec: 27881.903736\n",
            "Epoch Step: 1500 Loss: 8.528130 Tokens per Sec: 27649.952905\n",
            "Epoch Step: 1600 Loss: 45.492092 Tokens per Sec: 27498.978982\n",
            "Epoch Step: 1700 Loss: 30.372110 Tokens per Sec: 27848.841144\n",
            "Epoch Step: 1800 Loss: 48.040745 Tokens per Sec: 27460.518300\n",
            "Epoch Step: 1900 Loss: 21.665400 Tokens per Sec: 27200.214818\n",
            "Epoch Step: 2000 Loss: 51.432472 Tokens per Sec: 27547.068812\n",
            "Epoch Step: 2100 Loss: 48.127712 Tokens per Sec: 27461.565145\n",
            "Epoch Step: 2200 Loss: 30.319946 Tokens per Sec: 27828.567654\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was happy to a <unk> in the morning , and i was british <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father stopped on his little boy , including radio <unk> the bbc of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very happy , which was pretty unusual , because it was the news <unk> .\n",
            "\n",
            "Validation perplexity: 11.707481\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86rYGo03jHV8",
        "outputId": "c1c6a32c-35fa-49f9-a4aa-3833b01b1767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plot_perplexity(dev_perplexities)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqihuhJiFJBX"
      },
      "source": [
        "## Model Saving and Tracing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Inp02A4X2M",
        "outputId": "2a087bd5-b9ba-40f9-dce2-3b71d4d59e43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.save(model.state_dict(), \"attention_s11_new.pth\")\n",
        "#model.save(\"/content/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type EncoderDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type BahdanauAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtDjidZp_oer",
        "outputId": "463c9f31-2f34-428e-cb42-84a6bd705415",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
        "                   emb_size=256, hidden_size=256,\n",
        "                   num_layers=1, dropout=0.2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOu0QYLl4LHh"
      },
      "source": [
        "#!cp /content/attention_s11_new.pth /content/drive/My\\ Drive/EVA4/attention_s11_new.pth\n",
        "!cp /content/drive/My\\ Drive/EVA4/P2S11/* /content"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nprRLzUp_v1I"
      },
      "source": [
        "!cp /content/drive/My\\ Drive/EVA4/RekogNizer/torchversionchange.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBdLIPqE_Tty"
      },
      "source": [
        "torch.save(model.state_dict, \"attention_s11.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf4dQkpwzxnv"
      },
      "source": [
        "#model.state_dict = torch.load(\"/content/attention_s11.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9AKc92mxx7v",
        "outputId": "313ba92c-185a-41df-e918-70fc0ba26a4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.cuda()\n",
        "model.load_state_dict(torch.load(\"/content/attention_s11_new.pth\"))\n",
        "#traced_model = torch.jit.trace(model.to(\"cpu\"), torch.randn(1,3,512,512))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrdXM-aoAhmK",
        "outputId": "355b65b6-a43c-49b7-966b-a6f8ab5accd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "MAX_LENGTH = 25\n",
        "model = model.to(\"cpu\")\n",
        "src = torch.LongTensor(1, MAX_LENGTH).random_(0, len(SRC.vocab)).to(\"cpu\")\n",
        "trg = torch.LongTensor(1, MAX_LENGTH).random_(0, len(TRG.vocab)).to(\"cpu\")\n",
        "src_t = model.src_embed(src).to(\"cpu\")\n",
        "trg_t = model.trg_embed(trg).to(\"cpu\")\n",
        "test_mask = torch.BoolTensor(1,1,MAX_LENGTH).to(\"cpu\")\n",
        "test_length = torch.LongTensor([MAX_LENGTH]).to(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DozZFc_I7Pj7",
        "outputId": "5005e029-ca11-435a-ff78-4f31f0bd0065",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "traced_encoder = torch.jit.trace(model.encoder.to(\"cpu\"), \n",
        "                                 (src_t.to(\"cpu\"), \n",
        "                                  test_mask.to(\"cpu\"), \n",
        "                                  test_length.to(\"cpu\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "/usr/local/lib/python3.6/dist-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTYxaiogA417"
      },
      "source": [
        "enc_hidden,enc_final = traced_encoder(src_t.to(\"cpu\"), \n",
        "                                  test_mask.to(\"cpu\"), \n",
        "                                  test_length.to(\"cpu\"))\n",
        "test_mask, test_length = torch.BoolTensor(1,1,MAX_LENGTH).to(\"cpu\"), torch.LongTensor([MAX_LENGTH]).to(\"cpu\")\n",
        "# enc_hidden,enc_final = traced_encoder(input_embed.unsqueeze(0).to(\"cpu\"), \n",
        "#                                       my_op.src_mask.to(\"cpu\"), \n",
        "#                                       my_op.src_lengths.to(\"cpu\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUHooGg0AxDz",
        "outputId": "28256ae1-fb4e-4118-ace9-9f4a42f6bb65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "traced_decoder = torch.jit.trace(model.decoder.to(\"cpu\"), (trg_t, enc_hidden,enc_final, \n",
        "                                                           test_mask.to(\"cpu\"), \n",
        "                                                           test_length.to(\"cpu\"),torch.FloatTensor(1,1,256)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:64: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "/usr/local/lib/python3.6/dist-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1037: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 0, 218] (0.32699108123779297 vs. -0.8354699611663818) and 255 other locations (100.00%)\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1037: TracerWarning: Output nr 2. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 0, 218] (0.32699108123779297 vs. -0.8354699611663818) and 255 other locations (100.00%)\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1037: TracerWarning: Output nr 3. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 0, 77] (-2.0776357650756836 vs. 2.8516368865966797) and 255 other locations (100.00%)\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPaSdjz4kJsn"
      },
      "source": [
        "#traced_decoder = torch.jit.trace(model.decoder.to(\"cpu\"), (trg_embed, enc_hidden,enc_final, my_op.src_mask.to(\"cpu\"), my_op.trg_mask.to(\"cpu\")))\n",
        "model =model.to(\"cpu\")\n",
        "traced_src_embed = torch.jit.trace(model.src_embed, torch.LongTensor(1, MAX_LENGTH).random_(0, len(SRC.vocab)))\n",
        "traced_trg_embed = torch.jit.trace(model.trg_embed, torch.LongTensor(1, MAX_LENGTH).random_(0, len(TRG.vocab)))\n",
        "traced_generator = torch.jit.trace(model.generator, (torch.FloatTensor(1,256)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN12KBR7k1Lt"
      },
      "source": [
        "traced_decoder.save(\"decoder.pt\")\n",
        "traced_encoder.save(\"encoder.pt\")\n",
        "traced_src_embed.save(\"embed_src.pt\")\n",
        "traced_trg_embed.save(\"embed_trg.pt\")\n",
        "traced_generator.save(\"generator.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvzl10VktFxg"
      },
      "source": [
        "traced_decoder = torch.jit.load(\"/content/decoder.pt\")\n",
        "traced_encoder  =torch.jit.load(\"/content/encoder.pt\")\n",
        "traced_src_embed = torch.jit.load(\"/content/embed_src.pt\")\n",
        "traced_trg_embed = torch.jit.load(\"/content/embed_trg.pt\")\n",
        "traced_generator = torch.jit.load(\"/content/generator.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjFSzeF3Flk2"
      },
      "source": [
        "!cp /content/en_de_translator.pickle /content/drive/My\\ Drive/EVA4/P2S11\n",
        "!cp /content/attention_s11_new.pth /content/drive/My\\ Drive/EVA4/P2S11\n",
        "!cp /content/encoder.pt /content/decoder.pt /content/embed_src.pt /content/embed_trg.pt /content/drive/My\\ Drive/EVA4/P2S11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23XJLk1jHV_"
      },
      "source": [
        "## Prediction and Evaluation\n",
        "\n",
        "Once trained we can use the model to produce a set of translations. \n",
        "\n",
        "If we translate the whole validation set, we can use [SacreBLEU](https://github.com/mjpost/sacreBLEU) to get a [BLEU score](https://en.wikipedia.org/wiki/BLEU), which is the most common way to evaluate translations.\n",
        "\n",
        "#### Important sidenote\n",
        "Typically you would use SacreBLEU from the **command line** using the output file and original (possibly tokenized) development reference file. This will give you a nice version string that shows how the BLEU score was calculated; for example, if it was lowercased, if it was tokenized (and how), and what smoothing was used. If you want to learn more about how BLEU scores are (and should be) reported, check out [this paper](https://arxiv.org/abs/1804.08771).\n",
        "\n",
        "However, right now our pre-processed data is only in memory, so we'll calculate the BLEU score right from this notebook for demonstration purposes.\n",
        "\n",
        "We'll first test the raw BLEU function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4cGe3UOixB8",
        "outputId": "bdd066cb-b871-4cdc-de02-6ce169092039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gen_op.shape,gen_op.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 13002]), torch.float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__GQR5rPEJTf"
      },
      "source": [
        "!cp /content/encoder.pt /content/generator.pt /content/embed_trg.pt /content/embed_src.pt /content/decoder.pt /content/drive/My\\ Drive/EVA4/P2S11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w39oX5lBh4vB"
      },
      "source": [
        "def greedy_decode_traced( traced_src_embed, traced_trg_embed, \n",
        "                         encoder, decoder, generator,\n",
        "                         src, src_mask, \n",
        "                         src_lengths, max_len=100, sos_index=1, \n",
        "                         eos_index=None):\n",
        "    \"\"\"Greedily decode a sentence.\"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        #src_embed = model.src_embed(src)\n",
        "        src_embed = traced_src_embed(src)\n",
        "        print(\"#################### SRC Embedding receivd\")\n",
        "        encoder_hidden, encoder_final = encoder(src_embed, src_mask, src_lengths)\n",
        "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
        "        trg_mask = torch.ones_like(prev_y)\n",
        "\n",
        "    output = []\n",
        "    #attention_scores = []\n",
        "    hidden = torch.zeros_like(torch.FloatTensor(1,1,256))\n",
        "\n",
        "    for i in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            #trg_embed = model.trg_embed(prev_y)\n",
        "            trg_embed = traced_trg_embed(prev_y)\n",
        "            #print(\"#################### TRG Embedding receivd\")\n",
        "            out, hidden, pre_output = decoder(trg_embed, \n",
        "                                              encoder_hidden, encoder_final, \n",
        "                                              src_mask,trg_mask,hidden)\n",
        "\n",
        "            #   model.decode(\n",
        "            #   encoder_hidden, encoder_final, src_mask,\n",
        "            #   prev_y, trg_mask, hidden)\n",
        "\n",
        "            # we predict from the pre-output layer, which is\n",
        "            # a combination of Decoder state, prev emb, and context\n",
        "            #prob = model.generator(pre_output[:, -1])\n",
        "            prob = traced_generator(pre_output[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.data.item()\n",
        "        output.append(next_word)\n",
        "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
        "        #attention_scores.append(decoder.attention.alphas.cpu().numpy())\n",
        "    print(\"Decoding completed\",output,type(output))#, attention_scores)\n",
        "    output = np.array(output)\n",
        "        \n",
        "    # cut off everything starting from </s> \n",
        "    #(only when eos_index provided)\n",
        "    if eos_index is not None:\n",
        "        first_eos = np.where(output==eos_index)[0]\n",
        "        if len(first_eos) > 0:\n",
        "            output = output[:first_eos[0]]      \n",
        "    \n",
        "    return output#, np.concatenate(attention_scores, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY84gfH79MLj"
      },
      "source": [
        "hypotheses = []\n",
        "alphas = []  # save the last attention scores\n",
        "for idx, batch in enumerate(valid_iter):\n",
        "#   if(idx > 5):\n",
        "#       break\n",
        "  batch = rebatch(PAD_INDEX, batch)\n",
        "  #pred, attention = \n",
        "  #torch.jit.script()\n",
        "  pred = greedy_decode_traced(\n",
        "    traced_src_embed, traced_trg_embed, \n",
        "    traced_encoder, traced_decoder, traced_generator,\n",
        "    batch.src.to(\"cpu\"), batch.src_mask.to(\"cpu\"), \n",
        "    batch.src_lengths.to(\"cpu\"), max_len=30,\n",
        "    sos_index=TRG.vocab.stoi[SOS_TOKEN],\n",
        "    eos_index=TRG.vocab.stoi[EOS_TOKEN])\n",
        "  hypotheses.append(pred)\n",
        "  alphas.append(attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dsbDGF1Aqdz"
      },
      "source": [
        "def print_batch_params(batch_elem):\n",
        "    print(\"SRC Shape\",batch_elem.src.shape, batch_elem.src_mask.shape, batch_elem.src_lengths.shape)\n",
        "    print(\"TRG Shape\",batch_elem.trg.shape, batch_elem.trg_mask.shape, batch_elem.trg_lengths.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuuAq8hYfhm8"
      },
      "source": [
        "def convert_to_target(hypotheses):\n",
        "    hypotheses_ = [lookup_words(x, TRG.vocab) for x in hypotheses]\n",
        "    print(hypotheses_[0])\n",
        "    # finally, the SacreBLEU raw scorer requires string input, so we convert the lists to strings\n",
        "    hypotheses_ = [\" \".join(x) for x in hypotheses_]\n",
        "    print(len(hypotheses_))\n",
        "    print(hypotheses_[0])\n",
        "    return hypotheses_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDlrh8Dl9B7y"
      },
      "source": [
        "de_text = \"Warum gibt es immer unterschiedliche Antworten, wenn ich denselben Absatz auf Englisch schreibe, um ihn ins Deutsche zu übersetzen?\"\n",
        "token_de = tokenize_de(de_text)\n",
        "idx = [ SRC.vocab.stoi[x] for x in token_de] \n",
        "my_src = (torch.LongTensor(idx), )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo6BFIDV-KJo",
        "outputId": "7138a946-3c3f-4cf3-9dcc-18f4d3dedacd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "my_op_batch = Batch((torch.LongTensor(idx).unsqueeze(0), torch.LongTensor([len(idx)])),\n",
        "      (torch.LongTensor(idx).unsqueeze(0), torch.LongTensor([len(idx)])),\n",
        "       pad_index=1)\n",
        "\n",
        "pred = greedy_decode_traced(\n",
        "    traced_src_embed, traced_trg_embed, \n",
        "    traced_encoder, traced_decoder, traced_generator,\n",
        "    my_op_batch.src.to(\"cpu\"), my_op_batch.src_mask.to(\"cpu\"), \n",
        "    my_op_batch.src_lengths.to(\"cpu\"), max_len=30,\n",
        "    sos_index=TRG.vocab.stoi[SOS_TOKEN],\n",
        "    eos_index=TRG.vocab.stoi[EOS_TOKEN])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#################### SRC Embedding receivd\n",
            "Decoding completed [289, 8, 460, 104, 289, 23, 27, 17, 1479, 23, 3, 23, 3, 104, 289, 23, 3, 104, 6, 289, 23, 3, 8992, 23, 3, 23, 3, 104, 979, 23] <class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_1akKoYCRbn",
        "outputId": "592a1764-ab1a-413c-944f-5e5053a570aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "convert_to_target([pred])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['space', 'to', 'bring', 'into', 'space', '?', 'what', 'we', 'discover', '?']\n",
            "1\n",
            "space to bring into space ? what we discover ?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['space to bring into space ? what we discover ?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DGt97_mdSlO"
      },
      "source": [
        "## Pickling the VOCABs\n",
        "These will be used during model deployment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj9pR-50EPBF"
      },
      "source": [
        "TRG_stoi_dict = { key:value_ for key,value_ in TRG.vocab.stoi.items()}\n",
        "SRC_stoi_dict = { key:value_ for key,value_ in SRC.vocab.stoi.items()}\n",
        "translator_vocab = { \"TRG_stoi\": TRG_stoi_dict, \"TRG_itos\":TRG.vocab.itos, \"SRC_stoi\": SRC_stoi_dict, \"SRC_itos\":SRC.vocab.itos}\n",
        "import pickle\n",
        "with open('en_de_translator.pickle','wb') as f:\n",
        "    pickle.dump(translator_vocab, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izwlur3RE7qk",
        "outputId": "635e8b40-54dc-48a3-90d8-f88ce9c2c9fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "de_picked[\"TRG_stoi\"]['when'] == translator_vocab[\"TRG_stoi\"]['when']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcWxiDbbjHWC",
        "outputId": "9ac9f2ac-226d-462c-d61d-a8ea137215ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# this should result in a perfect BLEU of 100%\n",
        "hypotheses = [\"this is a test\"]\n",
        "references = [\"this is a test\"]\n",
        "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score\n",
        "print(bleu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100.00000000000004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P32Ig5fcjHWF",
        "outputId": "a110db73-0c29-4282-e510-d133668f1893",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# here the BLEU score will be lower, because some n-grams won't match\n",
        "hypotheses = [\"this is a test\"]\n",
        "references = [\"this is a fest\"]\n",
        "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score\n",
        "print(bleu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22.360679774997894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUlKm_5BjHWM",
        "outputId": "c382be9e-8d3f-411c-d8df-ab76043af154",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "references = [\" \".join(example.trg) for example in valid_data]\n",
        "print(len(references))\n",
        "print(references[0])\n",
        "references[-2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "690\n",
            "when i was 11 , i remember waking up one morning to the sound of joy in my house .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsRzS5YpjHWS",
        "outputId": "fc9b25c3-872a-48b0-db7b-9985cf25002a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hypotheses = []\n",
        "alphas = []  # save the last attention scores\n",
        "for batch in valid_iter:\n",
        "  batch = rebatch(PAD_INDEX, batch)\n",
        "  pred, attention = greedy_decode(\n",
        "    model.cuda(), batch.src, batch.src_mask, batch.src_lengths, max_len=25,\n",
        "    sos_index=TRG.vocab.stoi[SOS_TOKEN],\n",
        "    eos_index=TRG.vocab.stoi[EOS_TOKEN])\n",
        "  hypotheses.append(pred)\n",
        "  alphas.append(attention)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9XJv05_jHWY",
        "outputId": "84b082a7-dbea-4100-9c97-0442c086fef9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hypotheses = [lookup_words(x, TRG.vocab) for x in hypotheses]\n",
        "hypotheses[0]\n",
        "# finally, the SacreBLEU raw scorer requires string input, so we convert the lists to strings\n",
        "hypotheses = [\" \".join(x) for x in hypotheses]\n",
        "print(len(hypotheses))\n",
        "print(hypotheses[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "690\n",
            "when i was 11 years old , i was a <unk> in the morning of happiness .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86L1DGuCjHWb",
        "outputId": "cacb8c87-6ec9-4fe2-aca1-657bca4e94b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# finally, the SacreBLEU raw scorer requires string input, so we convert the lists to strings\n",
        "hypotheses = [\" \".join(x) for x in hypotheses]\n",
        "print(len(hypotheses))\n",
        "print(hypotheses[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "690\n",
            "thoroughly faster factory acknowledge likewise thrive brotherhood temperate undo edward pulls ! zone nobel 16th miller lip cognitive magazines substance powerless orville induced feed criminal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x6IpMdOjHWd"
      },
      "source": [
        "# now we can compute the BLEU score!\n",
        "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score\n",
        "print(bleu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz3lN4SKjHWf"
      },
      "source": [
        "## Attention Visualization\n",
        "\n",
        "We can also visualize the attention scores of the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bojCudDjHWg"
      },
      "source": [
        "def plot_heatmap(src, trg, scores):\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    heatmap = ax.pcolor(scores, cmap='viridis')\n",
        "\n",
        "    ax.set_xticklabels(trg, minor=False, rotation='vertical')\n",
        "    ax.set_yticklabels(src, minor=False)\n",
        "\n",
        "    # put the major ticks at the middle of each cell\n",
        "    # and the x-ticks on top\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.set_xticks(np.arange(scores.shape[1]) + 0.5, minor=False)\n",
        "    ax.set_yticks(np.arange(scores.shape[0]) + 0.5, minor=False)\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    plt.colorbar(heatmap)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMwAaUk4jHWk",
        "outputId": "bda3df07-33d9-4465-ad8c-ecb754e4ac0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "# This plots a chosen sentence, for which we saved the attention scores above.\n",
        "idx = 5\n",
        "src = valid_data[idx].src + [\"</s>\"]\n",
        "trg = valid_data[idx].trg + [\"</s>\"]\n",
        "pred = hypotheses[idx].split() + [\"</s>\"]\n",
        "pred_att = alphas[idx][0].T[:, :len(pred)]\n",
        "print(\"src\", src)\n",
        "print(\"ref\", trg)\n",
        "print(\"pred\", pred)\n",
        "plot_heatmap(src, pred, pred_att)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src ['\"', 'jetzt', 'kannst', 'du', 'auf', 'eine', 'richtige', 'schule', 'gehen', ',', '\"', 'sagte', 'er', '.', '</s>']\n",
            "ref ['\"', 'you', 'can', 'go', 'to', 'a', 'real', 'school', 'now', ',', '\"', 'he', 'said', '.', '</s>']\n",
            "pred ['\"', 'now', 'you', 'can', 'go', 'to', 'a', 'right', 'school', ',', '\"', 'he', 'said', '.', '</s>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjppRhdzdk2V"
      },
      "source": [
        "## Saved model pushed to AWS S3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Zjr2xAHgPK",
        "outputId": "2dbd0c6f-ed9d-41c1-aa8d-58ae16ef2be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "!pip install boto3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/99/3979b617c0cbb3d7260cd3357b4a06edaa91073dd252687b7502f6678bb8/boto3-1.16.9-py2.py3-none-any.whl (129kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 24.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 30.2MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 40kB 14.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 51kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 61kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 71kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 81kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 92kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 102kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 112kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 122kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 10.6MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 20kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 30kB 29.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 40kB 32.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 51kB 32.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 61kB 32.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 9.4MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/c7/ffd9653ac448eb4d1034b3422b4699b6e0a1d0550a33587f876efb14569b/botocore-1.19.9-py2.py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 23.7MB/s \n",
            "\u001b[?25hCollecting urllib3<1.26,>=1.25.4; python_version != \"3.4\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 56.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.9->boto3) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.9->boto3) (1.15.0)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, urllib3, botocore, s3transfer, boto3\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.16.9 botocore-1.19.9 jmespath-0.10.0 s3transfer-0.3.3 urllib3-1.25.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udYVWmBDHbXR"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bGhH4M9Hjvl"
      },
      "source": [
        "sys.path.append(\"/content/drive/My Drive/EVA4/RekogNizer\")\n",
        "sys.path.append(\"/content/drive/My Drive/EVA4/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPiq_HXsH04t",
        "outputId": "e654cad2-3534-4651-a60c-4b0e40079e81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "glob.glob(\"./*pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./generator.pt',\n",
              " './embed_src.pt',\n",
              " './decoder.pt',\n",
              " './encoder.pt',\n",
              " './embed_trg.pt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1-boGUmHvYu"
      },
      "source": [
        "from RekogNizer import awsutils\n",
        "\n",
        "for saved_model in glob.glob(\"./*pt\"):\n",
        "    awsutils.upload_model_to_s3('rekog-eva4s1',saved_model,os.path.basename(saved_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wddxQC5yH9cc"
      },
      "source": [
        "import boto3\n",
        "S3_BUCKET='rekog-eva4s1'\n",
        "s3 = boto3.client('s3')\n",
        "s3_res = boto3.resource('s3')\n",
        "bucket = s3_res.Bucket(S3_BUCKET)\n",
        "\n",
        "model_name='encoder.pt'\n",
        "obj = s3.get_object(Bucket=S3_BUCKET, Key=model_name)\n",
        "bytestream = io.BytesIO(obj['Body'].read() )\n",
        "print(\"Loading Model {}\".format(model_name))\n",
        "traced_model = torch.jit.load(bytestream)\n",
        "traced_model.eval()\n",
        "#MODELS_DICT[key] = model\n",
        "print( \"Models {} Loaded\".format(model_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WYwVpDFJijE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
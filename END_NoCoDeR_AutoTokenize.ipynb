{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "END_NoCoDeR_AutoTokenize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EVAP2/blob/master/END_NoCoDeR_AutoTokenize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOdNYOq_V5C3",
        "outputId": "8ce16d11-eb91-4131-e6c3-05d6faeea232"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Mar  7 04:10:12 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4uN4sSQlA7w"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import fileinput\n",
        "import re\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.display import display, HTML\n",
        "#import seaborn as sns\n",
        "import dateutil.parser\n",
        "import datetime\n",
        "#from ipyfilechooser import FileChooser\n",
        "import numpy as np\n",
        "import os\n",
        "import gzip\n",
        "import dateutil.parser\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import glob\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import timedelta\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "import spacy\n",
        "from pprint import pprint\n",
        "import six\n",
        "import sys, token, tokenize\n",
        "import ast\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "#from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm, trange\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRNwGADTvOuh",
        "outputId": "64d751d4-b6a4-4a29-936b-52866ad6a5ed"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 8.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=4508c133a96af1bb3fd69d5d1aaee995f6adc2ef0766fcecdf93c97f5a185c6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6XDZbeWK6u_"
      },
      "source": [
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_7b5amJJIQC"
      },
      "source": [
        "%%bash\n",
        "python -m spacy download en\n",
        "python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqKpFyhplfvZ"
      },
      "source": [
        "!wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n",
        "!wget -O gpt2_bpe_encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbnG7rH1Y5Ye"
      },
      "source": [
        "!unzip /content/python.zip\n",
        "!cp /content/python/final/jsonl/train/python_train_0.jsonl.gz .\n",
        "!gzip -d /content/python_train_0.jsonl.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzTyfBvwcT50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01998ec0-c630-47fe-cfca-7c74bba905db"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y61ETqwPu9rc"
      },
      "source": [
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/english_python_data.txt .\n",
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/english_python_cleaned.txt .\n",
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone.csv ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBoZC5u13DOb"
      },
      "source": [
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_baseline.pt ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGoF0UiKmLg-"
      },
      "source": [
        "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "spacy_en = spacy.load('en')\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "#model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "#model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NPC5Tjn-eHe"
      },
      "source": [
        "\"\"\"\n",
        "    Removes docstrings and comments\n",
        "\"\"\"\n",
        "def remove_docstrings_comments(src_string, doc_string=None, debug=False):\n",
        "    mod = []\n",
        "\n",
        "    prev_toktype = token.INDENT\n",
        "    first_line = None\n",
        "    last_lineno = -1\n",
        "    last_col = 0\n",
        "    try:\n",
        "        #tokgen = tokenize.generate_tokens(source.readline)\n",
        "        tokgen = tokenize.generate_tokens(six.StringIO(src_string.rstrip()).readline)\n",
        "        for toktype, ttext, (slineno, scol), (elineno, ecol), ltext in tokgen:\n",
        "            if 0:   # Change to if 1 to see the tokens fly by.\n",
        "                print(\"%10s %-14s %-20r %r\" % (\n",
        "                    tokenize.tok_name.get(toktype, toktype),\n",
        "                    \"%d.%d-%d.%d\" % (slineno, scol, elineno, ecol),\n",
        "                    ttext, ltext\n",
        "                    ))\n",
        "            if slineno > last_lineno:\n",
        "                last_col = 0\n",
        "            if scol > last_col:\n",
        "                mod.append(\" \" * (scol - last_col))\n",
        "            if toktype == token.STRING and prev_toktype == token.INDENT:\n",
        "                # Docstring\n",
        "                mod.append(\"#--\")\n",
        "            elif toktype == tokenize.COMMENT:\n",
        "                # Comment\n",
        "                mod.append(\"##\")\n",
        "            else:\n",
        "                mod.append(ttext)\n",
        "            prev_toktype = toktype\n",
        "            last_col = ecol\n",
        "            last_lineno = elineno\n",
        "        return \"\".join(mod)\n",
        "    except:\n",
        "        print(doc_string)\n",
        "        print(src_string )\n",
        "        print(sys.exc_info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwtHz8XxC8ep"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c35HYbYxWBk7"
      },
      "source": [
        "### Create datasets and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nobKWg_DJ5Xp"
      },
      "source": [
        "nl_to_pl_df = pd.read_csv('/content/end_capstone.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prv7EIAX5aOj"
      },
      "source": [
        "nl_to_pl_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "MR7nruWfNQNB",
        "outputId": "55ebd0fe-ef67-4758-c9b5-548818e783ca"
      },
      "source": [
        "print(nl_to_pl_df['code_len'].max(),nl_to_pl_df['code_len'].min())\n",
        "print(nl_to_pl_df['docstring_len'].max(),nl_to_pl_df['docstring_len'].min())\n",
        "nl_to_pl_df[nl_to_pl_df['code_len'] ==0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2443 11\n",
            "313 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>docstring</th>\n",
              "      <th>code</th>\n",
              "      <th>docstring_len</th>\n",
              "      <th>code_len</th>\n",
              "      <th>cleaned_code</th>\n",
              "      <th>cleaned_code_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [docstring, code, docstring_len, code_len, cleaned_code, cleaned_code_len]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbZrffwrSv05"
      },
      "source": [
        "nl_to_pl_df[(nl_to_pl_df['code_len'] > 256) & (nl_to_pl_df['code_len'] < 512)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJUyE3S39KWy"
      },
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    # if n_gpu > 0:\n",
        "    #     torch.cuda.manual_seed_all(seed)\n",
        "set_seed(0x1112233)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVD-3TjxY1SD"
      },
      "source": [
        "tok_ids_list=[]\n",
        "class NLPLSingleEntry(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 code_ids, \n",
        "                 code_mask, \n",
        "                 doc_ids,\n",
        "                 doc_mask,\n",
        "                 ):\n",
        "        self.code_ids = code_ids\n",
        "        self.code_mask = code_mask\n",
        "        self.doc_ids = doc_ids\n",
        "        self.doc_mask = doc_mask\n",
        "        #self.segment_ids = segment_ids\n",
        "        #self.label_id = label_id\n",
        "class NLPLDataSet():\n",
        "    def __init__(self, \n",
        "                 doc_tokenizer, \n",
        "                 code_tokenizer):\n",
        "        self.doc_tokenizer = doc_tokenizer\n",
        "        self.code_tokenizer = code_tokenizer\n",
        "\n",
        "    def prepare_tokens(self, \n",
        "                       samples, \n",
        "                       tokenizer, \n",
        "                       max_seq_length=0,\n",
        "                       data_type=None):\n",
        "        \"\"\"\n",
        "            Tokenizes an input sequence, adds padding and SOS+EOS \n",
        "        \"\"\"\n",
        "        toks = tokenizer.tokenize(samples)\n",
        "        # print(data_type)\n",
        "        # print(toks)\n",
        "        if max_seq_length > 2 and len(toks) > max_seq_length - 2:\n",
        "            toks = toks[:max_seq_length -2]\n",
        "        tok_ids =  tokenizer.convert_tokens_to_ids(toks)\n",
        "        ### We use pseudo-BERT process so we will add both CLS and SEP tokens for\n",
        "        ### src and target inputs\n",
        "        tok_ids = [tokenizer.cls_token_id] + tok_ids + [tokenizer.sep_token_id]\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [ 1 ] * len(tok_ids)\n",
        "\n",
        "        if len(tok_ids) < max_seq_length:\n",
        "            padding_length = max_seq_length - len(tok_ids)\n",
        "            tok_ids = tok_ids + ([tokenizer.pad_token_id] * padding_length)\n",
        "            input_mask = input_mask + ([ 0 ] * padding_length) ### Padded tokens are zero-masked\n",
        "        \n",
        "        # print(tok_ids)\n",
        "        return tok_ids, input_mask\n",
        "\n",
        "    def create_dataset(self,\n",
        "                    nl_to_pl_df,\n",
        "                    final_ds,\n",
        "                    sample_count=10000,\n",
        "                    max_doc_len=50,\n",
        "                    max_code_len=0):\n",
        "        \"\"\"\n",
        "            Reads from a dataframe, tokenizes and numericalizes both docstrings \n",
        "            and code. \n",
        "\n",
        "        \"\"\"\n",
        "        #final_ds = []\n",
        "        for idx in nl_to_pl_df.itertuples():\n",
        "            ## For SOS and EOS tokens 2 positions are left\n",
        "            if not idx.cleaned_code:\n",
        "                print(\"Invalid entry, No code found for:\", idx.docstring)\n",
        "            \n",
        "            doc_toks, doc_mask = self.prepare_tokens(idx.docstring,\n",
        "                                                      self.doc_tokenizer,\n",
        "                                                      max_doc_len,\n",
        "                                                      \"docs\")\n",
        "            code_toks, code_mask = self.prepare_tokens(idx.cleaned_code,\n",
        "                                                      self.code_tokenizer,\n",
        "                                                      max_code_len,\n",
        "                                                      \"code\")\n",
        "            #code_toks = None\n",
        "            ### Skip over current iteration if no valid code found\n",
        "\n",
        "            # print(code_toks)\n",
        "            # print(code_mask)\n",
        "            # print(doc_toks)\n",
        "            # print(doc_mask)\n",
        "            final_entry = NLPLSingleEntry(code_toks, \n",
        "                                            code_mask, \n",
        "                                            doc_toks, \n",
        "                                            doc_mask)\n",
        "            # print(final_entry.code_ids)\n",
        "            # print(final_entry.code_mask)\n",
        "            # print(final_entry.doc_ids)\n",
        "            # print(final_entry.doc_mask)\n",
        "            final_ds.append(final_entry)\n",
        "        #print(len(final_ds))\n",
        "        return final_ds\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rdWTWBQ6VJc"
      },
      "source": [
        "nl_to_pl_df[(nl_to_pl_df['cleaned_code_len'] >= MAX_VOCAB_LENGTH) & (nl_to_pl_df['cleaned_code_len'] >= MAX_LENGTH)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V71S9kbEyhv8"
      },
      "source": [
        "### We will use the same tokenizer for both docstrings and code\n",
        "final_ds = []\n",
        "MAX_LENGTH=256\n",
        "MAX_VOCAB_LENGTH=256\n",
        "assert(MAX_VOCAB_LENGTH <= MAX_LENGTH)\n",
        "#selected_elems = nl_to_pl_df[nl_to_pl_df['cleaned_code_len'] <= MAX_LENGTH]\n",
        "#selected_elems = my_df_copy[my_df_copy['code_len'] <= MAX_LENGTH]\n",
        "selected_elems = nl_to_pl_df[nl_to_pl_df['cleaned_code_len'] <= MAX_VOCAB_LENGTH]\n",
        "#selected_elems = nl_to_pl_df[(nl_to_pl_df['cleaned_code_len'] >= MAX_VOCAB_LENGTH) & (nl_to_pl_df['cleaned_code_len'] = MAX_LENGTH)]\n",
        "my_nlpl_ds = NLPLDataSet(auto_tokenizer, auto_tokenizer).create_dataset(selected_elems, \n",
        "                                                                        final_ds, \n",
        "                                                                        max_doc_len=MAX_VOCAB_LENGTH, \n",
        "                                                                        max_code_len=MAX_VOCAB_LENGTH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShA8ITXD9wfe"
      },
      "source": [
        "all_code_ids = torch.tensor([f.code_ids for f in my_nlpl_ds], dtype=torch.long)\n",
        "all_code_mask = torch.tensor([f.code_mask for f in my_nlpl_ds], dtype=torch.long)\n",
        "all_doc_ids = torch.tensor([f.doc_ids for f in my_nlpl_ds], dtype=torch.long)\n",
        "all_doc_mask = torch.tensor([f.doc_mask for f in my_nlpl_ds], dtype=torch.long)\n",
        "# if output_mode == \"classification\":\n",
        "#     all_label_ids = torch.tensor([f.label_id for f in my_nlpl_ds], dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(all_code_ids, \n",
        "                              all_code_mask, \n",
        "                              all_doc_ids, \n",
        "                              all_doc_mask)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKJG-nAJl3i9"
      },
      "source": [
        "dataset_size = len(train_dataset)\n",
        "dataset_indices = list(range(dataset_size))\n",
        "np.random.shuffle(dataset_indices)\n",
        "val_split_index = int(np.floor(0.2 * dataset_size))\n",
        "\n",
        "train_idx, val_idx = dataset_indices[val_split_index:], dataset_indices[:val_split_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grRIIkyMl4fY"
      },
      "source": [
        "BATCH_SIZE=16\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "\n",
        "#train_sampler = RandomSampler(train_dataset,) #if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_dataloader = DataLoader(train_dataset, sampler=val_sampler, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "#train_sampler = RandomSampler(train_dataset) #if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSmbw44v__OU"
      },
      "source": [
        "iter_one = next(iter(train_dataloader))\n",
        "iter_one_val = next(iter(val_dataloader))\n",
        "\n",
        "#iter_one[0]=all_code_ids, \n",
        "#iter_one[1]=all_code_mask, \n",
        "#iter_one[2]=all_doc_ids, \n",
        "#iter_one[3]=all_doc_mask\n",
        "#mask_reshape.shape\n",
        "# trg = iter_one[0]\n",
        "# trg_len = trg.shape[1]\n",
        "# trg_pad_mask = iter_one[1].unsqueeze(1).unsqueeze(2) \n",
        "# trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len))).bool()\n",
        "\n",
        "# #trg_sub_mask = [trg len, trg len]\n",
        "    \n",
        "# trg_mask = trg_pad_mask & trg_sub_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scH0xbQHKvS3"
      },
      "source": [
        "### Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2wiHHFBuXxi"
      },
      "source": [
        "class TransEncoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = MAX_LENGTH):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([TransEncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_PH9wz_KzuM"
      },
      "source": [
        "class TransEncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OodBNJ46LCQo"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK2KNXZSLEAz"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH18KwEnLFhF"
      },
      "source": [
        "class TransDecoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 1000):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([TransDecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "            \n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        #output =F.softmax(output, dim=2)  \n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IChtMK8QLHOP"
      },
      "source": [
        "class TransDecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        # query, key, value\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swxv3Y0SLJmE"
      },
      "source": [
        "class TransSeq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = src_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg, trg_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        trg_pad_mask = trg_mask.unsqueeze(1).unsqueeze(2) \n",
        "        \"\"\"\n",
        "            A boolean tensor of shape [batch size, 1, 1, trg len]\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, src_mask, trg, trg_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src_mask)\n",
        "        trg_mask = self.make_trg_mask(trg, trg_mask)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cJUUj5QLvbw"
      },
      "source": [
        "INPUT_DIM = auto_tokenizer.vocab_size\n",
        "OUTPUT_DIM = auto_tokenizer.vocab_size\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 4\n",
        "ENC_HEADS = 16\n",
        "DEC_HEADS = 16\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "enc = TransEncoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = TransDecoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device,\n",
        "              max_length=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK3PYSb6gT12"
      },
      "source": [
        "INPUT_DIM = auto_tokenizer.vocab_size\n",
        "OUTPUT_DIM = auto_tokenizer.vocab_size\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 1024\n",
        "DEC_PF_DIM = 1024\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "enc = TransEncoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = TransDecoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device,\n",
        "              max_length=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7yKcgxLPKiv"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olwlAq_B3boJ"
      },
      "source": [
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_baseline_128.pt ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lexhxG9rL8pQ"
      },
      "source": [
        "SRC_PAD_IDX = auto_tokenizer.pad_token_id #SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = auto_tokenizer.pad_token_id #TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "model = TransSeq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw9b7bQ-MIWM"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-yLC9A_Mmuo",
        "outputId": "3b7722c8-413b-4ea6-c731-0d18fab700e8"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 44,314,457 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cWIuwNUMQXs"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip, device):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        trg = batch[0].to(device)\n",
        "        trg_mask = batch[1].to(device)\n",
        "        src = batch[2].to(device)\n",
        "        src_mask = batch[3].to(device)\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, src_mask, trg[:,:-1], trg_mask[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26yD4gOnMr9U"
      },
      "source": [
        "def evaluate(model, iterator, criterion, device):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            trg = batch[0].to(device)\n",
        "            trg_mask = batch[1].to(device)\n",
        "            src = batch[2].to(device)\n",
        "            src_mask = batch[3].to(device)\n",
        "\n",
        "            output, _ = model(src, src_mask, trg[:,:-1], trg_mask[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGz-LB82MvFT"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBG82-UVaCab"
      },
      "source": [
        "loss = criterion(output, trg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBiqn6yYvgs7",
        "outputId": "fde60e14-f612-4635-f8e0-b7f4e2d9563f"
      },
      "source": [
        "loss.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.818306922912598"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hrlBV-kAM4h"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def run_train_eval_loop(model, \n",
        "                        train_dataloader,\n",
        "                        val_dataloader,\n",
        "                        optimizer,\n",
        "                        criterion,\n",
        "                        device,\n",
        "                        epochs=20,\n",
        "                        clip=1,\n",
        "                        best_valid_loss=float('inf'),\n",
        "                        file_path='end_capstone_baseline_128.pt',\n",
        "                        scheduler=None):\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "    \n",
        "        start_time = time.time()\n",
        "        \n",
        "        train_loss = train(model, train_dataloader, optimizer, criterion, clip, device)\n",
        "        valid_loss = evaluate(model, val_dataloader, criterion, device)\n",
        "        if(scheduler is not None):\n",
        "            scheduler.step(valid_loss)\n",
        "\n",
        "        end_time = time.time()\n",
        "        \n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save({\"model\":model.state_dict(),\n",
        "                \"optimizer\":optimizer.state_dict(),\n",
        "                \"loss\":valid_loss,\n",
        "                },file_path)\n",
        "        \n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYF9SIrotkAm"
      },
      "source": [
        "torch.save({\"model\":model.state_dict(),\n",
        "    \"optimizer\":optimizer.state_dict(),\n",
        "    \"loss\":7.88,\n",
        "    },'enc_mb_nowls_1024.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE1Xa6EJwk2F"
      },
      "source": [
        "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
        "    \n",
        "    assert n_rows * n_cols == n_heads\n",
        "    \n",
        "    fig = plt.figure(figsize=(15,25))\n",
        "    \n",
        "    for i in range(n_heads):\n",
        "        \n",
        "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
        "        \n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                           rotation=45)\n",
        "        ax.set_yticklabels(['']+translation)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiwj5jwOA_Lf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9ba708-e0f6-4e40-9bd7-b572b7aa5154"
      },
      "source": [
        "file_path='end_capstone_128sq_sth.pt'\n",
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-8,verbose=True)\n",
        "run_train_eval_loop(model,\n",
        "                    train_dataloader,\n",
        "                    val_dataloader,\n",
        "                    optimizer,\n",
        "                    criterion,\n",
        "                    device,\n",
        "                    epochs=100,\n",
        "                    clip=1,\n",
        "                    best_valid_loss=float('inf'),\n",
        "                    file_path=file_path,\n",
        "                    scheduler=scheduler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 14s\n",
            "\tTrain Loss: 5.885 | Train PPL: 359.771\n",
            "\t Val. Loss: 4.147 |  Val. PPL:  63.233\n",
            "Epoch: 02 | Time: 0m 14s\n",
            "\tTrain Loss: 3.572 | Train PPL:  35.599\n",
            "\t Val. Loss: 3.289 |  Val. PPL:  26.810\n",
            "Epoch: 03 | Time: 0m 14s\n",
            "\tTrain Loss: 2.956 | Train PPL:  19.221\n",
            "\t Val. Loss: 2.900 |  Val. PPL:  18.171\n",
            "Epoch: 04 | Time: 0m 14s\n",
            "\tTrain Loss: 2.583 | Train PPL:  13.236\n",
            "\t Val. Loss: 2.643 |  Val. PPL:  14.059\n",
            "Epoch: 05 | Time: 0m 14s\n",
            "\tTrain Loss: 2.308 | Train PPL:  10.056\n",
            "\t Val. Loss: 2.417 |  Val. PPL:  11.211\n",
            "Epoch: 06 | Time: 0m 14s\n",
            "\tTrain Loss: 2.071 | Train PPL:   7.933\n",
            "\t Val. Loss: 2.255 |  Val. PPL:   9.537\n",
            "Epoch: 07 | Time: 0m 14s\n",
            "\tTrain Loss: 1.886 | Train PPL:   6.593\n",
            "\t Val. Loss: 2.128 |  Val. PPL:   8.399\n",
            "Epoch: 08 | Time: 0m 14s\n",
            "\tTrain Loss: 1.710 | Train PPL:   5.530\n",
            "\t Val. Loss: 2.005 |  Val. PPL:   7.430\n",
            "Epoch: 09 | Time: 0m 14s\n",
            "\tTrain Loss: 1.565 | Train PPL:   4.782\n",
            "\t Val. Loss: 1.948 |  Val. PPL:   7.014\n",
            "Epoch: 10 | Time: 0m 14s\n",
            "\tTrain Loss: 1.436 | Train PPL:   4.205\n",
            "\t Val. Loss: 1.906 |  Val. PPL:   6.728\n",
            "Epoch: 11 | Time: 0m 14s\n",
            "\tTrain Loss: 1.311 | Train PPL:   3.709\n",
            "\t Val. Loss: 1.779 |  Val. PPL:   5.927\n",
            "Epoch: 12 | Time: 0m 14s\n",
            "\tTrain Loss: 1.210 | Train PPL:   3.354\n",
            "\t Val. Loss: 1.738 |  Val. PPL:   5.688\n",
            "Epoch: 13 | Time: 0m 14s\n",
            "\tTrain Loss: 1.110 | Train PPL:   3.035\n",
            "\t Val. Loss: 1.680 |  Val. PPL:   5.364\n",
            "Epoch: 14 | Time: 0m 14s\n",
            "\tTrain Loss: 1.026 | Train PPL:   2.789\n",
            "\t Val. Loss: 1.634 |  Val. PPL:   5.125\n",
            "Epoch: 15 | Time: 0m 15s\n",
            "\tTrain Loss: 0.953 | Train PPL:   2.593\n",
            "\t Val. Loss: 1.660 |  Val. PPL:   5.257\n",
            "Epoch: 16 | Time: 0m 15s\n",
            "\tTrain Loss: 0.882 | Train PPL:   2.417\n",
            "\t Val. Loss: 1.626 |  Val. PPL:   5.081\n",
            "Epoch: 17 | Time: 0m 15s\n",
            "\tTrain Loss: 0.829 | Train PPL:   2.291\n",
            "\t Val. Loss: 1.610 |  Val. PPL:   5.003\n",
            "Epoch: 18 | Time: 0m 15s\n",
            "\tTrain Loss: 0.777 | Train PPL:   2.174\n",
            "\t Val. Loss: 1.610 |  Val. PPL:   5.003\n",
            "Epoch: 19 | Time: 0m 15s\n",
            "\tTrain Loss: 0.726 | Train PPL:   2.066\n",
            "\t Val. Loss: 1.569 |  Val. PPL:   4.802\n",
            "Epoch: 20 | Time: 0m 15s\n",
            "\tTrain Loss: 0.686 | Train PPL:   1.986\n",
            "\t Val. Loss: 1.626 |  Val. PPL:   5.084\n",
            "Epoch: 21 | Time: 0m 15s\n",
            "\tTrain Loss: 0.646 | Train PPL:   1.908\n",
            "\t Val. Loss: 1.581 |  Val. PPL:   4.858\n",
            "Epoch: 22 | Time: 0m 15s\n",
            "\tTrain Loss: 0.615 | Train PPL:   1.850\n",
            "\t Val. Loss: 1.613 |  Val. PPL:   5.018\n",
            "Epoch: 23 | Time: 0m 15s\n",
            "\tTrain Loss: 0.587 | Train PPL:   1.798\n",
            "\t Val. Loss: 1.575 |  Val. PPL:   4.833\n",
            "Epoch: 24 | Time: 0m 15s\n",
            "\tTrain Loss: 0.552 | Train PPL:   1.737\n",
            "\t Val. Loss: 1.617 |  Val. PPL:   5.038\n",
            "Epoch    25: reducing learning rate of group 0 to 5.0000e-05.\n",
            "Epoch: 25 | Time: 0m 15s\n",
            "\tTrain Loss: 0.527 | Train PPL:   1.695\n",
            "\t Val. Loss: 1.607 |  Val. PPL:   4.990\n",
            "Epoch: 26 | Time: 0m 15s\n",
            "\tTrain Loss: 0.432 | Train PPL:   1.540\n",
            "\t Val. Loss: 1.552 |  Val. PPL:   4.721\n",
            "Epoch: 27 | Time: 0m 15s\n",
            "\tTrain Loss: 0.390 | Train PPL:   1.478\n",
            "\t Val. Loss: 1.535 |  Val. PPL:   4.643\n",
            "Epoch: 28 | Time: 0m 15s\n",
            "\tTrain Loss: 0.373 | Train PPL:   1.452\n",
            "\t Val. Loss: 1.575 |  Val. PPL:   4.829\n",
            "Epoch: 29 | Time: 0m 15s\n",
            "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
            "\t Val. Loss: 1.536 |  Val. PPL:   4.645\n",
            "Epoch: 30 | Time: 0m 15s\n",
            "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
            "\t Val. Loss: 1.556 |  Val. PPL:   4.739\n",
            "Epoch: 31 | Time: 0m 15s\n",
            "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
            "\t Val. Loss: 1.555 |  Val. PPL:   4.735\n",
            "Epoch: 32 | Time: 0m 15s\n",
            "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
            "\t Val. Loss: 1.564 |  Val. PPL:   4.778\n",
            "Epoch    33: reducing learning rate of group 0 to 5.0000e-06.\n",
            "Epoch: 33 | Time: 0m 15s\n",
            "\tTrain Loss: 0.325 | Train PPL:   1.383\n",
            "\t Val. Loss: 1.551 |  Val. PPL:   4.716\n",
            "Epoch: 34 | Time: 0m 15s\n",
            "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
            "\t Val. Loss: 1.595 |  Val. PPL:   4.927\n",
            "Epoch: 35 | Time: 0m 15s\n",
            "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
            "\t Val. Loss: 1.529 |  Val. PPL:   4.614\n",
            "Epoch: 36 | Time: 0m 15s\n",
            "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
            "\t Val. Loss: 1.570 |  Val. PPL:   4.806\n",
            "Epoch: 37 | Time: 0m 15s\n",
            "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
            "\t Val. Loss: 1.550 |  Val. PPL:   4.711\n",
            "Epoch: 38 | Time: 0m 15s\n",
            "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
            "\t Val. Loss: 1.585 |  Val. PPL:   4.878\n",
            "Epoch: 39 | Time: 0m 15s\n",
            "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
            "\t Val. Loss: 1.560 |  Val. PPL:   4.757\n",
            "Epoch: 40 | Time: 0m 15s\n",
            "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
            "\t Val. Loss: 1.534 |  Val. PPL:   4.635\n",
            "Epoch    41: reducing learning rate of group 0 to 5.0000e-07.\n",
            "Epoch: 41 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
            "\t Val. Loss: 1.578 |  Val. PPL:   4.846\n",
            "Epoch: 42 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
            "\t Val. Loss: 1.555 |  Val. PPL:   4.737\n",
            "Epoch: 43 | Time: 0m 15s\n",
            "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
            "\t Val. Loss: 1.563 |  Val. PPL:   4.773\n",
            "Epoch: 44 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
            "\t Val. Loss: 1.544 |  Val. PPL:   4.685\n",
            "Epoch: 45 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
            "\t Val. Loss: 1.588 |  Val. PPL:   4.893\n",
            "Epoch: 46 | Time: 0m 15s\n",
            "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
            "\t Val. Loss: 1.568 |  Val. PPL:   4.796\n",
            "Epoch: 47 | Time: 0m 15s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 1.505 |  Val. PPL:   4.505\n",
            "Epoch: 48 | Time: 0m 15s\n",
            "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
            "\t Val. Loss: 1.565 |  Val. PPL:   4.782\n",
            "Epoch: 49 | Time: 0m 15s\n",
            "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
            "\t Val. Loss: 1.579 |  Val. PPL:   4.849\n",
            "Epoch: 50 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
            "\t Val. Loss: 1.548 |  Val. PPL:   4.701\n",
            "Epoch: 51 | Time: 0m 15s\n",
            "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
            "\t Val. Loss: 1.555 |  Val. PPL:   4.735\n",
            "Epoch: 52 | Time: 0m 15s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 1.544 |  Val. PPL:   4.682\n",
            "Epoch    53: reducing learning rate of group 0 to 5.0000e-08.\n",
            "Epoch: 53 | Time: 0m 15s\n",
            "\tTrain Loss: 0.301 | Train PPL:   1.352\n",
            "\t Val. Loss: 1.593 |  Val. PPL:   4.917\n",
            "Epoch: 54 | Time: 0m 15s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 1.524 |  Val. PPL:   4.589\n",
            "Epoch: 55 | Time: 0m 15s\n",
            "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
            "\t Val. Loss: 1.564 |  Val. PPL:   4.776\n",
            "Epoch: 56 | Time: 0m 15s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 1.554 |  Val. PPL:   4.728\n",
            "Epoch: 57 | Time: 0m 15s\n",
            "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
            "\t Val. Loss: 1.527 |  Val. PPL:   4.607\n",
            "Epoch: 58 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
            "\t Val. Loss: 1.574 |  Val. PPL:   4.826\n",
            "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Epoch: 59 | Time: 0m 15s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
            "\t Val. Loss: 1.597 |  Val. PPL:   4.939\n",
            "Epoch: 60 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
            "\t Val. Loss: 1.561 |  Val. PPL:   4.765\n",
            "Epoch: 61 | Time: 0m 15s\n",
            "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
            "\t Val. Loss: 1.558 |  Val. PPL:   4.747\n",
            "Epoch: 62 | Time: 0m 15s\n",
            "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
            "\t Val. Loss: 1.616 |  Val. PPL:   5.032\n",
            "Epoch: 63 | Time: 0m 15s\n",
            "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
            "\t Val. Loss: 1.536 |  Val. PPL:   4.646\n",
            "Epoch: 64 | Time: 0m 15s\n",
            "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
            "\t Val. Loss: 1.567 |  Val. PPL:   4.792\n",
            "Epoch: 65 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
            "\t Val. Loss: 1.557 |  Val. PPL:   4.743\n",
            "Epoch: 66 | Time: 0m 15s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
            "\t Val. Loss: 1.559 |  Val. PPL:   4.754\n",
            "Epoch: 67 | Time: 0m 15s\n",
            "\tTrain Loss: 0.301 | Train PPL:   1.352\n",
            "\t Val. Loss: 1.577 |  Val. PPL:   4.842\n",
            "Epoch: 68 | Time: 0m 15s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 1.574 |  Val. PPL:   4.825\n",
            "Epoch: 69 | Time: 0m 15s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.353\n",
            "\t Val. Loss: 1.559 |  Val. PPL:   4.753\n",
            "Epoch: 70 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
            "\t Val. Loss: 1.539 |  Val. PPL:   4.660\n",
            "Epoch: 71 | Time: 0m 15s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
            "\t Val. Loss: 1.586 |  Val. PPL:   4.887\n",
            "Epoch: 72 | Time: 0m 15s\n",
            "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
            "\t Val. Loss: 1.565 |  Val. PPL:   4.784\n",
            "Epoch: 73 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
            "\t Val. Loss: 1.552 |  Val. PPL:   4.721\n",
            "Epoch: 74 | Time: 0m 15s\n",
            "\tTrain Loss: 0.300 | Train PPL:   1.349\n",
            "\t Val. Loss: 1.552 |  Val. PPL:   4.721\n",
            "Epoch: 75 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
            "\t Val. Loss: 1.582 |  Val. PPL:   4.862\n",
            "Epoch: 76 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
            "\t Val. Loss: 1.561 |  Val. PPL:   4.761\n",
            "Epoch: 77 | Time: 0m 15s\n",
            "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
            "\t Val. Loss: 1.585 |  Val. PPL:   4.878\n",
            "Epoch: 78 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
            "\t Val. Loss: 1.569 |  Val. PPL:   4.800\n",
            "Epoch: 79 | Time: 0m 15s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
            "\t Val. Loss: 1.531 |  Val. PPL:   4.622\n",
            "Epoch: 80 | Time: 0m 15s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 1.540 |  Val. PPL:   4.665\n",
            "Epoch: 81 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
            "\t Val. Loss: 1.568 |  Val. PPL:   4.798\n",
            "Epoch: 82 | Time: 0m 15s\n",
            "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
            "\t Val. Loss: 1.516 |  Val. PPL:   4.556\n",
            "Epoch: 83 | Time: 0m 15s\n",
            "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
            "\t Val. Loss: 1.554 |  Val. PPL:   4.730\n",
            "Epoch: 84 | Time: 0m 15s\n",
            "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
            "\t Val. Loss: 1.549 |  Val. PPL:   4.708\n",
            "Epoch: 85 | Time: 0m 15s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 1.533 |  Val. PPL:   4.630\n",
            "Epoch: 86 | Time: 0m 15s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
            "\t Val. Loss: 1.624 |  Val. PPL:   5.072\n",
            "Epoch: 87 | Time: 0m 15s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
            "\t Val. Loss: 1.558 |  Val. PPL:   4.747\n",
            "Epoch: 88 | Time: 0m 15s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
            "\t Val. Loss: 1.545 |  Val. PPL:   4.686\n",
            "Epoch: 89 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
            "\t Val. Loss: 1.587 |  Val. PPL:   4.891\n",
            "Epoch: 90 | Time: 0m 15s\n",
            "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
            "\t Val. Loss: 1.587 |  Val. PPL:   4.889\n",
            "Epoch: 91 | Time: 0m 15s\n",
            "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
            "\t Val. Loss: 1.591 |  Val. PPL:   4.911\n",
            "Epoch: 92 | Time: 0m 15s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 1.534 |  Val. PPL:   4.638\n",
            "Epoch: 93 | Time: 0m 15s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
            "\t Val. Loss: 1.531 |  Val. PPL:   4.625\n",
            "Epoch: 94 | Time: 0m 15s\n",
            "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
            "\t Val. Loss: 1.566 |  Val. PPL:   4.789\n",
            "Epoch: 95 | Time: 0m 15s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 1.524 |  Val. PPL:   4.591\n",
            "Epoch: 96 | Time: 0m 15s\n",
            "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
            "\t Val. Loss: 1.544 |  Val. PPL:   4.684\n",
            "Epoch: 97 | Time: 0m 15s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 1.635 |  Val. PPL:   5.129\n",
            "Epoch: 98 | Time: 0m 15s\n",
            "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
            "\t Val. Loss: 1.576 |  Val. PPL:   4.834\n",
            "Epoch: 99 | Time: 0m 15s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
            "\t Val. Loss: 1.554 |  Val. PPL:   4.729\n",
            "Epoch: 100 | Time: 0m 15s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
            "\t Val. Loss: 1.569 |  Val. PPL:   4.800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP8TCASrKRB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f2ab9b-bde3-4f16-ba7e-c16a37a74d76"
      },
      "source": [
        "file_path='end_capstone_128sq_sth.pt'\n",
        "\n",
        "chkpt = torch.load(file_path)\n",
        "print(chkpt['loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5051638835242815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H35YKslnmSWT",
        "outputId": "a902e384-8d35-4269-a18f-3451d748d961"
      },
      "source": [
        "model.load_state_dict(chkpt['model'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xbt0QojKy6M",
        "outputId": "045c9cb6-6ac9-4a49-c38a-1a6097cc346d"
      },
      "source": [
        "file_path='end_capstone_128sq_sth256_2.pt'\n",
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-8,verbose=True)\n",
        "run_train_eval_loop(model,\n",
        "                    train_dataloader,\n",
        "                    val_dataloader,\n",
        "                    optimizer,\n",
        "                    criterion,\n",
        "                    device,\n",
        "                    epochs=100,\n",
        "                    clip=1,\n",
        "                    best_valid_loss=chkpt['loss'],\n",
        "                    file_path=file_path,\n",
        "                    scheduler=scheduler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 50s\n",
            "\tTrain Loss: 2.898 | Train PPL:  18.143\n",
            "\t Val. Loss: 2.943 |  Val. PPL:  18.965\n",
            "Epoch: 02 | Time: 0m 52s\n",
            "\tTrain Loss: 2.889 | Train PPL:  17.974\n",
            "\t Val. Loss: 2.945 |  Val. PPL:  19.015\n",
            "Epoch: 03 | Time: 0m 51s\n",
            "\tTrain Loss: 2.882 | Train PPL:  17.858\n",
            "\t Val. Loss: 2.954 |  Val. PPL:  19.177\n",
            "Epoch: 04 | Time: 0m 51s\n",
            "\tTrain Loss: 2.889 | Train PPL:  17.984\n",
            "\t Val. Loss: 2.963 |  Val. PPL:  19.359\n",
            "Epoch: 05 | Time: 0m 51s\n",
            "\tTrain Loss: 2.893 | Train PPL:  18.043\n",
            "\t Val. Loss: 2.973 |  Val. PPL:  19.553\n",
            "Epoch: 06 | Time: 0m 51s\n",
            "\tTrain Loss: 2.888 | Train PPL:  17.957\n",
            "\t Val. Loss: 2.938 |  Val. PPL:  18.883\n",
            "Epoch: 07 | Time: 0m 51s\n",
            "\tTrain Loss: 2.894 | Train PPL:  18.061\n",
            "\t Val. Loss: 2.972 |  Val. PPL:  19.527\n",
            "Epoch: 08 | Time: 0m 51s\n",
            "\tTrain Loss: 2.887 | Train PPL:  17.931\n",
            "\t Val. Loss: 2.957 |  Val. PPL:  19.240\n",
            "Epoch: 09 | Time: 0m 51s\n",
            "\tTrain Loss: 2.893 | Train PPL:  18.056\n",
            "\t Val. Loss: 2.929 |  Val. PPL:  18.709\n",
            "Epoch: 10 | Time: 0m 51s\n",
            "\tTrain Loss: 2.872 | Train PPL:  17.673\n",
            "\t Val. Loss: 2.969 |  Val. PPL:  19.473\n",
            "Epoch: 11 | Time: 0m 51s\n",
            "\tTrain Loss: 2.879 | Train PPL:  17.801\n",
            "\t Val. Loss: 2.945 |  Val. PPL:  19.012\n",
            "Epoch: 12 | Time: 0m 51s\n",
            "\tTrain Loss: 2.883 | Train PPL:  17.866\n",
            "\t Val. Loss: 2.918 |  Val. PPL:  18.508\n",
            "Epoch: 13 | Time: 0m 51s\n",
            "\tTrain Loss: 2.886 | Train PPL:  17.916\n",
            "\t Val. Loss: 2.963 |  Val. PPL:  19.354\n",
            "Epoch: 14 | Time: 0m 51s\n",
            "\tTrain Loss: 2.875 | Train PPL:  17.717\n",
            "\t Val. Loss: 2.927 |  Val. PPL:  18.676\n",
            "Epoch: 15 | Time: 0m 51s\n",
            "\tTrain Loss: 2.888 | Train PPL:  17.955\n",
            "\t Val. Loss: 2.948 |  Val. PPL:  19.065\n",
            "Epoch: 16 | Time: 0m 51s\n",
            "\tTrain Loss: 2.886 | Train PPL:  17.926\n",
            "\t Val. Loss: 2.958 |  Val. PPL:  19.268\n",
            "Epoch: 17 | Time: 0m 51s\n",
            "\tTrain Loss: 2.867 | Train PPL:  17.583\n",
            "\t Val. Loss: 2.959 |  Val. PPL:  19.285\n",
            "Epoch: 18 | Time: 0m 51s\n",
            "\tTrain Loss: 2.864 | Train PPL:  17.528\n",
            "\t Val. Loss: 2.948 |  Val. PPL:  19.062\n",
            "Epoch: 19 | Time: 0m 51s\n",
            "\tTrain Loss: 2.879 | Train PPL:  17.804\n",
            "\t Val. Loss: 2.931 |  Val. PPL:  18.753\n",
            "Epoch: 20 | Time: 0m 51s\n",
            "\tTrain Loss: 2.882 | Train PPL:  17.846\n",
            "\t Val. Loss: 2.912 |  Val. PPL:  18.392\n",
            "Epoch: 21 | Time: 0m 51s\n",
            "\tTrain Loss: 2.874 | Train PPL:  17.709\n",
            "\t Val. Loss: 2.928 |  Val. PPL:  18.693\n",
            "Epoch: 22 | Time: 0m 51s\n",
            "\tTrain Loss: 2.863 | Train PPL:  17.512\n",
            "\t Val. Loss: 2.960 |  Val. PPL:  19.295\n",
            "Epoch: 23 | Time: 0m 51s\n",
            "\tTrain Loss: 2.863 | Train PPL:  17.520\n",
            "\t Val. Loss: 2.952 |  Val. PPL:  19.141\n",
            "Epoch: 24 | Time: 0m 51s\n",
            "\tTrain Loss: 2.875 | Train PPL:  17.727\n",
            "\t Val. Loss: 2.929 |  Val. PPL:  18.708\n",
            "Epoch: 25 | Time: 0m 51s\n",
            "\tTrain Loss: 2.861 | Train PPL:  17.481\n",
            "\t Val. Loss: 2.909 |  Val. PPL:  18.345\n",
            "Epoch: 26 | Time: 0m 51s\n",
            "\tTrain Loss: 2.867 | Train PPL:  17.583\n",
            "\t Val. Loss: 2.959 |  Val. PPL:  19.277\n",
            "Epoch: 27 | Time: 0m 51s\n",
            "\tTrain Loss: 2.866 | Train PPL:  17.566\n",
            "\t Val. Loss: 2.924 |  Val. PPL:  18.607\n",
            "Epoch: 28 | Time: 0m 51s\n",
            "\tTrain Loss: 2.871 | Train PPL:  17.649\n",
            "\t Val. Loss: 2.924 |  Val. PPL:  18.615\n",
            "Epoch: 29 | Time: 0m 51s\n",
            "\tTrain Loss: 2.868 | Train PPL:  17.595\n",
            "\t Val. Loss: 2.930 |  Val. PPL:  18.731\n",
            "Epoch: 30 | Time: 0m 51s\n",
            "\tTrain Loss: 2.867 | Train PPL:  17.588\n",
            "\t Val. Loss: 2.951 |  Val. PPL:  19.127\n",
            "Epoch: 31 | Time: 0m 51s\n",
            "\tTrain Loss: 2.873 | Train PPL:  17.693\n",
            "\t Val. Loss: 2.923 |  Val. PPL:  18.593\n",
            "Epoch: 32 | Time: 0m 51s\n",
            "\tTrain Loss: 2.854 | Train PPL:  17.355\n",
            "\t Val. Loss: 2.937 |  Val. PPL:  18.858\n",
            "Epoch: 33 | Time: 0m 51s\n",
            "\tTrain Loss: 2.878 | Train PPL:  17.774\n",
            "\t Val. Loss: 2.923 |  Val. PPL:  18.589\n",
            "Epoch: 34 | Time: 0m 51s\n",
            "\tTrain Loss: 2.864 | Train PPL:  17.532\n",
            "\t Val. Loss: 2.951 |  Val. PPL:  19.125\n",
            "Epoch: 35 | Time: 0m 51s\n",
            "\tTrain Loss: 2.862 | Train PPL:  17.505\n",
            "\t Val. Loss: 2.931 |  Val. PPL:  18.744\n",
            "Epoch: 36 | Time: 0m 51s\n",
            "\tTrain Loss: 2.854 | Train PPL:  17.357\n",
            "\t Val. Loss: 2.958 |  Val. PPL:  19.267\n",
            "Epoch: 37 | Time: 0m 51s\n",
            "\tTrain Loss: 2.855 | Train PPL:  17.368\n",
            "\t Val. Loss: 2.950 |  Val. PPL:  19.109\n",
            "Epoch: 38 | Time: 0m 51s\n",
            "\tTrain Loss: 2.863 | Train PPL:  17.508\n",
            "\t Val. Loss: 2.925 |  Val. PPL:  18.638\n",
            "Epoch: 39 | Time: 0m 51s\n",
            "\tTrain Loss: 2.860 | Train PPL:  17.454\n",
            "\t Val. Loss: 2.934 |  Val. PPL:  18.796\n",
            "Epoch: 40 | Time: 0m 51s\n",
            "\tTrain Loss: 2.852 | Train PPL:  17.327\n",
            "\t Val. Loss: 2.929 |  Val. PPL:  18.718\n",
            "Epoch: 41 | Time: 0m 51s\n",
            "\tTrain Loss: 2.867 | Train PPL:  17.578\n",
            "\t Val. Loss: 2.936 |  Val. PPL:  18.832\n",
            "Epoch: 42 | Time: 0m 51s\n",
            "\tTrain Loss: 2.870 | Train PPL:  17.635\n",
            "\t Val. Loss: 2.943 |  Val. PPL:  18.981\n",
            "Epoch: 43 | Time: 0m 51s\n",
            "\tTrain Loss: 2.859 | Train PPL:  17.444\n",
            "\t Val. Loss: 2.914 |  Val. PPL:  18.425\n",
            "Epoch: 44 | Time: 0m 51s\n",
            "\tTrain Loss: 2.869 | Train PPL:  17.614\n",
            "\t Val. Loss: 2.913 |  Val. PPL:  18.418\n",
            "Epoch: 45 | Time: 0m 51s\n",
            "\tTrain Loss: 2.861 | Train PPL:  17.478\n",
            "\t Val. Loss: 2.915 |  Val. PPL:  18.445\n",
            "Epoch: 46 | Time: 0m 51s\n",
            "\tTrain Loss: 2.857 | Train PPL:  17.404\n",
            "\t Val. Loss: 2.931 |  Val. PPL:  18.751\n",
            "Epoch: 47 | Time: 0m 51s\n",
            "\tTrain Loss: 2.854 | Train PPL:  17.351\n",
            "\t Val. Loss: 2.914 |  Val. PPL:  18.438\n",
            "Epoch: 48 | Time: 0m 51s\n",
            "\tTrain Loss: 2.858 | Train PPL:  17.433\n",
            "\t Val. Loss: 2.867 |  Val. PPL:  17.580\n",
            "Epoch: 49 | Time: 0m 51s\n",
            "\tTrain Loss: 2.843 | Train PPL:  17.161\n",
            "\t Val. Loss: 2.920 |  Val. PPL:  18.550\n",
            "Epoch: 50 | Time: 0m 51s\n",
            "\tTrain Loss: 2.862 | Train PPL:  17.494\n",
            "\t Val. Loss: 2.895 |  Val. PPL:  18.079\n",
            "Epoch: 51 | Time: 0m 51s\n",
            "\tTrain Loss: 2.852 | Train PPL:  17.316\n",
            "\t Val. Loss: 2.860 |  Val. PPL:  17.466\n",
            "Epoch: 52 | Time: 0m 51s\n",
            "\tTrain Loss: 2.834 | Train PPL:  17.015\n",
            "\t Val. Loss: 2.879 |  Val. PPL:  17.792\n",
            "Epoch: 53 | Time: 0m 51s\n",
            "\tTrain Loss: 2.841 | Train PPL:  17.125\n",
            "\t Val. Loss: 2.919 |  Val. PPL:  18.528\n",
            "Epoch: 54 | Time: 0m 51s\n",
            "\tTrain Loss: 2.843 | Train PPL:  17.167\n",
            "\t Val. Loss: 2.925 |  Val. PPL:  18.641\n",
            "Epoch: 55 | Time: 0m 51s\n",
            "\tTrain Loss: 2.834 | Train PPL:  17.016\n",
            "\t Val. Loss: 2.929 |  Val. PPL:  18.708\n",
            "Epoch: 56 | Time: 0m 51s\n",
            "\tTrain Loss: 2.849 | Train PPL:  17.277\n",
            "\t Val. Loss: 2.902 |  Val. PPL:  18.210\n",
            "Epoch: 57 | Time: 0m 51s\n",
            "\tTrain Loss: 2.842 | Train PPL:  17.153\n",
            "\t Val. Loss: 2.903 |  Val. PPL:  18.225\n",
            "Epoch: 58 | Time: 0m 51s\n",
            "\tTrain Loss: 2.837 | Train PPL:  17.059\n",
            "\t Val. Loss: 2.890 |  Val. PPL:  17.987\n",
            "Epoch: 59 | Time: 0m 51s\n",
            "\tTrain Loss: 2.847 | Train PPL:  17.241\n",
            "\t Val. Loss: 2.926 |  Val. PPL:  18.644\n",
            "Epoch: 60 | Time: 0m 51s\n",
            "\tTrain Loss: 2.839 | Train PPL:  17.104\n",
            "\t Val. Loss: 2.877 |  Val. PPL:  17.754\n",
            "Epoch: 61 | Time: 0m 51s\n",
            "\tTrain Loss: 2.832 | Train PPL:  16.976\n",
            "\t Val. Loss: 2.891 |  Val. PPL:  18.011\n",
            "Epoch: 62 | Time: 0m 51s\n",
            "\tTrain Loss: 2.832 | Train PPL:  16.974\n",
            "\t Val. Loss: 2.880 |  Val. PPL:  17.820\n",
            "Epoch: 63 | Time: 0m 51s\n",
            "\tTrain Loss: 2.835 | Train PPL:  17.024\n",
            "\t Val. Loss: 2.922 |  Val. PPL:  18.574\n",
            "Epoch: 64 | Time: 0m 51s\n",
            "\tTrain Loss: 2.844 | Train PPL:  17.190\n",
            "\t Val. Loss: 2.873 |  Val. PPL:  17.692\n",
            "Epoch: 65 | Time: 0m 51s\n",
            "\tTrain Loss: 2.846 | Train PPL:  17.225\n",
            "\t Val. Loss: 2.883 |  Val. PPL:  17.871\n",
            "Epoch: 66 | Time: 0m 51s\n",
            "\tTrain Loss: 2.833 | Train PPL:  16.994\n",
            "\t Val. Loss: 2.871 |  Val. PPL:  17.660\n",
            "Epoch: 67 | Time: 0m 51s\n",
            "\tTrain Loss: 2.838 | Train PPL:  17.081\n",
            "\t Val. Loss: 2.845 |  Val. PPL:  17.204\n",
            "Epoch: 68 | Time: 0m 51s\n",
            "\tTrain Loss: 2.834 | Train PPL:  17.015\n",
            "\t Val. Loss: 2.891 |  Val. PPL:  18.015\n",
            "Epoch: 69 | Time: 0m 51s\n",
            "\tTrain Loss: 2.844 | Train PPL:  17.187\n",
            "\t Val. Loss: 2.876 |  Val. PPL:  17.739\n",
            "Epoch: 70 | Time: 0m 51s\n",
            "\tTrain Loss: 2.832 | Train PPL:  16.974\n",
            "\t Val. Loss: 2.882 |  Val. PPL:  17.854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KrWgIHbvMVM"
      },
      "source": [
        "LEARNING_RATE = 0.00005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "run_train_eval_loop(model,\n",
        "                    train_dataloader,\n",
        "                    val_dataloader,\n",
        "                    optimizer,\n",
        "                    criterion,\n",
        "                    device,\n",
        "                    epochs=20,\n",
        "                    clip=1.4,\n",
        "                    best_valid_loss=chkpt['loss'],\n",
        "                    file_path=file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZO2dHwDSKQT",
        "outputId": "8a96c701-ae11-43fc-8dc4-820c39e61d7b"
      },
      "source": [
        "chkpt['loss']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.520801103340005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRBviYJnKOUT"
      },
      "source": [
        "file_path='end_capstone_codesearch_256_seqLen512.pt'\n",
        "LEARNING_RATE = 0.00005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "run_train_eval_loop(model,\n",
        "                    train_dataloader,\n",
        "                    val_dataloader,\n",
        "                    optimizer,\n",
        "                    criterion,\n",
        "                    device,\n",
        "                    epochs=20,\n",
        "                    clip=1.4,\n",
        "                    best_valid_loss=5.050,\n",
        "                    file_path=file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2husK3UdJv5"
      },
      "source": [
        "run_train_eval_loop(model,\n",
        "                    train_dataloader,\n",
        "                    val_dataloader,\n",
        "                    optimizer,\n",
        "                    criterion,\n",
        "                    device,\n",
        "                    epochs=20,\n",
        "                    clip=1,\n",
        "                    best_valid_loss=chkpt['loss'],\n",
        "                    file_path=file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvyYB8qwfCN1"
      },
      "source": [
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_256_mod2.pt /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxT0tD8jWK81"
      },
      "source": [
        "!cp /content/end_capstone_LRManaged_256.pt /content/drive/MyDrive/EVA4/END_Capstone/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNZ04ld7fJzE",
        "outputId": "c8f373a2-c95e-4920-902d-69170a2694d3"
      },
      "source": [
        "chkpt = torch.load(\"/content/end_capstone_128sq_sth.pt\")\n",
        "model.load_state_dict(chkpt['model'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZU107Z7y4IF"
      },
      "source": [
        "!cp tut6-model.pt /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_256_mod2.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJT9UuDB8zyk"
      },
      "source": [
        "sentence=\"Write a python program to add 2 numbers\"\n",
        "tokenizer = auto_tokenizer\n",
        "dataset_handler = NLPLDataSet(tokenizer, tokenizer)\n",
        "src_indexes, src_mask =  dataset_handler.prepare_tokens(sentence, dataset_handler.doc_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFGCa94292bR"
      },
      "source": [
        "src_mask = torch.LongTensor(src_mask).unsqueeze(0)#.to(device)\n",
        "    \n",
        "src_mask = model.make_src_mask(src_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghm2RaIG-CHn"
      },
      "source": [
        "!cp /content/end_capstone_codesearch_256_seqLen512.pt /content/drive/MyDrive/EVA4/END_Capstone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kvI-A00R3Bp"
      },
      "source": [
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_codesearch_256_seqLen512.pt ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imWxpZVvhqAD"
      },
      "source": [
        "def get_code(sentence,\n",
        "             tokenizer,\n",
        "             model, \n",
        "             device, \n",
        "             max_len = 100):\n",
        "    \n",
        "    model.eval()\n",
        "    dataset_handler = NLPLDataSet(tokenizer, tokenizer)\n",
        "    src_indexes, src_mask =  dataset_handler.prepare_tokens(sentence, dataset_handler.doc_tokenizer)\n",
        "    # if isinstance(sentence, str):\n",
        "    #     nlp = spacy.load('de')\n",
        "    #     tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    # else:\n",
        "    #     tokens = [token.lower() for token in sentence]\n",
        "    #dataset_handler.prepare_tokens()\n",
        "\n",
        "    #tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
        "        \n",
        "    #src_indexes = [tokenizer.convert_tokens_to_ids[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    src_mask = torch.LongTensor(src_mask).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_mask)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [tokenizer.cls_token_id]\n",
        "    #trg_mask = [1]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        base_mask = torch.LongTensor([1]*(i+1)).unsqueeze(0).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor, base_mask)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == tokenizer.sep_token_id:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [tokenizer.convert_ids_to_tokens(i) for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk1_6Jpc8o-x"
      },
      "source": [
        "model.load_state_dict(chkpt['model'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ammem8M7qAL5",
        "outputId": "6f8726cc-c777-4e60-93af-e5883b0060d0"
      },
      "source": [
        "input_text = \"Write a python program to check if a string has digits\"\n",
        "splitted_text = auto_tokenizer.tokenize(input_text)\n",
        "mycode, attention_val = get_code(input_text,auto_tokenizer, model, device,max_len=200)\n",
        "output = auto_tokenizer.convert_tokens_to_string(mycode[:-1])\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "if(isinstance(ch) == 1):\n",
            "    print(ord(ch)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzDP8GI2ExK4",
        "outputId": "9708f55c-9538-46c2-ad29-5dc5c01d4055"
      },
      "source": [
        "attention_val[:,1:2,:,:].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 135, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayl995l96d6L"
      },
      "source": [
        "import matplotlib.ticker as ticker\n",
        "\n",
        "display_attention(splitted_text, mycode, attention_val[:,1,:,:].unsqueeze(1), n_heads=1, n_rows=1, n_cols=1)\n",
        "#display_attention(splitted_text, mycode, attention_val[:,4:8,:,:], n_heads=4, n_rows=2, n_cols=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oo7rjqO83t6"
      },
      "source": [
        "[t.lower() for t in input_text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_Arp5Z30Hds"
      },
      "source": [
        "def right_posth_posth(pos):\n",
        "    global left_pos\n",
        "    while(pos >= 0):\n",
        "           if right += 1:\n",
        "                 break\n",
        "          return 'Not a palindrome'\n",
        "\n",
        "n = 'C'\n",
        "if right_pos >= left rotation:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAVE6pwqdNxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfb4cfe-ccab-450f-ad86-0267609cc766"
      },
      "source": [
        "selected_strings = nl_to_pl_df[nl_to_pl_df['cleaned_code_len'] > 128]['docstring'].values\n",
        "all_strings = selected_strings\n",
        "offset=2000\n",
        "for one_string in all_strings[offset:offset+50]:\n",
        "    cleaned_string = one_string.rstrip('\\n').lstrip('#')\n",
        "    mycode, attention_val = get_code(cleaned_string,auto_tokenizer, model, device, max_len=512)\n",
        "    print(cleaned_string)\n",
        "    print(auto_tokenizer.convert_tokens_to_string(mycode[:-1]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Remove special symbols/Punctuation from a given string\n",
            "import re\n",
            "regex = '\\.[0]*'\n",
            "def remove_leading_zeros(ip):\n",
            "    modified_ip = re.sub(regex, '.', ip)\n",
            "    return modified_ip\n",
            " From given string replace each punctuation with #\n",
            "sentence = 'the quick brown fox'\n",
            "words = sentence.split(' ')\n",
            "words.reverse()\n",
            "print(' '.join(words))\n",
            " Given a list iterate it and count the occurrence of each element and create a dictionary to show the count of each elemen\n",
            "\n",
            "list1 = ['apple', 'orange', 'banana']\n",
            "list2 = [1, 2, 3]\n",
            "print(list_list_list_to_list2))\n",
            " Given a two list of equal size create a set such that it shows the element from both lists in the pair\n",
            "list1 = [1, 2, 3, 4, 5]\n",
            "list2 = [5, 4, 3, 2, 2, 3, 4, 2, 3, 4]\n",
            "diff_list = [a-b for (a,b) in zip(list1, list2)]\n",
            "print(diff_list)\n",
            " Given a two sets find the intersection and remove those elements from the first set\n",
            "\n",
            "set1 = {10, 20, 30, 40, 50}\n",
            "set2 = {30, 40, 50, 60, 70}\n",
            "\n",
            "print(set1.symmetric_difference(set2))\n",
            " Given a dictionary get all values from the dictionary and add it in a list but don’t add duplicates\n",
            "my_list1 = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "print(my_list[:])\n",
            " Convert string into a datetime object\n",
            "from datetime import datetime\n",
            "datetime.utcnow()\n",
            "def log(msg, *, dt = datetime.utcnow()):\n",
            "    return f'Message at {dt} was {msg}')\n",
            " Subtract a week from a given date\n",
            "import subprocess\n",
            "subprocess.call([\"sudo\", \"apt\", \"update\"])\n",
            "  Add week (7 days) and 12 hours to a given date\n",
            "from datetime import datetime\n",
            "given_date = datetime(2020, 7, 26)\n",
            "print(given_date.strftime('%A'))\n",
            " Calculate number of days between two given dates\n",
            "set1 = {10, 20, 30, 40, 50}\n",
            "set2 = {30, 40, 50, 60, 70}\n",
            "print(set1.symmetric_difference(set2))\n",
            " Write a recursive function to calculate the sum of numbers from 0 to 10\n",
            "def sum_of_digits(num):\n",
            "    if num == 0:\n",
            "        return 0\n",
            "    else:\n",
            "          return num % 10 + sum_digits(num / 10))\n",
            "  Given a Python list, remove all occurrence of a given number from the list\n",
            "def remove_duplicatesinlist(lst):\n",
            "    return len(lst) == len(lst))\n",
            " Generate a random n-dimensional array of float numbers\n",
            "import random\n",
            "n = random.choice(n)\n",
            "print(n)\n",
            " Choose given number of elements from the list with different probability\n",
            "\n",
            "list1 = [10, 20, 1, 45, 99] \n",
            "\n",
            "print(\"Smallest element is:\", min(list1))\n",
            " Merge two dictionaries in a single expression\n",
            "thisdict = {\n",
            "  \"brand\": \"Ford\",\n",
            "  \"model\": \"Mustang\",\n",
            "  \"year\": 1964\n",
            "}\n",
            "print(f\"Sample Dictionary:{thisdict}\")\n",
            " Alternate cases in String \n",
            "\n",
            "import re\n",
            "def match(pattern, string):\n",
            "    if re.match(pattern, string):\n",
            "        return True\n",
            "    return True\n",
            "     return False\n",
            " 10 write a python function to return the median of a list\n",
            "def multiplyList(myList) :\n",
            "    result = 1\n",
            "    for x in myList:\n",
            "        result = result * x \n",
            "    return result\n",
            " 14 write a function to perform insertion sort on an arary\n",
            "\n",
            "def reverse_to_tuple(input_input_to_be_reversed):\n",
            "    return int(n)[::-1]\n",
            " 16 write a function to immplement insert in binary search tree\n",
            "\n",
            "\n",
            "def binary_to_gray(n):\n",
            "    #--\n",
            "    n = int(n, 2)\n",
            "    return bin(n)[2:]\n",
            " 17 write a function to initialize a linked list\n",
            "\n",
            "def multiplyList(myList) :\n",
            "    result = 1\n",
            "    for x in myList:\n",
            "        result = result * x \n",
            "    return result\n",
            " 18 write a function to create a linked list with given length and print the list after\n",
            "\n",
            "list = [10, 20, 1, 45, 99] \n",
            "\n",
            "\n",
            "print(\"Smallest element is:\", min(list1))\n",
            " 23 write a function to calculate the residual sum of squares between two lists of the same size\n",
            "\n",
            "def emptylist():\n",
            "    return list()\n",
            " 24 write a program to caclulate the approximate value of pi using the monte carlo method\n",
            "def derivative_sin(x:float)-> float:\n",
            "    import math\n",
            "    return math.cos(x))\n",
            " 26 write a program to calculate the root of a nonlinear equation using Newton's method\n",
            "\n",
            "def find_odd_occurring(alist):\n",
            "    #--\n",
            "    ans = 0\n",
            "\n",
            "    for element in alist:\n",
            "        ans ^= element\n",
            "\n",
            "    return ans\n",
            " 27 write a program that filters a list for even numbers only and returns their sum\n",
            "\n",
            "nums = [1, 2, 3, 4, 5, 6, 7, 8]\n",
            "no_even_nums = [i for i in nums if i % 2 == 0]\n",
            " 29 write a program to sort a list using bubblesort\n",
            "list = [1, 2, 3, 4, 5]\n",
            "list = [5, 3, 4, 1]\n",
            "print(list)\n",
            " 30 write a function that accepts two numbers or lists or dictionaries and returns True if the two are equal, and False otherwise\n",
            "def add_even_num(num1, num2):\n",
            "    sum = num1 + num2\n",
            "    return not sum % 2\n",
            " 31 write a function that checks if a number is an Armstrong number (sum of digits of the number = the number)\n",
            "def sum_of_digits(num):\n",
            "    if num == 0:\n",
            "        return 0\n",
            "    else:\n",
            "        return num % 10 + sum_of_digits(num / 10))\n",
            " 32 write a program in python to create a directed graph, and add an edge between two vertices\n",
            "num1 = 1.5\n",
            "num2 = 6.3\n",
            "sum = num1 * num2\n",
            "print(f'Sum: {sum}')\n",
            " 33 write a program that shows how child class can access the init method of the parent class using super\n",
            "import secrets\n",
            "print(\"Random integer number generated using secrets module is \")\n",
            "number = secrets.randbelow(30)\n",
            "print(number)\n",
            " 37 Write a python program to calculate the LCM and HCF of two given numbers\n",
            "num1 = 1.5\n",
            "num2 = 6.3\n",
            "product = num1 * num2\n",
            "print(f'Sub: {sum}')\n",
            " 38 write a python program which takes in a dictionary with unique values and converts keys into values and vice versa\n",
            "d = {'a':1,'b':2,'c':3,'d':4}\n",
            "print(d.keys())\n",
            " 42 write a python program that converts lower case letters to uppercase and vice versa\n",
            "s = \"Kilometer\"\n",
            "print(s.lower())\n",
            " 49 Write a generator that returns True / False randomly\n",
            "def printList():\n",
            "    li=list()\n",
            "    for i in range(1,21):\n",
            "        li.append(i**2)\n",
            "    print(li[-5:])\n",
            " 51 write a python program to perform Softmax operation on an input array\n",
            "a = 60\n",
            "b = 13\n",
            "a_or_b = a|b\n",
            "print(a_or_b)\n",
            " 52 Write a python program to calculate the slope of a line given two points\n",
            "import math\n",
            "\n",
            "num = 45\n",
            "print(\"Cosine\", math.cos(num))\n",
            " 53 write a python function which checks if a number is a perfect square\n",
            "def square(num):\n",
            "    return num ** 2\n",
            " 55 Write a python program that pads a given python list to a given length at the end and prints the modified list\n",
            "lst = [40, 10, 20, 30]\n",
            "\n",
            "\n",
            "print (f\"last element from the list:{lst.pop()}\")\n",
            " 55 Write a python program that pads a given python list to a given length at the start and prints the modified list\n",
            "my_list = [4,3,2,9,10,44,1]\n",
            "my_list.sort()\n",
            "print(f\"Ascending Order list:,{my_list}\")\n",
            " 58 Write a python program that calculates and prints the area of an ellipse\n",
            "\n",
            "h = 12\n",
            "w = 11\n",
            "area = 0.5*w\n",
            "print(area)\n",
            " 61 write a python function to return the standard deviation of a list of numbers\n",
            "def shift_and_scale(list_of_nums, mean, std):\n",
            "    return [ (x-mean) / std for x in list_of_nums ]\n",
            " 63 Write a python function which returns true if all the numbers in a list negative, else return False\n",
            "def add_even_num(l):\n",
            "    sum = 0\n",
            "    for i in range(min, max)      sum += i\n",
            "     return sum\n",
            " 64 Write a python function that checks if all the numbers in a list sum upto 1. Returns False otherwise\n",
            "def is_prod_even(num):\n",
            "    sum = num % 2\n",
            "    return not sum % 2\n",
            " 75 write a program to generate all sentences where subject is in [\"I\", \"You\"] and verb is in [\"Play\", \"Love\"] and the object is in [\"Hockey\",\"Football\"].\n",
            "import random\n",
            "\n",
            "print(random.choice([i for i in range(201) if i%2==0 and i%5==0]))\n",
            " 83 With a given list [12,24,35,24,88,120,155,88,120,155], write a program to print this list after removing all duplicate values with original order reserved.\n",
            "set1=set([1,3,6,78,35,55])\n",
            "set2=set([12,24,35,24,88,120,155])\n",
            "set1 &= set2\n",
            "li=list(set1)\n",
            "print li\n",
            " 84 Define a class Person and its two child classes: Male and Female. All classes have a method \"getGender\" which can print \"Male\" for Male class and \"Female\" for Female class.\n",
            "print('%o,' % (8))\n",
            " 85 write a program which count and print the numbers of each character in a string\n",
            "\n",
            "str1 = \"what a great day!\"\n",
            "print(\"\".join(set(str1)))\n",
            " 89 Write a program to solve a classic ancient Chinese puzzle:  We count 35 heads and 94 legs among the chickens and rabbits in a farm. How many rabbits and how many chickens do we have?\n",
            "\n",
            "import json\n",
            "data = {\"key1\", \"value1\", \"key2\" : \"value2\"}\n",
            "jsonData = json.dumps(data)\n",
            "print(jsonData)\n",
            " 93 write a python function to find One's compliment of a number\n",
            "\n",
            "def concat_two_numbers(num1, num2):\n",
            "    return mul\n",
            " 97 write a python function that accepts a number, and returns the nearest square number\n",
            "def square(x):\n",
            "    return x**2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
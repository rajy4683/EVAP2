{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA6Session3_FirstAt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EVAP2/blob/master/EVA6Session3_FirstAt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_Lx-4fj7zoO",
        "outputId": "631d23f3-8701-4c44-de2c-f2818033ddda"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri May 21 18:58:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    34W / 250W |   1179MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534b793c-3cdb-443a-ae66-c86cae05aa0c"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import codecs\n",
        "import string\n",
        "import gzip\n",
        "import lzma\n",
        "from typing import Any, Callable, Dict, IO, List, Optional, Tuple, Union\n",
        "from urllib.error import URLError"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OONup7Ukaulv"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVtbR5oV7mM8",
        "outputId": "f4fcba0e-de41-4b01-9583-26558fff1ce3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyaXPgnsC9O5"
      },
      "source": [
        "### Returns one element from MNIST dataset along with label, and random number\n",
        "class MNISTAdderDataSet():\n",
        "    training_file = 'training.pt'\n",
        "    test_file = 'test.pt'\n",
        "    classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four',\n",
        "               '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
        "    def __init__(\n",
        "            self,\n",
        "            path_name,\n",
        "            train,\n",
        "            transform,\n",
        "            target_transform=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path_name: String - Path to the file containing the training.pt and test.pt,\n",
        "            train: Boolean - Indicates whether to create training Dataset or Test Dataset,\n",
        "            transform: Object/None - Any torch.transform object,\n",
        "            target_transform: Object/None - Any torch.transform object (default: None),\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.train = train\n",
        "        self.path_name = path_name\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        if self.train:\n",
        "            data_file = self.training_file\n",
        "        else:\n",
        "            data_file = self.test_file\n",
        "\n",
        "        self.data, self.targets = torch.load(os.path.join(self.path_name, data_file))\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                image: transformed image as numpy.array,\n",
        "                test_ip: one-hot encoded array of length 10 with a random position set to 1,\n",
        "                target: class label of the image \n",
        "                result_add: sum of the random number and class label\n",
        "        \"\"\"\n",
        "        #random_int = torch.randint(0,9,(1,))\n",
        "        random_int = np.random.randint(0,9, (1,))[0]\n",
        "        test_ip = torch.zeros((10,))\n",
        "        test_ip[random_int ]=1\n",
        "        result_add = torch.zeros((19,))\n",
        "        \n",
        "        img, target = self.data[index], int(self.targets[index])\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img.numpy(), mode='L')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # if self.target_transform is not None:\n",
        "        #     target = self.target_transform(target)\n",
        "        \n",
        "        result_add = random_int+target\n",
        "\n",
        "        return img, test_ip, target, result_add\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hswUp3M2N_uv"
      },
      "source": [
        "class MNISTAdderNet(nn.Module):\n",
        "    def __init__(self,start_channels=32, \n",
        "                 exponetate_layers=True, \n",
        "                 max_random_value = 10, \n",
        "                 adder_classes=19,\n",
        "                 hidden_dim=20):\n",
        "        super(MNISTAdderNet, self).__init__()\n",
        "        self.start_channels = start_channels\n",
        "        self.multiplier = 2\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        if (exponetate_layers == False):\n",
        "            self.multiplier = 1\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, self.start_channels, 3, padding=1) # input - 3*28*28 Output: 16*28*28\n",
        "        self.conv1_bn = nn.BatchNorm2d(self.start_channels)\n",
        "        self.conv2 = nn.Conv2d(self.start_channels, self.start_channels*self.multiplier, 3, padding=1) # input - 16*28*28 Output: 32*28*28\n",
        "        self.conv2_bn = nn.BatchNorm2d(self.start_channels*self.multiplier)\n",
        "\n",
        "        self.start_channels = self.start_channels * self.multiplier\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # input - 32*28*28 Output: 32*14*14\n",
        "\n",
        "        self.conv3 = nn.Conv2d(self.start_channels, self.start_channels*self.multiplier, 3, padding=1)# input - 32*14*14 Output: 64*14*14\n",
        "        self.conv3_bn = nn.BatchNorm2d(self.start_channels*self.multiplier)\n",
        "        self.start_channels = self.start_channels * self.multiplier\n",
        "        self.conv4 = nn.Conv2d(self.start_channels, self.start_channels*self.multiplier, 3, padding=1)# input - 64*14*14 Output: 128*14*14\n",
        "        self.conv4_bn = nn.BatchNorm2d(self.start_channels*self.multiplier)\n",
        "        self.start_channels = self.start_channels * self.multiplier\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)# input - 128*14*14 Output: 128*7*7\n",
        "\n",
        "        self.conv5 = nn.Conv2d(self.start_channels, self.start_channels*self.multiplier, 3)# input - 128*7*7 Output: 256*5*5\n",
        "        self.conv5_bn = nn.BatchNorm2d(self.start_channels*self.multiplier)\n",
        "        self.start_channels = self.start_channels * self.multiplier\n",
        "        self.conv6 = nn.Conv2d(self.start_channels, self.start_channels*self.multiplier, 3)# input - 256*5*5 Output: 512*3*3\n",
        "        self.conv6_bn = nn.BatchNorm2d(self.start_channels*self.multiplier)\n",
        "        self.start_channels = self.start_channels * self.multiplier\n",
        "\n",
        "        self.conv7 = nn.Conv2d(self.start_channels, 10, 3)# input - 512*3*3 Output: 10*1*1\n",
        "        ### Fully connected layers for performing addition operation \n",
        "        self.embed_layer = nn.Linear(max_random_value, self.hidden_dim) # input - 10*1 Output: 20*1\n",
        "        self.conv_embed_layer = nn.Linear(max_random_value, self.hidden_dim) # input - 10*1 Output: 20*1 \n",
        "        self.final_adder_layer = nn.Linear(2*self.hidden_dim, adder_classes) # input - 20*1 Output: 19*1\n",
        "\n",
        "    def forward(self, x, rand_inp):\n",
        "        x = F.relu(self.conv1_bn(self.conv1(x))) ### Layer 1 ---->(Relu(BN(Conv)))\n",
        "        x = self.pool1(F.relu(self.conv2_bn(self.conv2(x)))) ### Layer 2---> MaxPool((Relu(BN(Conv))))\n",
        "        x = F.relu(self.conv3_bn(self.conv3(x))) ### Layer 3 ---->(Relu(BN(Conv)))\n",
        "        x = self.pool2(F.relu(self.conv4_bn(self.conv4(x)))) ### Layer 4---> MaxPool((Relu(BN(Conv))))\n",
        "        #########\n",
        "        x = F.relu(self.conv5_bn(self.conv5(x))) ### Layer 5---> (Relu(BN(Conv)))\n",
        "        x = F.relu(self.conv6_bn(self.conv6(x))) ### Layer 6 ---> (Relu(BN(Conv)))\n",
        "        x = self.conv7(x) #### Final Layer (BN(Conv)))\n",
        "        x = x.view(-1, 10)\n",
        "        conv_to_fc = F.relu(self.conv_embed_layer(x)) ### We pass the predicted class through an FC to create an embedding \n",
        "        adder_output = F.relu(self.embed_layer(rand_inp)) ### Random Input is passed through an FC layer to create an embedding\n",
        "        concat_layers = torch.cat((conv_to_fc, adder_output), dim=1)  ### Concat above two inputs for passing through final linear layer\n",
        "        final_adder_output = self.final_adder_layer(concat_layers) ### Outputs a class between 0 and 18. Max sum of digits =18 and min = 0\n",
        "\n",
        "        return F.log_softmax(x), F.log_softmax(final_adder_output)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d13IHwVEbloG"
      },
      "source": [
        "## Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80XTzsAwEpjd"
      },
      "source": [
        "batch_size=128\n",
        "\n",
        "### For training and test data we normalize the images by the MNIST dataset's mean and variance.\n",
        "my_ds = MNISTAdderDataSet('/content/data/MNIST/processed', True, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,)) \n",
        "                    ]))\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(my_ds,\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "my_test_ds = MNISTAdderDataSet('/content/data/MNIST/processed', False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,)) \n",
        "                    ]))\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "test_loader = torch.utils.data.DataLoader(my_test_ds,\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oshYxnLOxHY"
      },
      "source": [
        "model = MNISTAdderNet(start_channels=16,exponetate_layers=False).to(device)\n",
        "#model(a.to(device))\n",
        "img, input_adder_array, target, result_add = next(iter(train_loader))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1YEPqMiQ4zw",
        "outputId": "a02a4093-be38-4439-bd57-53ac688673f1"
      },
      "source": [
        "pred_image, pred_sum = model(img.to(device), input_adder_array.to(device))\n",
        "pred_image.shape, pred_sum.shape\n",
        "F.nll_loss(pred_sum, result_add.to(device))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([128, 10]), torch.Size([128, 19]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFLsmP-PbfAZ"
      },
      "source": [
        "## Train and Test function definitions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev3CKtCtVh28"
      },
      "source": [
        "### For \n",
        "\n",
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    #pbar = tqdm(train_loader)\n",
        "    for batch_idx, (img, input_adder_array, target, result_add) in enumerate(train_loader):\n",
        "        #data, target = data.to(device), target.to(device)\n",
        "        img = img.to(device)        \n",
        "        input_adder_array = input_adder_array.to(device)\n",
        "        target = target.to(device)\n",
        "        result_add = result_add.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred_image, pred_sum = model(img, input_adder_array)\n",
        "        loss = F.nll_loss(pred_image, target) + F.nll_loss(pred_sum, result_add) ### We use two loss functions - one for MNIST class loss and one for predicting the result of addition.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    class_loss = 0\n",
        "    sum_loss = 0\n",
        "    correct = 0\n",
        "    correct_sum = 0\n",
        "    with torch.no_grad():\n",
        "        for img, input_adder_array, target, result_add in test_loader:\n",
        "            # data, target = data.to(device), target.to(device)\n",
        "            img = img.to(device)        \n",
        "            input_adder_array = input_adder_array.to(device)\n",
        "            target = target.to(device)\n",
        "            result_add = result_add.to(device)\n",
        "            # output = model(data)\n",
        "            pred_image, pred_sum = model(img, input_adder_array)\n",
        "\n",
        "            class_loss += F.nll_loss(pred_image, target, reduction='sum').item() \n",
        "            sum_loss += F.nll_loss(pred_sum, result_add, reduction='sum').item()\n",
        "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = pred_image.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            pred_sum_vals = pred_sum.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct_sum += pred_sum_vals.eq(result_add.view_as(pred_sum_vals)).sum().item()\n",
        "\n",
        "    class_loss /= len(test_loader.dataset)\n",
        "    sum_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(f'Epoch: {epoch} Test set: Average class loss: {class_loss}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset)}%)')\n",
        "    print(f'Epoch: {epoch} Test set: Average Sum Loss:{sum_loss} Accuracy: {correct_sum}/{len(test_loader.dataset)} ({100. * correct_sum / len(test_loader.dataset)}%)')\n",
        "        # epoch, class_loss, sum_loss, correct, len(test_loader.dataset),\n",
        "        # 100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evCWH_xXbZ6B"
      },
      "source": [
        "## Train/Test Loops\n",
        "Accuracy on Images is ~99..64\n",
        "Accuracy on predicted sum is ~98.72"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kChGO-vYWayv",
        "outputId": "477461ad-bcc6-4686-cf36-d831f766129c"
      },
      "source": [
        "model = MNISTAdderNet(start_channels=16,exponetate_layers=True).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) \n",
        "\n",
        "\n",
        "for epoch in range(1, 50):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Test set: Average class loss: 0.029781136710895224, Accuracy: 9909/10000 (99.09%)\n",
            "Epoch: 1 Test set: Average Sum Loss:2.1067971336364746 Accuracy: 2077/10000 (20.77%)\n",
            "Epoch: 2 Test set: Average class loss: 0.02811167477183044, Accuracy: 9922/10000 (99.22%)\n",
            "Epoch: 2 Test set: Average Sum Loss:1.6257107707977294 Accuracy: 3710/10000 (37.1%)\n",
            "Epoch: 3 Test set: Average class loss: 0.021551052032783628, Accuracy: 9935/10000 (99.35%)\n",
            "Epoch: 3 Test set: Average Sum Loss:1.3302183277130126 Accuracy: 4979/10000 (49.79%)\n",
            "Epoch: 4 Test set: Average class loss: 0.020353321626968682, Accuracy: 9938/10000 (99.38%)\n",
            "Epoch: 4 Test set: Average Sum Loss:1.1584116191864013 Accuracy: 5411/10000 (54.11%)\n",
            "Epoch: 5 Test set: Average class loss: 0.023299573473073543, Accuracy: 9940/10000 (99.4%)\n",
            "Epoch: 5 Test set: Average Sum Loss:1.0022976754188537 Accuracy: 6771/10000 (67.71%)\n",
            "Epoch: 6 Test set: Average class loss: 0.02777718466236256, Accuracy: 9931/10000 (99.31%)\n",
            "Epoch: 6 Test set: Average Sum Loss:0.9111501329421997 Accuracy: 7214/10000 (72.14%)\n",
            "Epoch: 7 Test set: Average class loss: 0.021386632597073914, Accuracy: 9943/10000 (99.43%)\n",
            "Epoch: 7 Test set: Average Sum Loss:0.8184787579536438 Accuracy: 7434/10000 (74.34%)\n",
            "Epoch: 8 Test set: Average class loss: 0.023849810665560654, Accuracy: 9945/10000 (99.45%)\n",
            "Epoch: 8 Test set: Average Sum Loss:0.7537479559898377 Accuracy: 7798/10000 (77.98%)\n",
            "Epoch: 9 Test set: Average class loss: 0.02283721186830662, Accuracy: 9944/10000 (99.44%)\n",
            "Epoch: 9 Test set: Average Sum Loss:0.685794631767273 Accuracy: 8613/10000 (86.13%)\n",
            "Epoch: 10 Test set: Average class loss: 0.02070178262074478, Accuracy: 9950/10000 (99.5%)\n",
            "Epoch: 10 Test set: Average Sum Loss:0.6359557472229004 Accuracy: 8695/10000 (86.95%)\n",
            "Epoch: 11 Test set: Average class loss: 0.033498095539142375, Accuracy: 9938/10000 (99.38%)\n",
            "Epoch: 11 Test set: Average Sum Loss:0.6637102916717529 Accuracy: 8035/10000 (80.35%)\n",
            "Epoch: 12 Test set: Average class loss: 0.023731720602923452, Accuracy: 9949/10000 (99.49%)\n",
            "Epoch: 12 Test set: Average Sum Loss:0.5852170094013214 Accuracy: 8882/10000 (88.82%)\n",
            "Epoch: 13 Test set: Average class loss: 0.05262757916972041, Accuracy: 9870/10000 (98.7%)\n",
            "Epoch: 13 Test set: Average Sum Loss:0.6625428275108337 Accuracy: 8539/10000 (85.39%)\n",
            "Epoch: 14 Test set: Average class loss: 0.02647817108957097, Accuracy: 9942/10000 (99.42%)\n",
            "Epoch: 14 Test set: Average Sum Loss:0.5305684957504272 Accuracy: 8964/10000 (89.64%)\n",
            "Epoch: 15 Test set: Average class loss: 0.017459278390795224, Accuracy: 9955/10000 (99.55%)\n",
            "Epoch: 15 Test set: Average Sum Loss:0.46564145975112914 Accuracy: 9336/10000 (93.36%)\n",
            "Epoch: 16 Test set: Average class loss: 0.026828678897459757, Accuracy: 9940/10000 (99.4%)\n",
            "Epoch: 16 Test set: Average Sum Loss:0.5220946075439453 Accuracy: 8944/10000 (89.44%)\n",
            "Epoch: 17 Test set: Average class loss: 0.023511913765931966, Accuracy: 9953/10000 (99.53%)\n",
            "Epoch: 17 Test set: Average Sum Loss:0.4216055794715881 Accuracy: 9471/10000 (94.71%)\n",
            "Epoch: 18 Test set: Average class loss: 0.06339229862214997, Accuracy: 9857/10000 (98.57%)\n",
            "Epoch: 18 Test set: Average Sum Loss:0.6688988928794861 Accuracy: 9036/10000 (90.36%)\n",
            "Epoch: 19 Test set: Average class loss: 0.03318554861416633, Accuracy: 9941/10000 (99.41%)\n",
            "Epoch: 19 Test set: Average Sum Loss:0.43094079465866086 Accuracy: 9617/10000 (96.17%)\n",
            "Epoch: 20 Test set: Average class loss: 0.04862264170163544, Accuracy: 9930/10000 (99.3%)\n",
            "Epoch: 20 Test set: Average Sum Loss:0.40607184953689573 Accuracy: 9566/10000 (95.66%)\n",
            "Epoch: 21 Test set: Average class loss: 0.0254979210500911, Accuracy: 9950/10000 (99.5%)\n",
            "Epoch: 21 Test set: Average Sum Loss:0.35637129957675934 Accuracy: 9788/10000 (97.88%)\n",
            "Epoch: 22 Test set: Average class loss: 0.025529943281190935, Accuracy: 9949/10000 (99.49%)\n",
            "Epoch: 22 Test set: Average Sum Loss:0.3438975245475769 Accuracy: 9734/10000 (97.34%)\n",
            "Epoch: 23 Test set: Average class loss: 0.02189993600859307, Accuracy: 9956/10000 (99.56%)\n",
            "Epoch: 23 Test set: Average Sum Loss:0.3285067180633545 Accuracy: 9766/10000 (97.66%)\n",
            "Epoch: 24 Test set: Average class loss: 0.023458078561956062, Accuracy: 9958/10000 (99.58%)\n",
            "Epoch: 24 Test set: Average Sum Loss:0.30426155021190643 Accuracy: 9836/10000 (98.36%)\n",
            "Epoch: 25 Test set: Average class loss: 0.020542383010005143, Accuracy: 9962/10000 (99.62%)\n",
            "Epoch: 25 Test set: Average Sum Loss:0.2821488481283188 Accuracy: 9816/10000 (98.16%)\n",
            "Epoch: 26 Test set: Average class loss: 0.02290178811738151, Accuracy: 9965/10000 (99.65%)\n",
            "Epoch: 26 Test set: Average Sum Loss:0.28253089673519133 Accuracy: 9837/10000 (98.37%)\n",
            "Epoch: 27 Test set: Average class loss: 0.0242400248936945, Accuracy: 9957/10000 (99.57%)\n",
            "Epoch: 27 Test set: Average Sum Loss:0.2910739560723305 Accuracy: 9822/10000 (98.22%)\n",
            "Epoch: 28 Test set: Average class loss: 0.0254696618508955, Accuracy: 9963/10000 (99.63%)\n",
            "Epoch: 28 Test set: Average Sum Loss:0.28359835546016693 Accuracy: 9834/10000 (98.34%)\n",
            "Epoch: 29 Test set: Average class loss: 0.031546996685347405, Accuracy: 9945/10000 (99.45%)\n",
            "Epoch: 29 Test set: Average Sum Loss:0.38362774262428284 Accuracy: 9716/10000 (97.16%)\n",
            "Epoch: 30 Test set: Average class loss: 0.045796580664697106, Accuracy: 9928/10000 (99.28%)\n",
            "Epoch: 30 Test set: Average Sum Loss:0.4440179497361183 Accuracy: 9626/10000 (96.26%)\n",
            "Epoch: 31 Test set: Average class loss: 0.03860998377521755, Accuracy: 9943/10000 (99.43%)\n",
            "Epoch: 31 Test set: Average Sum Loss:0.3601237511396408 Accuracy: 9710/10000 (97.1%)\n",
            "Epoch: 32 Test set: Average class loss: 0.033688783714734015, Accuracy: 9942/10000 (99.42%)\n",
            "Epoch: 32 Test set: Average Sum Loss:0.3172424297332764 Accuracy: 9769/10000 (97.69%)\n",
            "Epoch: 33 Test set: Average class loss: 0.026887675154971657, Accuracy: 9957/10000 (99.57%)\n",
            "Epoch: 33 Test set: Average Sum Loss:0.25456808080673216 Accuracy: 9861/10000 (98.61%)\n",
            "Epoch: 34 Test set: Average class loss: 0.024305231332418045, Accuracy: 9959/10000 (99.59%)\n",
            "Epoch: 34 Test set: Average Sum Loss:0.23246212162971497 Accuracy: 9870/10000 (98.7%)\n",
            "Epoch: 35 Test set: Average class loss: 0.022887673134137004, Accuracy: 9958/10000 (99.58%)\n",
            "Epoch: 35 Test set: Average Sum Loss:0.2597497302055359 Accuracy: 9858/10000 (98.58%)\n",
            "Epoch: 36 Test set: Average class loss: 0.023152222563140094, Accuracy: 9962/10000 (99.62%)\n",
            "Epoch: 36 Test set: Average Sum Loss:0.248951704287529 Accuracy: 9862/10000 (98.62%)\n",
            "Epoch: 37 Test set: Average class loss: 0.025881195527051204, Accuracy: 9956/10000 (99.56%)\n",
            "Epoch: 37 Test set: Average Sum Loss:0.27403179619312285 Accuracy: 9849/10000 (98.49%)\n",
            "Epoch: 38 Test set: Average class loss: 0.022820679961220593, Accuracy: 9962/10000 (99.62%)\n",
            "Epoch: 38 Test set: Average Sum Loss:0.24652360544204713 Accuracy: 9874/10000 (98.74%)\n",
            "Epoch: 39 Test set: Average class loss: 0.02632438176128744, Accuracy: 9953/10000 (99.53%)\n",
            "Epoch: 39 Test set: Average Sum Loss:0.2742140605092049 Accuracy: 9810/10000 (98.1%)\n",
            "Epoch: 40 Test set: Average class loss: 0.025465426477925213, Accuracy: 9964/10000 (99.64%)\n",
            "Epoch: 40 Test set: Average Sum Loss:0.2462636006951332 Accuracy: 9861/10000 (98.61%)\n",
            "Epoch: 41 Test set: Average class loss: 0.024458243751239207, Accuracy: 9963/10000 (99.63%)\n",
            "Epoch: 41 Test set: Average Sum Loss:0.2562820773601532 Accuracy: 9860/10000 (98.6%)\n",
            "Epoch: 42 Test set: Average class loss: 0.02505271040558173, Accuracy: 9962/10000 (99.62%)\n",
            "Epoch: 42 Test set: Average Sum Loss:0.23173072811961173 Accuracy: 9863/10000 (98.63%)\n",
            "Epoch: 43 Test set: Average class loss: 0.024856062382276285, Accuracy: 9959/10000 (99.59%)\n",
            "Epoch: 43 Test set: Average Sum Loss:0.24460083354711532 Accuracy: 9868/10000 (98.68%)\n",
            "Epoch: 44 Test set: Average class loss: 0.02416417478397143, Accuracy: 9962/10000 (99.62%)\n",
            "Epoch: 44 Test set: Average Sum Loss:0.2557578363180161 Accuracy: 9863/10000 (98.63%)\n",
            "Epoch: 45 Test set: Average class loss: 0.024762619278957208, Accuracy: 9961/10000 (99.61%)\n",
            "Epoch: 45 Test set: Average Sum Loss:0.24993746840953826 Accuracy: 9867/10000 (98.67%)\n",
            "Epoch: 46 Test set: Average class loss: 0.024542020087075435, Accuracy: 9964/10000 (99.64%)\n",
            "Epoch: 46 Test set: Average Sum Loss:0.2646421900510788 Accuracy: 9848/10000 (98.48%)\n",
            "Epoch: 47 Test set: Average class loss: 0.02350032809365621, Accuracy: 9964/10000 (99.64%)\n",
            "Epoch: 47 Test set: Average Sum Loss:0.23227927067875861 Accuracy: 9861/10000 (98.61%)\n",
            "Epoch: 48 Test set: Average class loss: 0.02483714062999611, Accuracy: 9965/10000 (99.65%)\n",
            "Epoch: 48 Test set: Average Sum Loss:0.2246130622267723 Accuracy: 9858/10000 (98.58%)\n",
            "Epoch: 49 Test set: Average class loss: 0.025573136833245372, Accuracy: 9961/10000 (99.61%)\n",
            "Epoch: 49 Test set: Average Sum Loss:0.2610436644792557 Accuracy: 9852/10000 (98.52%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
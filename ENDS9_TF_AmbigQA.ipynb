{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "ENDS9_TF_AmbigQA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yUQMjegvDkpX",
        "WIGxiriM4GRC"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EVAP2/blob/master/ENDS9_TF_AmbigQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzxOJwN7EzFO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1YC7Z0guYGX",
        "outputId": "6eddfc3a-1436-4d93-97e6-f3f2e4184c4d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jan  7 17:38:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUnMLdevEzFT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og-MDQc4hKL7",
        "outputId": "79dc595f-6559-4a05-b113-56cb589fb5a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzErHAsEzFU"
      },
      "source": [
        "Then set a random seed for deterministic results/reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_wP4J4LEzFX"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyaSxNdzxPE8",
        "outputId": "df6acc24-bbd0-45ef-ec9b-0be389e6d2b0"
      },
      "source": [
        "!pip install jsonlines\r\n",
        "import jsonlines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.15.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "789WUpyCxVIq"
      },
      "source": [
        "with jsonlines.open('/content/dev_rand_split.jsonl') as reader:\r\n",
        "    # reader.read()\r\n",
        "    for out in reader:\r\n",
        "        print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnbsQg6YPLFs"
      },
      "source": [
        "fout = open(f\"openqatrain.tsv\", \"w+\")\r\n",
        "fout_meta = open(f\"openqatrain_meta.tsv\", \"w+\")\r\n",
        "with open(\"/content/train.jsonl\") as f:\r\n",
        "    for line in f.readlines():\r\n",
        "        json_line = json.loads(line)\r\n",
        "        question = json_line['question']['stem']\r\n",
        "        choices = json_line['question']['choices']\r\n",
        "        candidates = \" \".join([f\"({c['label']}) {c['text']}\" for c in choices]).replace(\"\\n\", \" \")\r\n",
        "        print(json_line)\r\n",
        "        answer_key = json_line['answerKey']\r\n",
        "        answer_key_idx = ord(answer_key[0]) - ord('A')\r\n",
        "        answer_text = choices[answer_key_idx]['text']\r\n",
        "        id = \"OBQA_\" + json_line['id']\r\n",
        "        para = \"\"\r\n",
        "        # if with_para:\r\n",
        "        #     para = \"\\\\n\" + oyvind_paragraphs[id].replace(\"\\n\", \" \").replace(\"\\t\", \" \")\r\n",
        "        fout.write(f\"{question} \\\\n {candidates}{para}\\t{answer_text}\\n\")\r\n",
        "        fout_meta.write(f\"{json_line['id']}\\t{answer_key[0]}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC_7SAqaBu3M"
      },
      "source": [
        "def read_file(file, split):\r\n",
        "    fout = open(f\"./{split}.tsv\", \"w\")\r\n",
        "    fout_meta = open(f\"./{split}_meta.tsv\", \"w\")\r\n",
        "    with open(file) as f:\r\n",
        "        for line in f.readlines():\r\n",
        "            json_line = json.loads(line)\r\n",
        "\r\n",
        "            candidates_str = \" \".join([f\"({x['label']}) {x['text']}\" for x in json_line['question']['choices']])\r\n",
        "            if split != \"test\":\r\n",
        "                selected_ans_string = [x['text'] for x in json_line['question']['choices'] if\r\n",
        "                                        json_line['answerKey'] == x['label']]\r\n",
        "                assert len(selected_ans_string) == 1, f\"{len(selected_ans_string)} -- {json_line['answerKey']}\"\r\n",
        "            json_line['question']['stem'] = json_line['question']['stem'].replace(\"\\t\", \" \").replace(\"\\n\", \"\")\r\n",
        "            candidates_str = candidates_str.replace(\"\\t\", \" \").replace(\"\\n\", \"\")\r\n",
        "\r\n",
        "            if split == \"test\":\r\n",
        "                fout_meta.write(f\"{json_line['id']}\\t-\\n\")\r\n",
        "                fout.write(f\"{json_line['question']['stem']} \\\\n {candidates_str}\\t-\\n\")\r\n",
        "            else:\r\n",
        "                fout_meta.write(f\"{json_line['id']}\\t{json_line['answerKey']}\\n\")\r\n",
        "                selected_ans_string[0] = selected_ans_string[0].replace(\"\\t\", \" \").replace(\"\\n\", \"\")\r\n",
        "                fout.write(f\"{json_line['question']['stem']} \\\\n {candidates_str}\\t{selected_ans_string[0]}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-7us2LRkpCh"
      },
      "source": [
        "read_file(\"/content/CommonQA/train_rand_split.jsonl\", \"dev\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Vn-t6Elk1u"
      },
      "source": [
        "sentence = []\r\n",
        "label = []\r\n",
        "with open('/content/CommonQA/train_rand_split.jsonl') as h:\r\n",
        "    for line in h:\r\n",
        "        example = json.loads(line)\r\n",
        "        scores = []\r\n",
        "        merged_choices = ' A: '.join([choice['text'] for choice in example['question']['choices']])\r\n",
        "        input = 'Q: ' + example['question']['stem'] + ' A: ' + merged_choices\r\n",
        "        correct_answer = [ choice['text'] for choice in example['question']['choices'] if choice['label'] == example['answerKey'] ][0]\r\n",
        "        sentence.append(input)\r\n",
        "        label.append(correct_answer)\r\n",
        "        #print(input, correct_answer)\r\n",
        "dataset_df = pd.DataFrame({'sentence':sentence, 'label':label})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NciIxvgjzHwM"
      },
      "source": [
        "sentence = []\r\n",
        "label = []\r\n",
        "with open('/content/OpenBookQa/train.jsonl') as h:\r\n",
        "    for line in h:\r\n",
        "        example = json.loads(line)\r\n",
        "        print(example.keys())\r\n",
        "        # scores = []\r\n",
        "        merged_choices = ' A: '.join([choice['text'] for choice in example['question']['choices']])\r\n",
        "        input = 'Q: ' + example['question']['stem'] + ' A: ' + merged_choices\r\n",
        "        correct_answer = [ choice['text'] for choice in example['question']['choices'] if choice['label'] == example['answerKey'] ][0]\r\n",
        "        sentence.append(input)\r\n",
        "        label.append(correct_answer)\r\n",
        "        #print(input, correct_answer)\r\n",
        "dataset_df = pd.DataFrame({'sentence':sentence, 'label':label})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "DphX8AeVXmCz",
        "outputId": "a76c37f9-5cab-40c8-b7f9-07f67548c023"
      },
      "source": [
        "dataset_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q: The sun is responsible for A: puppies learn...</td>\n",
              "      <td>plants sprouting, blooming and wilting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q: When standing miles away from Mount Rushmor...</td>\n",
              "      <td>the mountains seem smaller than in photographs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q: When food is reduced in the stomach A: the ...</td>\n",
              "      <td>nutrients are being deconstructed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q: Stars are A: warm lights that float A: made...</td>\n",
              "      <td>great balls of gas burning billions of miles away</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q: You can make a telescope with a A: straw A:...</td>\n",
              "      <td>mailing tube</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3409</th>\n",
              "      <td>Q: Bears instinctively know when its time to h...</td>\n",
              "      <td>Their parents</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3410</th>\n",
              "      <td>Q: An example of a fossil is a paw print in wh...</td>\n",
              "      <td>hard stones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3411</th>\n",
              "      <td>Q: Which is likeliest to make light pass throu...</td>\n",
              "      <td>any kind of tangible object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3412</th>\n",
              "      <td>Q: What most likely caused the castle built by...</td>\n",
              "      <td>very strong eddies of fast moving air carried ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3413</th>\n",
              "      <td>Q: Splitting and fusing billions of atoms at t...</td>\n",
              "      <td>illumination</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3414 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence                                              label\n",
              "0     Q: The sun is responsible for A: puppies learn...             plants sprouting, blooming and wilting\n",
              "1     Q: When standing miles away from Mount Rushmor...     the mountains seem smaller than in photographs\n",
              "2     Q: When food is reduced in the stomach A: the ...                  nutrients are being deconstructed\n",
              "3     Q: Stars are A: warm lights that float A: made...  great balls of gas burning billions of miles away\n",
              "4     Q: You can make a telescope with a A: straw A:...                                       mailing tube\n",
              "...                                                 ...                                                ...\n",
              "3409  Q: Bears instinctively know when its time to h...                                      Their parents\n",
              "3410  Q: An example of a fossil is a paw print in wh...                                        hard stones\n",
              "3411  Q: Which is likeliest to make light pass throu...                        any kind of tangible object\n",
              "3412  Q: What most likely caused the castle built by...  very strong eddies of fast moving air carried ...\n",
              "3413  Q: Splitting and fusing billions of atoms at t...                                       illumination\n",
              "\n",
              "[3414 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiAbm2GrX1-x"
      },
      "source": [
        "import itertools\r\n",
        "import json\r\n",
        "import os\r\n",
        "import csv\r\n",
        "import errno\r\n",
        "import random\r\n",
        "from random import shuffle\r\n",
        "from typing import List\r\n",
        "import spacy\r\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgMBfKvxfhr7",
        "outputId": "f9f6342f-2e94-4868-d7e4-0bf10d2ec29a"
      },
      "source": [
        "!unzip /content/ambignq_light.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/ambignq_light.zip\n",
            "   creating: ambignq_light/\n",
            "  inflating: ambignq_light/dev_light.json  \n",
            "  inflating: ambignq_light/LICENSE   \n",
            "  inflating: ambignq_light/train_light.json  \n",
            "  inflating: dev_light.json          \n",
            "  inflating: LICENSE                 \n",
            "  inflating: train_light.json        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkQHn9K5XBfG"
      },
      "source": [
        "def ambigqa():\r\n",
        "    def read_file(file, dir, split):\r\n",
        "        outfile = open(f\"{dir}/{split}.tsv\", \"w\")\r\n",
        "        outfile_meta = open(f\"{dir}/{split}_meta.tsv\", \"w\")\r\n",
        "        size = 0\r\n",
        "        with open(file, \"r\") as f:\r\n",
        "            json_file = json.load(f)\r\n",
        "            for item in tqdm(json_file):\r\n",
        "                question = item['question'].replace(\"\\n\", \" \").replace(\"\\t\", \" \")\r\n",
        "                single_answers_already_included = []\r\n",
        "                for anno in item[\"annotations\"]:\r\n",
        "                    if anno['type'] == \"singleAnswer\":\r\n",
        "                        for ans in anno['answer']:\r\n",
        "                            if ans not in single_answers_already_included:\r\n",
        "                                ans = ans.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\r\n",
        "                                outfile.write(f\"{question}\\t{ans}\\n\")\r\n",
        "                                outfile_meta.write(item['id'] + \"\\n\")\r\n",
        "                                single_answers_already_included.append(ans)\r\n",
        "                                size += 1\r\n",
        "                    else:\r\n",
        "                        answers = []\r\n",
        "                        for x in anno['qaPairs']:\r\n",
        "                            answers.append(x['answer'][0])\r\n",
        "\r\n",
        "                        answers = [x.strip() for x in answers]\r\n",
        "                        answers = list(set(answers))  # to drop duplicate answers\r\n",
        "                        for i, ordering in enumerate(itertools.permutations(answers)):\r\n",
        "                            if i >= 3:\r\n",
        "                                break\r\n",
        "                            ans_str = \" [SEP] \".join(ordering).replace(\"\\n\", \" \").replace(\"\\t\", \" \")\r\n",
        "                            outfile.write(f\"{question}\\t{ans_str}\\n\")\r\n",
        "                            outfile_meta.write(item['id'] + \"\\n\")\r\n",
        "                            size += 1\r\n",
        "        return size\r\n",
        "\r\n",
        "    count_dev = read_file(\"/content/ambignq_light/dev_light.json\", \"/content/ambigqa\", \"dev\")\r\n",
        "    count_train = read_file(\"/content/ambignq_light/train_light.json\", \"/content/ambigqa\", \"train\")\r\n",
        "    count_test = 0\r\n",
        "\r\n",
        "    # Create TSVs and get counts.\r\n",
        "    with open(\"ambigqa/counts.json\", \"w\") as outfile:\r\n",
        "        json.dump({\"train\": count_train, \"dev\": count_dev, \"test\": count_test}, outfile)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENbZZKb9Xd6A",
        "outputId": "25c89806-f38c-468c-d5b5-08650a34c5c7"
      },
      "source": [
        "ambigqa()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2002/2002 [00:00<00:00, 112391.54it/s]\n",
            "100%|██████████| 10036/10036 [00:00<00:00, 219098.26it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGYiDLywFypD"
      },
      "source": [
        "#dataset = pd.read_csv(\"/content/dev.tsv\", sep='\\t', header=None)\r\n",
        "dataset = pd.read_csv(\"/content/ambigqa/train.tsv\", sep='\\t', header=None)\r\n",
        "dataset.columns = [\"sentence\", \"label\"]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-7zLo-ZyqwS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "b9698f78-21b5-4645-f542-3492dd36e8a7"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When did the simpsons first air on television?</td>\n",
              "      <td>April 19, 1987 [SEP] December 17, 1989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When did the simpsons first air on television?</td>\n",
              "      <td>December 17, 1989 [SEP] April 19, 1987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who played george washington in the john adams...</td>\n",
              "      <td>David Morse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the legal age of marriage in usa?</td>\n",
              "      <td>0 [SEP] 21 [SEP] 18 years of age [SEP] 19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the legal age of marriage in usa?</td>\n",
              "      <td>0 [SEP] 21 [SEP] 19 [SEP] 18 years of age</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19801</th>\n",
              "      <td>Who was the last person in the uk to be executed?</td>\n",
              "      <td>Gwynne Evans and Peter Allen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19802</th>\n",
              "      <td>Who does wonder woman end up with in the comics?</td>\n",
              "      <td>Superman and Batman [SEP] General Steven Rockw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19803</th>\n",
              "      <td>Who does wonder woman end up with in the comics?</td>\n",
              "      <td>General Steven Rockwell Trevor [SEP] Superman ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19804</th>\n",
              "      <td>When were the first pair of jordans released?</td>\n",
              "      <td>early 1984 [SEP] November 17, 1984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19805</th>\n",
              "      <td>When were the first pair of jordans released?</td>\n",
              "      <td>November 17, 1984 [SEP] early 1984</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19806 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence                                              label\n",
              "0         When did the simpsons first air on television?             April 19, 1987 [SEP] December 17, 1989\n",
              "1         When did the simpsons first air on television?             December 17, 1989 [SEP] April 19, 1987\n",
              "2      Who played george washington in the john adams...                                        David Morse\n",
              "3              What is the legal age of marriage in usa?          0 [SEP] 21 [SEP] 18 years of age [SEP] 19\n",
              "4              What is the legal age of marriage in usa?          0 [SEP] 21 [SEP] 19 [SEP] 18 years of age\n",
              "...                                                  ...                                                ...\n",
              "19801  Who was the last person in the uk to be executed?                       Gwynne Evans and Peter Allen\n",
              "19802   Who does wonder woman end up with in the comics?  Superman and Batman [SEP] General Steven Rockw...\n",
              "19803   Who does wonder woman end up with in the comics?  General Steven Rockwell Trevor [SEP] Superman ...\n",
              "19804      When were the first pair of jordans released?                 early 1984 [SEP] November 17, 1984\n",
              "19805      When were the first pair of jordans released?                 November 17, 1984 [SEP] early 1984\n",
              "\n",
              "[19806 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oazmnaraL0A7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7337da9d-18e2-41fd-e076-5390eea3e831"
      },
      "source": [
        "!gsutil cp -r \\\r\n",
        "  gs://unifiedqa/data/qasc/ \\\r\n",
        "  gs://unifiedqa/data/qasc_test/ \\\r\n",
        "  gs://unifiedqa/data/qasc_with_ir/ \\\r\n",
        "  gs://unifiedqa/data/qasc_with_ir_test/ \\\r\n",
        "  ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://unifiedqa/data/qasc/counts.json...\n",
            "Copying gs://unifiedqa/data/qasc/dev.tsv...\n",
            "Copying gs://unifiedqa/data/qasc/dev_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc/test.tsv...\n",
            "- [4 files][352.9 KiB/352.9 KiB]                                                \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://unifiedqa/data/qasc/test_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc/train.tsv...\n",
            "Copying gs://unifiedqa/data/qasc/train_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_test/counts.json...\n",
            "Copying gs://unifiedqa/data/qasc_test/dev.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_test/dev_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_test/test.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_test/test_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_test/train.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_test/train_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/counts.json...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/dev.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/dev_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/test.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/test_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/train.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/train_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/counts.json...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/dev.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/dev_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/test.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/test_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/train.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/train_meta.tsv...\n",
            "| [28 files][ 38.7 MiB/ 38.7 MiB]                                               \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "\n",
            "Operation completed over 28 objects/38.7 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQGi1iSYTBjj",
        "outputId": "c1bebf9d-9954-4c6f-d1c7-0c846089cd15"
      },
      "source": [
        "!gsutil -m cp -r gs://unifiedqa/data/squad2/dev.tsv ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://unifiedqa/data/squad2/dev.tsv...\n",
            "/ [0/1 files][    0.0 B/ 10.3 MiB]   0% Done                                    \r/ [1/1 files][ 10.3 MiB/ 10.3 MiB] 100% Done                                    \r\n",
            "Operation completed over 1 objects/10.3 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH9ZJ6ZVMzWG",
        "outputId": "d3723f75-1851-4b08-ed6f-c505a1060f22"
      },
      "source": [
        "!gsutil -m cp -r \\\r\n",
        "  gs://unifiedqa/data/ambigqa/ \\\r\n",
        "  ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://unifiedqa/data/ambigqa/counts.json...\n",
            "/ [0/5 files][    0.0 B/  2.8 MiB]   0% Done                                    \rCopying gs://unifiedqa/data/ambigqa/dev.tsv...\n",
            "/ [0/5 files][    0.0 B/  2.8 MiB]   0% Done                                    \rCopying gs://unifiedqa/data/ambigqa/train_meta.tsv...\n",
            "/ [0/5 files][    0.0 B/  2.8 MiB]   0% Done                                    \rCopying gs://unifiedqa/data/ambigqa/train.tsv...\n",
            "Copying gs://unifiedqa/data/ambigqa/dev_meta.tsv...\n",
            "/ [5/5 files][  2.8 MiB/  2.8 MiB] 100% Done                                    \n",
            "Operation completed over 5 objects/2.8 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87RC_1U5BwuC"
      },
      "source": [
        "read_file(\"/content/dev_rand_split.jsonl\", \"dev\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hawianoFEzFY"
      },
      "source": [
        "Instantiate our German and English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLSPj13L2yAr",
        "outputId": "1c58e588-d2f1-4f23-8e71-43bf669c2b96"
      },
      "source": [
        "!python3 -m spacy download de_core_news_sm"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVVbBjSb3MO1"
      },
      "source": [
        "import de_core_news_sm"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BP3YSvJEzFY"
      },
      "source": [
        "spacy_de =  de_core_news_sm.load()\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bEkyPt5EzFY"
      },
      "source": [
        "Previously we reversed the source (German) sentence, however in the paper we are implementing they don't do this, so neither will we."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KaGEZ45EzFZ"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7rbQLYHEzFZ"
      },
      "source": [
        "Create our fields to process our data. This will append the \"start of sentence\" and \"end of sentence\" tokens as well as converting all words to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-J1Zy4QEzFa"
      },
      "source": [
        "# SRC = Field(tokenize=tokenize_de, \n",
        "#             init_token='<sos>', \n",
        "#             eos_token='<eos>', \n",
        "#             lower=True)\n",
        "SRC = Field(tokenize=tokenize_de, \n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>', \n",
        "            lower=True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>', \n",
        "            lower=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0ivNcNSldI6"
      },
      "source": [
        "from IPython.display import display, HTML\r\n",
        "display(HTML(dataset.to_html()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "BuC9C7w6wf1w",
        "outputId": "c3d2092c-4abf-404a-bd9b-0c0969e54321"
      },
      "source": [
        "dataset[18880:18889]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18880</th>\n",
              "      <td>What grade does high school start in japan?</td>\n",
              "      <td>seven [SEP] ten</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18881</th>\n",
              "      <td>What grade does high school start in japan?</td>\n",
              "      <td>ten [SEP] seven</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18882</th>\n",
              "      <td>Where is blanche from in streetcar named desire?</td>\n",
              "      <td>Laurel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18883</th>\n",
              "      <td>Where is blanche from in streetcar named desire?</td>\n",
              "      <td>Laurel, Mississippi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18884</th>\n",
              "      <td>Which two plates meet along the west coast of ...</td>\n",
              "      <td>Pacific Plate [SEP] North American Plate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18885</th>\n",
              "      <td>Which two plates meet along the west coast of ...</td>\n",
              "      <td>North American Plate [SEP] Pacific Plate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18886</th>\n",
              "      <td>What is the current version of php and mysql?</td>\n",
              "      <td>7.4.2 [SEP] 8.0.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18887</th>\n",
              "      <td>What is the current version of php and mysql?</td>\n",
              "      <td>8.0.18 [SEP] 7.4.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18888</th>\n",
              "      <td>When are you supposed to bring elf on the shel...</td>\n",
              "      <td>Thanksgiving to Christmas Eve</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence                                     label\n",
              "18880        What grade does high school start in japan?                           seven [SEP] ten\n",
              "18881        What grade does high school start in japan?                           ten [SEP] seven\n",
              "18882   Where is blanche from in streetcar named desire?                                    Laurel\n",
              "18883   Where is blanche from in streetcar named desire?                       Laurel, Mississippi\n",
              "18884  Which two plates meet along the west coast of ...  Pacific Plate [SEP] North American Plate\n",
              "18885  Which two plates meet along the west coast of ...  North American Plate [SEP] Pacific Plate\n",
              "18886      What is the current version of php and mysql?                        7.4.2 [SEP] 8.0.18\n",
              "18887      What is the current version of php and mysql?                        8.0.18 [SEP] 7.4.2\n",
              "18888  When are you supposed to bring elf on the shel...             Thanksgiving to Christmas Eve"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sytzu5pTnSCa"
      },
      "source": [
        "from torchtext import data \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "Sentence = data.Field(sequential = True, tokenize = 'spacy', batch_first =False, include_lengths=False, lower=False)\r\n",
        "Label = data.Field(sequential =True, tokenize ='spacy', is_target=False, batch_first =False, include_lengths=False, lower=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDuSMFrQwrXC"
      },
      "source": [
        "#dataset = dataset_df\r\n",
        "fields = [('sentence', Sentence),('label',Label)]\r\n",
        "# example = [data.Example.fromlist([dataset.sentence[i],dataset.label[i]], fields) for i in range(dataset.shape[0])] \r\n",
        "example = [data.Example.fromlist([dataset.sentence[i],dataset.label[i]], fields) for i in range(10000)] \r\n",
        "\r\n",
        "commonqa_ds = data.Dataset(example, fields)\r\n",
        "(train, valid) = commonqa_ds.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97K5lZwYx0BB"
      },
      "source": [
        "Sentence.build_vocab(train)\r\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IclMiJOv9Sdz",
        "outputId": "3a920cec-8b4e-4fc3-dcd9-e3536b5a86f7"
      },
      "source": [
        "SRC.sequential, SRC.batch_first, SRC.include_lengths, SRC.is_target, TRG.sequential, TRG.batch_first, TRG.include_lengths, TRG.is_target"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, False, False, False, True, False, False, False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZlXYwYyyz46",
        "outputId": "3bd1098f-aafa-419b-cb3c-b343c2ce66cc"
      },
      "source": [
        "print('Size of input vocab : ', len(Sentence.vocab))\r\n",
        "print('Size of label vocab : ', len(Label.vocab))\r\n",
        "print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\r\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  6784\n",
            "Size of label vocab :  9945\n",
            "Top 10 words appreared repeatedly : [('?', 8500), ('the', 7320), ('Who', 3537), ('in', 3129), ('of', 2834), ('is', 1729), ('When', 1711), ('did', 1218), ('What', 1038), ('was', 1031)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7f25c1ca2ea0>, {'<unk>': 0, '<pad>': 1, '[': 2, ']': 3, 'SEP': 4, ',': 5, 'and': 6, 'the': 7, 'of': 8, '2017': 9, '-': 10, '\"': 11, 'May': 12, 'The': 13, '2018': 14, 'October': 15, 'September': 16, 'March': 17, 'November': 18, 'July': 19, 'April': 20, 'December': 21, 'June': 22, '2016': 23, 'January': 24, 'August': 25, '1': 26, '2015': 27, 'John': 28, 'in': 29, 'New': 30, '6': 31, '10': 32, '2': 33, '17': 34, '12': 35, '15': 36, \"'s\": 37, 'February': 38, 'United': 39, '13': 40, '25': 41, '5': 42, 'to': 43, '8': 44, '2014': 45, 'James': 46, '26': 47, '7': 48, '18': 49, '4': 50, '9': 51, 'Michael': 52, 'a': 53, '20': 54, '11': 55, '3': 56, '16': 57, '14': 58, '21': 59, 'States': 60, '23': 61, '24': 62, '2019': 63, '30': 64, '27': 65, '(': 66, ')': 67, '.': 68, '19': 69, '22': 70, '2006': 71, 'de': 72, '2013': 73, '28': 74, '31': 75, 'York': 76, 'George': 77, '2008': 78, '2009': 79, '29': 80, 'Paul': 81, '2007': 82, 'William': 83, '1998': 84, 'South': 85, 'with': 86, '2012': 87, 'Stadium': 88, 'Robert': 89, 'Charles': 90, 'David': 91, 'II': 92, 'Jr.': 93, 'Richard': 94, 'or': 95, 'Thomas': 96, '1991': 97, 'City': 98, 'France': 99, '/': 100, '2005': 101, '2011': 102, 'King': 103, 'from': 104, '2010': 105, 'Chris': 106, '&': 107, 'Season': 108, 'years': 109, 'Joseph': 110, '1995': 111, 'Martin': 112, 'Taylor': 113, '2001': 114, '°': 115, 'British': 116, 'India': 117, 'Jones': 118, 'million': 119, '2003': 120, 'Smith': 121, 'England': 122, 'Peter': 123, 'on': 124, '1994': 125, 'American': 126, '2004': 127, 'Henry': 128, 'Republic': 129, '1990': 130, '2000': 131, 'Christopher': 132, 'Angeles': 133, 'Canada': 134, 'Daniel': 135, 'Edward': 136, 'London': 137, 'Los': 138, 'Louis': 139, 'Sam': 140, 'Washington': 141, '1980': 142, '1992': 143, '1997': 144, 'Episode': 145, 'Mike': 146, '–': 147, 'between': 148, 'Mexico': 149, 'Patrick': 150, 'for': 151, '1976': 152, '2002': 153, 'General': 154, 'Kevin': 155, 'North': 156, 'University': 157, 'V.': 158, 'World': 159, '1977': 160, 'A': 161, 'Australia': 162, 'California': 163, 'Frank': 164, 'Harris': 165, 'Italy': 166, 'Jeff': 167, 'River': 168, 'Sir': 169, 'late': 170, '%': 171, '1974': 172, '1988': 173, '1999': 174, 'Alexander': 175, 'III': 176, 'Park': 177, 'State': 178, 'Tom': 179, 'Al': 180, 'Hall': 181, 'Kingdom': 182, 'Mark': 183, 'by': 184, 'v.': 185, '1960': 186, '2020': 187, 'Davis': 188, 'Jennifer': 189, 'Mary': 190, 'Tony': 191, 'ago': 192, '1987': 193, '1996': 194, 'Alan': 195, 'Jordan': 196, 'billion': 197, 'is': 198, '1982': 199, 'Cristiano': 200, 'Harry': 201, 'Johnny': 202, 'Kelly': 203, 'President': 204, 'Roger': 205, 'Ronaldo': 206, 'War': 207, 'Williams': 208, '1954': 209, '1973': 210, '1984': 211, 'Ashley': 212, 'Bill': 213, 'Brazil': 214, 'Columbia': 215, 'Eric': 216, 'Lake': 217, 'West': 218, 'Wilson': 219, 'season': 220, '1971': 221, 'Bay': 222, 'Congress': 223, 'Jack': 224, 'Jason': 225, 'Philip': 226, 'Russell': 227, 'St.': 228, 'as': 229, 'at': 230, '1942': 231, '1981': 232, 'Adam': 233, 'Germany': 234, 'I': 235, 'Joe': 236, 'Lee': 237, 'San': 238, 'Scott': 239, 'Vancouver': 240, 'Zealand': 241, 'two': 242, \"'\": 243, '1993': 244, 'Aaron': 245, 'Arthur': 246, 'Carolina': 247, 'House': 248, 'Jackson': 249, 'Jean': 250, 'Johnson': 251, 'Miller': 252, 'Ray': 253, 'Saint': 254, 'century': 255, 'team': 256, 'three': 257, '1969': 258, '1985': 259, '?': 260, 'Albert': 261, 'B.': 262, 'China': 263, 'Field': 264, 'International': 265, 'Kim': 266, 'Ryan': 267, 'Thompson': 268, 'USS': 269, 'Union': 270, 'Young': 271, 'being': 272, '1945': 273, '1965': 274, '1968': 275, '1986': 276, 'Alex': 277, 'Anna': 278, 'Anthony': 279, 'Ben': 280, 'C.': 281, 'Dylan': 282, 'Florida': 283, 'Howard': 284, 'J.': 285, 'Moore': 286, 'Pennsylvania': 287, 'Philadelphia': 288, 'Rachel': 289, 'Singh': 290, 'Stephen': 291, 'Tyler': 292, 'after': 293, 'are': 294, ' ': 295, '1961': 296, '1964': 297, '1966': 298, ':': 299, 'A.': 300, 'Adams': 301, 'Bobby': 302, 'County': 303, 'Gary': 304, 'Green': 305, 'Jimmy': 306, 'Justin': 307, 'La': 308, 'Soviet': 309, 'Spain': 310, 'Stewart': 311, 'Texas': 312, 'Tim': 313, 'episode': 314, 'national': 315, '1620': 316, 'Africa': 317, 'Andy': 318, 'Bob': 319, 'Carter': 320, 'Franklin': 321, 'Marie': 322, 'Matt': 323, 'Men': 324, 'Real': 325, 'Winter': 326, 'five': 327, '1938': 328, '1939': 329, '1957': 330, '1962': 331, '1989': 332, 'Amy': 333, 'Blue': 334, 'Brown': 335, 'Constitution': 336, 'Drew': 337, 'Francis': 338, 'Jerry': 339, 'Kennedy': 340, 'Matthew': 341, 'POWs': 342, 'Pittsburgh': 343, 'Rogers': 344, 'Rose': 345, 'not': 346, 'tendon': 347, '1937': 348, '1967': 349, 'Andrew': 350, 'Black': 351, 'English': 352, 'Francisco': 353, 'French': 354, 'Georgia': 355, 'Japan': 356, 'Jonathan': 357, 'Luke': 358, 'Madrid': 359, 'Nathan': 360, 'Neil': 361, 'Perry': 362, 'Randy': 363, 'Red': 364, 'School': 365, 'Timothy': 366, 'Virginia': 367, 'Wayne': 368, 'one': 369, 'system': 370, '1947': 371, 'Airport': 372, 'Anderson': 373, 'Charlie': 374, 'Chicago': 375, 'Christian': 376, 'Club': 377, 'Cook': 378, 'Daniels': 379, 'Diego': 380, 'Donald': 381, 'Ed': 382, 'Islands': 383, 'Jay': 384, 'Lieutenant': 385, 'Lincoln': 386, 'Lynn': 387, 'Manchester': 388, 'Prince': 389, 'Sean': 390, 'Steve': 391, 'Sunday': 392, 'Van': 393, 'Walter': 394, 'When': 395, 'White': 396, 'early': 397, 'football': 398, 'northern': 399, '100': 400, '1920': 401, '1983': 402, 'Anne': 403, 'Bell': 404, 'Captain': 405, 'Company': 406, 'Day': 407, 'Dean': 408, 'Elizabeth': 409, 'Gordon': 410, 'Harrison': 411, 'Jersey': 412, 'Jim': 413, 'Karl': 414, 'Keith': 415, 'Kentucky': 416, 'National': 417, 'Ohio': 418, 'Old': 419, 'Pacific': 420, 'Plate': 421, 'R.': 422, 'Robinson': 423, 'Shrek': 424, 'Western': 425, 'Will': 426, 'near': 427, 'six': 428, '€': 429, 'Atlanta': 430, 'Barry': 431, 'Boston': 432, 'Bowl': 433, 'Burt': 434, 'Caroline': 435, 'Dick': 436, 'Emma': 437, 'Evans': 438, 'Grant': 439, 'Gray': 440, 'Houston': 441, 'Isaac': 442, 'Jefferson': 443, 'Kansas': 444, 'Lopez': 445, 'Minnesota': 446, 'Street': 447, 'Valley': 448, 'Vegas': 449, 'it': 450, 'other': 451, 'over': 452, 'usually': 453, 'weeks': 454, '!': 455, '0': 456, '1914': 457, '1949': 458, '1970': 459, '34': 460, '70': 461, 'Air': 462, 'Alison': 463, 'Amendment': 464, 'Apostle': 465, 'Austin': 466, 'Band': 467, 'Bonnie': 468, 'Congo': 469, 'Double': 470, 'Eva': 471, 'F': 472, 'F.': 473, 'Illinois': 474, 'Jake': 475, 'Las': 476, 'Lewis': 477, 'M.': 478, 'Nadal': 479, 'Nicholas': 480, 'Ocean': 481, 'Rafael': 482, 'Roberts': 483, 'Speaker': 484, 'Terry': 485, 'Turner': 486, 'Victoria': 487, 'W.': 488, 'Wright': 489, 'You': 490, 'bones': 491, 'four': 492, 'km': 493, 'president': 494, 'seven': 495, 'that': 496, 'their': 497, 'were': 498, '1783': 499, '1927': 500, '1930': 501, '1955': 502, '1958': 503, '1978': 504, 'Alyson': 505, 'America': 506, 'Bailey': 507, 'Billy': 508, 'CBS': 509, 'Chamberlain': 510, 'Chapman': 511, 'Charlotte': 512, 'Deputy': 513, 'East': 514, 'First': 515, 'Force': 516, 'Garuda': 517, 'Giants': 518, 'Hasdrubal': 519, 'High': 520, 'Jeffrey': 521, 'Julie': 522, 'Lisa': 523, 'Ltd': 524, 'Mann': 525, 'Maria': 526, 'Milioti': 527, 'Nancy': 528, 'P.': 529, 'Pakistan': 530, 'Princess': 531, 'Roosevelt': 532, 'Sabha': 533, 'Sharma': 534, 'Show': 535, 'Steven': 536, 'Studios': 537, 'Sun': 538, 'Super': 539, 'T.': 540, 'Thunder': 541, 'Tracy': 542, 'coast': 543, 'hours': 544, 'mL': 545, 'novel': 546, 'state': 547, 'they': 548, 'was': 549, '1921': 550, '1946': 551, '1948': 552, 'Allen': 553, 'Andre': 554, 'Arizona': 555, 'Arnold': 556, 'Beach': 557, 'Boys': 558, 'Brad': 559, 'Brady': 560, 'Brian': 561, 'Bryant': 562, 'Commission': 563, 'Dan': 564, 'Dawn': 565, 'Depp': 566, 'German': 567, 'Grace': 568, 'Inc.': 569, 'Jessica': 570, 'Khan': 571, 'Kumar': 572, 'L.': 573, 'Lionel': 574, 'Lloyd': 575, 'Lord': 576, 'Lythgoe': 577, 'Man': 578, 'Max': 579, 'Murray': 580, 'Netherlands': 581, 'Nick': 582, 'No': 583, 'Parker': 584, 'Price': 585, 'Priscilla': 586, 'Quincy': 587, 'Ricky': 588, 'Robertson': 589, 'Sarah': 590, 'Second': 591, 'Senate': 592, 'Spencer': 593, 'Sr': 594, 'Susan': 595, 'Theodore': 596, 'Victor': 597, 'Warren': 598, 'Water': 599, 'his': 600, 'joint': 601, 'm': 602, 'rock': 603, 'second': 604, 'which': 605, '1776': 606, '1901': 607, '1925': 608, '1935': 609, '1951': 610, '53': 611, 'Amanda': 612, 'Antonio': 613, 'Atlantic': 614, 'BCE': 615, 'Cameron': 616, 'Canadian': 617, 'Carlos': 618, 'Carson': 619, 'Christ': 620, 'Church': 621, 'Colin': 622, 'Craig': 623, 'Crystal': 624, 'Cup': 625, 'Curry': 626, 'Dark': 627, 'Dead': 628, 'Delta': 629, 'Denver': 630, 'Dr.': 631, 'Earl': 632, 'Eddie': 633, 'Eddy': 634, 'El': 635, 'Emily': 636, 'Entertainment': 637, 'Fast': 638, 'Ferguson': 639, 'Ford': 640, 'Four': 641, 'Fred': 642, 'Furious': 643, 'Gabrielle': 644, 'Girl': 645, 'Great': 646, 'Hamilton': 647, 'Hans': 648, 'Hardy': 649, 'Ian': 650, 'Jennings': 651, 'Jesus': 652, 'Josef': 653, 'Jules': 654, 'Kathleen': 655, 'Lakers': 656, 'Marquis': 657, 'Melissa': 658, 'Messi': 659, 'Michigan': 660, 'Mitchell': 661, 'Morris': 662, 'Mosby': 663, 'NC': 664, 'Nelson': 665, 'Nigel': 666, 'Norman': 667, 'Paris': 668, 'Paula': 669, 'Poland': 670, 'Representatives': 671, 'Ron': 672, 'Russia': 673, 'Samuel': 674, 'Seattle': 675, 'Seth': 676, 'Simon': 677, 'Spanish': 678, 'Testament': 679, 'Theatre': 680, 'Trump': 681, 'Turkey': 682, 'US': 683, 'Wales': 684, 'Wall': 685, 'be': 686, 'blood': 687, 'east': 688, 'mm': 689, 'per': 690, 'percent': 691, 'south': 692, '$': 693, '1864': 694, '1892': 695, '1910': 696, '1916': 697, '1941': 698, '32': 699, '50': 700, 'Ali': 701, 'Antioch': 702, 'Arsenal': 703, 'Asia': 704, 'Baldwin': 705, 'Barnabas': 706, 'Brees': 707, 'Bridges': 708, 'C': 709, 'Catherine': 710, 'Celtics': 711, 'Center': 712, 'Christine': 713, 'Cleveland': 714, 'Colorado': 715, 'Crosby': 716, 'Dale': 717, 'Dance': 718, 'Dartmouth': 719, 'Democratic': 720, 'Denis': 721, 'District': 722, 'Dorothy': 723, 'Dwight': 724, 'E.': 725, 'Elena': 726, 'End': 727, 'Excel': 728, 'FIFA': 729, 'Fox': 730, 'Girls': 731, 'Glen': 732, 'God': 733, 'Hale': 734, 'Halliwell': 735, 'Hannibal': 736, 'Hughes': 737, 'Indian': 738, 'Instagram': 739, 'Island': 740, 'Ivan': 741, 'Jon': 742, 'Judah': 743, 'Kohli': 744, 'Korea': 745, 'Kristen': 746, 'Kurt': 747, 'Larry': 748, 'Lindsay': 749, 'Lucas': 750, 'Lynne': 751, 'Manoj': 752, 'Marshall': 753, 'Martha': 754, 'Maryland': 755, 'Memorial': 756, 'Merckx': 757, 'Minister': 758, 'Montreal': 759, 'Morgan': 760, 'Murphy': 761, 'Nevada': 762, 'Newman': 763, 'Oliver': 764, 'Packers': 765, 'Party': 766, 'Penguins': 767, 'Pete': 768, 'Phil': 769, 'Pierce': 770, 'Ralph': 771, 'Ram': 772, 'Rebecca': 773, 'Richardson': 774, 'Rob': 775, 'Sinatra': 776, 'Stuart': 777, 'Team': 778, 'Third': 779, 'Tina': 780, 'Tommy': 781, 'U.S.': 782, 'V': 783, 'Vincent': 784, 'Virat': 785, 'Walker': 786, 'an': 787, 'area': 788, 'chloride': 789, 'convert': 790, 'do': 791, 'end': 792, 'ended': 793, 'fiction': 794, 'first': 795, 'intestine': 796, 'plates': 797, 'salt': 798, 'states': 799, 'unknown': 800, 'unnamed': 801, 'up': 802, 'where': 803, 'who': 804, '≥': 805, '100,000': 806, '1775': 807, '18th': 808, '1909': 809, '1917': 810, '1931': 811, '1936': 812, '1940': 813, '1943': 814, '1950': 815, '1975': 816, '1979': 817, '35': 818, '36': 819, '39': 820, '40': 821, '500': 822, '60': 823, 'Abdul': 824, 'After': 825, 'Airlines': 826, 'Alabama': 827, 'Alaska': 828, 'Alberta': 829, 'Arena': 830, 'Argentina': 831, 'Avenue': 832, 'Bacharach': 833, 'Bank': 834, 'Berlin': 835, 'Beyoncé': 836, 'Boyz': 837, 'Brett': 838, 'Bruce': 839, 'Bulldogs': 840, 'Carl': 841, 'Cassidy': 842, 'Christie': 843, 'Christy': 844, 'Cody': 845, 'Coleman': 846, 'Conroy': 847, 'Cornwall': 848, 'Corporation': 849, 'Cristin': 850, 'D': 851, 'Derek': 852, 'Dhawan': 853, 'Douglas': 854, 'Drummond': 855, 'Duke': 856, 'Elliott': 857, 'Emperor': 858, 'Empire': 859, 'Enterprises': 860, 'Erik': 861, 'F.C.': 862, 'Fair': 863, 'Falcons': 864, 'Federer': 865, 'Felix': 866, 'Financial': 867, 'Fiona': 868, 'Fire': 869, 'Flag': 870, 'Football': 871, 'Forest': 872, 'Frances': 873, 'Freddie': 874, 'G.': 875, 'Geoff': 876, 'Graham': 877, 'Hal': 878, 'Hannigan': 879, 'Hart': 880, 'Home': 881, 'IV': 882, 'Iran': 883, 'Israel': 884, 'Jill': 885, 'Kane': 886, 'Kate': 887, 'Kenny': 888, 'Kings': 889, 'L4': 890, 'Last': 891, 'Le': 892, 'LeBron': 893, 'Leo': 894, 'Leviticus': 895, 'Major': 896, 'Marching': 897, 'McCartney': 898, 'McConnell': 899, 'Mediterranean': 900, 'Missouri': 901, 'Morocco': 902, 'Mountains': 903, 'Muhammad': 904, 'Nehru': 905, 'On': 906, 'Peters': 907, 'Philippines': 908, 'Phoebe': 909, 'Queen': 910, 'Quinn': 911, 'Ramirez': 912, 'Rams': 913, 'Ross': 914, 'Saturday': 915, 'Saul': 916, 'Syed': 917, 'Tell': 918, 'Tigers': 919, 'Ulf': 920, 'Wood': 921, 'approximately': 922, 'army': 923, 'around': 924, 'below': 925, 'during': 926, 'eight': 927, 'group': 928, 'her': 929, 'into': 930, 'large': 931, 'layer': 932, 'league': 933, 'mid': 934, 'minutes': 935, 'nine': 936, 'people': 937, 'red': 938, 'small': 939, 'still': 940, 'tax': 941, '×': 942, '1792': 943, '1801': 944, '1908': 945, '1928': 946, '1933': 947, '1956': 948, '1963': 949, '1st': 950, '78': 951, '80': 952, 'Act': 953, 'Alicia': 954, 'Ann': 955, 'Ariana': 956, 'Assembly': 957, 'Barack': 958, 'Barbra': 959, 'Beijing': 960, 'Benjamin': 961, 'Blood': 962, 'Bowman': 963, 'Brenda': 964, 'Brooks': 965, 'Bryan': 966, 'Bulgaria': 967, 'Bush': 968, 'Carey': 969, 'Carpenter': 970, 'Chelsea': 971, 'Cheryl': 972, 'Chief': 973, 'Clause': 974, 'Clement': 975, 'Council': 976, 'Court': 977, 'Cree': 978, 'Curtis': 979, 'DC': 980, 'Damon': 981, 'Danny': 982, 'Dave': 983, 'Dewey': 984, 'Diggy': 985, 'Do': 986, 'Dutch': 987, 'Earth': 988, 'Edelman': 989, 'Egypt': 990, 'Exodus': 991, 'Faye': 992, 'Felton': 993, 'Free': 994, 'Friedrich': 995, 'Gen.': 996, 'Glenn': 997, 'Grande': 998, 'Gregory': 999, 'Haley': 1000, 'Hastings': 1001, 'Hathaway': 1002, 'Holland': 1003, 'Hudson': 1004, 'Hugo': 1005, 'Hurwitz': 1006, 'Hussein': 1007, 'Iraq': 1008, 'Jacob': 1009, 'Javier': 1010, 'Jeremy': 1011, 'Jo': 1012, 'Josh': 1013, 'Judy': 1014, 'Kirsten': 1015, 'Kovind': 1016, 'Lafayette': 1017, 'Land': 1018, 'Latin': 1019, 'League': 1020, 'Lennon': 1021, 'Lester': 1022, 'Lily': 1023, 'Little': 1024, 'Lok': 1025, 'Loren': 1026, 'Lou': 1027, 'López': 1028, 'Malone': 1029, 'Mansion': 1030, 'Mercury': 1031, 'Michelle': 1032, 'Middle': 1033, 'Moody': 1034, 'Museum': 1035, 'N': 1036, 'Nath': 1037, 'Obama': 1038, 'Officer': 1039, 'Olivia': 1040, 'Paige': 1041, 'Pasadena': 1042, 'Pitt': 1043, 'Point': 1044, 'Prime': 1045, 'Raymond': 1046, 'Reynolds': 1047, 'Romano': 1048, 'Rouge': 1049, 'Sebastian': 1050, 'Shepard': 1051, 'Sisters': 1052, 'Sports': 1053, 'Stanton': 1054, 'Stevie': 1055, 'Streisand': 1056, 'Sudan': 1057, 'Sullivan': 1058, 'Tampa': 1059, 'Thailand': 1060, 'Timberlake': 1061, 'Todd': 1062, 'Two': 1063, 'Urban': 1064, 'Vanessa': 1065, 'Vladimir': 1066, 'Wallace': 1067, 'Waylon': 1068, 'Whitney': 1069, 'Wisconsin': 1070, 'Wonder': 1071, 'about': 1072, 'along': 1073, 'before': 1074, 'court': 1075, 'different': 1076, 'found': 1077, 'glands': 1078, 'halite': 1079, 'most': 1080, 'north': 1081, 'original': 1082, 'papillae': 1083, 'plate': 1084, 's': 1085, 'sixth': 1086, 'tectonic': 1087, 'ten': 1088, 'various': 1089, 'very': 1090, 'white': 1091, 'young': 1092, '01': 1093, '105': 1094, '11th': 1095, '130': 1096, '150': 1097, '1895': 1098, '1911': 1099, '1915': 1100, '1934': 1101, '1944': 1102, '1959': 1103, '1960s': 1104, '1990s': 1105, '22nd': 1106, '258': 1107, '300,000': 1108, '338': 1109, '44': 1110, '49': 1111, '6:31': 1112, '7:12': 1113, '8:00': 1114, 'AD': 1115, 'Adcock': 1116, 'Adrian': 1117, 'African': 1118, 'Alexandra': 1119, 'Algeria': 1120, 'All': 1121, 'Amsterdam': 1122, 'Andreas': 1123, 'Andrews': 1124, 'Anglo': 1125, 'Antoine': 1126, 'Apollos': 1127, 'Arlington': 1128, 'Army': 1129, 'Arvid': 1130, 'Attlee': 1131, 'Audrey': 1132, 'Australian': 1133, 'Badale': 1134, 'Baker': 1135, 'Barca': 1136, 'Beatles': 1137, 'Belinda': 1138, 'Bennett': 1139, 'Beth': 1140, 'Big': 1141, 'Bloom': 1142, 'Bonds': 1143, 'Bone': 1144, 'Books': 1145, 'Britain': 1146, 'Brock': 1147, 'Bronco': 1148, 'Brosnan': 1149, 'Buster': 1150, 'Byron': 1151, 'CE': 1152, 'CSS': 1153, 'Camille': 1154, 'Cardinals': 1155, 'Catholic': 1156, 'Chad': 1157, 'Chairman': 1158, 'Chhote': 1159, 'Chloe': 1160, 'Chuck': 1161, 'Clemson': 1162, 'Clifton': 1163, 'Cobie': 1164, 'Cole': 1165, 'Connick': 1166, 'Cooke': 1167, 'Cove': 1168, 'Crows': 1169, 'Czech': 1170, 'Czechoslavak': 1171, 'D.': 1172, 'Darling': 1173, 'De': 1174, 'DeAndre': 1175, 'Diane': 1176, 'Doctor': 1177, 'Dragons': 1178, 'Dyke': 1179, 'Eastern': 1180, 'Eduardo': 1181, 'Ernest': 1182, 'Estate': 1183, 'Ethiopia': 1184, 'Europe': 1185, 'Evgenia': 1186, 'Fatt': 1187, 'Feast': 1188, 'Federal': 1189, 'Fisher': 1190, 'Fort': 1191, 'Frederick': 1192, 'Freed': 1193, 'G': 1194, 'Gaines': 1195, 'Galilei': 1196, 'Galileo': 1197, 'Gene': 1198, 'Gerard': 1199, 'Giblin': 1200, 'Graves': 1201, 'Greek': 1202, 'Greg': 1203, 'Griffin': 1204, 'Han': 1205, 'Hanno': 1206, 'Harold': 1207, 'Hope': 1208, 'Hotels': 1209, 'Hungary': 1210, 'Hunter': 1211, 'Hurst': 1212, 'Ironsides': 1213, 'Jacques': 1214, 'Jets': 1215, 'Joan': 1216, 'Joel': 1217, 'Johnathan': 1218, 'Joshi': 1219, 'José': 1220, 'Joyce': 1221, 'Judge': 1222, 'Just': 1223, 'Kaitlyn': 1224, 'Katy': 1225, 'Kenya': 1226, 'Kjellberg': 1227, 'Knight': 1228, 'Kris': 1229, 'L5': 1230, 'Lalong': 1231, 'Lange': 1232, 'Late': 1233, 'Laura': 1234, 'Lauren': 1235, 'Lawes': 1236, 'Lepiato': 1237, 'Lot': 1238, 'Lusail': 1239, 'Maccabees': 1240, 'Maciej': 1241, 'Magic': 1242, 'Main': 1243, 'Maj': 1244, 'Mali': 1245, 'Manuel': 1246, 'Marc': 1247, 'Mariyappan': 1248, 'Massey': 1249, 'McEwen': 1250, 'McFly': 1251, 'McLean': 1252, 'Me': 1253, 'Medvedeva': 1254, 'Miles': 1255, 'Mimi': 1256, 'Minstrels': 1257, 'Molly': 1258, 'Momsen': 1259, 'Montgomery': 1260, 'Motor': 1261, 'My': 1262, 'Neal': 1263, 'Network': 1264, 'Newton': 1265, 'Niger': 1266, 'Normandy': 1267, 'Norway': 1268, 'Now': 1269, 'O': 1270, \"O'Connor\": 1271, 'One': 1272, 'Orji': 1273, 'Orleans': 1274, 'Owen': 1275, 'Palace': 1276, 'Panthers': 1277, 'Payne': 1278, 'Pearl': 1279, 'Pearson': 1280, 'Peck': 1281, 'Peninsula': 1282, 'Portugal': 1283, 'Post': 1284, 'Preston': 1285, 'Punjab': 1286, 'Rani': 1287, 'Rayyan': 1288, 'Religion': 1289, 'Rice': 1290, 'Rights': 1291, 'Roderick': 1292, 'Rodgers': 1293, 'Rohit': 1294, 'Romania': 1295, 'Ronald': 1296, 'Royal': 1297, 'Salt': 1298, 'Sankt': 1299, 'Santa': 1300, 'Schmit': 1301, 'Sea': 1302, 'Seasons': 1303, 'Seely': 1304, 'Seven': 1305, 'Shaw': 1306, 'Shearer': 1307, 'Sherwood': 1308, 'Shikhar': 1309, 'Sidney': 1310, 'Simone': 1311, 'Singapore': 1312, 'Slowly': 1313, 'Smulders': 1314, 'Solstice': 1315, 'Spaniards': 1316, 'Starlighters': 1317, 'Steph': 1318, 'Stevens': 1319, 'Strange': 1320, 'Subah': 1321, 'Tanzania': 1322, 'Tennessee': 1323, 'Thangavelu': 1324, 'This': 1325, 'Thornton': 1326, 'Time': 1327, 'Townsend': 1328, 'Trumbo': 1329, 'USMC': 1330, 'Villanova': 1331, 'Vivian': 1332, 'Voyager': 1333, 'W': 1334, 'Ward': 1335, 'Wardell': 1336, 'Warner': 1337, 'Warwick': 1338, 'Whiteside': 1339, 'Winds': 1340, 'Woods': 1341, 'Yankees': 1342, 'Yorkshire': 1343, 'Zimmer': 1344, 'act': 1345, 'bailiff': 1346, 'best': 1347, 'border': 1348, 'both': 1349, 'called': 1350, 'capital': 1351, 'cell': 1352, 'center': 1353, 'constable': 1354, 'cricket': 1355, 'crossing': 1356, 'current': 1357, 'di': 1358, 'disciples': 1359, 'discovered': 1360, 'earthquakes': 1361, 'eruptions': 1362, 'explosion': 1363, 'featuring': 1364, 'fifty': 1365, 'front': 1366, 'game': 1367, 'gypsum': 1368, 'h': 1369, 'hard': 1370, 'have': 1371, 'just': 1372, 'last': 1373, 'membrane': 1374, 'metal': 1375, 'middle': 1376, 'mph': 1377, 'muscle': 1378, 'named': 1379, 'natural': 1380, 'new': 1381, 'number': 1382, 'off': 1383, 'p.m.': 1384, 'phase': 1385, 'power': 1386, 'process': 1387, 'quickly': 1388, 'raw': 1389, 'republic': 1390, 'rugby': 1391, 'salamander': 1392, 'salts': 1393, 'shaped': 1394, 'she': 1395, 'sheriff': 1396, 'sodium': 1397, 'son': 1398, 'subducted': 1399, 'symbol': 1400, 'third': 1401, 'tissue': 1402, 'town': 1403, 'triceps': 1404, 'upcoming': 1405, 'western': 1406, 'within': 1407, 'y': 1408, 'year': 1409, '1,000': 1410, '10,000': 1411, '145': 1412, '167': 1413, '1789': 1414, '1793': 1415, '17th': 1416, '180': 1417, '1834': 1418, '1848': 1419, '1857': 1420, '186': 1421, '1898': 1422, '1899': 1423, '1912': 1424, '1970s': 1425, '1980s': 1426, '19th': 1427, '2000s': 1428, '2nd': 1429, '304': 1430, '330': 1431, '370': 1432, '41': 1433, '45': 1434, '47': 1435, '48': 1436, '63': 1437, '69': 1438, '72': 1439, '82': 1440, 'Acts': 1441, 'Addison': 1442, 'Adolph': 1443, 'Affleck': 1444, 'Ages': 1445, 'Alastair': 1446, 'Alpha': 1447, 'Amelia': 1448, 'Angela': 1449, 'Annette': 1450, 'Arabian': 1451, 'Area': 1452, 'Arkansas': 1453, 'Arterton': 1454, 'Article': 1455, 'Auerbach': 1456, 'B': 1457, 'B.I.G.': 1458, 'BBC': 1459, 'Banks': 1460, 'Baxter': 1461, 'Belafonte': 1462, 'Bernard': 1463, 'Bican': 1464, 'Blackburn': 1465, 'Blunt': 1466, 'Bonham': 1467, 'Book': 1468, 'Brendan': 1469, 'Bronson': 1470, 'Brooke': 1471, 'Brothers': 1472, 'Buck': 1473, 'CA': 1474, 'Caesar': 1475, 'Cain': 1476, 'Carole': 1477, 'Carolyn': 1478, 'Cat': 1479, 'Chas': 1480, 'Chase': 1481, 'Cher': 1482, 'Claire': 1483, 'Clark': 1484, 'Clarkson': 1485, 'Cloris': 1486, 'Clyde': 1487, 'Coliseum': 1488, 'College': 1489, 'Connie': 1490, 'Cornwallis': 1491, 'Crimson': 1492, 'Cynthia': 1493, 'Cyrus': 1494, 'D.C.': 1495, 'Daei': 1496, 'Dallas': 1497, 'Daltrey': 1498, 'Dana': 1499, 'Dennis': 1500, 'Detroit': 1501, 'Diggins': 1502, 'Dil': 1503, 'Disney': 1504, 'Djokovic': 1505, 'Doug': 1506, 'Edition': 1507, 'Executive': 1508, 'Fanning': 1509, 'Finley': 1510, 'Flack': 1511, 'Fontana': 1512, 'Foster': 1513, 'Fourth': 1514, 'Franco': 1515, 'Friday': 1516, 'Fuller': 1517, 'Gabriel': 1518, 'Gaga': 1519, 'Games': 1520, 'Garbiñe': 1521, 'Gardner': 1522, 'Genesis': 1523, 'Ghost': 1524, 'Gielgud': 1525, 'Goodman': 1526, 'Grand': 1527, 'Greece': 1528, 'Guy': 1529, 'H.': 1530, 'Hackett': 1531, 'Harding': 1532, 'Hayes': 1533, 'Health': 1534, 'Heather': 1535, 'Helena': 1536, 'Henri': 1537, 'Hepburn': 1538, 'Herbert': 1539, 'Hernandez': 1540, 'Hill': 1541, 'Holdings': 1542, 'Hollywood': 1543, 'Holt': 1544, 'Hopkins': 1545, 'Hudgens': 1546, 'Hunt': 1547, 'Idina': 1548, 'Ike': 1549, 'Indonesia': 1550, 'Irving': 1551, 'Jamie': 1552, 'Jane': 1553, 'Janeiro': 1554, 'Johan': 1555, 'Johansson': 1556, 'Jonah': 1557, 'Judith': 1558, 'Julius': 1559, 'Justice': 1560, 'Kalu': 1561, 'Katherine': 1562, 'Keating': 1563, 'Keller': 1564, 'Kilmer': 1565, 'Kirk': 1566, 'Lacey': 1567, 'Lady': 1568, 'Lana': 1569, 'Larson': 1570, 'Law': 1571, 'Leachman': 1572, 'Leah': 1573, 'Leigh': 1574, 'Lesley': 1575, 'Life': 1576, 'Lightning': 1577, 'Linden': 1578, 'Léon': 1579, 'MBE': 1580, 'MacDonald': 1581, 'Manekshaw': 1582, 'Manning': 1583, 'Marcus': 1584, 'Mariana': 1585, 'Marina': 1586, 'Marvin': 1587, 'Massachusetts': 1588, 'Maureen': 1589, 'Maurice': 1590, 'Mayweather': 1591, 'McDermott': 1592, 'McGrath': 1593, 'McGuire': 1594, 'Melanie': 1595, 'Menzel': 1596, 'Meredith': 1597, 'Miami': 1598, 'Millennium': 1599, 'Minneapolis': 1600, 'Minoru': 1601, 'Miranda': 1602, 'Monica': 1603, 'Montana': 1604, 'Moran': 1605, 'Morton': 1606, 'Motorsports': 1607, 'Moulin': 1608, 'Mount': 1609, 'Muguruza': 1610, 'Mukherjee': 1611, 'Natalie': 1612, 'Nathaniel': 1613, 'Nazareth': 1614, 'Ned': 1615, 'Nice': 1616, 'Nichols': 1617, 'Notorious': 1618, 'Novak': 1619, \"O'Donnell\": 1620, \"O'Donoghue\": 1621, 'Olympiastadion': 1622, 'Oregon': 1623, 'Orlando': 1624, 'Orthodox': 1625, 'Osmond': 1626, 'Pagans': 1627, 'Pasek': 1628, 'Patel': 1629, 'Patil': 1630, 'Patriots': 1631, 'Patti': 1632, 'Pentateuch': 1633, 'Persian': 1634, 'Placid': 1635, 'Porco': 1636, 'Postl': 1637, 'Powell': 1638, 'Powers': 1639, 'Pranab': 1640, 'Pratibha': 1641, 'Puerto': 1642, 'Pujols': 1643, 'Quartet': 1644, 'Rajya': 1645, 'Reese': 1646, 'Reeves': 1647, 'Regina': 1648, 'Resort': 1649, 'Rewind': 1650, 'Rexha': 1651, 'Rhodes': 1652, 'Rhys': 1653, 'Richie': 1654, 'Rico': 1655, 'Ridge': 1656, 'Rio': 1657, 'Roberta': 1658, 'Rollins': 1659, 'Root': 1660, 'Rosalind': 1661, 'Sally': 1662, 'Sammy': 1663, 'Sandra': 1664, 'Schwartz': 1665, 'Serena': 1666, 'Sergeant': 1667, 'Sevilla': 1668, 'Seymour': 1669, 'Shada': 1670, 'Shadows': 1671, 'Shi': 1672, 'Sinclair': 1673, 'Sirach': 1674, 'Son': 1675, 'Song': 1676, 'Sophie': 1677, 'Spotlight': 1678, 'Springs': 1679, 'St': 1680, 'Staff': 1681, 'Stanley': 1682, 'Stars': 1683, 'Station': 1684, 'Stranger': 1685, 'Strong': 1686, 'Sugar': 1687, 'Summer': 1688, 'Sunil': 1689, 'Suzanne': 1690, 'Swift': 1691, 'Tales': 1692, 'Tanakh': 1693, 'Task': 1694, 'Tenochtitlan': 1695, 'Tibbets': 1696, 'Tide': 1697, 'Tides': 1698, 'Tiger': 1699, 'Times': 1700, 'Tlacopan': 1701, 'Tower': 1702, 'Town': 1703, 'Travis': 1704, 'Trevor': 1705, 'Tuscany': 1706, 'Up': 1707, 'Utah': 1708, 'Vettel': 1709, 'Vikander': 1710, 'Vikings': 1711, 'Walsh': 1712, 'Weather': 1713, 'Wendy': 1714, 'Who': 1715, 'Wilt': 1716, 'Wind': 1717, 'Winston': 1718, 'Witherspoon': 1719, 'Wong': 1720, 'Wynn': 1721, 'YouTube': 1722, 'Yusuf': 1723, 'a.m.': 1724, 'back': 1725, 'body': 1726, 'bone': 1727, 'brother': 1728, 'car': 1729, 'card': 1730, 'cathedral': 1731, 'cells': 1732, 'central': 1733, 'children': 1734, 'city': 1735, 'coal': 1736, 'college': 1737, 'comedy': 1738, 'crucifixes': 1739, 'down': 1740, 'estuary': 1741, 'fall': 1742, 'fat': 1743, 'father': 1744, 'folklore': 1745, 'ft': 1746, 'government': 1747, 'governments': 1748, 'groups': 1749, 'human': 1750, 'information': 1751, 'interest': 1752, 'its': 1753, 'largely': 1754, 'leaves': 1755, 'less': 1756, 'life': 1757, 'like': 1758, 'lower': 1759, 'manuscript': 1760, 'may': 1761, 'members': 1762, 'memory': 1763, \"n't\": 1764, 'neglect': 1765, 'network': 1766, 'née': 1767, 'plant': 1768, 'player': 1769, 'pm': 1770, 'proximal': 1771, 'radiation': 1772, 'rate': 1773, 'right': 1774, 'school': 1775, 'series': 1776, 'smaller': 1777, 'some': 1778, 'sometimes': 1779, 'steroid': 1780, 'surface': 1781, 'territory': 1782, 'time': 1783, 'tower': 1784, 'tract': 1785, 'until': 1786, 'video': 1787, 'well': 1788, 'wife': 1789, 'winning': 1790, 'wooden': 1791, 'world': 1792, 'younger': 1793, '#': 1794, \"'re\": 1795, '+': 1796, '104': 1797, '108': 1798, '125': 1799, '136': 1800, '15th': 1801, '1600': 1802, '1765': 1803, '1781': 1804, '1847': 1805, '1853': 1806, '1871': 1807, '1893': 1808, '1907': 1809, '1924': 1810, '1953': 1811, '2021': 1812, '2029': 1813, '20:00': 1814, '24th': 1815, '311': 1816, '33': 1817, '38': 1818, '3rd': 1819, '43': 1820, '4th': 1821, '53.5': 1822, '56': 1823, '597': 1824, '84': 1825, '99': 1826, '9th': 1827, 'Abraham': 1828, 'Aces': 1829, 'Achilles': 1830, 'Acker': 1831, 'Additions': 1832, 'Administration': 1833, 'Admiral': 1834, 'Aisha': 1835, 'Alec': 1836, 'Allyn': 1837, 'Anaheim': 1838, 'Andersen': 1839, 'Andrés': 1840, 'Annapolis': 1841, 'Antarctica': 1842, 'Arabia': 1843, 'Archduke': 1844, 'Armstrong': 1845, 'Asteroid': 1846, 'At': 1847, 'Atlas': 1848, 'Atticus': 1849, 'Aveiro': 1850, 'B.F.': 1851, 'Back': 1852, 'Bagley': 1853, 'Baltimore': 1854, 'Barnes': 1855, 'Bart': 1856, 'Battle': 1857, 'Baylor': 1858, 'Bears': 1859, 'Bellinger': 1860, 'Bengal': 1861, 'Benoist': 1862, 'Betty': 1863, 'Billie': 1864, 'Bloomington': 1865, 'Boeser': 1866, 'Bolton': 1867, 'Bordeaux': 1868, 'Brahma': 1869, 'Brake': 1870, 'Breda': 1871, 'Bridge': 1872, 'Brightman': 1873, 'Brijwasi': 1874, 'Britton': 1875, 'Broncos': 1876, 'Buffalo': 1877, 'Burch': 1878, 'CC': 1879, 'CK': 1880, 'Caleb': 1881, 'Camp': 1882, 'Canadiens': 1883, 'Cannavale': 1884, 'Canning': 1885, 'Carla': 1886, 'Carol': 1887, 'Carroll': 1888, 'Carthage': 1889, 'Castro': 1890, 'Cates': 1891, 'Cayman': 1892, 'Cedric': 1893, 'Central': 1894, 'Charli': 1895, 'Charter': 1896, 'Chestnut': 1897, 'Chile': 1898, 'Choo': 1899, 'Christina': 1900, 'Christmas': 1901, 'Chu': 1902, 'Clarke': 1903, 'Clooney': 1904, 'Cobb': 1905, 'Collins': 1906, 'Columbus': 1907, 'Comics': 1908, 'Complex': 1909, 'Connecticut': 1910, 'Connors': 1911, 'Convention': 1912, 'Cooper': 1913, 'Corey': 1914, 'Crawford': 1915, 'Creek': 1916, 'Cuccittini': 1917, 'Cullen': 1918, 'Cummings': 1919, 'Cunningham': 1920, 'Curt': 1921, 'Daily': 1922, 'Dalton': 1923, 'Dame': 1924, 'Davidson': 1925, 'Declaration': 1926, 'Defense': 1927, 'Derulo': 1928, 'Desmond': 1929, 'Deuteronomy': 1930, 'Deventer': 1931, 'Diana': 1932, 'Diddle': 1933, 'Dido': 1934, 'Disneyland': 1935, 'Dodger': 1936, 'Doetinchem': 1937, 'Doha': 1938, 'Donna': 1939, 'Donny': 1940, 'Dragon': 1941, 'Driscoll': 1942, 'Drive': 1943, 'Dubois': 1944, 'Duncan': 1945, 'Eagles': 1946, 'Early': 1947, 'Edmund': 1948, 'Edwards': 1949, 'Edwin': 1950, 'Egan': 1951, 'Einziger': 1952, 'Elaine': 1953, 'Elgin': 1954, 'Elias': 1955, 'Elle': 1956, 'Ellis': 1957, 'Emilio': 1958, 'Enschede': 1959, 'Ephron': 1960, 'Ernie': 1961, 'Estates': 1962, 'Evan': 1963, 'Eve': 1964, 'FM': 1965, 'Ferrell': 1966, 'Final': 1967, 'Finch': 1968, 'Five': 1969, 'Flora': 1970, 'Forrest': 1971, 'François': 1972, 'Freedoms': 1973, 'Frey': 1974, 'Gage': 1975, 'Genevieve': 1976, 'Gere': 1977, 'Gilmour': 1978, 'Gold': 1979, 'Gore': 1980, 'Governor': 1981, 'Greatest': 1982, 'Grimes': 1983, 'Grounds': 1984, 'Ha': 1985, 'Hannah': 1986, 'Hansen': 1987, 'Harden': 1988, 'Harvey': 1989, 'Hawaii': 1990, 'Hawkins': 1991, 'Head': 1992, 'Heavy': 1993, 'Helen': 1994, 'Helms': 1995, 'Hemant': 1996, 'Henderson': 1997, 'Henley': 1998, 'Hero': 1999, 'Hirohito': 2000, 'Holliday': 2001, 'Holloway': 2002, 'Holly': 2003, 'Hong': 2004, 'Hotel': 2005, 'Howe': 2006, 'Hutchinson': 2007, 'Ice': 2008, 'Indiana': 2009, 'Indians': 2010, 'Inn': 2011, 'Internet': 2012, 'Investments': 2013, 'Iqbal': 2014, 'Irish': 2015, 'Isle': 2016, 'J': 2017, \"Ja'net\": 2018, 'Jardine': 2019, 'Jawaharlal': 2020, 'Jenny': 2021, 'Jeremiah': 2022, 'Jericho': 2023, 'Ji': 2024, 'Joachim': 2025, 'Jodi': 2026, 'Johann': 2027, 'Jong': 2028, 'Jorge': 2029, 'Jose': 2030, 'Jude': 2031, 'Kamalesvaran': 2032, 'Karen': 2033, 'Kathryn': 2034, 'Ken': 2035, 'Key': 2036, 'Khor': 2037, 'Knowles': 2038, 'Kong': 2039, 'Kossoy': 2040, 'Krauss': 2041, 'Kristin': 2042, 'Kuwait': 2043, 'Kylo': 2044, 'L3': 2045, 'Lanka': 2046, 'Lara': 2047, 'Lausanne': 2048, 'Lawrence': 2049, 'Legend': 2050, 'Leiber': 2051, 'Lenin': 2052, 'Lenny': 2053, 'Libya': 2054, 'Limited': 2055, 'Linnaeus': 2056, 'Lorraine': 2057, 'Louise': 2058, 'Louisiana': 2059, 'Ltd.': 2060, 'Luther': 2061, 'Mac': 2062, 'MacArthur': 2063, 'MacFarlane': 2064, 'Madeline': 2065, 'Majorino': 2066, 'Mallorca': 2067, 'Manfred': 2068, 'Mara': 2069, 'Margaret': 2070, 'Marseille': 2071, 'Marshal': 2072, 'Mastiff': 2073, 'Mattel': 2074, 'Mauna': 2075, 'Maxwell': 2076, 'McDonald': 2077, 'Melbourne': 2078, 'Melora': 2079, 'Menken': 2080, 'Mervyn': 2081, 'Mick': 2082, 'Miki': 2083, 'Milton': 2084, 'Min': 2085, 'Mississippi': 2086, 'Mohawk': 2087, 'Morrison': 2088, 'Mother': 2089, 'Mrs.': 2090, 'Mulvaney': 2091, 'Murdoch': 2092, 'Musa': 2093, 'NBA': 2094, 'Natural': 2095, 'Nayyar': 2096, 'Nebraska': 2097, 'Nellie': 2098, 'Nigeria': 2099, 'Nighy': 2100, 'Niven': 2101, 'Noah': 2102, 'Northern': 2103, 'Not': 2104, 'Oak': 2105, 'Of': 2106, 'Organization': 2107, 'Orioles': 2108, 'Ottoman': 2109, 'Over': 2110, 'Owens': 2111, 'PM': 2112, 'PST': 2113, 'Pat': 2114, 'People': 2115, 'Petersburg': 2116, 'PewDiePie': 2117, 'Philbin': 2118, 'Philippine': 2119, 'Phillip': 2120, 'Phoenix': 2121, 'Placement': 2122, 'Plan': 2123, 'Porter': 2124, 'Potsdam': 2125, 'Potter': 2126, 'Prasad': 2127, 'Pratt': 2128, 'Prayer': 2129, 'Protocol': 2130, 'Qatar': 2131, 'Quebec': 2132, 'Queensland': 2133, 'RAF': 2134, 'Rabbit': 2135, 'Rachael': 2136, 'Rajagopalachari': 2137, 'Raleigh': 2138, 'Rampone': 2139, 'Rana': 2140, 'Randolph': 2141, 'Rashida': 2142, 'Rear': 2143, 'Redmond': 2144, 'Remy': 2145, 'Rene': 2146, 'Rich': 2147, 'Rita': 2148, 'Robby': 2149, 'Robin': 2150, 'Robyn': 2151, 'Roman': 2152, 'Rome': 2153, 'Romeo': 2154, 'Ronan': 2155, 'Ronnie': 2156, 'Rotterdam': 2157, 'Roy': 2158, 'Ruth': 2159, 'S.': 2160, 'Sabella': 2161, 'Saddam': 2162, 'Salem': 2163, 'Samantha': 2164, 'Sanderson': 2165, 'Sands': 2166, 'Santos': 2167, 'Sardar': 2168, 'Saskatchewan': 2169, 'Saudi': 2170, 'Scarlett': 2171, 'Schon': 2172, 'Scottish': 2173, 'Script': 2174, 'Senators': 2175, 'Sex': 2176, 'Sharon': 2177, 'Shep': 2178, 'Shortbread': 2179, 'Skinner': 2180, 'Sloan': 2181, 'Smurf': 2182, 'Sony': 2183, 'Sri': 2184, 'Stan': 2185, 'Stella': 2186, 'Sterling': 2187, 'Stoller': 2188, 'Stone': 2189, 'Stonie': 2190, 'Storm': 2191, 'Sudo': 2192, 'Supreme': 2193, 'Sweden': 2194, 'Switzerland': 2195, 'Syracuse': 2196, 'Syria': 2197, \"T'Chaka\": 2198, 'TBD': 2199, 'Tecna': 2200, 'Teresa': 2201, 'Territory': 2202, 'Thanks': 2203, 'Thorogood': 2204, 'Thursday': 2205, 'Tilburg': 2206, 'Tisdale': 2207, 'Tokyo': 2208, 'Tozier': 2209, 'Tribe': 2210, 'US$': 2211, 'USA': 2212, 'USC': 2213, 'Ukrainian': 2214, 'Unexpected': 2215, 'Val': 2216, 'VanderWaal': 2217, 'Verne': 2218, 'Versailles': 2219, 'Wade': 2220, 'Wagner': 2221, 'Wakrah': 2222, 'Walt': 2223, 'Warburton': 2224, 'Warriors': 2225, 'Watson': 2226, 'Welch': 2227, 'Welles': 2228, 'Wellington': 2229, 'Westbrook': 2230, 'Wheel': 2231, 'Whispers': 2232, 'Wilder': 2233, 'Wilhelm': 2234, 'XCX': 2235, 'Yamamoto': 2236, 'Yellowstone': 2237, 'Yoon': 2238, 'Yuengling': 2239, 'Zeta': 2240, 'advice': 2241, 'agency': 2242, 'all': 2243, 'ball': 2244, 'belt': 2245, 'black': 2246, 'blue': 2247, 'boundary': 2248, 'bowling': 2249, 'brush': 2250, 'cancer': 2251, 'choir': 2252, 'class': 2253, 'clear': 2254, 'clock': 2255, 'comes': 2256, 'company': 2257, 'consent': 2258, 'cultural': 2259, 'da': 2260, 'days': 2261, 'digits': 2262, 'dos': 2263, 'driver': 2264, 'each': 2265, 'earth': 2266, 'edition': 2267, 'ends': 2268, 'energy': 2269, 'epithelium': 2270, 'era': 2271, 'fantasy': 2272, 'feet': 2273, 'flag': 2274, 'fluoride': 2275, 'food': 2276, 'force': 2277, 'full': 2278, 'gospel': 2279, 'grams': 2280, 'groundwater': 2281, 'has': 2282, 'hip': 2283, 'hyun': 2284, 'immunity': 2285, 'inches': 2286, 'including': 2287, 'kilometres': 2288, 'lb': 2289, 'limited': 2290, 'living': 2291, 'long': 2292, 'made': 2293, 'meat': 2294, 'mid-1990s': 2295, 'miles': 2296, 'mosque': 2297, 'mouth': 2298, 'much': 2299, 'naval': 2300, 'optic': 2301, 'overs': 2302, 'parts': 2303, 'patient': 2304, 'peptide': 2305, 'person': 2306, 'present': 2307, 'presidential': 2308, 'prohibited': 2309, 'property': 2310, 'reactions': 2311, 'region': 2312, 'required': 2313, 'restaurant': 2314, 'scenes': 2315, 'sciences': 2316, 'secondary': 2317, 'seconds': 2318, 'sex': 2319, 'since': 2320, 'single': 2321, 'sixty': 2322, 'size': 2323, 'social': 2324, 'strikeouts': 2325, 'students': 2326, 'studio': 2327, 'sung': 2328, 'support': 2329, 'surrounding': 2330, 'syllables': 2331, 'tail': 2332, 'tea': 2333, 'through': 2334, 'titles': 2335, 'trade': 2336, 'transport': 2337, 'tropics': 2338, 'uncertain': 2339, 'union': 2340, 'vote': 2341, 'war': 2342, 'west': 2343, 'when': 2344, 'will.i.am': 2345, 'women': 2346, 'working': 2347, '£': 2348, \"'S\": 2349, '...': 2350, '0.64': 2351, '03': 2352, '04': 2353, '05:50': 2354, '06:00': 2355, '08': 2356, '1,381': 2357, '1,400': 2358, '1.2': 2359, '10.3.3': 2360, '10.7': 2361, '104.5': 2362, '111,701': 2363, '114': 2364, '11:26': 2365, '12,000': 2366, '12,714': 2367, '12,756.3': 2368, '12th': 2369, '1309': 2370, '134.3': 2371, '137': 2372, '1378': 2373, '1393': 2374, '1394': 2375, '1395': 2376, '14,2019': 2377, '140,000': 2378, '1414': 2379, '1492': 2380, '14′58″E': 2381, '15,2018': 2382, '1500': 2383, '1500s': 2384, '1513': 2385, '156': 2386, '158.2': 2387, '1602': 2388, '1612': 2389, '1628': 2390, '166': 2391, '1689': 2392, '17.0': 2393, '173': 2394, '1746': 2395, '1757': 2396, '18.0': 2397, '1802': 2398, '1826': 2399, '1850': 2400, '1858': 2401, '188': 2402, '1896': 2403, '1897': 2404, '18:57': 2405, '1906': 2406, '1913': 2407, '1922': 2408, '193': 2409, '1937–40': 2410, '19:18': 2411, '19:34': 2412, '2,000': 2413, '2,2019': 2414, '2.31': 2415, '2.5': 2416, '20,000': 2417, '200,000': 2418, '2022': 2419, '2028': 2420, '2040': 2421, '207,778': 2422, '210,565': 2423, '219,773': 2424, '22,000': 2425, '220': 2426, '231,636': 2427, '252': 2428, '27,310': 2429, '27,387': 2430, '27.5': 2431, '270': 2432, '28.1': 2433, '29,330': 2434, '293': 2435, '3.465-billion': 2436, '3.77': 2437, '306': 2438, '31:15': 2439, '31′17″N': 2440, '320': 2441, '340': 2442, '350,000': 2443, '37': 2444, '3:43.13': 2445, '3:44.39': 2446, '3:46.32': 2447, '4.26': 2448, '4.5': 2449, '4.54': 2450, '40-hour': 2451, '400,000': 2452, '417': 2453, '445,000': 2454, '445.6': 2455, '450,000': 2456, '4:12.56': 2457, '4:15.61': 2458, '4:16.71': 2459, '4–13': 2460, '5,000': 2461, '50,000': 2462, '52': 2463, '54': 2464, '54,154': 2465, '55': 2466, '550': 2467, '568.26': 2468, '57.2': 2469, '570': 2470, '587': 2471, '5:14': 2472, '5th': 2473, '6,200': 2474, '609–598': 2475, '66': 2476, '68': 2477, '6th': 2478, '7.1.2': 2479, '7.3': 2480, '7.40': 2481, '7.50': 2482, '7.60': 2483, '704': 2484, '77.9': 2485, '800,000': 2486, '833': 2487, '85': 2488, '867': 2489, '89': 2490, '9,600': 2491, '9.3.5': 2492, '9.5': 2493, '90,368': 2494, '93.3': 2495, '94th': 2496, '95': 2497, '96': 2498, '9:00': 2499, 'AV': 2500, 'AVRI': 2501, 'Abhishek': 2502, 'Abigail': 2503, 'Abrams': 2504, 'Achaemenid': 2505, 'Acorn': 2506, 'Adair': 2507, 'Adele': 2508, 'Adepo': 2509, 'Adherbal': 2510, 'Admission': 2511, 'Afghanistan': 2512, 'Agnes': 2513, 'Aguinaldo': 2514, 'Ahamefule': 2515, 'Ainsworth': 2516, 'Airplanes': 2517, 'Airways': 2518, 'Ajit': 2519, 'Akinwande': 2520, 'Akira': 2521, 'Albana': 2522, 'Albany': 2523, 'Alisha': 2524, 'Allan': 2525, 'Allies': 2526, 'Allister': 2527, 'Ambedkar': 2528, 'Ambulatory': 2529, 'Amisha': 2530, 'Amityville': 2531, 'Amleto': 2532, 'Anastacia': 2533, 'Anastasia': 2534, 'Andersson': 2535, 'Andmesh': 2536, 'Anil': 2537, 'Annika': 2538, 'Anybody': 2539, 'Appalachian': 2540, 'Apple': 2541, 'Apurímac': 2542, 'Arab': 2543, 'Arabs': 2544, 'Archibald': 2545, 'Aretha': 2546, 'Ariel': 2547, 'Aristotle': 2548, 'Aroma': 2549, 'Arundell': 2550, 'Arvin': 2551, 'Arya': 2552, 'Aryan': 2553, 'Asa': 2554, 'Ash': 2555, 'Ashish': 2556, 'Asians': 2557, 'Assistant': 2558, 'Associates': 2559, 'Astasio': 2560, 'Athena': 2561, 'Attack': 2562, 'Auburn': 2563, 'Augustana': 2564, 'Authentication': 2565, 'Authors': 2566, 'Avengers': 2567, 'Awakens': 2568, 'Axton': 2569, 'Ayola': 2570, 'Azalea': 2571, 'BC': 2572, 'Babatunde': 2573, 'Babette': 2574, 'Bacall': 2575, 'Bacon': 2576, 'Badelt': 2577, 'Baffert': 2578, 'Bahadur': 2579, 'Bakay': 2580, 'Bako': 2581, 'Bananas': 2582, 'Bandura': 2583, 'Bankers': 2584, 'Banking': 2585, 'Banner': 2586, 'Barbeau': 2587, 'Barbier': 2588, 'Barbossa': 2589, 'Barcelona': 2590, 'Barker': 2591, 'Baron': 2592, 'Bartel': 2593, 'Bartholomew': 2594, 'Baseball': 2595, 'Based': 2596, 'Basic': 2597, 'Basil': 2598, 'Bassi': 2599, 'Bassin': 2600, 'Batman': 2601, 'Batoide': 2602, 'Battleworld': 2603, 'Baume': 2604, 'Bayard': 2605, 'Beachcombers': 2606, 'Beat': 2607, 'Beaumont': 2608, 'Beck': 2609, 'Behura': 2610, 'Beitia': 2611, 'Belfast': 2612, 'Belgian': 2613, 'Belgium': 2614, 'Belief': 2615, 'Bella': 2616, 'Bellard': 2617, 'Beni': 2618, 'Benn': 2619, 'Benny': 2620, 'Benson': 2621, 'Benteke': 2622, 'Bentinck': 2623, 'Berkshire': 2624, 'Bernadette': 2625, 'Bernanke': 2626, 'Berruti': 2627, 'Berry': 2628, 'Bertrup': 2629, 'Bessemer': 2630, 'Beta': 2631, 'Bethanie': 2632, 'Bette': 2633, 'Better': 2634, 'Beulah': 2635, 'Bhim': 2636, 'Bhonsle': 2637, 'Bhuvneshwar': 2638, 'Bieber': 2639, 'Bijender': 2640, 'Biles': 2641, 'Bing': 2642, 'Binghamton': 2643, 'Birdman': 2644, 'Birender': 2645, 'Birtwhistle': 2646, 'Bjørgen': 2647, 'Blackbeard': 2648, 'Blackhawks': 2649, 'Blethyn': 2650, 'Bliss': 2651, 'Blount': 2652, 'Boardwalk': 2653, 'BodyRockers': 2654, 'Boeotarch': 2655, 'Boi': 2656, 'Boland': 2657, 'Bolshevik': 2658, 'Bolt': 2659, 'Bom': 2660, 'Bon': 2661, 'Bondi': 2662, 'Booke': 2663, 'Booker': 2664, 'Borough': 2665, 'Bortolotti': 2666, 'Bosch': 2667, 'Bose': 2668, 'Bouche': 2669, 'Bougainville': 2670, 'Bounty': 2671, 'Bourne': 2672, 'Boyd': 2673, 'Brackett': 2674, 'Bragg': 2675, 'Brand': 2676, 'Bratton': 2677, 'Brent': 2678, 'Bridget': 2679, 'Brighton': 2680, 'Broadsands': 2681, 'Brody': 2682, 'Brolin': 2683, 'Brooklyn': 2684, 'Brookman': 2685, 'Bros.': 2686, 'Browns': 2687, 'Bruno': 2688, 'Bryn': 2689, 'Buckingham': 2690, 'Bucks': 2691, 'Bucky': 2692, 'Bukhari': 2693, 'Bull': 2694, 'Bunche': 2695, 'Burgess': 2696, 'Burnaby': 2697, 'Burnley': 2698, 'Burns': 2699, 'Burton': 2700, 'Buxton': 2701, 'Bythias': 2702, 'C.D.': 2703, 'CBE': 2704, 'CON': 2705, 'Caine': 2706, 'Calabria': 2707, 'Calderón': 2708, 'Cale': 2709, 'Calene': 2710, 'Calhern': 2711, 'Callista': 2712, 'Candice': 2713, 'Cape': 2714, 'Cara': 2715, 'Cardiff': 2716, 'Care': 2717, 'Cargreen': 2718, 'Carli': 2719, 'Carmen': 2720, 'Carnes': 2721, 'Carney': 2722, 'Carolan': 2723, 'Cary': 2724, 'Casino': 2725, 'Cassie': 2726, 'Castree': 2727, 'Catechism': 2728, 'Cathleen': 2729, 'Celestial': 2730, 'Cena': 2731, 'Cencio': 2732, 'Centre': 2733, 'Chabert': 2734, 'Chait': 2735, 'Chamberlin': 2736, 'Chancourtois': 2737, 'Channing': 2738, 'Charity': 2739, 'Chaudhary': 2740, 'Checkmate': 2741, 'Cheese': 2742, 'Cheng': 2743, 'Chesapeake': 2744, 'Chest': 2745, 'Chickens': 2746, 'Chicora': 2747, 'Chiefs': 2748, 'Chimuanya': 2749, 'Chip': 2750, 'Choi': 2751, 'Chondrichthyes': 2752, 'Chong': 2753, 'Chopra': 2754, 'Chori': 2755, 'Chow': 2756, 'Christianity': 2757, 'Christians': 2758, 'Christoph': 2759, 'Chrowder': 2760, 'Ciara': 2761, 'Cimorelli': 2762, 'Circle': 2763, 'Circumvallate': 2764, 'Civil': 2765, 'Civilization': 2766, 'Clack': 2767, 'Claude': 2768, 'Claudio': 2769, 'Clay': 2770, 'Clergy': 2771, 'Clift': 2772, 'Clinton': 2773, 'Clive': 2774, 'Clones': 2775, 'Clotworthy': 2776, 'Clown': 2777, 'Clydie': 2778, 'Co.': 2779, 'Coach': 2780, 'Coast': 2781, 'Coco': 2782, 'Cocos': 2783, 'Coldplay': 2784, 'Colleen': 2785, 'Come': 2786, 'Commerce': 2787, 'Commercial': 2788, 'Common': 2789, 'Conforti': 2790, 'Connery': 2791, 'Connor': 2792, 'Conrad': 2793, 'Constituent': 2794, 'Conte': 2795, 'Conway': 2796, 'Cordereau': 2797, 'Cori': 2798, 'Corlett': 2799, 'Corliss': 2800, 'Cornelia': 2801, 'Cornelius': 2802, 'Correa': 2803, 'Coulson': 2804, 'Courtney': 2805, 'Cox': 2806, 'Coyne': 2807, 'Cravalho': 2808, 'Crazy': 2809, 'Cream': 2810, 'Creighton': 2811, 'Crennel': 2812, 'Cricket': 2813, 'Cris': 2814, 'Crooked': 2815, 'Crosbys': 2816, 'Crossfire': 2817, 'Crouch': 2818, 'Culver': 2819, 'Curse': 2820, 'Curtiss': 2821, 'Custom': 2822, 'Customs': 2823, 'Cy': 2824, 'Czechia': 2825, 'DVD': 2826, 'Daddy': 2827, 'Daeg': 2828, 'Dakota': 2829, 'Daleman': 2830, 'Danielle': 2831, 'Darbyshire': 2832, 'Darci': 2833, 'Dardanelle': 2834, 'Dari': 2835, 'Darien': 2836, 'Daron': 2837, 'Darren': 2838, 'Dartington': 2839, 'Davide': 2840, 'Davidtz': 2841, 'Daylights': 2842, 'Daymond': 2843, 'Dede': 2844, 'Dee': 2845, 'Del': 2846, 'Delano': 2847, 'Delhi': 2848, 'Democritus': 2849, 'Denmark': 2850, 'Department': 2851, 'Depot': 2852, 'Destiny': 2853, 'Devil': 2854, 'Devin': 2855, 'Devisingh': 2856, 'Devon': 2857, 'Diaz': 2858, 'Didier': 2859, 'Diesterweg': 2860, 'Dikembe': 2861, 'Dillane': 2862, 'Dingle': 2863, 'Dino': 2864, 'Dinosaur': 2865, 'Diogenes': 2866, 'Discrimination': 2867, 'Divas': 2868, 'Diệm': 2869, 'Dmitri': 2870, 'Dodig': 2871, 'Dollard': 2872, 'Dolphy': 2873, 'Don': 2874, 'Donahue': 2875, 'Donnie': 2876, 'Donovan': 2877, 'Dorsey': 2878, 'Dotel': 2879, 'Dowd': 2880, 'Downing': 2881, 'Downtown': 2882, 'Draw': 2883, 'Drizella': 2884, 'Drogba': 2885, 'Dua': 2886, 'Dudamel': 2887, 'Duly': 2888, 'Dumerkhan': 2889, 'Dumezweni': 2890, 'Dunaway': 2891, 'Dunlap': 2892, 'Dye': 2893, 'Dynamo': 2894, 'E': 2895, 'ET': 2896, 'Eaton': 2897, 'Eazy': 2898, 'Ebina': 2899, 'Ecma': 2900, 'Edge': 2901, 'Edinburgh': 2902, 'Edith': 2903, 'Edy': 2904, 'Ehrenfried': 2905, 'Eiko': 2906, 'Einhorn': 2907, 'Elasmobranchii': 2908, 'Elba': 2909, 'Elder': 2910, 'Eli': 2911, 'Elimination': 2912, 'Elizaveta': 2913, 'Elles': 2914, 'Ellison': 2915, 'Elrod': 2916, 'Ely': 2917, 'Emanuel': 2918, 'Embeth': 2919, 'Emerald': 2920, 'Emerging': 2921, 'Engels': 2922, 'Engineering': 2923, 'Englad': 2924, 'Enrique': 2925, 'Enterprise': 2926, 'Entner': 2927, 'Epistle': 2928, 'Eritrea': 2929, 'Erland': 2930, 'Erlanger': 2931, 'Errico': 2932, 'Erskine': 2933, 'Essam': 2934, 'Estuary': 2935, 'Eugene': 2936, 'Europeans': 2937, 'Evangelist': 2938, 'Everton': 2939, 'Exercise': 2940, 'Exosphere': 2941, 'FDR': 2942, 'FTC': 2943, 'Fabrice': 2944, 'Faerch': 2945, 'Fagerbakke': 2946, 'Falls': 2947, 'Famy': 2948, 'Faraday': 2949, 'Farooq': 2950, 'Fata': 2951, 'Fear': 2952, 'Fedele': 2953, 'Feige': 2954, 'Feliciano': 2955, 'Felipe': 2956, 'Fennerman': 2957, 'Ferdinand': 2958, 'Ferlin': 2959, 'Fernandez': 2960, 'Ferrigno': 2961, 'Fiennes': 2962, 'Fifi': 2963, 'Fifth': 2964, 'Filfiley': 2965, 'Film': 2966, 'Finals': 2967, 'Finegold': 2968, 'Fizz': 2969, 'Flackett': 2970, 'Flatt': 2971, 'Flores': 2972, 'Flower': 2973, 'Floyd': 2974, 'Foliate': 2975, 'Follows': 2976, 'Foray': 2977, 'Forbush': 2978, 'Forever': 2979, 'Fork': 2980, 'Forke': 2981, 'Forms': 2982, 'Forsythe': 2983, 'Franz': 2984, 'Fray': 2985, 'Freddy': 2986, 'Freitas': 2987, 'Friend': 2988, 'Frobe': 2989, 'Front': 2990, 'Fulmer': 2991, 'Fulton': 2992, 'Fungiform': 2993, 'GW': 2994, 'Gabriela': 2995, 'Gadot': 2996, 'Gagnon': 2997, 'Gal': 2998, 'Galatians': 2999, 'Galena': 3000, 'Galjaard': 3001, 'Galway': 3002, 'Gambon': 3003, 'Gandhi': 3004, 'Gang': 3005, 'Ganus': 3006, 'Garber': 3007, 'Garcia': 3008, 'Garland': 3009, 'Garonne': 3010, 'Gastown': 3011, 'Gates': 3012, 'Genny': 3013, 'Geoffrey': 3014, 'Georg': 3015, 'Gerry': 3016, 'Gert': 3017, 'Geto': 3018, 'Giancarlo': 3019, 'Gianfranco': 3020, 'Gibsons': 3021, 'Gilmore': 3022, 'Gingrich': 3023, 'Gisco': 3024, 'Giulio': 3025, 'Globe': 3026, 'Gloucestershire': 3027, 'Glover': 3028, 'Golden': 3029, 'Goldie': 3030, 'Goldwyn': 3031, 'Goodluck': 3032, 'Government': 3033, 'Gramm': 3034, 'Grammer': 3035, 'Grass': 3036, 'Grattan': 3037, 'Grave': 3038, 'Grayling': 3039, 'Greaves': 3040, 'Greening': 3041, 'Greenspan': 3042, 'Greitens': 3043, 'Gribben': 3044, 'Griffith': 3045, 'Grill': 3046, 'Group': 3047, 'Grove': 3048, 'Gruber': 3049, 'Guard': 3050, 'Gubby': 3051, 'Guedj': 3052, 'Gunther': 3053, 'Gustavo': 3054, 'Gwenn': 3055, 'Gymnodinium': 3056, 'Hadary': 3057, 'Hagel': 3058, 'Hakan': 3059, 'Halloween': 3060, 'Hamilcar': 3061, 'Hamm': 3062, 'Hammer': 3063, 'Handwritten': 3064, 'Handy': 3065, 'Hannaford': 3066, 'Hanseatic': 3067, 'Hanyu': 3068, 'Haraguchi': 3069, 'Hardin': 3070, 'Harlin': 3071, 'Hartwell': 3072, 'Hassan': 3073, 'Hathiramani': 3074, 'Hawn': 3075, 'Hayley': 3076, 'Haysbert': 3077, 'Healey': 3078, 'Heart': 3079, 'Heavies': 3080, 'Hefford': 3081, 'Helensvale': 3082, 'Helpmann': 3083, 'Hemisphere': 3084, 'Hendrick': 3085, 'Hendrikus': 3086, 'Hensley': 3087, 'Herbie': 3088, 'Hercules': 3089, 'Herrmann': 3090, 'Highwaymen': 3091, 'Hilliard': 3092, 'Hillis': 3093, 'Hills': 3094, 'Hilsdon': 3095, 'Hilton': 3096, 'Him': 3097, 'Himanshu': 3098, 'Himilco': 3099, 'Hinojosa': 3100, 'History': 3101, 'Hit': 3102, 'Hobbes': 3103, 'Hobson': 3104, 'Hoesecker': 3105, 'Holmgren': 3106, 'Homan': 3107, 'Honeymoon': 3108, 'Hoo': 3109, 'Hoon': 3110, 'Horton': 3111, 'Huang': 3112, 'Hubert': 3113, 'Huddie': 3114, 'Huey': 3115, 'Hugh': 3116, 'Humeroradial': 3117, 'Humeroulnar': 3118, 'Husky': 3119, 'Hutchison': 3120, 'I-64': 3121, 'ICC': 3122, 'IPL': 3123, 'Iceland': 3124, 'Iden': 3125, 'Ifans': 3126, 'Iggy': 3127, 'Ignorance': 3128, 'Iim': 3129, 'Ikpeazu': 3130, 'Imperial': 3131, 'In': 3132, 'Ina': 3133, 'Inconclusive': 3134, 'Ingo': 3135, 'Ingredient': 3136, 'Insane': 3137, 'Interstate': 3138, 'Intolerance': 3139, 'Irons': 3140, 'Irvine': 3141, 'Irwin': 3142, 'Isaiah': 3143, 'Ishan': 3144, 'Israelite': 3145, 'It': 3146, 'Izabela': 3147, 'J.W.': 3148, 'JD': 3149, 'JZ': 3150, 'Jace': 3151, 'Jag': 3152, 'Jammu': 3153, 'Jandamarra': 3154, 'Janet': 3155, 'Jang': 3156, 'Jared': 3157, 'Jasmine': 3158, 'Javine': 3159, 'Jayna': 3160, 'Jayne': 3161, 'Jeffries': 3162, 'Jelena': 3163, 'Jenkins': 3164, 'Jessie': 3165, 'Jesús': 3166, 'Jet': 3167, 'Jett': 3168, 'Jews': 3169, 'Jiles': 3170, 'Joanne': 3171, 'Joaquin': 3172, 'Joaquín': 3173, 'Joemat': 3174, 'Joey': 3175, 'Johannes': 3176, 'Jonas': 3177, 'Jongh': 3178, 'Joy': 3179, 'Juan': 3180, 'Judd': 3181, 'Juliet': 3182, 'Justify': 3183, 'Justine': 3184, 'K.': 3185, 'KSA': 3186, 'Kaetlyn': 3187, 'Kalibangan': 3188, \"Kamakawiwo'ole\": 3189, 'Kamaleng': 3190, 'Kamchatka': 3191, 'Karenia': 3192, 'Kashmir': 3193, 'Kasznar': 3194, 'Kathy': 3195, 'Kaun': 3196, 'Kavanaugh': 3197, 'Keaton': 3198, 'Keenan': 3199, 'Kei': 3200, 'Kelsey': 3201, 'Kendrick': 3202, 'Keokuk': 3203, 'Keppel': 3204, 'Kept': 3205, 'Kerr': 3206, 'Kerry': 3207, 'Kgalema': 3208, 'Khadirbet': 3209, 'Kida': 3210, 'Kiiza': 3211, 'Killa': 3212, 'Kilpatrick': 3213, 'Kimmy': 3214, 'Kinnear': 3215, 'Kishan': 3216, 'Klaus': 3217, 'Klaw': 3218, 'Klay': 3219, 'Klerk': 3220, 'Klugman': 3221, 'Knecht': 3222, 'Knochenhauer': 3223, 'Kocheril': 3224, 'Kondo': 3225, 'Kottonmouth': 3226, 'Koyama': 3227, 'Kravitz': 3228, 'Kreviazuk': 3229, 'Krieger': 3230, 'Krishnamachari': 3231, 'Krista': 3232, 'Kristaps': 3233, 'Kristie': 3234, 'Kristina': 3235, 'Kristofferson': 3236, 'Kristyn': 3237, 'Kroll': 3238, 'Krzyzewski': 3239, 'Kuan': 3240, 'Kuki': 3241, 'Kunia': 3242, 'Kurtis': 3243, 'Kyla': 3244, 'Kyle': 3245, 'Kyong': 3246, 'L.A.': 3247, 'LaMarche': 3248, 'Lachlan': 3249, 'Lagoon': 3250, 'Lakk': 3251, 'Lamar': 3252, 'Lambert': 3253, 'Lambs': 3254, 'Lampard': 3255, 'Landsteiner': 3256, 'Langridge': 3257, 'Lannister': 3258, 'Larkana': 3259, 'Larossi': 3260, 'Latia': 3261, 'Lavoie': 3262, 'Lawn': 3263, 'Lawrance': 3264, 'Layla': 3265, 'Lazenby': 3266, 'LeGros': 3267, 'Ledbetter': 3268, 'Leningrad': 3269, 'Lent': 3270, 'Leon': 3271, 'Let': 3272, 'Levine': 3273, 'León': 3274, 'Lightfoot': 3275, 'Lighthaven': 3276, 'Lighting': 3277, 'Like': 3278, 'Lil': 3279, 'Lin': 3280, 'Lindley': 3281, 'Lindsey': 3282, 'Linster': 3283, 'Lions': 3284, 'Lipton': 3285, 'Liu': 3286, 'Living': 3287, 'Lobster': 3288, 'Locke': 3289, 'Lockhart': 3290, 'Lockington': 3291, 'Lockwood': 3292, 'Londonbeat': 3293, 'Loretta': 3294, 'Losing': 3295, 'Louvin': 3296, 'Love': 3297, 'Luca': 3298, 'Lucie': 3299, 'Lucy': 3300, 'Ludi': 3301, 'Lufthansa': 3302, 'Lukis': 3303, 'Lulama': 3304, 'Lulu': 3305, 'Luxembourg': 3306, 'Luzon': 3307, 'Luzonians': 3308, 'Lyceum': 3309, 'M.A.C.C.': 3310, 'MAR': 3311, 'MC': 3312, 'MLB': 3313, 'MacGregor': 3314, 'MacTavish': 3315, 'Macedon': 3316, 'Maciel': 3317, 'Mackenzie': 3318, 'Macpherson': 3319, 'Madagascar': 3320, 'Maddie': 3321, 'Madison': 3322, 'Maetens': 3323, 'Mafabi': 3324, 'Mager': 3325, 'Maggie': 3326, 'Magik': 3327, 'Mago': 3328, 'Maisy': 3329, 'Malakian': 3330, 'Malina': 3331, 'Mallard': 3332, 'Malory': 3333, 'Mamas': 3334, 'Mammy': 3335, 'Mandarin': 3336, 'Mandela': 3337, 'Mane': 3338, 'Manhunter': 3339, 'Manpreet': 3340, 'Mantaro': 3341, 'Mao': 3342, 'Marcelo': 3343, 'Margarete': 3344, 'Margaretha': 3345, 'Marian': 3346, 'Marianas': 3347, 'Marit': 3348, 'Marius': 3349, 'Marks': 3350, 'Marlo': 3351, 'Mars': 3352, 'Martelli': 3353, 'Martina': 3354, 'Marty': 3355, 'Marytheresa': 3356, 'Mascolo': 3357, 'Master': 3358, 'Material': 3359, 'Mattea': 3360, 'Mattek': 3361, 'Mattis': 3362, 'Matula': 3363, 'Maudsland': 3364, 'Maya': 3365, 'Mayer': 3366, 'Maza': 3367, 'Mbatha': 3368, 'Mbeki': 3369, 'McAuley': 3370, 'McCann': 3371, 'McClurkin': 3372, 'McCrary': 3373, 'McCreary': 3374, 'McCririck': 3375, 'McDavid': 3376, 'McElhone': 3377, 'McGill': 3378, 'McIntosh': 3379, 'McKennon': 3380, 'McLoughlin': 3381, 'McMahon': 3382, 'McMaster': 3383, 'McMillan': 3384, 'Mckenzie': 3385, 'Meadows': 3386, 'Media': 3387, 'Medieval': 3388, 'Megan': 3389, 'Mehta': 3390, 'Mei': 3391, 'Mel': 3392, 'Melo': 3393, 'Melvin': 3394, 'Mendeleev': 3395, 'Mera': 3396, 'Mercy': 3397, 'Mesosphere': 3398, 'Methuselah': 3399, 'Metro': 3400, 'Mic': 3401, 'Michèle': 3402, 'Midler': 3403, 'Miko': 3404, 'Mila': 3405, 'Mildred': 3406, 'Miletus': 3407, 'Minor': 3408, 'Minskoff': 3409, 'Minutes': 3410, 'Miracles': 3411, 'Miraildes': 3412, 'Miskew': 3413, 'Mismi': 3414, 'Missy': 3415, 'Miyahara': 3416, 'Mladenovic': 3417, 'Mobster': 3418, 'Mol': 3419, 'Monitor': 3420, 'Montauk': 3421, 'Montegut': 3422, 'Montford': 3423, 'Moonlight': 3424, 'Morell': 3425, 'Morgana': 3426, 'Morley': 3427, 'Mormon': 3428, 'Moscow': 3429, 'Most': 3430, 'Mota': 3431, 'Motlanthe': 3432, 'Moulton': 3433, 'Mourinho': 3434, 'Movement': 3435, 'Much': 3436, 'Muirhead': 3437, 'Muldoon': 3438, 'Mullan': 3439, 'Munich': 3440, 'Muses': 3441, 'Music': 3442, 'Mutombo': 3443, 'Mvuyelwa': 3444, 'Myron': 3445, 'Nachdi': 3446, 'Nadav': 3447, 'Naek': 3448, 'Nagaraj': 3449, 'Naked': 3450, 'Nandala': 3451, 'Nanjing': 3452, 'Nantz': 3453, 'Napes': 3454, 'Napoleon': 3455, 'Narayanan': 3456, 'Narendra': 3457, 'Narine': 3458, 'Nashville': 3459, 'Nasim': 3460, 'Natascha': 3461, 'Natorp': 3462, 'Natwick': 3463, 'Naval': 3464, 'Navratilova': 3465, 'Nayudu': 3466, 'Nazca': 3467, 'Necessary': 3468, 'Neeskens': 3469, 'Nehra': 3470, 'Nehwal': 3471, 'Neosho': 3472, 'Nevado': 3473, 'Newcastle': 3474, 'Newfoundland': 3475, 'Newham': 3476, 'News': 3477, 'Nguyễn': 3478, 'Ngô': 3479, 'Nicole': 3480, 'Nicolette': 3481, 'Nieto': 3482, 'Nights': 3483, 'Niles': 3484, 'Nina': 3485, 'Nissan': 3486, 'Nitty': 3487, 'Nixon': 3488, 'Niʻihau': 3489, 'Noam': 3490, 'Noel': 3491, 'Noelle': 3492, 'Noma': 3493, 'Norfolk': 3494, 'Norwich': 3495, 'Notre': 3496, 'Novelli': 3497, \"O'Brien\": 3498, \"O'Flanagan\": 3499, \"O'Jays\": 3500, \"O'Kaysions\": 3501, \"O'Mara\": 3502, \"O'Shaughnessy\": 3503, 'OBE': 3504, 'OH': 3505, 'OU': 3506, 'Oblong': 3507, 'Octavio': 3508, 'Odadjian': 3509, 'Odom': 3510, 'Oguttu': 3511, 'Okezie': 3512, 'Oldway': 3513, 'Oluwole': 3514, 'Olympic': 3515, 'Olympics': 3516, 'Omaha': 3517, 'Ono': 3518, 'Oppenheimer': 3519, 'Orchestra': 3520, 'Ormsby': 3521, 'Osborne': 3522, 'Oscar': 3523, 'Osta': 3524, 'Ostapenko': 3525, 'Otranto': 3526, 'Ottawa': 3527, 'Ouellette': 3528, 'Out': 3529, 'Oz': 3530, 'Ozone': 3531, 'P.R.': 3532, 'PR': 3533, 'PRC': 3534, 'PVSM': 3535, 'Pace': 3536, 'Paignton': 3537, 'Palmer': 3538, 'Palmetto': 3539, 'Pam': 3540, 'Panzer': 3541, 'Papas': 3542, 'Pappas': 3543, 'Parliament': 3544, 'Partida': 3545, 'Partners': 3546, 'Parton': 3547, 'Pascoe': 3548, 'Pastoral': 3549, 'Pathan': 3550, 'Patricia': 3551, 'Pattie': 3552, 'Pattison': 3553, 'Paulo': 3554, 'Pebworth': 3555, 'Peg': 3556, 'Pei': 3557, 'Pekinese': 3558, 'Pele': 3559, 'Pemulwuy': 3560, 'Penn': 3561, 'Pentatonix': 3562, 'Peoria': 3563, 'Period': 3564, 'Perlman': 3565, 'Persia': 3566, 'Peterburg': 3567, 'Peterman': 3568, 'Peterson': 3569, 'Petrus': 3570, 'Peyton': 3571, 'Peña': 3572, 'Phameas': 3573, 'Phelps': 3574, 'Phi': 3575, 'Phillips': 3576, 'Pickles': 3577, 'Picnic': 3578, 'Pictures': 3579, 'Piedmont': 3580, 'Pieter': 3581, 'Pinson': 3582, 'Pioresan': 3583, 'Piper': 3584, 'Pistons': 3585, 'Pitch': 3586, 'Plaza': 3587, 'Pleiß': 3588, 'Plewman': 3589, 'Plumette': 3590, 'Poem': 3591, 'Poetry': 3592, 'Pogorilaya': 3593, 'Pointed': 3594, 'Poiret': 3595, 'Ponce': 3596, 'Pontiac': 3597, 'Popplewell': 3598, 'Portman': 3599, 'Porziņģis': 3600, 'Posey': 3601, 'Posse': 3602, 'Potts': 3603, 'Pou': 3604, 'Povenmire': 3605, 'Power': 3606, 'Pozzo': 3607, 'Pradeep': 3608, 'Pradesh': 3609, 'Predators': 3610, 'Presidency': 3611, 'Preventing': 3612, 'Prima': 3613, 'Private': 3614, 'Priyanka': 3615, 'Productions': 3616, 'Profanities': 3617, 'Progressive': 3618, 'Promises': 3619, 'Proper': 3620, 'Proximal': 3621, 'Prudence': 3622, 'Prue': 3623, 'Pry': 3624, 'Prytz': 3625, 'Pseudo': 3626, 'Pusher': 3627, 'Pélissier': 3628, 'Q4': 3629, 'Quân': 3630, 'RFK': 3631, 'Rabbani': 3632, 'Rabe': 3633, 'Raby': 3634, 'Radionova': 3635, 'Radnor': 3636, 'Radson': 3637, 'Raf': 3638, 'Ragland': 3639, 'Rahne': 3640, 'Rainer': 3641, 'Rainn': 3642, 'Raitt': 3643, 'Rakie': 3644, 'Raman': 3645, 'Ramonda': 3646, 'Rampal': 3647, 'Ramsay': 3648, 'Ranchero': 3649, 'Rangaswamy': 3650, 'Ranieri': 3651, 'Raposo': 3652, 'Raw': 3653, 'Raza': 3654, 'Reagan': 3655, 'Realistic': 3656, 'Reason': 3657, 'Redden': 3658, 'Reddy': 3659, 'Redford': 3660, 'Reggio': 3661, 'Reginald': 3662, 'Regional': 3663, 'Reid': 3664, 'Reimers': 3665, 'Reinsdorf': 3666, 'Remar': 3667, 'Remember': 3668, 'Reports': 3669, 'Revenant': 3670, 'Revenge': 3671, 'Revolution': 3672, 'Rey': 3673, 'Rheostatics': 3674, 'Rhesus': 3675, 'Rhomboid': 3676, 'Rian': 3677, 'Rick': 3678, 'Rihanna': 3679, 'Rikiya': 3680, 'Ringham': 3681, 'Rinna': 3682, 'Rino': 3683, 'Ripka': 3684, 'Rises': 3685, 'Rising': 3686, 'Roach': 3687, 'Road': 3688, 'Roadshow': 3689, 'Rock': 3690, 'Rockey': 3691, 'Rocklin': 3692, 'Rodrigo': 3693, 'Rokes': 3694, 'Romdhane': 3695, 'Romo': 3696, 'Ronson': 3697, 'Rooker': 3698, 'Rooney': 3699, 'Roots': 3700, 'Rory': 3701, 'Rosie': 3702, 'Round': 3703, 'Rousseau': 3704, 'Rowell': 3705, 'Royales': 3706, 'Ruben': 3707, 'Rupert': 3708, 'Ruprecht': 3709, 'Rusev': 3710, 'Russa': 3711, 'Russian': 3712, 'Russo': 3713, 'Rusty': 3714, 'Ryo': 3715, 'Río': 3716, 'Rücker': 3717, 'SM': 3718, 'Sabara': 3719, 'Sabihy': 3720, 'Safronov': 3721, 'Sagar': 3722, 'Sahiwal': 3723, 'Sain': 3724, 'Saina': 3725, 'Salaza': 3726, 'Salerno': 3727, 'Sara': 3728, 'Sarstedt': 3729, 'Satin': 3730, 'Satish': 3731, 'Satoko': 3732, 'Saw': 3733, 'Saxon': 3734, 'Sayer': 3735, 'Schmidt': 3736, 'Schrager': 3737, 'Schrute': 3738, 'Scots': 3739, 'Scrimgeour': 3740, 'Scruggs': 3741, 'Seager': 3742, 'Secondary': 3743, 'Secretary': 3744, 'Seduction': 3745, 'See': 3746, 'Seeger': 3747, 'Seekers': 3748, 'Segal': 3749, 'Segel': 3750, 'Sehwag': 3751, 'Selden': 3752, 'Selfridge': 3753, 'Senator': 3754, 'Sentro': 3755, 'Senzeni': 3756, 'Sepulveda': 3757, 'Serj': 3758, 'Servizio': 3759, 'Seurat': 3760, 'Seventeenth': 3761, 'Severn': 3762, 'Seycove': 3763, 'Shai': 3764, 'Shane': 3765, 'Shannon': 3766, 'Shantha': 3767, 'Shape': 3768, 'Sharks': 3769, 'Shastri': 3770, 'Shattered': 3771, 'Shave': 3772, 'Shavo': 3773, 'She': 3774, 'Sheen': 3775, 'Sheila': 3776, 'Shepherd': 3777, 'Sher': 3778, 'Shetty': 3779, 'Shiavone': 3780, 'Shigeru': 3781, 'Shine': 3782, 'Shinji': 3783, 'Shirelles': 3784, 'Shiva': 3785, 'Shivangi': 3786, 'Shiwen': 3787, 'Shoes': 3788, 'Shoma': 3789, 'Shooter': 3790, 'Showcase': 3791, 'Shuri': 3792, 'Sigfridsson': 3793, 'Sigrid': 3794, 'Silence': 3795, 'Silphium': 3796, 'Silver': 3797, 'Simeon': 3798, 'Simple': 3799, 'Singer': 3800, 'Singers': 3801, 'Singleton': 3802, 'Sisto': 3803, 'Sith': 3804, 'Sixth': 3805, 'Skating': 3806, 'Skerritt': 3807, 'Slim': 3808, 'Smita': 3809, 'Smithsonian': 3810, 'Smokey': 3811, 'Snead': 3812, 'So': 3813, 'SoHo': 3814, 'Sokol': 3815, 'Soles': 3816, 'Solomon': 3817, 'Solti': 3818, 'Sonesta': 3819, 'Songs': 3820, 'Soni': 3821, 'Sooyoung': 3822, 'Sorcerer': 3823, 'Sorrell': 3824, 'Soul': 3825, 'Soviets': 3826, 'Soyinka': 3827, 'Spears': 3828, 'Spectre': 3829, 'Spieth': 3830, 'Spinners': 3831, 'Split': 3832, 'Spring': 3833, 'Spruance': 3834, 'Square': 3835, 'Squaw': 3836, 'Sreejesh': 3837, 'Srikkanth': 3838, 'Stacy': 3839, 'Standardization': 3840, 'Stanford': 3841, 'Stapleton': 3842, 'Stardust': 3843, 'Steadman': 3844, 'Steele': 3845, 'Stiller': 3846, 'Stormtrooper': 3847, 'Stratosphere': 3848, 'String': 3849, 'Submandibular': 3850, 'Succession': 3851, 'Summers': 3852, 'Sundarbans': 3853, 'Sunderland': 3854, 'Sunny': 3855, 'Suppan': 3856, 'Suárez': 3857, 'Sweatman': 3858, 'Sweety': 3859, 'Swords': 3860, 'Syphax': 3861, 'São': 3862, \"T'Challa\": 3863, 'TBS': 3864, 'TMS': 3865, 'Taiwan': 3866, 'Taj': 3867, 'Tamim': 3868, 'Tankian': 3869, 'Tanne': 3870, 'Tanya': 3871, 'Tara': 3872, 'Taranto': 3873, 'Tarzana': 3874, 'Tasker': 3875, 'Tau': 3876, 'Tavares': 3877, 'Tavárez': 3878, 'Ted': 3879, 'Tera': 3880, 'Texans': 3881, 'Thabo': 3882, 'Thales': 3883, 'Thau': 3884, 'Theodoric': 3885, 'Thermosphere': 3886, 'They': 3887, 'Thicknesse': 3888, 'Tiffin': 3889, 'Tigre': 3890, 'Tiki': 3891, 'Timi': 3892, 'Titus': 3893, 'Today': 3894, 'Tolkien': 3895, 'Tomar': 3896, 'Tommies': 3897, 'Toms': 3898, 'Tonga': 3899, 'Toni': 3900, 'Torquay': 3901, 'Torrens': 3902, 'Toulouse': 3903, 'Trade': 3904, 'Trench': 3905, 'Tresco': 3906, 'Tretiak': 3907, 'Trevithick': 3908, 'Troposhere': 3909, 'Trouble': 3910, 'Troy': 3911, 'TruTV': 3912, 'Trueb': 3913, 'Truman': 3914, 'Trust': 3915, 'Tuktamysheva': 3916, 'Tuzenbach': 3917, 'Tōdō': 3918, 'U2': 3919, 'UYSM': 3920, 'Ufodike': 3921, 'Unique': 3922, 'Unknown': 3923, 'Uno': 3924, 'Upper': 3925, 'Urinary': 3926, 'Urleen': 3927, 'Ursula': 3928, 'Uruguay': 3929, 'Usos': 3930, 'Uzor': 3931, 'VA': 3932, 'VI': 3933, 'VII': 3934, 'VSM': 3935, 'Vaccaro': 3936, 'Vallabhbhai': 3937, 'VanVelzen': 3938, 'Vanek': 3939, 'Vanity': 3940, 'Vannary': 3941, 'Vartan': 3942, 'Varuna': 3943, 'Veeck': 3944, 'Veiga': 3945, 'Venus': 3946, 'Verma': 3947, 'Verna': 3948, 'Vernon': 3949, 'Veronica': 3950, 'Versio': 3951, 'Veterans': 3952, 'Vicente': 3953, 'Vicki': 3954, 'Victory': 3955, 'Vidal': 3956, 'Vidovic': 3957, 'Village': 3958, 'Villone': 3959, 'Vinci': 3960, 'Virender': 3961, 'Virtue': 3962, 'Vishnu': 3963, 'Vitamin': 3964, 'Vittorio': 3965, 'Vladislav': 3966, 'Voice': 3967, 'Volaris': 3968, 'Vonda': 3969, 'W3C': 3970, 'WMIB': 3971, 'Wafula': 3972, 'Wakeman': 3973, 'Walking': 3974, 'Wanamaker': 3975, 'Warfield': 3976, 'Warne': 3977, 'WarnerMedia': 3978, 'Wash': 3979, 'Wat': 3980, 'Waterford': 3981, 'Wawrinka': 3982, 'Weagle': 3983, 'Weaver': 3984, 'Weaving': 3985, 'Wedding': 3986, 'Weekly': 3987, 'Wei': 3988, 'Weinberg': 3989, 'Weissman': 3990, 'Wennerström': 3991, 'Westwood': 3992, 'Wheeler': 3993, 'Whitfield': 3994, 'Whitsunday': 3995, 'Whyte': 3996, 'Wickenheiser': 3997, 'Wieden': 3998, 'Wieden+Kennedy': 3999, 'Wieners': 4000, 'Wiggles': 4001, 'Wilbur': 4002, 'Wilcher': 4003, 'Wildcats': 4004, 'Wilderness': 4005, 'Wilding': 4006, 'Wilkins': 4007, 'Winnie': 4008, 'Winters': 4009, 'Wisocky': 4010, 'Wizard': 4011, 'Women': 4012, 'Wonderful': 4013, 'Works': 4014, 'Wray': 4015, 'Wren': 4016, 'Writing': 4017, 'Wroldsen': 4018, 'Wyte': 4019, 'X': 4020, 'XLVIII': 4021, 'Xanthippus': 4022, 'Xingwana': 4023, 'Yaar': 4024, 'Yamada': 4025, 'Yanna': 4026, 'Yarborough': 4027, 'Years': 4028, 'Yee': 4029, 'Yell': 4030, 'Yellen': 4031, 'Yong': 4032, 'Yoona': 4033, 'Yoshida': 4034, 'Youth': 4035, 'Yuletide': 4036, 'Yuling': 4037, 'Yuro': 4038, 'Yuzuru': 4039, 'Zaentz': 4040, 'Zaidi': 4041, 'Zaire': 4042, 'Zanelli': 4043, 'Zant': 4044, 'Zerdin': 4045, 'Zhu': 4046, 'Zoanne': 4047, 'Zoff': 4048, 'Zokwana': 4049, 'Zuri': 4050, 'abietorum': 4051, 'accountant': 4052, 'achilles': 4053, 'across': 4054, 'adult': 4055, 'advertising': 4056, 'age': 4057, 'aged': 4058, 'airplane': 4059, 'al': 4060, 'alascensis': 4061, 'album': 4062, 'algae': 4063, 'alternate': 4064, 'among': 4065, 'ancestor': 4066, 'anhydrite': 4067, 'ankle': 4068, 'anterior': 4069, 'apostate': 4070, 'arcs': 4071, 'areas': 4072, 'arrondissement': 4073, 'articulate': 4074, 'artistic': 4075, 'artists': 4076, 'astronomer': 4077, 'bachelor': 4078, 'bank': 4079, 'basara': 4080, 'baseball': 4081, 'began': 4082, 'behind': 4083, 'belts': 4084, 'beneath': 4085, 'biceps': 4086, 'bike': 4087, 'biological': 4088, 'biology': 4089, 'booking': 4090, 'bookmakers': 4091, 'brachii': 4092, 'brachioradialis': 4093, 'brackish': 4094, 'breads': 4095, 'breeches': 4096, 'brevis': 4097, 'brought': 4098, 'but': 4099, 'c.': 4100, 'calcium': 4101, 'cannabis': 4102, 'capitalists': 4103, 'carbonates': 4104, 'cascadensis': 4105, 'cause': 4106, 'character': 4107, 'charge': 4108, 'child': 4109, 'ching': 4110, 'chloroplasts': 4111, 'clitoris': 4112, 'close': 4113, 'cm': 4114, 'co': 4115, 'codes': 4116, 'collision': 4117, 'collisions': 4118, 'combined': 4119, 'come': 4120, 'connectionless': 4121, 'consisted': 4122, 'consuls': 4123, 'contamination': 4124, 'content': 4125, 'continue': 4126, 'converge': 4127, 'conversion': 4128, 'cooked': 4129, 'core': 4130, 'criminology': 4131, 'cross': 4132, 'crucigera': 4133, 'currently': 4134, 'daimyō': 4135, 'dairy': 4136, 'dead': 4137, 'deathbed': 4138, 'debut': 4139, 'decorating': 4140, 'delusion': 4141, 'depictions': 4142, 'deshi': 4143, 'desperation': 4144, 'dialect': 4145, 'digestive': 4146, 'dishwater': 4147, 'dollar': 4148, 'driving': 4149, 'drought': 4150, 'du': 4151, 'duty': 4152, 'either': 4153, 'eleven': 4154, 'elf': 4155, 'employment': 4156, 'en': 4157, 'enabling': 4158, 'entire': 4159, 'episodes': 4160, 'eponymous': 4161, 'ethmoid': 4162, 'exercise': 4163, 'exile': 4164, 'explosive': 4165, 'external': 4166, 'extracellular': 4167, 'extraction': 4168, 'family': 4169, 'fashion': 4170, 'fashionable': 4171, 'fewer': 4172, 'fifteen': 4173, 'film': 4174, 'filmed': 4175, 'finance': 4176, 'fitting': 4177, 'florida': 4178, 'folk': 4179, 'forced': 4180, 'form': 4181, 'forward': 4182, 'founded': 4183, 'friend': 4184, 'fruit': 4185, 'fruits': 4186, 'fudge': 4187, 'futsal': 4188, 'g': 4189, 'games': 4190, 'garden': 4191, 'gets': 4192, 'gland': 4193, 'golden': 4194, 'gonad': 4195, 'governor': 4196, 'gun': 4197, 'habaneros': 4198, 'half': 4199, 'halides': 4200, 'harem': 4201, 'headwaters': 4202, 'health': 4203, 'heart': 4204, 'hired': 4205, 'hockey': 4206, 'hole': 4207, 'hop': 4208, 'hopping': 4209, 'hormones': 4210, 'hosen': 4211, 'ice': 4212, 'if': 4213, 'immediately': 4214, 'incentives': 4215, 'income': 4216, 'independent': 4217, 'injures': 4218, 'inspired': 4219, 'interval': 4220, 'introduced': 4221, 'intrusion': 4222, 'invasion': 4223, 'islands': 4224, 'job': 4225, 'jury': 4226, 'katers': 4227, 'kill': 4228, 'kite': 4229, 'kn': 4230, 'knees': 4231, 'knocked': 4232, 'la': 4233, 'labyrinth': 4234, 'lads': 4235, 'legislature': 4236, 'level': 4237, 'levels': 4238, 'lingual': 4239, 'lit': 4240, 'lithospheric': 4241, 'loan': 4242, 'loose': 4243, 'lungs': 4244, 'macroura': 4245, 'magazine': 4246, 'magazines': 4247, 'majority': 4248, 'male': 4249, 'management': 4250, 'manager': 4251, 'many': 4252, 'marshal': 4253, 'maxilla': 4254, 'maximizing': 4255, 'meats': 4256, 'meatus': 4257, 'medial': 4258, 'medially': 4259, 'men': 4260, 'mid-2016': 4261, 'mine': 4262, 'miniature': 4263, 'minimum': 4264, 'moderating': 4265, 'monasteries': 4266, 'monasticae': 4267, 'movement': 4268, 'movements': 4269, 'moving': 4270, 'music': 4271, 'nagasakiense': 4272, 'name': 4273, 'names': 4274, 'nasal': 4275, 'necator': 4276, 'nervous': 4277, 'next': 4278, 'nitzschia': 4279, 'nobles': 4280, 'node': 4281, 'nor': 4282, 'nuclear': 4283, 'occurs': 4284, 'ocean': 4285, 'old': 4286, 'organic': 4287, 'originated': 4288, 'others': 4289, 'out': 4290, 'overall': 4291, 'oxygen': 4292, 'pairs': 4293, 'palate': 4294, 'palatine': 4295, 'pants': 4296, 'party': 4297, 'patella': 4298, 'patellar': 4299, 'patwin': 4300, 'peepal': 4301, 'penchant': 4302, 'photosynthetic': 4303, 'plasmalemma': 4304, 'players': 4305, 'playing': 4306, 'poire': 4307, 'popularized': 4308, 'poultry': 4309, 'premier': 4310, 'prices': 4311, 'prison': 4312, 'prisoners': 4313, 'protocol': 4314, 'protozoans': 4315, 'provinces': 4316, 'pterygoid': 4317, 'radio': 4318, 'radioulnar': 4319, 'rapper': 4320, 'rat': 4321, 'rates': 4322, 'reaction': 4323, 'rebates': 4324, 'rebuild': 4325, 'receptors': 4326, 'regalis': 4327, 'ren': 4328, 'representatives': 4329, 'republika': 4330, 'riding': 4331, 'rise': 4332, 'river': 4333, 'rodents': 4334, 'rubricosa': 4335, 'rules': 4336, 'runners': 4337, 'safe': 4338, 'salivary': 4339, 'saltwater': 4340, 'scale': 4341, 'scholae': 4342, 'schools': 4343, 'seafood': 4344, 'senior': 4345, 'sent': 4346, 'server': 4347, 'service': 4348, 'several': 4349, 'shortly': 4350, 'show': 4351, 'shōgun': 4352, 'sick': 4353, 'singing': 4354, 'smooth': 4355, 'socialist': 4356, 'southwest': 4357, 'spread': 4358, 'stabilizing': 4359, 'stealth': 4360, 'stepsister': 4361, 'streets': 4362, 'suburb': 4363, 'sugar': 4364, 'sulfate': 4365, 'summer': 4366, 'table': 4367, 'tagged': 4368, 'tapping': 4369, 'taught': 4370, 'teacher': 4371, 'teaching': 4372, 'teams': 4373, 'tenden': 4374, 'term': 4375, 'test': 4376, 'them': 4377, 'tibiae': 4378, 'tiger': 4379, 'times': 4380, 'tonttu': 4381, 'top': 4382, 'touching': 4383, 'toward': 4384, 'transfusion': 4385, 'tribe': 4386, 'tubes': 4387, 'typically': 4388, 'under': 4389, 'unicellular': 4390, 'unified': 4391, 'universes': 4392, 'university': 4393, 'use': 4394, 'used': 4395, 'vagina': 4396, 'vegetable': 4397, 'vehicle': 4398, 'ventricles': 4399, 'ventriloquism': 4400, 'vestibule': 4401, 'veteran': 4402, 'vice': 4403, 'volcano': 4404, 'vulpes': 4405, 'vulval': 4406, 'walking': 4407, 'westward': 4408, 'wood': 4409, 'woolery': 4410, 'words': 4411, 'Česko': 4412, 'Česká': 4413, 'Đình': 4414, 'Đăng': 4415, 'Şükür': 4416, 'Šafářová': 4417, '−89.2': 4418, '..': 4419, '0.08': 4420, '0.19': 4421, '0.4': 4422, '0.68': 4423, '05': 4424, '08:15': 4425, '1,202': 4426, '1,464': 4427, '1,602': 4428, '1,710': 4429, '1.3': 4430, '1.5': 4431, '1000': 4432, '106': 4433, '1080000000': 4434, '11,000': 4435, '11,500': 4436, '11,745': 4437, '11.5': 4438, '110': 4439, '117': 4440, '117.92': 4441, '119': 4442, '11:15': 4443, '123': 4444, '124': 4445, '1260': 4446, '12:00': 4447, '1313': 4448, '133': 4449, '1348': 4450, '1368–1644': 4451, '139': 4452, '14-year': 4453, '14.23': 4454, '14.9': 4455, '1400': 4456, '151': 4457, '1526': 4458, '1535': 4459, '153rd': 4460, '159': 4461, '15–18': 4462, '1611': 4463, '1665': 4464, '1684': 4465, '16–17': 4466, '16–19': 4467, '1709': 4468, '172.2': 4469, '1721': 4470, '1754': 4471, '1763': 4472, '177–189': 4473, '1784': 4474, '18.5': 4475, '1800': 4476, '1803': 4477, '1804': 4478, '1815': 4479, '1816': 4480, '1819': 4481, '1820': 4482, '1824': 4483, '1828': 4484, '1835': 4485, '1836': 4486, '1838': 4487, '1849': 4488, '1861': 4489, '1862': 4490, '1863': 4491, '1865': 4492, '1867': 4493, '1870': 4494, '1875': 4495, '1876': 4496, '1877': 4497, '1878': 4498, '1882': 4499, '1887': 4500, '189': 4501, '1890': 4502, '18–21': 4503, '19-August': 4504, '19-March': 4505, '1900': 4506, '1905': 4507, '191': 4508, '1918': 4509, '1920s': 4510, '1926': 4511, '1930s': 4512, '1932': 4513, '195': 4514, '1950s': 4515, '1972': 4516, '197–206': 4517, '199': 4518, '1996–1997': 4519, '1:11–38': 4520, '20.00': 4521, '200916': 4522, '2010–2013': 4523, '2016–17': 4524, '2017-October': 4525, '2017–18': 4526, '201–215': 4527, '202': 4528, '2020-March': 4529, '2024': 4530, '2030': 4531, '2044': 4532, '205': 4533, '207': 4534, '20:14': 4535, '212': 4536, '21st': 4537, '225': 4538, '22:25': 4539, '22:50': 4540, '23,093': 4541, '232': 4542, '243': 4543, '244': 4544, '25,233': 4545, '250': 4546, '266': 4547, '27,282': 4548, '276': 4549, '28,162': 4550, '29.78': 4551, '29.95': 4552, '291': 4553, '299700': 4554, '2:45pm': 4555, '300': 4556, '300000': 4557, '3000BCE': 4558, '32,292': 4559, '328': 4560, '32′E': 4561, '33,636': 4562, '33.81': 4563, '332': 4564, '33′N': 4565, '3708': 4566, '395': 4567, '4,540': 4568, '4-door': 4569, '4.6': 4570, '410': 4571, '414': 4572, '42': 4573, '5,987': 4574, '513': 4575, '52,388': 4576, '52nd': 4577, '538': 4578, '555': 4579, '572': 4580, '5:22': 4581, '5–4': 4582, '6,2002': 4583, '6,2013': 4584, '6.5': 4585, '6.8': 4586, '61': 4587, '626': 4588, '633': 4589, '634': 4590, '7.62': 4591, '74.5': 4592, '771–476': 4593, '79.5': 4594, '7th': 4595, '7–14': 4596, '8,696': 4597, '8-February': 4598, '81': 4599, '83.5': 4600, '84.9': 4601, '87': 4602, '88': 4603, '89-acre': 4604, '8:15–26': 4605, '8–8.5': 4606, '90': 4607, '90.08': 4608, '950': 4609, '97': 4610, '973': 4611, '976': 4612, '986': 4613, '9:21–27': 4614, '9:45': 4615, '9:55': 4616, '@BarackObama': 4617, '@instagram': 4618, 'AAVE': 4619, 'ACT': 4620, 'AD393': 4621, 'AFC': 4622, 'AG': 4623, 'AJK': 4624, 'ANZ': 4625, 'AOC': 4626, 'APs': 4627, 'Abaire': 4628, 'Abbie': 4629, 'Aboud': 4630, 'About': 4631, 'Absolem': 4632, 'Abstergo': 4633, 'Abu': 4634, 'Acorah': 4635, 'Acquired': 4636, 'Acres': 4637, 'Ademir': 4638, 'Adlai': 4639, 'Ado': 4640, 'Adolphe': 4641, 'Adrienne': 4642, 'Aduwak': 4643, 'Advanced': 4644, 'Advertiser': 4645, 'Aeneas': 4646, 'Afanasieff': 4647, 'Age': 4648, 'Agriope': 4649, 'Ahmad': 4650, 'Ahmed': 4651, 'Aiden': 4652, 'Aileen': 4653, 'Aintoinette': 4654, 'Aires': 4655, 'Ajax': 4656, 'Akashi': 4657, 'Akashic': 4658, 'Akbar': 4659, 'Akhenaten': 4660, 'Alamut': 4661, 'Alanis': 4662, 'Alberto': 4663, 'Alcoholics': 4664, 'Aldrich': 4665, 'Alecia': 4666, 'Aleksandrovna': 4667, 'Alexandria': 4668, 'Algonkian': 4669, 'Alien': 4670, 'Alliance': 4671, 'Alma': 4672, 'Almeida': 4673, 'Aloe': 4674, 'Aloisius': 4675, 'Alone': 4676, 'Alpert': 4677, 'Amaravati': 4678, 'Ambar': 4679, 'Ambarnath': 4680, 'Ameche': 4681, 'Amok': 4682, 'Analysis': 4683, 'Analytical': 4684, 'Angelina': 4685, 'Angola': 4686, 'Anguillara': 4687, 'Angélique': 4688, 'Anigbogu': 4689, 'Anika': 4690, 'Animals': 4691, 'Annamite': 4692, 'Annie': 4693, 'Anonymous': 4694, 'Ant': 4695, 'Antilles': 4696, 'Antonina': 4697, 'Apophenia': 4698, 'Apparent': 4699, 'Appeal': 4700, 'Apr': 4701, 'Aquitaine': 4702, 'Aquostic': 4703, 'Arbour': 4704, 'Archangel': 4705, 'Argenziano': 4706, 'Armato': 4707, 'Armisen': 4708, 'Arnhim': 4709, 'Arora': 4710, 'Articles': 4711, 'Artie': 4712, 'Ascension': 4713, 'Assets': 4714, 'Aston': 4715, 'Astros': 4716, 'Atkinson': 4717, 'Attenborough': 4718, 'Atwood': 4719, 'Auckland': 4720, 'Augsburg': 4721, 'Augustine': 4722, 'Ava': 4723, 'Aves': 4724, 'Aviazione': 4725, 'Avicii': 4726, 'Away': 4727, 'Axis': 4728, 'Axl': 4729, 'Aztec': 4730, 'BA': 4731, 'BASIC': 4732, 'BOS': 4733, 'BRT': 4734, 'Babyface': 4735, 'Backstreet': 4736, 'Bader': 4737, 'Badgers': 4738, 'Bae': 4739, 'Bahama': 4740, 'Balboa': 4741, 'Balfe': 4742, 'Ball': 4743, 'Balladeer': 4744, 'Ballaugh': 4745, 'Banerjee': 4746, 'Bani': 4747, 'Bansal': 4748, 'Bantam': 4749, 'Barbara': 4750, 'Barbary': 4751, 'Barden': 4752, 'Bare': 4753, 'Barr': 4754, 'Barrett': 4755, 'Barroso': 4756, 'Barrymore': 4757, 'Baruch': 4758, 'Basile': 4759, 'Basilica': 4760, 'Bastille': 4761, 'Batabyal': 4762, 'Baton': 4763, 'Batt': 4764, 'Bauhin': 4765, 'Bayelsa': 4766, 'Bayern': 4767, 'Bayt': 4768, 'Bear': 4769, 'Beard': 4770, 'Beauregard': 4771, 'Beaver': 4772, 'Bebe': 4773, 'Before': 4774, 'Begins': 4775, 'Belgrade': 4776, 'Bellagio': 4777, 'Bellas': 4778, 'Bello': 4779, 'Bellona': 4780, 'Belshazzar': 4781, 'Belvedere': 4782, 'Bender': 4783, 'Benedict': 4784, 'Benj': 4785, 'Benkirane': 4786, 'Benz': 4787, 'Berda': 4788, 'Bergeron': 4789, 'Bergling': 4790, 'Bergoglio': 4791, 'Berkeley': 4792, 'Bert': 4793, 'Bessel': 4794, 'Bett': 4795, 'Bettis': 4796, 'Bharat': 4797, 'Bhatt': 4798, 'Bhraonáin': 4799, 'Bhushan': 4800, 'Bhutan': 4801, 'Bichir': 4802, 'Bidwell': 4803, 'Biermann': 4804, 'Bio': 4805, 'Birinus': 4806, 'Bischoff': 4807, 'Bishop': 4808, 'Blacc': 4809, 'Blackwood': 4810, 'Blake': 4811, 'Blakeley': 4812, 'Blando': 4813, 'Blizzard': 4814, 'Blondes': 4815, 'Blumer': 4816, 'Bo': 4817, 'Bobo': 4818, 'Boente': 4819, 'Boldt': 4820, 'Bolger': 4821, 'Bomb': 4822, 'Bomba': 4823, 'Bonney': 4824, 'Boogie': 4825, 'Bordeauxdog': 4826, 'Botswana': 4827, 'Botto': 4828, 'Boulevard': 4829, 'Bowen': 4830, 'Bowie': 4831, 'Boxer': 4832, 'Boyle': 4833, 'Bracco': 4834, 'Braddan': 4835, 'Bradford': 4836, 'Bradley': 4837, 'Braid': 4838, 'Braised': 4839, 'Bran': 4840, 'Branch': 4841, 'Brandon': 4842, 'Braxton': 4843, 'Brazzi': 4844, 'Breck': 4845, 'Bregman': 4846, 'Brel': 4847, 'Bremsstrahlung': 4848, 'Brest': 4849, 'Bridgeland': 4850, 'Briggs': 4851, 'Britney': 4852, 'Britten': 4853, 'Brockville': 4854, 'Brooklynn': 4855, 'Browne': 4856, 'Brundage': 4857, 'Bruns': 4858, 'Brynner': 4859, 'Buccaneers': 4860, 'Buenos': 4861, 'Buffer': 4862, 'Buffon': 4863, 'Building': 4864, 'Bullock': 4865, 'Burbank': 4866, 'Burger': 4867, 'Burgundy': 4868, 'Burnett': 4869, 'Burrafato': 4870, 'Burrell': 4871, 'Burruss': 4872, 'Bushman': 4873, 'Business': 4874, 'Butler': 4875, 'Buzz': 4876, 'Bynum': 4877, 'C$': 4878, 'C.S.C.': 4879, 'CAF': 4880, 'CHAP': 4881, 'CONCACAF': 4882, 'CONMEBOL': 4883, 'CT': 4884, 'Cabot': 4885, 'Cafe': 4886, 'Cage': 4887, 'Caitlin': 4888, 'Calahan': 4889, 'Calais': 4890, 'Calcutta': 4891, 'Caldera': 4892, 'Caldiero': 4893, 'Calgary': 4894, 'Calixa': 4895, 'Calment': 4896, 'Camacho': 4897, 'Camilla': 4898, 'Cammack': 4899, 'Cammie': 4900, 'Campania': 4901, 'Cana': 4902, 'Canarinha': 4903, 'Cannes': 4904, 'Canterbury': 4905, 'Cantor': 4906, 'Capaldi': 4907, 'Capita': 4908, 'Cardinal': 4909, 'Caribbean': 4910, 'Carlin': 4911, 'Carlson': 4912, 'Carmel': 4913, 'Case': 4914, 'Cash': 4915, 'Caspar': 4916, 'Castle': 4917, 'Caswell': 4918, 'Catalonia': 4919, 'Cathedral': 4920, 'Cathy': 4921, 'Cato': 4922, 'Caudill': 4923, 'Caviezel': 4924, 'Cecilienhof': 4925, 'Celtic': 4926, 'Cetera': 4927, 'Chalet': 4928, 'Chambers': 4929, 'Chamonix': 4930, 'Chandan': 4931, 'Chapel': 4932, 'Characteristic': 4933, 'Charleston': 4934, 'Chatsworth': 4935, 'Cheap': 4936, 'Cheating': 4937, 'Chikwendu': 4938, 'Child': 4939, 'Chinn': 4940, 'Chittell': 4941, 'Chiwetel': 4942, 'Chocolate': 4943, 'Choir': 4944, 'Christensen': 4945, 'Chrysostom': 4946, 'Churchill': 4947, 'Chyler': 4948, 'Château': 4949, 'Châteauneuf': 4950, 'Cinélu': 4951, 'Cities': 4952, 'Claramae': 4953, 'Clarence': 4954, 'Clegane': 4955, 'Cliffe': 4956, 'Clint': 4957, 'Cobain': 4958, 'Coburn': 4959, 'Cocchiola': 4960, 'Cocker': 4961, 'Coghlan': 4962, 'Cohen': 4963, 'Cohenour': 4964, 'Coinman': 4965, 'Colby': 4966, 'Collar': 4967, 'Collection': 4968, 'Collide': 4969, 'Collingwood': 4970, 'Colombia': 4971, 'Colossus': 4972, 'Compaq': 4973, 'Computers': 4974, 'Concord': 4975, 'Condon': 4976, 'Confederate': 4977, 'Confederation': 4978, 'Conference': 4979, 'Conqueror': 4980, 'Continental': 4981, 'Contours': 4982, 'Controls': 4983, 'Cookies': 4984, 'Cooley': 4985, 'Coots': 4986, 'Coppee': 4987, 'Corbett': 4988, 'Corbin': 4989, 'Cordillera': 4990, 'Corinne': 4991, 'Cornelissen': 4992, 'Cortez': 4993, 'Cortona': 4994, 'Cosmic': 4995, 'Cosmos': 4996, 'Costa': 4997, 'Costello': 4998, 'Cotton': 4999, 'Countess': 5000, 'Country': 5001, 'Courageous': 5002, 'Course': 5003, 'Cozumel': 5004, 'Craddick': 5005, 'Creme': 5006, 'Creole': 5007, 'Crews': 5008, 'Crispen': 5009, 'Croce': 5010, 'Crosbie': 5011, 'Crossroads': 5012, 'Cruise': 5013, 'Cruz': 5014, 'Cuba': 5015, 'Cuizza': 5016, 'Cyril': 5017, \"D'Arcy\": 5018, 'D.G.': 5019, 'DSM': 5020, 'Da': 5021, 'Dad': 5022, 'Daddario': 5023, 'Dae': 5024, 'Daft': 5025, 'Dahlia': 5026, 'Daisyjo': 5027, 'Dales': 5028, 'Dalung': 5029, 'Dambulla': 5030, 'Dampier': 5031, 'Dane': 5032, 'Dannii': 5033, 'Danyang': 5034, 'Dapo': 5035, 'Darkseid': 5036, 'Darrowby': 5037, 'Dashwood': 5038, 'Daveed': 5039, 'Davetta': 5040, 'Dawkins': 5041, 'DeHaan': 5042, 'DeKay': 5043, 'DeLoach': 5044, 'Deborah': 5045, 'Dec': 5046, 'Decoration': 5047, 'Deele': 5048, 'Deeley': 5049, 'Deep': 5050, 'Dehone': 5051, 'Delaware': 5052, 'Delevingne': 5053, 'Delivery': 5054, 'Della': 5055, 'Demián': 5056, 'Denise': 5057, 'Derby': 5058, 'Destroyers': 5059, 'Deutsche': 5060, 'Devine': 5061, 'Dexter': 5062, 'Dianne': 5063, 'Dibaba': 5064, 'Diede': 5065, 'Diggs': 5066, 'Dignity': 5067, 'Dinklage': 5068, 'Dinsmore': 5069, 'Dixon': 5070, 'Django': 5071, 'Djenné': 5072, 'Dobbyn': 5073, 'Dodgers': 5074, 'Dogue': 5075, 'Dohrn': 5076, 'Doina': 5077, 'Dollar': 5078, 'Dolly': 5079, 'Dolman': 5080, 'Dominican': 5081, 'Dorabji': 5082, 'Doris': 5083, 'Dormer': 5084, 'Dorn': 5085, 'Dorothea': 5086, 'Dottie': 5087, 'Downey': 5088, 'Dr': 5089, 'Dragan': 5090, 'Draper': 5091, 'Dravid': 5092, 'Drayton': 5093, 'Drift': 5094, 'Dubin': 5095, 'Dudley': 5096, 'Dumoulin': 5097, 'Dunant': 5098, 'Dunk': 5099, 'Dunne': 5100, 'Dunsinane': 5101, 'Dunst': 5102, 'Dutt': 5103, 'Duvall': 5104, 'Duvernay': 5105, 'Dwan': 5106, 'Dworsky': 5107, 'EAP': 5108, 'EST': 5109, 'Eamonn': 5110, 'Earnhardt': 5111, 'Eastwood': 5112, 'Ebb': 5113, 'Eccleston': 5114, 'Eckels': 5115, 'Edberg': 5116, 'Eddard': 5117, 'Eden': 5118, 'Education': 5119, 'Edwardian': 5120, 'Efe': 5121, 'Effie': 5122, 'Egbert': 5123, 'Egyptian': 5124, 'Eion': 5125, 'Eithne': 5126, 'Ejeba': 5127, 'Ejiofor': 5128, 'Ekaterina': 5129, 'Eleanore': 5130, 'Elections': 5131, 'Electric': 5132, 'Eleventh': 5133, 'Elise': 5134, 'Elisha': 5135, 'Ellsworth': 5136, 'Elsa': 5137, 'Elsinore': 5138, 'Elton': 5139, 'Elveden': 5140, 'Elysian': 5141, 'Emile': 5142, 'Emmett': 5143, 'Emry': 5144, 'Encino': 5145, 'Engelbrecht': 5146, 'Enlisted': 5147, 'Entrepreneurship': 5148, 'Eratosthenes': 5149, 'Esdras': 5150, 'Esmeralda': 5151, 'Esparza': 5152, 'Esther': 5153, 'Eswatini': 5154, 'Ethelbert': 5155, 'Euro': 5156, 'Eustace': 5157, 'Evelyn': 5158, 'Everett': 5159, 'Everidge': 5160, 'Everyone': 5161, 'Evoy': 5162, 'Ewan': 5163, 'Ewing': 5164, 'Experimentation': 5165, 'Exploratory': 5166, 'Express': 5167, 'FBN': 5168, 'FL': 5169, 'FPL': 5170, 'Fallopian': 5171, 'Family': 5172, 'Fargo': 5173, 'Faris': 5174, 'Farley': 5175, 'Farr': 5176, 'Farrah': 5177, 'Farrar': 5178, 'Farrell': 5179, 'Fat': 5180, 'Fault': 5181, 'Fawcett': 5182, 'Fe': 5183, 'Fedora': 5184, 'Feinstein': 5185, 'Ferdin': 5186, 'Fernand': 5187, 'Ferrari': 5188, 'Fess': 5189, 'Fidel': 5190, 'Fields': 5191, 'Fiji': 5192, 'Filiform': 5193, 'Fillion': 5194, 'Films': 5195, 'Firth': 5196, 'Fish': 5197, 'Fishburne': 5198, 'Fitz': 5199, 'Fitzpatrick': 5200, 'Florence': 5201, 'Flu': 5202, 'Flynn': 5203, 'Fodé': 5204, 'Fools': 5205, 'Foote': 5206, 'Forget': 5207, 'Forsyth': 5208, 'Forty': 5209, 'Fossard': 5210, 'Fowler': 5211, 'Foy': 5212, 'Fracture': 5213, 'Frakes': 5214, 'Francesco': 5215, 'Francks': 5216, 'Frankie': 5217, 'Freedom': 5218, 'Freeman': 5219, 'Freiherr': 5220, 'Freuder': 5221, 'Friedle': 5222, 'Frisco': 5223, 'From': 5224, 'Frontier': 5225, 'Frédéric': 5226, 'Funmilayo': 5227, 'Furlong': 5228, 'Fury': 5229, 'G1': 5230, 'G2': 5231, 'GRS': 5232, 'Gaerlan': 5233, 'Gail': 5234, 'Galerie': 5235, 'Gammon': 5236, 'Gananoque': 5237, 'Ganges': 5238, 'Ganguly': 5239, 'Gangwon': 5240, 'Gap': 5241, 'Garfield': 5242, 'Garnett': 5243, 'Garrod': 5244, 'Garudeshwar': 5245, 'Gashuin': 5246, 'Gaulos': 5247, 'Gauss': 5248, 'Gemberling': 5249, 'Gemma': 5250, 'Gen': 5251, 'Genda': 5252, 'Geneva': 5253, 'Gentlemen': 5254, 'Genzebe': 5255, 'Georges': 5256, 'Germain': 5257, 'Germaine': 5258, 'Germans': 5259, 'Gertz': 5260, 'Ghebreslassie': 5261, 'Ghirmay': 5262, 'Ghufran': 5263, 'Gianluigi': 5264, 'Gibbons': 5265, 'Gibbs': 5266, 'Gideon': 5267, 'Gila': 5268, 'Gilbert': 5269, 'Gilchrist': 5270, 'Gilead': 5271, 'Giles': 5272, 'Gilkison': 5273, 'Gillan': 5274, 'Gillespie': 5275, 'Gillian': 5276, 'Ginnifer': 5277, 'Giselle': 5278, 'Git': 5279, 'Givney': 5280, 'Glacier': 5281, 'Glasgow': 5282, 'Glencrutchery': 5283, 'Gluck': 5284, 'Goatherd': 5285, 'Godfrey': 5286, 'Goffin': 5287, 'Goings': 5288, 'Goldberg': 5289, 'Goldenberg': 5290, 'Goldfields': 5291, 'Golding': 5292, 'Goliath': 5293, 'Gonsalves': 5294, 'Gonzaga': 5295, 'Gonzalez': 5296, 'Goodwin': 5297, 'Gottfried': 5298, 'Gotye': 5299, 'Goulding': 5300, 'Gradishar': 5301, 'Grandstand': 5302, 'Graphics': 5303, 'Gravitron': 5304, 'Grayson': 5305, 'Greater': 5306, 'Greeba': 5307, 'Greenbaum': 5308, 'Grey': 5309, 'Griffiths': 5310, 'Grillz': 5311, 'Grimm': 5312, 'Groot': 5313, 'Gross': 5314, 'Guan': 5315, 'Guerrouj': 5316, 'Guess': 5317, 'Guillaume': 5318, 'Gulf': 5319, 'Gunn': 5320, 'Guns': 5321, 'Gurbachan': 5322, 'Guss': 5323, 'Guyton': 5324, 'Gwen': 5325, 'Généreux': 5326, 'Géza': 5327, 'Gómez': 5328, 'Göckel': 5329, 'H': 5330, 'HS': 5331, 'Hachero': 5332, 'Hagar': 5333, 'Haitian': 5334, 'Haldane': 5335, 'Half': 5336, 'Halfday': 5337, 'Hamblen': 5338, 'Hamill': 5339, 'Hamlin': 5340, 'Hamlisch': 5341, 'Hammond': 5342, 'Hampton': 5343, 'Hand': 5344, 'Hank': 5345, 'Hanks': 5346, 'Hanshaw': 5347, 'Hardwicke': 5348, 'Harington': 5349, 'Harishankar': 5350, 'Harlem': 5351, 'Harline': 5352, 'Harrington': 5353, 'Harshman': 5354, 'Hatter': 5355, 'Havana': 5356, 'Haven': 5357, \"Hawai'i\": 5358, 'Hawaiʻi': 5359, 'Hawkesbury': 5360, 'Haydn': 5361, 'Hazratullah': 5362, 'Healy': 5363, 'Heathrow': 5364, 'Hecht': 5365, 'Heinrich': 5366, 'Heir': 5367, 'Hellerstedt': 5368, 'Hellfighters': 5369, 'Hemswell': 5370, 'Henson': 5371, 'Her': 5372, 'Herb': 5373, 'Heritage': 5374, 'Hermann': 5375, 'Herod': 5376, 'Herschel': 5377, 'Herschelle': 5378, 'Hertfordshire': 5379, 'Hertz': 5380, 'Hessian': 5381, 'Hesston': 5382, 'Hicham': 5383, 'Hicks': 5384, 'Hiddleston': 5385, 'Highmore': 5386, 'Himadri': 5387, 'Himalayas': 5388, 'Hina': 5389, 'Hindu': 5390, 'Hira': 5391, 'Hiroshima': 5392, 'Hispaniola': 5393, 'Hits': 5394, 'Hoboken': 5395, 'Hodges': 5396, 'Hoffman': 5397, 'Hofheinz': 5398, 'Hoi': 5399, 'Hollie': 5400, 'Holtzapffel': 5401, 'Hon': 5402, 'Hook': 5403, 'Hopper': 5404, 'Horacio': 5405, 'Horia': 5406, 'Horse': 5407, 'Hoss': 5408, 'Houghton': 5409, 'Howie': 5410, 'Hoyt': 5411, 'Hughie': 5412, 'Hughley': 5413, 'Hundley': 5414, 'Huns': 5415, 'Hussain': 5416, 'Hwang': 5417, 'Hyderabad': 5418, 'Hydrogen': 5419, 'Hérault': 5420, 'Iberian': 5421, 'Ibrahim': 5422, 'Ibrahimović': 5423, 'Iconic': 5424, 'Identity': 5425, 'Igbo': 5426, 'Igraine': 5427, 'IndARC': 5428, 'Independence': 5429, 'Indiands': 5430, 'Indies': 5431, 'Industrial': 5432, 'Infantry': 5433, 'Infared': 5434, 'Inglewood': 5435, 'Inside': 5436, 'Interior': 5437, 'Intestine': 5438, 'Inverness': 5439, 'Io': 5440, 'Ireland': 5441, 'Iris': 5442, 'Isa': 5443, 'Isaak': 5444, 'Isley': 5445, 'Isoroku': 5446, 'Israeli': 5447, 'Italian': 5448, 'Iēsus': 5449, 'Iūdaeōrum': 5450, 'J.Hud': 5451, 'J.J.': 5452, 'JFK': 5453, 'JR': 5454, 'Jackie': 5455, 'Jacks': 5456, 'Jacobite': 5457, 'Jacqy': 5458, 'Jade': 5459, 'Jae': 5460, 'Jai': 5461, 'Jaime': 5462, 'Jain': 5463, 'Jaisalmer': 5464, 'Jaitapur': 5465, 'Jamestown': 5466, 'Jami': 5467, 'Janoub': 5468, 'Januare': 5469, 'Jeanne': 5470, 'Jedi': 5471, 'Jemma': 5472, 'Jeri': 5473, 'Jessi': 5474, 'Jewel': 5475, 'Jewish': 5476, 'Jiani': 5477, 'Jin': 5478, 'Jobs': 5479, 'Jolie': 5480, 'Jordanne': 5481, 'Jourdan': 5482, 'Journey': 5483, 'Jovi': 5484, 'Judgment': 5485, 'Judiasm': 5486, 'Julien': 5487, 'Juncker': 5488, 'Juolevi': 5489, 'Jupiter': 5490, 'KST': 5491, 'Kaew': 5492, 'Kahoʻolawe': 5493, 'Kaikyō': 5494, 'Kalyan': 5495, 'Kamala': 5496, 'Kameto': 5497, 'Kami': 5498, 'Kamiji': 5499, 'Kan': 5500, 'Kandi': 5501, 'Kang': 5502, 'Karimi': 5503, 'Karla': 5504, 'Karnataka': 5505, 'Karun': 5506, 'Kasaila': 5507, 'Kasirer': 5508, 'Katarina': 5509, 'Kater': 5510, 'Katharine': 5511, 'Katie': 5512, 'Katung': 5513, \"Kaua'i\": 5514, 'Kavan': 5515, 'Kaya': 5516, 'Kayla': 5517, 'Ke': 5518, 'Kea': 5519, 'Keala': 5520, 'Keefe': 5521, 'Keiino': 5522, 'Keitany': 5523, 'Kenichi': 5524, 'Kenickie': 5525, 'Kenneth': 5526, 'Kennicott': 5527, 'Kerala': 5528, 'Ketch': 5529, 'Khabib': 5530, 'Khalifa': 5531, 'Khalil': 5532, 'Khesar': 5533, 'Kickoff': 5534, 'Kid': 5535, 'Kilda': 5536, 'Kimberli': 5537, 'Kimbra': 5538, 'Kimmel': 5539, 'Kinkakuji': 5540, 'Kinks': 5541, 'Kinnaman': 5542, 'Kiplingcotes': 5543, 'Kit': 5544, 'Kita': 5545, 'Kittles': 5546, 'Klementieff': 5547, 'Kliment': 5548, 'Kloss': 5549, 'Knauff': 5550, 'Knickerbocker': 5551, 'Knopfler': 5552, 'Koch': 5553, 'Kon': 5554, 'Konstantine': 5555, 'Korman': 5556, 'Korsakov': 5557, 'Kowalski': 5558, 'Krause': 5559, 'Krishan': 5560, 'Kristien': 5561, 'Kristle': 5562, 'Krosney': 5563, 'Kudankulam': 5564, 'Kunshan': 5565, 'Kuroshima': 5566, 'Kusaka': 5567, 'Kuti': 5568, 'Kyles': 5569, 'Kyoto': 5570, 'Kyōto': 5571, 'LBB': 5572, 'LaShawn': 5573, 'Lac': 5574, 'Lacs': 5575, 'Ladakh': 5576, 'Laffy': 5577, 'Lahr': 5578, 'Lai': 5579, 'Lajpat': 5580, 'Lakehurst': 5581, 'Laker': 5582, 'Lala': 5583, 'Lancashire': 5584, 'Lane': 5585, 'Laney': 5586, 'Langle': 5587, 'Lanning': 5588, 'Lao': 5589, 'Lapa': 5590, 'Laurence': 5591, 'Laurent': 5592, 'Lavallee': 5593, 'Lawson': 5594, 'Laye': 5595, 'Lazzeri': 5596, 'Lea': 5597, 'Leaders': 5598, 'Leakes': 5599, 'Leal': 5600, 'Leary': 5601, 'Leavesden': 5602, 'Leeza': 5603, 'Legendary': 5604, 'Legionaria': 5605, 'Legowelt': 5606, 'Leick': 5607, 'Leif': 5608, 'Leilah': 5609, 'Leipzig': 5610, 'Lemonades': 5611, 'Lennie': 5612, 'Leonardo': 5613, 'Leoni': 5614, 'Lerner': 5615, 'Lesnar': 5616, 'Less': 5617, 'Letter': 5618, 'Lettie': 5619, 'Lexington': 5620, 'Liberian': 5621, 'Lighthouse': 5622, 'Lightyear': 5623, 'Lillian': 5624, 'Lindesnes': 5625, 'Lineker': 5626, 'Linley': 5627, 'Lionsgate': 5628, 'Lisbon': 5629, 'List': 5630, 'Lithgow': 5631, 'Litovsk': 5632, 'Liza': 5633, 'Loa': 5634, 'Logan': 5635, 'Lonely': 5636, 'Lonnie': 5637, 'Lost': 5638, 'Loughman': 5639, 'Louisville': 5640, 'Lousiana': 5641, 'Lt': 5642, 'Lubowitz': 5643, 'Lucia': 5644, 'Luddington': 5645, 'Ludwig': 5646, 'Lutz': 5647, 'Lymphatic': 5648, 'Lynda': 5649, 'Lyon': 5650, 'Lyrae': 5651, 'Lānaʻi': 5652, 'M': 5653, 'MEO': 5654, 'MP': 5655, 'MPS': 5656, 'MSK': 5657, 'Ma': 5658, 'Mabe': 5659, 'Mace': 5660, 'Machaut': 5661, 'Macklin': 5662, 'Madhya': 5663, 'Mae': 5664, 'Magee': 5665, 'Mahony': 5666, 'Maitland': 5667, 'Majority': 5668, 'Makarova': 5669, 'Makkena': 5670, 'Mako': 5671, 'Malahide': 5672, 'Malala': 5673, 'Malek': 5674, 'Mallika': 5675, 'Manasseh': 5676, 'Mandylor': 5677, 'Mangroves': 5678, 'Manny': 5679, 'Mantegna': 5680, 'Maple': 5681, 'Maracanã': 5682, 'Marcille': 5683, 'Marco': 5684, 'Mare': 5685, 'Margo': 5686, 'Margot': 5687, 'Mariah': 5688, 'Marilyn': 5689, 'Mariners': 5690, 'Mario': 5691, 'Marjorie': 5692, 'Marley': 5693, 'Maroulis': 5694, 'Marsay': 5695, 'Marsha': 5696, 'Marshak': 5697, 'Martel': 5698, 'Martins': 5699, 'Martínez': 5700, 'Marulo': 5701, 'Masala': 5702, 'Mason': 5703, 'Massena': 5704, 'Masterson': 5705, 'Mastrantonio': 5706, 'Masur': 5707, 'Matale': 5708, 'Mathai': 5709, 'Matthews': 5710, 'Maui': 5711, 'Mauritania': 5712, 'Mausolus': 5713, 'Maximus': 5714, 'Maynila': 5715, 'Mazouz': 5716, 'McAdams': 5717, 'McArthur': 5718, 'McCormack': 5719, 'McCown': 5720, 'McCoy': 5721, 'McDonagh': 5722, 'McEnroe': 5723, 'McEntire': 5724, 'McGranger': 5725, 'McGregor': 5726, 'McInerney': 5727, 'McInnerny': 5728, 'McKeague': 5729, 'McKean': 5730, 'McNamara': 5731, 'McQueen': 5732, 'McWherter': 5733, 'Mead': 5734, 'Mears': 5735, 'Medallions': 5736, 'Mehmed': 5737, 'Mehran': 5738, 'Meisner': 5739, 'Melinda': 5740, 'Melinte': 5741, 'Melvil': 5742, 'Mendoza': 5743, 'Mercer': 5744, 'Merkel': 5745, 'Merton': 5746, 'Mesopotamia': 5747, 'Meza': 5748, 'Mia': 5749, 'Midway': 5750, 'Mighty': 5751, 'Milana': 5752, 'Miley': 5753, 'Millar': 5754, 'Mille': 5755, 'Millicent': 5756, 'Millie': 5757, 'Mills': 5758, 'Milne': 5759, 'Milner': 5760, 'Milo': 5761, 'Mindbenders': 5762, 'Minelli': 5763, 'Mink': 5764, 'Minogue': 5765, 'Mirrors': 5766, 'Mitch': 5767, 'Mitchel': 5768, 'Mitsubishi': 5769, 'Mix': 5770, 'Moana': 5771, 'Mohammed': 5772, \"Moloka'i\": 5773, 'Monroe': 5774, 'Montano': 5775, 'Montaño': 5776, 'Monthly': 5777, 'Mopti': 5778, 'Moreno': 5779, 'Morissette': 5780, 'Moritz': 5781, 'Morristown': 5782, 'Moseley': 5783, 'Moses': 5784, 'Moss': 5785, 'Mottola': 5786, 'Mound': 5787, 'Mountbatten': 5788, 'Mt': 5789, 'Mudie': 5790, 'Mumbai': 5791, 'Mundaka': 5792, 'Muniz': 5793, 'Muralitharan': 5794, 'Murtala': 5795, 'Muslim': 5796, 'Musso': 5797, 'Muyingo': 5798, 'Myers': 5799, 'Mylod': 5800, 'Mysore': 5801, 'MØ': 5802, 'NADP': 5803, 'NADPH': 5804, 'NBC': 5805, 'NFL': 5806, 'NORAD': 5807, 'NY': 5808, 'Nadine': 5809, 'Nadiya': 5810, 'Nadu': 5811, 'Nair': 5812, 'Nam': 5813, 'NameOurShip': 5814, 'Nanny': 5815, 'Naples': 5816, 'Naranjo': 5817, 'Narmada': 5818, 'Narrenbeschwörung': 5819, 'Nas': 5820, 'Nassau': 5821, 'Nasseri': 5822, 'Natale': 5823, 'Nathanael': 5824, 'Nationalist': 5825, 'Navanethem': 5826, 'Navarro': 5827, 'Naveen': 5828, 'Navi': 5829, 'Naykia': 5830, 'Nazarene': 5831, 'Nazarēnus': 5832, 'Ndukwe': 5833, 'NeNe': 5834, 'Neely': 5835, 'Neeraj': 5836, 'Neill': 5837, 'Neolithic': 5838, 'Nepean': 5839, 'Nesbitt': 5840, 'Neuer': 5841, 'Neues': 5842, 'Neuilly': 5843, 'Never': 5844, 'Neymar': 5845, 'Nicco': 5846, 'Nicklaus': 5847, 'Niebuhr': 5848, 'Nielsen': 5849, 'Nigerian': 5850, 'Nikki': 5851, 'Nilsson': 5852, 'Nishant': 5853, 'Nobel': 5854, 'Nobles': 5855, 'Noise': 5856, 'None': 5857, 'Noni': 5858, 'Norsemen': 5859, 'Northeastern': 5860, 'Norwegian': 5861, 'Notebook': 5862, 'Numbers': 5863, 'Nunnally': 5864, 'Ní': 5865, \"O'Brian\": 5866, \"O'Donis\": 5867, \"O'Hara\": 5868, \"O'Kane\": 5869, \"O'Keefe\": 5870, \"O'Loughlin\": 5871, \"O'Neill\": 5872, \"O'ahu\": 5873, \"O'farrell\": 5874, 'OFC': 5875, 'Oct.': 5876, 'Odo': 5877, 'Oduber': 5878, 'Ogdensburg': 5879, 'Oklahoma': 5880, 'Olli': 5881, 'Once': 5882, 'Ong': 5883, 'Onorati': 5884, 'Ontario': 5885, 'Opening': 5886, 'Opposition': 5887, 'Oran': 5888, 'Orange': 5889, 'Oriakhi': 5890, 'Ornette': 5891, 'Orson': 5892, 'Osment': 5893, 'Osmon': 5894, 'Ostinato': 5895, 'Othmani': 5896, 'Otis': 5897, 'Outfield': 5898, 'Oval': 5899, 'Ownership': 5900, 'PAP': 5901, 'PC': 5902, 'PCS': 5903, 'Padmini': 5904, 'Page': 5905, 'Paine': 5906, 'Paing': 5907, 'Pak': 5908, 'Pale': 5909, 'Palestinian': 5910, 'Palmeiro': 5911, 'Pamelyn': 5912, 'Panama': 5913, 'Pandit': 5914, 'Pantera': 5915, 'Pape': 5916, 'Paramount': 5917, 'Pareidolia': 5918, 'Parish': 5919, 'Parrilla': 5920, 'Partridge': 5921, 'Passport': 5922, 'Passy': 5923, 'Past': 5924, 'Patchy': 5925, 'Paterson': 5926, 'Patmos': 5927, 'Patterson': 5928, 'Patty': 5929, 'Pavilion': 5930, 'Peasants': 5931, 'Peifer': 5932, 'Pence': 5933, 'Pench': 5934, 'Penetrating': 5935, 'Pennines': 5936, 'Pepperdine': 5937, 'Pepys': 5938, 'Perpetual': 5939, 'Perth': 5940, 'Pettersson': 5941, 'Photina': 5942, 'Phra': 5943, 'Picayune': 5944, 'Picon': 5945, 'Pierre': 5946, 'Pietersen': 5947, 'Pillay': 5948, 'Pimps': 5949, 'Pinxtones': 5950, 'Pitton': 5951, 'Plantagenet': 5952, 'Platine': 5953, 'Play': 5954, 'Plc': 5955, 'Plymouth': 5956, 'Pogba': 5957, 'Poitier': 5958, 'Poledouris': 5959, 'Polk': 5960, 'Polly': 5961, 'Polo': 5962, 'Ponting': 5963, 'Pooja': 5964, 'Poppy': 5965, 'Porsha': 5966, 'Prater': 5967, 'Prefecture': 5968, 'Prefer': 5969, 'Pressly': 5970, 'Priest': 5971, 'Pripyat': 5972, 'Prize': 5973, 'Procurement': 5974, 'Prodi': 5975, 'Profane': 5976, 'Professional': 5977, 'Proof': 5978, 'Protection': 5979, 'Protoliterate': 5980, 'Proulx': 5981, 'Province': 5982, 'Psalm': 5983, 'Public': 5984, 'Punk': 5985, 'Punta': 5986, 'Purcell': 5987, 'Putin': 5988, 'Pyeongchang': 5989, 'Pyramid': 5990, 'Pádraigín': 5991, 'Qin': 5992, 'Qualtrough': 5993, 'Quan': 5994, 'QuickBASIC': 5995, 'Quintessence': 5996, 'Quirós': 5997, 'Qusay': 5998, 'RKO': 5999, 'Raad': 6000, 'Race': 6001, 'Radbourn': 6002, 'Rafe': 6003, 'Rahul': 6004, 'Rai': 6005, 'Railway': 6006, 'Rainy': 6007, 'Rajasthan': 6008, 'Rall': 6009, 'Ramachandran': 6010, 'Rami': 6011, 'Ramona': 6012, 'Ramsey': 6013, 'Randall': 6014, 'Randamie': 6015, 'Raphaël': 6016, 'Raquel': 6017, 'Ras': 6018, 'Ratzinger': 6019, 'Ravi': 6020, 'Realm': 6021, 'Redbone': 6022, 'Redfield': 6023, 'Redvers': 6024, 'Refugee': 6025, 'Region': 6026, 'Reich': 6027, 'Reilly': 6028, 'Reinhold': 6029, 'Ren': 6030, 'Renault': 6031, 'Renshaw': 6032, 'Renteria': 6033, 'Republican': 6034, 'Research': 6035, 'Resorts': 6036, 'Reuter': 6037, 'Rex': 6038, 'Reyes': 6039, 'Rhea': 6040, 'Rhonda': 6041, 'Rhythm': 6042, 'Rica': 6043, 'Ricardo': 6044, 'Richards': 6045, 'Richter': 6046, 'Richthofen': 6047, 'Rickman': 6048, 'Rickon': 6049, 'Riggs': 6050, 'Rimsky': 6051, 'Ring': 6052, 'Riot': 6053, 'Rippon': 6054, 'Ritchie': 6055, 'Rivers': 6056, 'Riviera': 6057, 'Rivières': 6058, 'Robb': 6059, 'Robbie': 6060, 'Rockapella': 6061, 'Rocket': 6062, 'Rockets': 6063, 'Rockies': 6064, 'Rod': 6065, 'Rodriguez': 6066, 'Rodríguez': 6067, 'Rogen': 6068, 'Roiland': 6069, 'Rojer': 6070, 'Roker': 6071, 'Roll': 6072, 'Ronda': 6073, 'Roopa': 6074, 'Roper': 6075, 'Rosemary': 6076, 'Roses': 6077, 'Rossano': 6078, 'Rotor': 6079, 'Rouse': 6080, 'Rousey': 6081, 'Routhier': 6082, 'Roxanne': 6083, 'Roxy': 6084, 'RuPaul': 6085, 'Rudin': 6086, 'Rudolph': 6087, 'Ruhr': 6088, 'Runners': 6089, 'Rupsa': 6090, 'Rush': 6091, 'Ryūnosuke': 6092, 'Röntgen': 6093, 'Rēx': 6094, 'S': 6095, 'S.J.': 6096, 'S1': 6097, 'SDCCU': 6098, 'SH': 6099, 'Saadeddine': 6100, 'Saba': 6101, 'Sabazia': 6102, 'Salaria': 6103, 'Saltoro': 6104, 'Samhain': 6105, 'Samira': 6106, 'Samson': 6107, 'Sandin': 6108, 'Sandor': 6109, 'Sangster': 6110, 'Sanjay': 6111, 'Sansa': 6112, 'Santiago': 6113, 'Saoirse': 6114, 'Sarabhai': 6115, 'Sassette': 6116, 'Satine': 6117, 'Savery': 6118, 'Savile': 6119, 'Scampton': 6120, 'Scanavino': 6121, 'Scanlan': 6122, 'Schaal': 6123, 'Schatz': 6124, 'Scheff': 6125, 'Scheme': 6126, 'Schock': 6127, 'Schoeffling': 6128, 'Schumer': 6129, 'Schuster': 6130, 'Scodelario': 6131, 'Scooter': 6132, 'Scot': 6133, 'Scotland': 6134, 'Scranton': 6135, 'Scrappy': 6136, 'Scriven': 6137, 'Scully': 6138, 'Se': 6139, 'Seahawks': 6140, 'Seaview': 6141, 'Secor': 6142, 'Security': 6143, 'Sedella': 6144, 'Seeley': 6145, 'Seine': 6146, 'Selcap': 6147, 'Semon': 6148, 'Seninde': 6149, 'Senja': 6150, 'Sense': 6151, 'Seong': 6152, 'Sepse': 6153, 'Serafinowicz': 6154, 'Serbia': 6155, 'Serenity': 6156, 'Serge': 6157, 'Sertoli': 6158, 'Service': 6159, 'Services': 6160, 'Sessions': 6161, 'Set': 6162, 'Settle': 6163, 'Sevier': 6164, 'Shah': 6165, 'Shamea': 6166, 'Shanna': 6167, 'Shante': 6168, 'Sharad': 6169, 'Shaun': 6170, 'Shazwan': 6171, 'Shedd': 6172, 'Shelby': 6173, 'Shellback': 6174, 'Shereé': 6175, 'Sheridan': 6176, 'Sherilyn': 6177, 'Sherman': 6178, 'Sherri': 6179, 'Shikai': 6180, 'Shinichiro': 6181, 'Shot': 6182, 'Shukla': 6183, 'Shura': 6184, 'Siachen': 6185, 'Sidik': 6186, 'Siegfried': 6187, 'Sierra': 6188, 'Sikandar': 6189, 'Silentó': 6190, 'Silesia': 6191, 'Silverheels': 6192, 'Silverstone': 6193, 'Silvestri': 6194, 'Sinderby': 6195, 'Sinise': 6196, 'Sirius': 6197, 'Sistine': 6198, 'Sitka': 6199, 'Six': 6200, 'Skarsgård': 6201, 'Slate': 6202, 'Sloane': 6203, 'Slough': 6204, 'Small': 6205, 'Smithland': 6206, 'Smurfblossom': 6207, 'Smurfette': 6208, 'Smurfijade': 6209, 'Smurflily': 6210, 'Smurfmelody': 6211, 'Smurfstorm': 6212, 'Smurfwillow': 6213, 'Snider': 6214, 'Sobers': 6215, 'Social': 6216, 'Soers': 6217, 'Sofie': 6218, 'Solom': 6219, 'Sonia': 6220, 'Soucie': 6221, 'Sound': 6222, 'Southern': 6223, 'Southport': 6224, 'Spirit': 6225, 'Springbank': 6226, 'Spruce': 6227, 'Spurs': 6228, 'Squamish': 6229, 'Stackridge': 6230, 'Stafford': 6231, 'Stalin': 6232, 'Stark': 6233, 'Stassen': 6234, 'Steamhammer': 6235, 'Steelers': 6236, 'Stefan': 6237, 'Stefanik': 6238, 'Steinbeck': 6239, 'Stephens': 6240, 'Steppenwolf': 6241, 'Stevenson': 6242, 'Stilgoe': 6243, 'Stockard': 6244, 'Stoltz': 6245, 'Stoner': 6246, 'Stop': 6247, 'Strae': 6248, 'Straus': 6249, 'Strauss': 6250, 'Stripped': 6251, 'Stryker': 6252, 'Studio': 6253, 'Sue': 6254, 'Suffolk': 6255, 'Sulaiman': 6256, 'Sulby': 6257, 'Sully': 6258, 'Sumalee': 6259, 'Sumerian': 6260, 'Summit': 6261, 'Sumter': 6262, 'Sung': 6263, 'Suns': 6264, 'Sunweb': 6265, 'Supervision': 6266, 'Supervolcano': 6267, 'Supply': 6268, 'Sweet': 6269, 'Swinton': 6270, 'Swiss': 6271, 'Sylvester': 6272, 'Sánchez': 6273, 'Sèvres': 6274, 'Séchelles': 6275, 'TCU': 6276, 'TLC': 6277, 'TT': 6278, 'TV': 6279, 'Tabla': 6280, 'Tae': 6281, 'Taffy': 6282, 'Taft': 6283, 'Tagore': 6284, 'Tamil': 6285, 'Tamina': 6286, 'Tarapur': 6287, 'Tardif': 6288, 'Tarsus': 6289, 'Tata': 6290, 'Tatasciore': 6291, 'Tatsuya': 6292, 'Teas': 6293, 'Tecău': 6294, 'Telangana': 6295, 'Telekom': 6296, 'Temprees': 6297, 'Temuera': 6298, 'Tencent': 6299, 'Terence': 6300, 'Terminal': 6301, 'Terrebonne': 6302, 'Terrence': 6303, 'Tesori': 6304, 'Tessie': 6305, 'Thakuri': 6306, 'Thanisson': 6307, 'Thea': 6308, 'Theodosius': 6309, 'Thewlis': 6310, 'Thirteen': 6311, 'Thirty': 6312, 'Thomason': 6313, 'Three': 6314, 'Thumama': 6315, 'Thursdays': 6316, 'Tiggy': 6317, 'Tilzer': 6318, 'Timbuk': 6319, 'Timlin': 6320, 'Tino': 6321, 'Tippin': 6322, 'Titchener': 6323, 'Tobit': 6324, 'Toltec': 6325, 'Tomb': 6326, 'Toné': 6327, 'Torre': 6328, 'Torres': 6329, 'Torsen': 6330, 'Tortuguero': 6331, 'Toscano': 6332, 'Touchwood': 6333, 'Tournefort': 6334, 'Toyota': 6335, 'Train': 6336, 'Transamerica': 6337, 'Transvaal': 6338, 'Trestyn': 6339, 'Trey': 6340, 'Trick': 6341, 'Trip': 6342, 'Tristano': 6343, 'Triumphs': 6344, 'Trois': 6345, 'Trojans': 6346, 'Trotti': 6347, 'Trudy': 6348, 'Truist': 6349, 'Truth': 6350, 'Tré': 6351, 'Tsar': 6352, 'Tsewang': 6353, 'Tuck': 6354, 'Tufeld': 6355, 'Tunick': 6356, 'Tunisia': 6357, 'Turman': 6358, 'Turtle': 6359, 'Twenty': 6360, 'Twin': 6361, 'Twins': 6362, 'Tylo': 6363, 'Type': 6364, 'Tyra': 6365, 'Tyrian': 6366, 'Tyrion': 6367, 'Tzu': 6368, 'UEFA': 6369, 'USD': 6370, 'Uday': 6371, 'Ukulele': 6372, 'Ultron': 6373, 'Umeadi': 6374, 'Upanishad': 6375, 'Us': 6376, 'Usain': 6377, 'Utechin': 6378, 'Utility': 6379, 'Utrecht': 6380, 'Valeryevna': 6381, 'Valois': 6382, 'Vande': 6383, 'Vanshidhar': 6384, 'Varane': 6385, 'Varun': 6386, 'Vaughan': 6387, 'Vaughn': 6388, 'Vayntrub': 6389, 'Vedas': 6390, 'Vega': 6391, 'Venipuncture': 6392, 'Verde': 6393, 'Vermont': 6394, 'Vernacular': 6395, 'Vesnina': 6396, 'Vespasian': 6397, 'Vexy': 6398, 'Vickery': 6399, 'Victorian': 6400, 'Vidisha': 6401, 'Vieira': 6402, 'Villa': 6403, 'Villeneuve': 6404, 'Vin': 6405, 'Vincenzo': 6406, 'Vine': 6407, 'Virgin': 6408, 'Visa': 6409, 'VoiceStream': 6410, 'Volez': 6411, 'Von': 6412, 'Voroshilov': 6413, 'Vries': 6414, 'WW': 6415, 'Wainwright': 6416, 'Waite': 6417, 'Wallach': 6418, 'Walmart': 6419, 'Wang': 6420, 'Warrick': 6421, 'Watanabe': 6422, 'Watford': 6423, 'Watt': 6424, 'Webb': 6425, 'Webber': 6426, 'Webster': 6427, 'Weil': 6428, 'Weinrib': 6429, 'Weiss': 6430, 'Wellesley': 6431, 'Wen': 6432, 'Wengert': 6433, 'Wentworth': 6434, 'Westchester': 6435, 'Westen': 6436, 'Westinghouse': 6437, 'Westlake': 6438, 'Weston': 6439, 'Wharton': 6440, 'Wheen': 6441, 'Whiley': 6442, 'Whistle': 6443, 'Whiteley': 6444, 'Whitey': 6445, 'Whiting': 6446, 'Whitley': 6447, 'Whitmore': 6448, 'Whitten': 6449, 'Whole': 6450, 'Whoopi': 6451, 'Wicket': 6452, 'Wight': 6453, 'Wiley': 6454, 'Wilkinson': 6455, 'Willis': 6456, 'Willow': 6457, 'Wilmington': 6458, 'Wily': 6459, 'Winchester': 6460, 'Windu': 6461, 'Winwood': 6462, 'Wireless': 6463, 'Wisdom': 6464, 'Wishes': 6465, 'Wittet': 6466, 'Wolfram': 6467, 'Wolter': 6468, 'Wolverines': 6469, 'Woman': 6470, 'Woo': 6471, 'Wormer': 6472, 'Worthington': 6473, 'Wow': 6474, 'Wroblewitz': 6475, 'Wundt': 6476, 'Wupper': 6477, 'Wymark': 6478, 'XI': 6479, 'XIX': 6480, 'XXVI': 6481, 'XXXIX': 6482, 'Y.': 6483, 'Yangtze': 6484, 'Yann': 6485, 'Yates': 6486, 'Year': 6487, 'Yellow': 6488, 'Yente': 6489, 'Yoakam': 6490, 'Yoko': 6491, 'Yondu': 6492, 'Yuan': 6493, 'Yugoslavia': 6494, 'Yui': 6495, 'Yul': 6496, 'Yunnan': 6497, 'Yuvraj': 6498, 'Zara': 6499, 'Zaw': 6500, 'Zazai': 6501, 'Zeid': 6502, 'Zerubbabel': 6503, 'Zhané': 6504, 'Ziegler': 6505, 'Zlatan': 6506, 'Zoe': 6507, 'Zolciak': 6508, 'Zora': 6509, 'Zradicka': 6510, 'abscissa': 6511, 'acid': 6512, 'acting': 6513, 'activity': 6514, 'administrative': 6515, 'administrator': 6516, 'agent': 6517, 'agnatic': 6518, 'airdate': 6519, 'allies': 6520, 'alphabet': 6521, 'alternating': 6522, 'ampere': 6523, 'ampulla': 6524, 'anchor': 6525, 'androgen': 6526, 'anointed': 6527, 'antagonistic': 6528, 'antiX': 6529, 'apparent': 6530, 'appendix': 6531, 'apple': 6532, 'art': 6533, 'article': 6534, 'ask': 6535, 'assassins': 6536, 'assistance': 6537, 'asteroid': 6538, 'attempt': 6539, 'aurantia': 6540, 'auxiliaries': 6541, 'award': 6542, 'azu': 6543, 'ban': 6544, 'band': 6545, 'basin': 6546, 'bassist': 6547, 'bc': 6548, 'beaches': 6549, 'beaver': 6550, 'became': 6551, 'because': 6552, 'become': 6553, 'bell': 6554, 'bergamot': 6555, 'beta': 6556, 'bid': 6557, 'bin': 6558, 'birth': 6559, 'blackness': 6560, 'bloc': 6561, 'blues': 6562, 'blurred': 6563, 'boilermaker': 6564, 'bomber': 6565, 'books': 6566, 'boost': 6567, 'boundaries': 6568, 'bourgeoisie': 6569, 'boxer': 6570, 'brachiocephalic': 6571, 'bradycardia': 6572, 'brain': 6573, 'broadens': 6574, 'broken': 6575, 'burning': 6576, 'buy': 6577, 'c.1944': 6578, 'came': 6579, 'can': 6580, 'canal': 6581, 'canisters': 6582, 'capitalised': 6583, 'caregiver': 6584, 'cats': 6585, 'ceiling': 6586, 'centred': 6587, 'certain': 6588, 'charcoal': 6589, 'check': 6590, 'church': 6591, 'chō': 6592, 'cigarettes': 6593, 'citizens': 6594, 'citrus': 6595, 'civil': 6596, 'classical': 6597, 'clearing': 6598, 'cochineal': 6599, 'coffins': 6600, 'cold': 6601, 'collection': 6602, 'commission': 6603, 'common': 6604, 'concealed': 6605, 'concluding': 6606, 'conduct': 6607, 'confluence': 6608, 'connecting': 6609, 'connective': 6610, 'connects': 6611, 'consist': 6612, 'construction': 6613, 'consumption': 6614, 'continents': 6615, 'conus': 6616, 'convergent': 6617, 'cooler': 6618, 'cord': 6619, 'countries': 6620, 'countryside': 6621, 'courtyard': 6622, 'created': 6623, 'creation': 6624, 'creator': 6625, 'critical': 6626, 'cuba': 6627, 'culture': 6628, 'curricula': 6629, 'cytoplasm': 6630, 'dancers': 6631, 'dark': 6632, 'database': 6633, 'databases': 6634, 'dedicated': 6635, 'deficiency': 6636, 'democracy': 6637, 'democratic': 6638, 'depth': 6639, 'des': 6640, 'desert': 6641, 'deuterocanonical': 6642, 'diastereomers': 6643, 'differentiated': 6644, 'dinosaur': 6645, 'direct': 6646, 'directly': 6647, 'director': 6648, 'disappeared': 6649, 'disc': 6650, 'discoverers': 6651, 'divergent': 6652, 'divine': 6653, 'dog': 6654, 'dogs': 6655, 'domestication': 6656, 'doo': 6657, 'drama': 6658, 'dustbin': 6659, 'dustcart': 6660, 'eastern': 6661, 'eavesden': 6662, 'economically': 6663, 'egg': 6664, 'eggs': 6665, 'eighteen': 6666, 'electors': 6667, 'elephant': 6668, 'emission': 6669, 'emperor': 6670, 'enantiomers': 6671, 'endometrium': 6672, 'engine': 6673, 'enterocytes': 6674, 'entrances': 6675, 'epidermis': 6676, 'episodic': 6677, 'escape': 6678, 'evacuated': 6679, 'even': 6680, 'event': 6681, 'every': 6682, 'everyman': 6683, 'exceeds': 6684, 'executive': 6685, 'exhibit': 6686, 'extended': 6687, 'extensive': 6688, 'extreme': 6689, 'facilitated': 6690, 'faction': 6691, 'fallopian': 6692, 'families': 6693, 'farm': 6694, 'female': 6695, 'fertilization': 6696, 'few': 6697, 'fiberglass': 6698, 'fighting': 6699, 'financial': 6700, 'flat': 6701, 'flood': 6702, 'flopping': 6703, 'flowing': 6704, 'follower': 6705, 'following': 6706, 'forces': 6707, 'fraction': 6708, 'free': 6709, 'freedom': 6710, 'frescos': 6711, 'functionary': 6712, 'further': 6713, 'gameplay': 6714, 'ganjira': 6715, 'gas': 6716, 'gestation': 6717, 'glass': 6718, 'goaltending': 6719, 'governing': 6720, 'grandmother': 6721, 'gravitational': 6722, 'green': 6723, 'ground': 6724, 'grown': 6725, 'growth': 6726, 'guilder': 6727, 'had': 6728, 'ham': 6729, 'handkerchief': 6730, 'hardware': 6731, 'heir': 6732, 'hematite': 6733, 'heritage': 6734, 'hieroglyph': 6735, 'high': 6736, 'himself': 6737, 'ho': 6738, 'hoon': 6739, 'horticulture': 6740, 'house': 6741, 'humanity': 6742, 'hwan': 6743, 'hwi': 6744, 'hypothetical': 6745, 'ileum': 6746, 'illegitimate': 6747, 'image': 6748, 'immune': 6749, 'imperial': 6750, 'included': 6751, 'influenced': 6752, 'inspirational': 6753, 'installment': 6754, 'instead': 6755, 'insurance': 6756, 'intentionally': 6757, 'international': 6758, 'internet': 6759, 'interspinous': 6760, 'irregular': 6761, 'items': 6762, 'jejunum': 6763, 'ji': 6764, 'joining': 6765, 'jointed': 6766, 'juice': 6767, 'junta': 6768, 'kevlar': 6769, 'kick': 6770, 'kicked': 6771, 'killed': 6772, 'kilogram': 6773, 'kin': 6774, 'kings': 6775, 'ku': 6776, 'lamina': 6777, 'landlocked': 6778, 'landscaping': 6779, 'lane': 6780, 'language': 6781, 'law': 6782, 'lead': 6783, 'leave': 6784, 'legislative': 6785, 'legs': 6786, 'ler': 6787, 'letter': 6788, 'letters': 6789, 'ligature': 6790, 'lin': 6791, 'lining': 6792, 'literary': 6793, 'live': 6794, 'longest': 6795, 'lorry': 6796, 'losing': 6797, 'love': 6798, 'lumbar': 6799, 'luminous': 6800, 'magnetite': 6801, 'maintenance': 6802, 'major': 6803, 'making': 6804, 'man': 6805, 'manufacturing': 6806, 'march': 6807, 'marmots': 6808, 'mass': 6809, 'meaning': 6810, 'medullaris': 6811, 'mentioned': 6812, 'method': 6813, 'metre': 6814, 'mick': 6815, 'mid-1940s': 6816, 'mid-1997': 6817, 'middle-': 6818, 'midnight': 6819, 'midway': 6820, 'millennium': 6821, 'min': 6822, 'miners': 6823, 'mining': 6824, 'minister': 6825, 'minor': 6826, 'minstrelsy': 6827, 'mitochondria': 6828, 'mitosis': 6829, 'mix': 6830, 'model': 6831, 'monarchy': 6832, 'monster': 6833, 'months': 6834, 'morale': 6835, 'more': 6836, 'move': 6837, 'movie': 6838, 'mud': 6839, 'multilingual': 6840, 'multiplier': 6841, 'mya': 6842, 'nationalistic': 6843, 'nations': 6844, 'nature': 6845, 'naïve': 6846, 'need': 6847, 'networking': 6848, 'neurology': 6849, 'neutron': 6850, 'never': 6851, 'nichts': 6852, 'nickel': 6853, 'nitrogen': 6854, 'non': 6855, 'none': 6856, 'noodles': 6857, 'normally': 6858, 'northeastern': 6859, 'northwestern': 6860, 'novelty': 6861, 'nucleon': 6862, 'nwa': 6863, 'office': 6864, 'often': 6865, 'oil': 6866, 'only': 6867, 'ordinate': 6868, 'organised': 6869, 'organizations': 6870, 'oversee': 6871, 'ovule': 6872, 'owned': 6873, 'owners': 6874, 'package': 6875, 'pancreas': 6876, 'pancreatic': 6877, 'parasitism': 6878, 'part': 6879, 'passing': 6880, 'patriarchal': 6881, 'patronymic': 6882, 'pedosphere': 6883, 'penalty': 6884, 'peninsula': 6885, 'performance': 6886, 'period': 6887, 'persons': 6888, 'physical': 6889, 'plain': 6890, 'planetary': 6891, 'play': 6892, 'poll': 6893, 'polyglot': 6894, 'popular': 6895, 'population': 6896, 'powers': 6897, 'preparing': 6898, 'prevent': 6899, 'primeval': 6900, 'primogeniture': 6901, 'princess': 6902, 'produced': 6903, 'production': 6904, 'pronunciation': 6905, 'protected': 6906, 'proverb': 6907, 'publication': 6908, 'purpose': 6909, 'qualify': 6910, 'quarter': 6911, 'question': 6912, 'quote': 6913, 'rabbit': 6914, 'races': 6915, 'radiant': 6916, 'radiocarbon': 6917, 'rain': 6918, 'ranch': 6919, 'ratified': 6920, 'ray': 6921, 'rebel': 6922, 'recipients': 6923, 'recombination': 6924, 'refer': 6925, 'refinery': 6926, 'regions': 6927, 'registered': 6928, 'removed': 6929, 'renewable': 6930, 'require': 6931, 'requirement': 6932, 'resembling': 6933, 'resigns': 6934, 'resisting': 6935, 'respective': 6936, 'restatement': 6937, 'retaken': 6938, 'rich': 6939, 'roof': 6940, 'roughly': 6941, 'routers': 6942, 'sacred': 6943, 'sacrificed': 6944, 'safety': 6945, 'sake': 6946, 'saliva': 6947, 'same': 6948, 'saved': 6949, 'scattered': 6950, 'science': 6951, 'seat': 6952, 'seawater': 6953, 'second^2': 6954, 'secretary': 6955, 'sections': 6956, 'sector': 6957, 'sedan': 6958, 'sedimentary': 6959, 'seed': 6960, 'segments': 6961, 'self': 6962, 'sell': 6963, 'selling': 6964, 'serratus': 6965, 'severe': 6966, 'shared': 6967, 'shareholders': 6968, 'shifting': 6969, 'shipping': 6970, 'short': 6971, 'sides': 6972, 'singer': 6973, 'sitting': 6974, 'situations': 6975, 'skill': 6976, 'slope': 6977, 'slopes': 6978, 'slow': 6979, 'smile': 6980, 'solstice': 6981, 'song': 6982, 'soon': 6983, 'southeast': 6984, 'southern': 6985, 'southwestern': 6986, 'space': 6987, 'specialties': 6988, 'species': 6989, 'spectral': 6990, 'speech': 6991, 'spider': 6992, 'spotted': 6993, 'stable': 6994, 'stars': 6995, 'station': 6996, 'storm': 6997, 'striking': 6998, 'student': 6999, 'studies': 7000, 'style': 7001, 'suppleness': 7002, 'suppressed': 7003, 'sur': 7004, 'swarthy': 7005, 'swiftness': 7006, 'symbolizes': 7007, 'sympathetic': 7008, 'syndrome': 7009, 'synthetic': 7010, 'systems': 7011, 'tag': 7012, 'tank': 7013, 'taunting': 7014, 'taxed': 7015, 'terrestrial': 7016, 'testes': 7017, 'testing': 7018, 'thalassemia': 7019, 'theonomy': 7020, 'therapist': 7021, 'therefore': 7022, 'thickness': 7023, 'threatening': 7024, 'throughout': 7025, 'throw': 7026, 'tiny': 7027, 'tissues': 7028, 'tobacco': 7029, 'together': 7030, 'tops': 7031, 'tortured': 7032, 'total': 7033, 'totalitarian': 7034, 'towns': 7035, 'troposphere': 7036, 'true': 7037, 'tube': 7038, 'twelve': 7039, 'type': 7040, 'ul': 7041, 'un': 7042, 'unsportsmanlike': 7043, 'urban': 7044, 'urine': 7045, 'usherette': 7046, 'uterus': 7047, 'vein': 7048, 'venom': 7049, 'ventralis': 7050, 'vermiform': 7051, 'version': 7052, 'vertebrae': 7053, 'vikings': 7054, 'villi': 7055, 'visa': 7056, 'visualization': 7057, 'vocabulary': 7058, 'von': 7059, 'wall': 7060, 'wartime': 7061, 'waters': 7062, 'way': 7063, 'weary': 7064, 'week': 7065, 'weir': 7066, 'wheat': 7067, 'widow': 7068, 'wild': 7069, 'will': 7070, 'winter': 7071, 'wireless': 7072, 'woo': 7073, 'woodland': 7074, 'wool': 7075, 'woon': 7076, 'worker': 7077, 'wreck': 7078, 'written': 7079, 'yellow': 7080, 'yeon': 7081, 'yes': 7082, 'you': 7083, 'zebra': 7084, 'Île': 7085, 'Österreichische': 7086, 'Čas': 7087, 'Škrtel': 7088, 'اسلامی': 7089, 'افغانستان': 7090, 'د': 7091, 'دولت': 7092, 'شورای': 7093, 'ملی': 7094, '+22': 7095, '+394421': 7096, '-0': 7097, '00': 7098, '000': 7099, '00′': 7100, '01906': 7101, '05h': 7102, '09/10/1995': 7103, '1,256': 7104, '1,600': 7105, '1/2': 7106, '100–110': 7107, '102': 7108, '103,875–124,232': 7109, '1045903': 7110, '10th': 7111, '11,2018': 7112, '115': 7113, '118': 7114, '11–14': 7115, '12-October-2005': 7116, '1215': 7117, '122': 7118, '12:33:51': 7119, '13,017': 7120, '1300': 7121, '1330s': 7122, '1362': 7123, '13′21″W': 7124, '143': 7125, '1447': 7126, '15,1987': 7127, '1517': 7128, '1524': 7129, '1546': 7130, '1549': 7131, '155': 7132, '1560': 7133, '16,1987': 7134, '1608': 7135, '161–177': 7136, '164': 7137, '169': 7138, '1699': 7139, '16–18': 7140, '16′29″E': 7141, '17,000': 7142, '17.5': 7143, '1707': 7144, '1726': 7145, '1740': 7146, '1756': 7147, '1769': 7148, '1787': 7149, '1788': 7150, '1798': 7151, '17:8–16': 7152, '18,800': 7153, '1812': 7154, '1830s': 7155, '1841': 7156, '1845': 7157, '1846': 7158, '184th': 7159, '1859': 7160, '1868': 7161, '1870s': 7162, '1874': 7163, '1886': 7164, '1889': 7165, '1894': 7166, '19,101': 7167, '19,950': 7168, '1902': 7169, '1903': 7170, '1919': 7171, '1923': 7172, '1929': 7173, '1946–1958': 7174, '1952': 7175, '1961–62': 7176, '1965–1980': 7177, '1967,1970': 7178, '1970–71': 7179, '1994–95': 7180, '1995–1998': 7181, '1999–00': 7182, '1⁄2': 7183, '1⁄4': 7184, '2,562–2,900': 7185, '2-D': 7186, '2013–present': 7187, '2018-December': 7188, '2018–19': 7189, '2026': 7190, '214': 7191, '21:6': 7192, '223': 7193, '22–26': 7194, '233': 7195, '260,897': 7196, '26′00″E': 7197, '28th': 7198, '28–29': 7199, '29.1': 7200, '2:3': 7201, '2–3': 7202, '3.5': 7203, '30,2010': 7204, '3000': 7205, '31,419': 7206, '31.94s': 7207, '34.98': 7208, '350': 7209, '353': 7210, '362': 7211, '371': 7212, '393': 7213, '3d': 7214, '4,200': 7215, '4,294,967,296': 7216, '4.3': 7217, '41′15″N': 7218, '428': 7219, '441': 7220, '4568.2': 7221, '45′4″N': 7222, '4898': 7223, '4–11': 7224, '5-pin': 7225, '51': 7226, '512': 7227, '52.2″': 7228, '530': 7229, '55425': 7230, '58.933': 7231, '59′10″N': 7232, '5:30': 7233, '6.18': 7234, '67': 7235, '7.53': 7236, '71': 7237, '74': 7238, '741': 7239, '75': 7240, '76': 7241, '76.3': 7242, '776': 7243, '8:05': 7244, '8:20': 7245, '9,974': 7246, '91350': 7247, '91390': 7248, '92': 7249, '93': 7250, '949': 7251, '9:43': 7252, ';': 7253, \"A'ja\": 7254, 'ABCCBA': 7255, 'AC': 7256, 'AM': 7257, 'AO': 7258, 'AST': 7259, 'ATP': 7260, 'AUH': 7261, 'AUS': 7262, 'AZ': 7263, 'Abagnale': 7264, 'Abdulmanapovich': 7265, 'Abreu': 7266, 'Absolute': 7267, 'Absolutely': 7268, 'Accommodation': 7269, 'Achilleus': 7270, 'Aconcagua': 7271, 'Acronym': 7272, 'Acrostic': 7273, 'Ada': 7274, 'Adelantado': 7275, 'Aditi': 7276, 'Adl': 7277, 'Afghan': 7278, 'Afton': 7279, 'Aga': 7280, 'Agatha': 7281, 'Agency': 7282, 'Agustín': 7283, 'Akane': 7284, 'Alam': 7285, 'Alamo': 7286, 'Alaric': 7287, 'Albarn': 7288, 'Aldean': 7289, 'Aldine': 7290, 'Aldridge': 7291, 'Aldrin': 7292, 'Alessandro': 7293, 'Alexandre': 7294, 'Allende': 7295, 'Allman': 7296, 'Almsgiving': 7297, 'Aloha': 7298, 'Aloysius': 7299, 'Alpo': 7300, 'Alps': 7301, 'Alternative': 7302, 'Aluminum': 7303, 'Amal': 7304, 'Amber': 7305, 'Amenorrhea': 7306, 'Americas': 7307, 'Amphion': 7308, 'An': 7309, 'Andrea': 7310, 'Angel': 7311, 'Angelica': 7312, 'Angellica': 7313, 'Angus': 7314, 'Anikulapo': 7315, 'Ankrum': 7316, 'Anshuman': 7317, 'Ansoff': 7318, 'Antiquity': 7319, 'Antonin': 7320, 'Antonoff': 7321, 'Anything': 7322, 'Apatow': 7323, 'Apes': 7324, 'Approximately': 7325, 'Appuleius': 7326, 'Ara': 7327, 'Aram': 7328, 'Arenado': 7329, 'Arenberg': 7330, 'Aristotelian': 7331, 'Aristotelianism': 7332, 'Arlanda': 7333, 'Arnel': 7334, 'Arroyo': 7335, 'Art': 7336, 'Arvand': 7337, 'Ashe': 7338, 'Asher': 7339, 'Ashly': 7340, 'Ashnoor': 7341, 'Ashok': 7342, 'Ashoka': 7343, 'Aspartate': 7344, 'Assad': 7345, 'Assam': 7346, 'Assassin': 7347, 'Association': 7348, 'Atal': 7349, 'Athenian': 7350, 'Athlone': 7351, 'Atkin': 7352, 'Atmosphere': 7353, 'Atomic': 7354, 'Attainder': 7355, 'Audra': 7356, 'Auger': 7357, 'Augu': 7358, 'Augustin': 7359, 'Auliʻi': 7360, 'Aurangzeb': 7361, 'Austrian': 7362, 'Autumn': 7363, 'Avenues': 7364, 'Avonlea': 7365, 'Ayyangar': 7366, 'Azie': 7367, 'Azikiwe': 7368, 'Aztecs': 7369, 'B1': 7370, 'BB': 7371, 'BCA': 7372, 'BEM': 7373, 'BMW': 7374, 'Babasaheb': 7375, 'Babel': 7376, 'Baby': 7377, 'Babylon': 7378, 'Backer': 7379, 'Bailee': 7380, 'Bald': 7381, 'Balraj': 7382, 'Bamford': 7383, 'BanX': 7384, 'Bangladesh': 7385, 'Barbé': 7386, 'Bardi': 7387, 'Barracks': 7388, 'Base': 7389, 'Bashar': 7390, 'Bates': 7391, 'Bautista': 7392, 'Bavarian': 7393, 'Bavier': 7394, 'Bayes': 7395, 'Beals': 7396, 'Beast': 7397, 'Beatrice': 7398, 'Beatty': 7399, 'Beau': 7400, 'Beauty': 7401, 'Beavers': 7402, 'Beckenbauer': 7403, 'Becky': 7404, 'Beefeaters': 7405, 'Beeson': 7406, 'Beevi': 7407, 'Behan': 7408, 'Behrens': 7409, 'Belles': 7410, 'Bengt': 7411, 'Bennet': 7412, 'Bennington': 7413, 'Berger': 7414, 'Bernardino': 7415, 'Bernini': 7416, 'Besson': 7417, 'Bethsaida': 7418, 'Betto': 7419, 'Bettye': 7420, 'Bev': 7421, 'Beverley': 7422, 'Bezzoubenko': 7423, 'Bhabha': 7424, 'Bhagiratha': 7425, 'Bharata': 7426, 'Bhaskar': 7427, 'Bible': 7428, 'Bidet': 7429, 'Bihari': 7430, 'Bills': 7431, 'Bilson': 7432, 'Bizzy': 7433, 'Blanc': 7434, 'Blanchflower': 7435, 'Bland': 7436, 'Blight': 7437, 'Blondie': 7438, 'Blues': 7439, 'Blériot': 7440, 'Bocephus': 7441, 'Bodett': 7442, 'Body': 7443, 'Boeck': 7444, 'Boeing': 7445, 'Bogataj': 7446, 'Boggess': 7447, 'Bologna': 7448, 'Bolshakova': 7449, 'Bolsheviks': 7450, 'Bonaparte': 7451, 'Boomer': 7452, 'Boomerang': 7453, 'Boopadoop': 7454, 'Borden': 7455, 'Borgo': 7456, 'Borlaug': 7457, 'Born': 7458, 'Borneo': 7459, 'Bosa': 7460, 'Boseman': 7461, 'Boshū': 7462, 'Bosnian': 7463, 'Botticelli': 7464, 'Boutin': 7465, 'Bow': 7466, 'Bowell': 7467, 'Bowling': 7468, 'Boxing': 7469, 'Boy': 7470, 'Boyce': 7471, 'Bracciano': 7472, 'Bracknell': 7473, 'Bradshaw': 7474, 'Brahmacharini': 7475, 'Brain': 7476, 'Brando': 7477, 'Brass': 7478, 'Brave': 7479, 'Bread': 7480, 'Break': 7481, 'Bree': 7482, 'Brenton': 7483, 'Brewing': 7484, 'Bridal': 7485, 'Brilliant': 7486, 'Britannia': 7487, 'Broadway': 7488, 'Brogdon': 7489, 'Brook': 7490, 'Bruddah': 7491, 'Brumley': 7492, 'Brun': 7493, 'Brunson': 7494, 'Brussels': 7495, 'Bryce': 7496, 'Bryson': 7497, 'Buchanan': 7498, 'Buchholz': 7499, 'Buddh': 7500, 'Buddha': 7501, 'Buddy': 7502, 'Bulldog': 7503, 'Buonarroti': 7504, 'Burdon': 7505, 'Bure': 7506, 'Burrill': 7507, 'Butea': 7508, 'Buttes': 7509, 'Béguyer': 7510, 'C.I.': 7511, 'C.R.': 7512, 'C6H12O6': 7513, 'CF': 7514, 'CH': 7515, 'CHFRS': 7516, 'CQ': 7517, 'CSULA': 7518, 'CTV': 7519, 'Cabinet': 7520, 'Cabrillo': 7521, 'Cactus': 7522, 'Cadillac': 7523, 'Cai': 7524, 'Caligula': 7525, 'Callies': 7526, 'Calloway': 7527, 'Caltech': 7528, 'Calvin': 7529, 'Cam': 7530, 'Camberley': 7531, 'Campbell': 7532, 'Cancer': 7533, 'Caoife': 7534, 'Capercaillie': 7535, 'Capital': 7536, 'Capitals': 7537, 'Caprio': 7538, 'Capt': 7539, 'Car': 7540, 'Cardle': 7541, 'Carly': 7542, 'Carmichael': 7543, 'Carradine': 7544, 'Carrie': 7545, 'Cars': 7546, 'Carta': 7547, 'Cartersville': 7548, 'Casablanca': 7549, 'Casey': 7550, 'Category': 7551, 'Cattle': 7552, 'Caucasus': 7553, 'Cavendish': 7554, 'Cavitt': 7555, 'Celeste': 7556, 'Cellier': 7557, 'Cells': 7558, 'Chadwick': 7559, 'Chaka': 7560, 'Chakravarthi': 7561, 'Chakravarti': 7562, 'Challenge': 7563, 'Chamling': 7564, 'Champaign': 7565, 'Championship': 7566, 'Chandler': 7567, 'Chandra': 7568, 'Chandraghanta': 7569, 'Charlemagne': 7570, 'Cheez': 7571, 'Chemjong': 7572, 'Cheque': 7573, 'Chester': 7574, 'Cheswick': 7575, 'Chevy': 7576, 'Chimanimani': 7577, 'Chlumsky': 7578, 'Choldenko': 7579, 'Christa': 7580, 'Christal': 7581, 'Christoffel': 7582, 'Chukwu': 7583, 'Circuit': 7584, 'Ciudad': 7585, 'Clapton': 7586, 'Claver': 7587, 'Clawson': 7588, 'Clayton': 7589, 'Cleary': 7590, 'Clifford': 7591, 'Clufetos': 7592, 'Code': 7593, 'Coe': 7594, 'Coercive': 7595, 'Coitus': 7596, 'Cold': 7597, 'Coles': 7598, 'Collabro': 7599, 'Cologne': 7600, 'Colonel': 7601, 'Colonies': 7602, 'Colón': 7603, 'Commissioners': 7604, 'Comorin': 7605, 'CompactFlash': 7606, 'Compiègne': 7607, 'Compston': 7608, 'Computer': 7609, 'Condit': 7610, 'Conformity': 7611, 'Congregation': 7612, 'Constitutional': 7613, 'Continuous': 7614, 'Contracts': 7615, 'Copland': 7616, 'Copperfield': 7617, 'Corden': 7618, 'Corporate': 7619, 'Cosby': 7620, 'Costas': 7621, 'Cott': 7622, 'Cottrell': 7623, 'Courier': 7624, 'Cowell': 7625, 'Coyle': 7626, 'Crash': 7627, 'Creatine': 7628, 'Creed': 7629, 'Creem': 7630, 'Crescent': 7631, 'Criminal': 7632, 'Crohn': 7633, 'Crumpit': 7634, 'Culross': 7635, 'Cunégonde': 7636, 'Curb': 7637, 'Cyrene': 7638, 'Cytoplasm': 7639, 'DBAs': 7640, 'DEFFED': 7641, 'DJ': 7642, 'DMC': 7643, 'DMX': 7644, 'DNA': 7645, 'DVC': 7646, 'Dada': 7647, 'Dadasaheb': 7648, 'Dainik': 7649, 'Daisy': 7650, 'Daiya': 7651, 'Dak': 7652, 'Danae': 7653, 'Danai': 7654, 'Dania': 7655, 'Danneel': 7656, 'Dapidran': 7657, 'Darbuka': 7658, 'Daredevil': 7659, 'Darin': 7660, 'Darleen': 7661, 'Darlene': 7662, 'Davenport': 7663, 'Dawan': 7664, 'Dawnn': 7665, 'Dawson': 7666, 'Daz': 7667, 'DeLorean': 7668, 'DeLorenzo': 7669, 'DePrima': 7670, 'DeWitt': 7671, 'Death': 7672, 'Debbie': 7673, 'Debby': 7674, 'Debicki': 7675, 'Debra': 7676, 'Delk': 7677, 'Dells': 7678, 'Delorean': 7679, 'Demetria': 7680, 'Demonte': 7681, 'Dench': 7682, 'Dent': 7683, 'Deon': 7684, 'Deoxyribonucleic': 7685, 'Desai': 7686, 'Desalinated': 7687, 'Desert': 7688, 'Deutsches': 7689, 'Devenski': 7690, 'Devonian': 7691, 'Devonne': 7692, 'Dewan': 7693, 'Dhami': 7694, 'Dharma': 7695, 'Di': 7696, 'DiCaprio': 7697, 'DiMarco': 7698, 'Diablos': 7699, 'Diadophis': 7700, 'Diagnosis': 7701, 'Dian': 7702, 'Dichen': 7703, 'Diehl': 7704, 'Digital': 7705, 'Digman': 7706, 'Dimas': 7707, 'Dinaric': 7708, 'Diocles': 7709, 'Dion': 7710, 'Dionne': 7711, 'Dire': 7712, 'Djeser': 7713, 'Djibouti': 7714, 'Djoser': 7715, 'Dobbs': 7716, 'Dobro': 7717, 'Doctrine': 7718, 'Doeschate': 7719, 'Dogg': 7720, 'Dolphins': 7721, 'Dom': 7722, 'Dominic': 7723, 'Donato': 7724, 'Doolan': 7725, 'Doors': 7726, 'Dophalene': 7727, 'Dorje': 7728, 'Dortmund': 7729, 'Doubleday': 7730, 'Downes': 7731, 'Doyle': 7732, 'Draft': 7733, 'Drashti': 7734, 'Driver': 7735, 'Dualers': 7736, 'Dub': 7737, 'Duczmal': 7738, 'Duggan': 7739, 'Duritz': 7740, 'Dwayne': 7741, 'Düsseldorf': 7742, 'E.G.': 7743, 'E21': 7744, 'EDT': 7745, 'EM': 7746, 'EMR': 7747, 'EPA': 7748, 'ESPN': 7749, 'Earp': 7750, 'Earvin': 7751, 'Eatenton': 7752, 'Echosmith': 7753, 'Edgar': 7754, 'Edmonds': 7755, 'Educational': 7756, 'Efren': 7757, 'Efron': 7758, 'Eileen': 7759, 'Eiriksson': 7760, 'Eisenhower': 7761, 'Eisenmann': 7762, 'Eldredge': 7763, 'Elfman': 7764, 'Elijah': 7765, 'Elizondo': 7766, 'Ella': 7767, 'Ellen': 7768, 'Ellie': 7769, 'Elliot': 7770, 'Elvis': 7771, 'Emil': 7772, 'Emirates': 7773, 'Emmanuel': 7774, 'Emmanuelle': 7775, 'Emotional': 7776, 'Ends': 7777, 'Enlightened': 7778, 'Enola': 7779, 'Environmental': 7780, 'Equal': 7781, 'Eradicator': 7782, 'Erbe': 7783, 'Ericson': 7784, 'Erikson': 7785, 'Erin': 7786, 'Essendon': 7787, 'Estelle': 7788, 'Estes': 7789, 'Estevez': 7790, 'Estonia': 7791, 'Estridge': 7792, 'Euclid': 7793, 'European': 7794, 'Eurowings': 7795, 'Evie': 7796, 'Excavators': 7797, 'Experimental': 7798, 'Extensible': 7799, 'Ezra': 7800, 'FA': 7801, 'FCIP': 7802, 'FRS': 7803, 'Fab': 7804, 'Fabrizio': 7805, 'Faceless': 7806, 'Factors': 7807, 'Faggin': 7808, 'Fairley': 7809, 'Faison': 7810, 'Fakhr': 7811, 'Falkor': 7812, 'Fanny': 7813, 'Fans': 7814, 'Fantasy': 7815, 'Faranan': 7816, 'Fascinator': 7817, 'Fasting': 7818, 'Fatima': 7819, 'Fator': 7820, 'Fauntleroy': 7821, 'Fauves': 7822, 'Feat': 7823, 'Federico': 7824, 'Federkiewicz': 7825, 'Felder': 7826, 'Fennoy': 7827, 'Fertile': 7828, 'Fevola': 7829, 'Fiddle': 7830, 'Filipepi': 7831, 'Filippo': 7832, 'Finland': 7833, 'Firkus': 7834, 'Fischer': 7835, 'Flags': 7836, 'Flash': 7837, 'Flats': 7838, 'Flattened': 7839, 'Flav': 7840, 'Flavor': 7841, 'Flax': 7842, 'Flay': 7843, 'Fleet': 7844, 'Flemyng': 7845, 'Flesh': 7846, 'Foch': 7847, 'Fogle': 7848, 'Fold': 7849, 'Fonsi': 7850, 'Fontaine': 7851, 'Foody': 7852, 'For': 7853, 'Forces': 7854, 'Foreman': 7855, 'Foresighted': 7856, 'Fossey': 7857, 'Fr': 7858, 'Framji': 7859, 'Francois': 7860, 'Francport': 7861, 'Frankfurt': 7862, 'Freeway': 7863, 'Friel': 7864, 'Frigyes': 7865, 'Frizzell': 7866, 'Frosty': 7867, 'Frye': 7868, 'Fuchur': 7869, 'Future': 7870, 'Fútbol': 7871, 'Gabel': 7872, 'Gaekwad': 7873, 'Gaelic': 7874, 'Gaius': 7875, 'Galaxy': 7876, 'Gale': 7877, 'Galleria': 7878, 'Galveston': 7879, 'Gandalf': 7880, 'Ganesh': 7881, 'Ganymede': 7882, 'Gardens': 7883, 'Garibaldi': 7884, 'Gastrointestinal': 7885, 'Gaughan': 7886, 'Gautama': 7887, 'Gay': 7888, 'Gayla': 7889, 'Gayle': 7890, 'Gazette': 7891, 'Gears': 7892, 'Gehlot': 7893, 'Geldof': 7894, 'Geller': 7895, 'Gelsenkirche': 7896, 'Gems': 7897, 'Gennifer': 7898, 'Geocentric': 7899, 'Georgetown': 7900, 'Gerald': 7901, 'Germanic': 7902, 'Germanwings': 7903, 'Geron': 7904, 'Gerstein': 7905, 'Ghosh': 7906, 'Gian': 7907, 'Gilda': 7908, 'Gilley': 7909, 'Gina': 7910, 'Gingivitis': 7911, 'Giuseppe': 7912, 'Glucose': 7913, 'Glycogen': 7914, 'Glycolysis': 7915, 'Gobind': 7916, 'Goldstein': 7917, 'Gomes': 7918, 'Gomez': 7919, 'Gondwana': 7920, 'González': 7921, 'Gooden': 7922, 'Gorrell': 7923, 'Gosselaar': 7924, 'Gottwald': 7925, 'Gouges': 7926, 'Gounaris': 7927, 'Governance': 7928, 'Governorate': 7929, 'Governors': 7930, 'Grandmaster': 7931, 'Grateau': 7932, 'Greenbriar': 7933, 'Greenbush': 7934, 'Greenleaf': 7935, 'Gregg': 7936, 'Greta': 7937, 'Grindecobbe': 7938, 'Grinnell': 7939, 'Grocery': 7940, 'Grosz': 7941, 'Guano': 7942, 'Guffey': 7943, 'Guglielmo': 7944, 'Guilherme': 7945, 'Gurira': 7946, 'Guru': 7947, 'Gwendoline': 7948, 'Gypsy': 7949, 'HEB': 7950, 'Habsburg': 7951, 'Hackman': 7952, 'Hadfield': 7953, 'Hadley': 7954, 'Hafner': 7955, 'Hagara': 7956, 'Halfman': 7957, 'Halford': 7958, 'Halim': 7959, 'Halle': 7960, 'Hamburg': 7961, 'Handshake': 7962, 'Haney': 7963, 'Hanging': 7964, 'Hangzhou': 7965, 'Hanley': 7966, 'Hanoi': 7967, 'Hanover': 7968, 'Hanuman': 7969, 'Haq': 7970, 'Harbour': 7971, 'Hard': 7972, 'Harishchandra': 7973, 'Harmon': 7974, 'Harrop': 7975, 'Harroun': 7976, 'Haryana': 7977, 'Hasbro': 7978, 'Hauer': 7979, 'Haugh': 7980, 'Haunted': 7981, 'Havilland': 7982, 'Hawk': 7983, 'Hayama': 7984, 'Hayat': 7985, 'Hayden': 7986, 'Hays': 7987, 'Hazzard': 7988, 'Heche': 7989, 'Heidi': 7990, 'Heike': 7991, 'Helium': 7992, 'Hellard': 7993, 'Heloise': 7994, 'Hendrickson': 7995, 'Herman': 7996, 'Hershlag': 7997, 'Hesiod': 7998, 'Hewlett': 7999, 'Hey': 8000, 'Hi': 8001, 'Hicky': 8002, 'Hidden': 8003, 'Highschool': 8004, 'Hilltop': 8005, 'Himachal': 8006, 'Himself': 8007, 'Hindle': 8008, 'Hinson': 8009, 'Hinterwald': 8010, 'Hoagy': 8011, 'Hobbit': 8012, 'Hobbs': 8013, 'Hoffmann': 8014, 'Hofford': 8015, 'Hohne': 8016, 'Holes': 8017, 'Holiday': 8018, 'Hologram': 8019, 'Holzhauer': 8020, 'Homi': 8021, 'Honourable': 8022, 'Hormusji': 8023, 'Horst': 8024, 'Hoskins': 8025, 'Hosterman': 8026, 'Hotspur': 8027, 'How': 8028, 'Hoyer': 8029, 'Hu': 8030, 'Hudspeth': 8031, 'Humphrey': 8032, 'Hunnam': 8033, 'Hurd': 8034, 'Hurling': 8035, 'Hurtado': 8036, 'Husk': 8037, 'Hutton': 8038, 'Huy': 8039, 'Hyat': 8040, 'IDF': 8041, 'IIS': 8042, 'IMA': 8043, 'INFJ': 8044, 'IST': 8045, 'IX': 8046, 'IZ': 8047, 'Iain': 8048, 'Icona': 8049, 'Igor': 8050, 'Ilgenfritz': 8051, 'Ilyich': 8052, 'Imamah': 8053, 'Imbrium': 8054, 'Imp': 8055, 'Imperfecta': 8056, 'Index': 8057, 'Indigo': 8058, 'Information': 8059, 'Ingeborg': 8060, 'Inkjet': 8061, 'Institute': 8062, 'Insular': 8063, 'Intolerable': 8064, 'Investigative': 8065, 'Inzamam': 8066, 'Inzi': 8067, 'Ira': 8068, 'Iraiyanar': 8069, 'Irene': 8070, 'Iron': 8071, 'Isabella': 8072, 'Ischemia': 8073, 'Islamic': 8074, 'Iturbide': 8075, 'Iver': 8076, 'Ivers': 8077, 'Iwo': 8078, 'Iyengar': 8079, 'Iz': 8080, 'J.E.B.': 8081, 'J.M.': 8082, 'Jaden': 8083, 'Jagger': 8084, 'Jagodowski': 8085, 'Jaguars': 8086, 'Jahan': 8087, 'Jainism': 8088, 'Jalacy': 8089, 'Jalen': 8090, 'Jalisco': 8091, 'Jamaica': 8092, 'Jamshedji': 8093, 'Janel': 8094, 'Janis': 8095, 'Japanese': 8096, 'Jarvis': 8097, 'Jasmin': 8098, 'Jauna': 8099, 'Jehangir': 8100, 'Jenna': 8101, 'Jennie': 8102, 'Jerome': 8103, 'Jianyu': 8104, 'Jigme': 8105, 'Jima': 8106, 'Jingzhong': 8107, 'Jody': 8108, 'Johnston': 8109, 'Joint': 8110, 'Joplin': 8111, 'Jorrel': 8112, 'Joshua': 8113, 'Juarez': 8114, 'Judaism': 8115, 'Judi': 8116, 'Judicial': 8117, 'Julia': 8118, 'Juliette': 8119, 'Jung': 8120, 'Junkers': 8121, 'Juscelino': 8122, 'KCMG': 8123, 'KG': 8124, 'Kaalratri': 8125, 'Kabaka': 8126, 'Kaepernick': 8127, 'Kajagoogoo': 8128, 'Kalia': 8129, 'Kamahl': 8130, 'Kamakawiwoʻole': 8131, 'Kamal': 8132, 'Kamboh': 8133, 'Kamerlingh': 8134, 'Kander': 8135, 'Kandiah': 8136, 'Kandpal': 8137, 'Kangana': 8138, 'Kanniyakumari': 8139, 'Kanto': 8140, 'Kanyakumari': 8141, 'Kar': 8142, 'Karamchand': 8143, 'Karin': 8144, 'Karina': 8145, 'Karinthy': 8146, 'Karlen': 8147, 'Karolyn': 8148, 'Katrina': 8149, 'Katyayani': 8150, 'Kaur': 8151, 'Kay': 8152, 'Keen(e': 8153, 'Keira': 8154, 'Kellaway': 8155, 'Kenner': 8156, 'Kent': 8157, 'Kepler': 8158, 'Kerosene': 8159, 'Kerri': 8160, 'Keystone': 8161, 'Khachaturian': 8162, 'Khaddam': 8163, 'Khaled': 8164, 'Khalsa': 8165, 'Khoisan': 8166, 'Khullar': 8167, 'Kidneys': 8168, 'Kills': 8169, 'Kimbrell': 8170, 'Kimple': 8171, 'Kinase': 8172, 'Kingsley': 8173, 'Kingston': 8174, 'Kiran': 8175, 'Kirby': 8176, 'Kirkpatrick': 8177, 'Kivilev': 8178, 'Kjeldsen': 8179, 'Klinefelter': 8180, 'Knightley': 8181, 'Knox': 8182, 'Kobe': 8183, 'Kochi': 8184, 'Kotecha': 8185, 'Kotkin': 8186, 'Kramer': 8187, 'Krantivira': 8188, 'Krayzie': 8189, 'Krogh': 8190, 'Krul': 8191, 'Kubitschek': 8192, 'Kunal': 8193, 'Kuntal': 8194, 'Kushmanda': 8195, 'Kylie': 8196, 'Kīlauea': 8197, 'L.I.V.E.': 8198, 'L.T.': 8199, 'LDH': 8200, 'LI': 8201, 'LII': 8202, 'LP': 8203, 'LaBelle': 8204, 'LaVette': 8205, 'LaVoie': 8206, 'Labrador': 8207, 'Lachman': 8208, 'Lactate': 8209, 'Lagos': 8210, 'Laina': 8211, 'Laird': 8212, 'Lakshadweep': 8213, 'Lal': 8214, 'Lamb': 8215, 'Lamont': 8216, 'Lancie': 8217, 'Landen': 8218, 'Landon': 8219, 'Landry': 8220, 'Laozi': 8221, 'Latcherie': 8222, 'Latvia': 8223, 'Laverne': 8224, 'Lawrenceton': 8225, 'Layzie': 8226, 'Lazarus': 8227, 'Leadon': 8228, 'Leano': 8229, 'Learning': 8230, 'Lebanon': 8231, 'Legault': 8232, 'Legazpi': 8233, 'Legislative': 8234, 'Legolas': 8235, 'Leiv': 8236, 'Len': 8237, 'Leto': 8238, 'Letters': 8239, 'Levin': 8240, 'Levite': 8241, 'Li': 8242, 'Liberal': 8243, 'Liboiron': 8244, 'Light': 8245, 'Lima': 8246, 'Limestone': 8247, 'Linda': 8248, 'Lines': 8249, 'Linkin': 8250, 'Linus': 8251, 'Linwood': 8252, 'Lion': 8253, 'Lisle': 8254, 'Litella': 8255, 'Lively': 8256, 'Lobo': 8257, 'Lochkovian': 8258, 'Lodovico': 8259, 'Lokmanya': 8260, 'Lolita': 8261, 'Lombard': 8262, 'Long': 8263, 'Longcross': 8264, 'Longoria': 8265, 'Longstreet': 8266, 'Lorenzo': 8267, 'Lori': 8268, 'Lorne': 8269, 'Louvre': 8270, 'Lovato': 8271, 'Loyola': 8272, 'Luc': 8273, 'Ludacris': 8274, 'Lue': 8275, 'Luis': 8276, 'Luisa': 8277, 'Lukasz': 8278, 'Lumbricus': 8279, 'Lun': 8280, 'Luna': 8281, 'Lunik': 8282, 'Lusha': 8283, 'Lvovich': 8284, 'Lyte': 8285, 'Léa': 8286, 'MB': 8287, 'MGM': 8288, 'MN8': 8289, 'MV': 8290, 'Mabel': 8291, 'MacLeod': 8292, 'Macaulay': 8293, 'Macfadyen': 8294, 'Mad': 8295, 'Madeleine': 8296, 'Mader': 8297, 'Maersk': 8298, 'Magaziner': 8299, 'Magna': 8300, 'Mahagauri': 8301, 'Mahatma': 8302, 'Maiman': 8303, 'Mairi': 8304, 'Malcolm': 8305, 'Maldives': 8306, 'Malik': 8307, 'Mall': 8308, 'Malloy': 8309, 'Mamata': 8310, 'Mamta': 8311, 'Mangekyo': 8312, 'Manhattan': 8313, 'Manyu': 8314, 'Marbella': 8315, 'Marbois': 8316, 'Marcel': 8317, 'Marconi': 8318, 'Margrethe': 8319, 'Mariano': 8320, 'Marietta': 8321, 'Marini': 8322, 'Marié': 8323, 'Marla': 8324, 'Marlon': 8325, 'Marmon': 8326, 'Maroon': 8327, 'Marquesas': 8328, 'Marquess': 8329, 'Marriott': 8330, 'Marsh': 8331, 'Marshmello': 8332, 'Marsters': 8333, 'Marta': 8334, 'Martinez': 8335, 'Martyn': 8336, 'Marx': 8337, 'Mase': 8338, 'Maseo': 8339, 'Masiela': 8340, 'Massalia': 8341, 'Mathew': 8342, 'Matilda': 8343, 'Matter': 8344, 'Matthai': 8345, 'Mattingly': 8346, 'Maude': 8347, 'Maury': 8348, 'Mavalankar': 8349, 'Mawhood': 8350, 'Mayberry': 8351, 'Mayhew': 8352, 'Mb': 8353, 'Mbakwe': 8354, 'McCain': 8355, 'McChrystal': 8356, 'McClain': 8357, 'McClure': 8358, 'McDaniel': 8359, 'McDiarmid': 8360, 'McDowall': 8361, 'McEwan': 8362, 'McKinley': 8363, 'McKinnies': 8364, 'McLachlan': 8365, 'Meara': 8366, 'Medical': 8367, 'Medlicott': 8368, 'Meera': 8369, 'Megh': 8370, 'Meghalaya': 8371, 'Meghan': 8372, 'Meiosis': 8373, 'Melba': 8374, 'Melody': 8375, 'Members': 8376, 'Menedez': 8377, 'Mercié': 8378, 'Mercutio': 8379, 'Merry': 8380, 'Messing': 8381, \"Mi'ad\": 8382, 'Micheal': 8383, 'Michel': 8384, 'Michelangelo': 8385, 'Mickey': 8386, 'Microsoft': 8387, 'Mid': 8388, 'Midge': 8389, 'Miguel': 8390, 'Mili': 8391, 'Milli': 8392, 'Milwaukee': 8393, 'Mina': 8394, 'Mind': 8395, 'Mineral': 8396, 'Ministers': 8397, 'Mint': 8398, 'Minty': 8399, 'Mintz': 8400, 'Miriam': 8401, 'Mishay': 8402, 'Mishima': 8403, 'Mishka': 8404, 'Miss': 8405, 'Mission': 8406, 'Mizoram': 8407, 'Mjolnir': 8408, 'Mohandas': 8409, 'Momordica': 8410, 'Mona': 8411, 'Monash': 8412, 'Monday': 8413, 'Mongo': 8414, 'Monsignor': 8415, 'Monsters': 8416, 'Montalbán': 8417, 'Montepulciano': 8418, 'Morarji': 8419, 'Morganfield': 8420, 'Morse': 8421, 'Mortimer': 8422, 'Morup': 8423, 'Morwenna': 8424, 'Mosley': 8425, 'Motel': 8426, 'Motera': 8427, 'Moudros': 8428, 'Mouse': 8429, 'Mr.': 8430, 'Muir': 8431, 'Mulk': 8432, 'Muni': 8433, 'Munro': 8434, 'Murdoc': 8435, 'Murrah': 8436, 'Mushu': 8437, 'Mustangs': 8438, 'Mutarazi': 8439, 'Mutiny': 8440, 'Muttaqi': 8441, 'Muttiah': 8442, 'Myoglobin': 8443, 'México': 8444, 'NATO': 8445, 'NGA': 8446, 'NHL': 8447, 'Nagaland': 8448, 'Naismith': 8449, 'Namba': 8450, 'Nameless': 8451, 'Namgyel': 8452, 'Nandalal': 8453, 'Nat': 8454, 'Nationals': 8455, 'Nations': 8456, 'Navadurga': 8457, 'Navigator': 8458, 'Navy': 8459, 'Nawab': 8460, 'Nazi': 8461, 'Near': 8462, 'NehruPandit': 8463, 'Nell': 8464, 'Nepal': 8465, 'Neptune': 8466, 'Nero': 8467, 'Nerve': 8468, 'Neta': 8469, 'Newbern': 8470, 'Newly': 8471, 'Next': 8472, 'Niccals': 8473, 'Niccolò': 8474, 'Nieuport': 8475, 'Niihau': 8476, 'Nikolai': 8477, 'Nikolaj': 8478, 'Nile': 8479, 'Ninety': 8480, 'Nkem': 8481, 'Nnamdi': 8482, 'Noirmoutier': 8483, 'Nolan': 8484, 'Nondisjunction': 8485, 'Nondo': 8486, 'Noodle': 8487, 'Norma': 8488, 'Norton': 8489, 'Nouveau': 8490, 'Nubuwwah': 8491, 'Nucleotide': 8492, 'Nurmagomedo': 8493, 'Nurmagomedov': 8494, 'Nursery': 8495, 'Nyle': 8496, 'Nájera': 8497, 'OM': 8498, 'OMAA': 8499, 'Obedience': 8500, 'Occupations': 8501, 'Octomom': 8502, 'Oddie': 8503, 'Oden': 8504, 'Odyssey': 8505, 'Ogg': 8506, 'Olmec': 8507, 'Olmecs': 8508, 'Olmos': 8509, 'Olufunmilayo': 8510, 'Olympe': 8511, 'Onnes': 8512, 'Operation': 8513, 'Opt': 8514, 'Orcein': 8515, 'Oriental': 8516, 'Original': 8517, 'Ormond': 8518, 'Orthographic': 8519, 'Osteogenesis': 8520, 'Outer': 8521, 'Ove': 8522, 'Overstreet': 8523, 'Own': 8524, 'Oyama': 8525, 'P': 8526, 'P.A.': 8527, 'PCP': 8528, 'PVC': 8529, 'Pablo': 8530, 'Pacers': 8531, 'Pacquiao': 8532, 'Pagan': 8533, 'Paice': 8534, 'Pain': 8535, 'Palash': 8536, 'Paleozoic': 8537, 'Palghar': 8538, 'Paljor': 8539, 'Panini': 8540, 'Panipat': 8541, 'Pantages': 8542, 'Paradise': 8543, 'Paramesh': 8544, 'Parera': 8545, 'Parissi': 8546, 'Parks': 8547, 'Parkwa': 8548, 'Parrish': 8549, 'Parsons': 8550, 'Parti': 8551, 'Parvati': 8552, 'Pasemaster': 8553, 'Paso': 8554, 'Password': 8555, 'Patrice': 8556, 'Paulette': 8557, 'Pavlov': 8558, 'Pawan': 8559, 'Peabo': 8560, 'Peace': 8561, 'Peacock': 8562, 'Peevey': 8563, 'Pelling': 8564, 'Pelosi': 8565, 'Pembrokeshire': 8566, 'Pendleton': 8567, 'Pentafluoridophosphorus': 8568, 'Pentafluorophosphorane': 8569, 'Pepper': 8570, 'Per': 8571, 'Percy': 8572, 'Perkins': 8573, 'Perla': 8574, 'Perot': 8575, 'Persuasions': 8576, 'Perú': 8577, 'Pescetarian': 8578, 'Petr': 8579, 'Petricca': 8580, 'Petronelli': 8581, 'Petrovich': 8582, 'Pets': 8583, 'Petty': 8584, 'Phalke': 8585, 'Phidias': 8586, 'Phosphorous': 8587, 'Phosphorus': 8588, 'Phosphorus(V': 8589, 'Physical': 8590, 'Picaresque': 8591, 'Picasso': 8592, 'Pier': 8593, 'Pilgrimage': 8594, 'Pinciotti': 8595, 'Pine': 8596, 'Pineda': 8597, 'Pinney': 8598, 'Pipes': 8599, 'Pisa': 8600, 'Pizarro': 8601, 'Plains': 8602, 'Planas': 8603, 'Planck': 8604, 'Plassey': 8605, 'Plato': 8606, 'Pleistocene': 8607, 'Plummer': 8608, 'Poehler': 8609, 'Pojas': 8610, 'Politiká': 8611, 'Pollard': 8612, 'Polynesia': 8613, 'Pond': 8614, 'Poonam': 8615, 'Pop': 8616, 'Pope': 8617, 'Poppe': 8618, 'Port': 8619, 'Possum': 8620, 'Preet': 8621, 'Prepon': 8622, 'Prescott': 8623, 'Prewinter': 8624, 'Prine': 8625, 'Pro': 8626, 'Procter': 8627, 'Productivity': 8628, 'Professor': 8629, 'Progress': 8630, 'Promontory': 8631, 'Propellant-1': 8632, 'Property': 8633, 'Ptolemy': 8634, 'Publishers': 8635, 'Puente': 8636, 'Purusha': 8637, 'Puth': 8638, 'Putsoa': 8639, 'Pyle': 8640, 'Pytheas': 8641, 'Père': 8642, 'QC': 8643, 'Quadri': 8644, 'Quantum': 8645, 'Quarter': 8646, 'Quartering': 8647, 'Queenstown': 8648, 'Quigley': 8649, 'RP-1': 8650, 'Ra': 8651, 'Racine': 8652, 'Radiant': 8653, 'Raditz': 8654, 'Radner': 8655, 'Rafa': 8656, 'Rafferty': 8657, 'Raimondo': 8658, 'Raipur': 8659, 'Raja': 8660, 'Rajasmita': 8661, 'Raje': 8662, 'Rajendra': 8663, 'Ramaphosa': 8664, 'Ramesh': 8665, 'Ramis': 8666, 'Ranaut': 8667, 'Ranch': 8668, 'Rand': 8669, 'Randle': 8670, 'Rangers': 8671, 'Ransome': 8672, 'Raphael': 8673, 'Raptors': 8674, 'Rauch': 8675, 'Rayanna': 8676, 'Raymonde': 8677, 'Re': 8678, 'Reality': 8679, 'Realty': 8680, 'Reba': 8681, 'Rebellion': 8682, 'Redeemer': 8683, 'Redgrave': 8684, 'Regnall': 8685, 'Remi': 8686, 'Renata': 8687, 'Reserve': 8688, 'Retriever': 8689, 'Retting': 8690, 'Return': 8691, 'Reuben': 8692, 'Rev.': 8693, 'Revenue': 8694, 'Revolutionary': 8695, 'Reye': 8696, 'Rhett': 8697, 'Richmond': 8698, 'Riders': 8699, 'Ridley': 8700, 'Right': 8701, 'Riley': 8702, 'Ripon': 8703, 'Roache': 8704, 'Robard': 8705, 'Robbins': 8706, 'Roc': 8707, 'Rochelle': 8708, 'Rochester': 8709, 'Rockville': 8710, 'Roday': 8711, 'Roddy': 8712, 'Rodney': 8713, 'Roland': 8714, 'Romans': 8715, 'Rondel': 8716, 'Ronny': 8717, 'Rosado': 8718, 'Rosenberg': 8719, 'Rossana': 8720, 'Rota': 8721, 'Rouget': 8722, 'Route': 8723, 'Rp': 8724, 'Rubble': 8725, 'Rud': 8726, 'Rudolf': 8727, 'Rupee': 8728, 'Rupp': 8729, 'Russel': 8730, 'Rutger': 8731, 'Rutter': 8732, 'Ryder': 8733, 'S.S.R.N.': 8734, 'S11': 8735, 'Sacagawea': 8736, 'Sacajawea': 8737, 'Sachin': 8738, 'Sacramento': 8739, 'Saddler': 8740, 'Safety': 8741, 'Salat': 8742, 'Salazar': 8743, 'Salivary': 8744, 'Salter': 8745, 'Salvador': 8746, 'Salvatore': 8747, 'Samanla': 8748, 'Samberg': 8749, 'Sambora': 8750, 'Samm': 8751, 'Sampson': 8752, 'Sanctuary': 8753, 'Sandro': 8754, 'Sangolli': 8755, 'Sarajevo': 8756, 'Saratoga': 8757, 'Satyendranath': 8758, 'Savage': 8759, 'Savan': 8760, 'Save': 8761, 'Savi': 8762, 'Sawai': 8763, 'Scarecrow': 8764, 'Scarlet': 8765, 'Schnatter': 8766, 'Schuyler': 8767, 'Scindia': 8768, 'Scout': 8769, 'Scramble': 8770, 'Scáthach': 8771, 'Seabiscuit': 8772, 'Seals': 8773, 'Seaport': 8774, 'Searcy': 8775, 'Secaucus': 8776, 'Seco': 8777, 'Sedgwick': 8778, 'SeegerPeter': 8779, 'Seminole': 8780, 'Senegal': 8781, 'Senior': 8782, 'Sepoy': 8783, 'Serial': 8784, 'Series': 8785, 'Serkis': 8786, 'Server': 8787, 'Settlement': 8788, 'Seydoux': 8789, 'Shaffer': 8790, 'Shailaputri': 8791, 'Shankha': 8792, 'Shanley': 8793, 'Shanté': 8794, 'Sharingan': 8795, 'Shatt': 8796, 'Shawn': 8797, 'Shawna': 8798, 'Sheeran': 8799, 'Shelley': 8800, 'Sherpa': 8801, 'Shinedown': 8802, 'Shinpo': 8803, 'Shogi': 8804, 'Shook': 8805, 'Shorthair': 8806, 'Shoup': 8807, 'Siddhidhatri': 8808, 'Side': 8809, 'Sienna': 8810, 'Sikhs': 8811, 'Silurian': 8812, 'Silvassa': 8813, 'Simhan': 8814, 'Simmons': 8815, 'Simoni': 8816, 'Simpsons': 8817, 'Sindhu': 8818, 'Sindhushree': 8819, 'Siobhán': 8820, 'Sisyphus': 8821, 'Siva': 8822, 'Skandamata': 8823, 'Skelton': 8824, 'Skunk': 8825, 'Sky': 8826, 'Slater': 8827, 'Slayer': 8828, 'Sledge': 8829, 'Sletten': 8830, 'Slocum': 8831, 'Smallwood': 8832, 'Smirnoff': 8833, 'Snoop': 8834, 'Sobolov': 8835, 'Sobule': 8836, 'Sokoloff': 8837, 'Solace': 8838, 'Solicitor': 8839, 'Something': 8840, 'Sonenclar': 8841, 'Sonny': 8842, 'Sophia': 8843, 'Soup': 8844, 'Southeast': 8845, 'Southeastern': 8846, 'Southside': 8847, 'Southwest': 8848, 'Space': 8849, 'Spargur': 8850, 'Sparrow': 8851, 'Spektor': 8852, 'Sperber': 8853, 'Spoken': 8854, 'Springfield': 8855, 'Sprouse': 8856, 'Squadron': 8857, 'Srivastava': 8858, 'Steamboat': 8859, 'Steel': 8860, 'Stefania': 8861, 'Steny': 8862, 'Step': 8863, 'Steveston': 8864, 'Stock': 8865, 'Stockholm': 8866, 'Stokes': 8867, 'Stonetown': 8868, 'Stonewall': 8869, 'Straight': 8870, 'Straits': 8871, 'Strug': 8872, 'Students': 8873, 'Stuttgart': 8874, 'Su': 8875, 'Sublimation': 8876, 'Suites': 8877, 'Sukirti': 8878, 'Sultan': 8879, 'Sulzberger': 8880, 'Sundram': 8881, 'Sundwall': 8882, 'Sunshine': 8883, 'Superman': 8884, 'Surrey': 8885, 'Sursok': 8886, 'Sutton': 8887, 'Swampy': 8888, 'Sweeney': 8889, 'T': 8890, 'T.A.P.S.': 8891, 'TBA': 8892, 'THP': 8893, 'TNZ': 8894, 'Taheny': 8895, 'Tahoe': 8896, 'Talkeetna': 8897, 'Tamatoa': 8898, 'Tammin': 8899, 'Tammy': 8900, 'Tamsin': 8901, 'Tan': 8902, 'Tanaka': 8903, 'Tango': 8904, 'Tania': 8905, 'Tanit': 8906, 'Taqvi': 8907, 'Taurasi': 8908, 'Tawhid': 8909, 'Taxiarch': 8910, 'Tecumseh': 8911, 'Tenace': 8912, 'Tendulkar': 8913, 'Tenth': 8914, 'Terrier': 8915, 'Thaba': 8916, 'Thakur': 8917, 'Thatch': 8918, 'Themyscira': 8919, 'Thom': 8920, 'Thor': 8921, 'Thrissur': 8922, 'Thwaites': 8923, 'Thyroid': 8924, 'Tidal': 8925, 'Tigress': 8926, 'Tilak': 8927, 'Tilda': 8928, 'Tilley': 8929, 'Tilly': 8930, 'Tin': 8931, 'Titanium': 8932, 'Tito': 8933, 'Tobacco': 8934, 'Tobago': 8935, 'Toland': 8936, 'Topher': 8937, 'Topper': 8938, 'Tori': 8939, 'Toronto': 8940, 'Totalitarianism': 8941, 'Tottenham': 8942, 'Tourism': 8943, 'Towns': 8944, 'Trainor': 8945, 'Tranquility': 8946, 'Transitional': 8947, 'Translation': 8948, 'Transportation': 8949, 'Trashmen': 8950, 'Traylor': 8951, 'Trea': 8952, 'Tribunal': 8953, 'Triennium': 8954, 'Trinbago': 8955, 'Trinidad': 8956, 'Trixie': 8957, 'Tropicana': 8958, 'Troponin': 8959, 'Trout': 8960, 'Trowbridge': 8961, 'Tucci': 8962, 'Tuesday': 8963, 'Tughluq': 8964, 'Tunstall': 8965, 'Twelfth': 8966, 'Twelve': 8967, 'Twitmyer': 8968, 'Tyson': 8969, 'Tze': 8970, 'Téa': 8971, 'UES': 8972, 'UN': 8973, 'USN': 8974, 'USOS': 8975, 'UTC': 8976, 'Ucluelet': 8977, 'Udaipur': 8978, 'Uganda': 8979, 'Uijeongbu': 8980, 'Ultimate': 8981, 'Ulugh': 8982, 'Ulyanov': 8983, 'Una': 8984, 'Underwood': 8985, 'Unics': 8986, 'Unix': 8987, 'Ural': 8988, 'Urbana': 8989, 'Ure': 8990, 'Uttarakhand': 8991, 'VIII': 8992, 'Vajpayee': 8993, 'Valderrama': 8994, 'Valentine': 8995, 'Valeri': 8996, 'Valli': 8997, 'Valluvar': 8998, 'VanWarmer': 8999, 'VanZant': 9000, 'Vanderford': 9001, 'Vanderwaal': 9002, 'Vanilla': 9003, 'Vanni': 9004, 'Varble': 9005, 'Varsano': 9006, 'Vasudev': 9007, 'Vasundhara': 9008, 'Vatican': 9009, 'Vaugier': 9010, 'Vegard': 9011, 'Veil': 9012, 'Velline': 9013, 'Venetian': 9014, 'Veneto': 9015, 'Vengaram': 9016, 'Very': 9017, 'Vestager': 9018, 'Vesuvius': 9019, 'Vice': 9020, 'Viejo': 9021, 'Vignette': 9022, 'Vinko': 9023, 'Virginian': 9024, 'Viscount': 9025, 'Vivat': 9026, 'Vivien': 9027, 'Volcano': 9028, 'Vornado': 9029, 'W16': 9030, 'Wadlow': 9031, 'Wagyu': 9032, 'Wally': 9033, 'Wangchuck': 9034, 'Waqar': 9035, 'Wardrobe': 9036, 'Wasylenko': 9037, 'Watley': 9038, 'Watts': 9039, 'Waugaman': 9040, 'Way': 9041, 'Weatherwax': 9042, 'Weavers': 9043, 'Wedekind': 9044, 'Wednesday': 9045, 'Wehle': 9046, 'Weighing': 9047, 'Weldon': 9048, 'Welliver': 9049, 'Wendie': 9050, 'Wengie': 9051, 'Wes': 9052, 'Wesley': 9053, 'Wessels': 9054, 'Wessex': 9055, 'Westmacott': 9056, 'Westminster': 9057, 'Weymouth': 9058, 'Whaite': 9059, 'What': 9060, 'Whipple': 9061, 'Whitman': 9062, 'Whiz': 9063, 'Whoville': 9064, 'Wicks': 9065, 'Widget': 9066, 'Wiggins': 9067, 'Wil': 9068, 'Wild': 9069, 'Williamson': 9070, 'Willie': 9071, 'Willy': 9072, 'Wilmer': 9073, 'Wilton': 9074, 'Wince': 9075, 'Windows': 9076, 'Winningham': 9077, 'Winona': 9078, 'Wish': 9079, 'Witch': 9080, 'Woking': 9081, 'Wolf': 9082, 'Wolves': 9083, 'Womesh': 9084, 'Wondolowski': 9085, 'Woodard': 9086, 'Woodland': 9087, 'Woodlands': 9088, 'Woodman': 9089, 'Woodrow': 9090, 'Woodsmen': 9091, 'Woodstock': 9092, 'Work': 9093, 'Worley': 9094, 'Wrawe': 9095, 'Wrightsville': 9096, 'Wyatt': 9097, 'Wynne': 9098, 'Wyoming': 9099, 'Wåhlin': 9100, 'XIV': 9101, 'XXVII': 9102, 'XXX': 9103, 'XXXVII': 9104, 'Xia': 9105, 'Yacht': 9106, 'Yacoub': 9107, 'Yang': 9108, 'Yasuko': 9109, 'Ylvis': 9110, 'Ylvisåker': 9111, 'Yoren': 9112, 'Yu': 9113, 'Yung': 9114, 'Yuriko': 9115, 'Yvette': 9116, 'Zabka': 9117, 'Zach': 9118, 'Zachary': 9119, 'Zedong': 9120, 'Zeishan': 9121, 'Zhaoxuan': 9122, 'Zhenwei': 9123, 'Ziffel': 9124, 'Zofya': 9125, 'Zoo': 9126, '^': 9127, 'abilities': 9128, 'acceptance': 9129, 'accordion': 9130, 'acetate': 9131, 'action': 9132, 'actor': 9133, 'additional': 9134, 'additions': 9135, 'adjacent': 9136, 'administrators': 9137, 'adoptive': 9138, 'adornment': 9139, 'aerobic': 9140, 'aesthetic': 9141, 'against': 9142, 'agencies': 9143, 'agricultural': 9144, 'albumin': 9145, 'allows': 9146, 'alteration': 9147, 'alternative': 9148, 'aluminum': 9149, 'am': 9150, 'anaerobic': 9151, 'anthem': 9152, 'antibodies': 9153, 'any': 9154, 'aorta': 9155, 'apellant': 9156, 'appear': 9157, 'appointment': 9158, 'appoints': 9159, 'april': 9160, 'aqueducts': 9161, 'archaeological': 9162, 'archil': 9163, 'argon': 9164, 'associated': 9165, 'atmosphere': 9166, 'aunt': 9167, 'authoritarian': 9168, 'ayllu': 9169, 'azygos': 9170, 'badensis': 9171, 'bag': 9172, 'balance': 9173, 'bandwidth': 9174, 'barns': 9175, 'base': 9176, 'based': 9177, 'batholiths': 9178, 'bathroom': 9179, 'battle': 9180, 'bearing': 9181, 'beds': 9182, 'beginning': 9183, 'begins': 9184, 'bellies': 9185, 'benefit': 9186, 'better': 9187, 'biplane': 9188, 'birthday': 9189, 'blocks': 9190, 'bluestone': 9191, 'bonds': 9192, 'bottom': 9193, 'bound': 9194, 'brainstem': 9195, 'brass': 9196, 'breast': 9197, 'bride': 9198, 'bridge': 9199, 'brilliant': 9200, 'bronchial': 9201, 'brown': 9202, 'browser': 9203, 'bug': 9204, 'bugle': 9205, 'building': 9206, 'bum': 9207, 'bush': 9208, 'business': 9209, 'businesses': 9210, 'calf': 9211, 'caliber': 9212, 'canals': 9213, 'canard': 9214, 'cap': 9215, 'capsule': 9216, 'captured': 9217, 'cards': 9218, 'care': 9219, 'carina': 9220, 'cat': 9221, 'category': 9222, 'cattle': 9223, 'centers': 9224, 'centre': 9225, 'cerebellum': 9226, 'chain': 9227, 'channels': 9228, 'charantia': 9229, 'cheese': 9230, 'chicken': 9231, 'chitin': 9232, 'chocolate': 9233, 'cholesterol': 9234, 'chromophore': 9235, 'cigi': 9236, 'citizen': 9237, 'civilizations': 9238, 'climate': 9239, 'club': 9240, 'coastal': 9241, 'cognition': 9242, 'column': 9243, 'columnar': 9244, 'command': 9245, 'commanded': 9246, 'commerce': 9247, 'commissioner': 9248, 'communication': 9249, 'communism': 9250, 'communist': 9251, 'communists': 9252, 'communities': 9253, 'composed': 9254, 'composition': 9255, 'computational': 9256, 'concentrated': 9257, 'conditioning': 9258, 'congress': 9259, 'consensus': 9260, 'consequently': 9261, 'constitutional': 9262, 'consumers': 9263, 'containment': 9264, 'continent': 9265, 'continuous': 9266, 'continuously': 9267, 'contracted': 9268, 'contractors': 9269, 'control': 9270, 'controller': 9271, 'convention': 9272, 'coordination': 9273, 'copepod': 9274, 'copper': 9275, 'corporate': 9276, 'counties': 9277, 'county': 9278, 'courts': 9279, 'coven': 9280, 'cows': 9281, 'crab': 9282, 'crash': 9283, 'credit': 9284, 'cribriform': 9285, 'cribrosa': 9286, 'criminal': 9287, 'cure': 9288, 'cutscenes': 9289, \"d'Abo\": 9290, 'dancer': 9291, 'dangerous': 9292, 'date': 9293, 'daughter': 9294, 'day': 9295, 'deadline': 9296, 'death': 9297, 'decay': 9298, 'deceive': 9299, 'deep': 9300, 'dehydrogenase': 9301, 'delegated': 9302, \"dell'Accademia\": 9303, 'delusions': 9304, 'demand': 9305, 'depletion': 9306, 'deputy': 9307, 'dermis': 9308, 'desalinated': 9309, 'descendant': 9310, 'descended': 9311, 'designers': 9312, 'destroyer': 9313, 'deterrence': 9314, 'development': 9315, 'developmental': 9316, 'deviations': 9317, 'dialogue': 9318, 'died': 9319, 'differential': 9320, 'difficulty': 9321, 'diffusion': 9322, 'digital': 9323, 'dimensional': 9324, 'disabilities': 9325, 'disbanded': 9326, 'dish': 9327, 'dock': 9328, 'does': 9329, 'domain': 9330, 'downhill': 9331, 'drive': 9332, 'dry': 9333, 'duck': 9334, 'duodenum': 9335, 'dying': 9336, 'e': 9337, 'eagle': 9338, 'ear': 9339, 'earthquake': 9340, 'earthworm': 9341, 'easier': 9342, 'easterly': 9343, 'edge': 9344, 'elder': 9345, 'elect': 9346, 'elected': 9347, 'electromagnetic': 9348, 'electronic': 9349, 'electrons': 9350, 'empire': 9351, 'enemies': 9352, 'engineer': 9353, 'engineers': 9354, 'england': 9355, 'enjoy': 9356, 'ensi': 9357, 'enterprises': 9358, 'et': 9359, 'everest': 9360, 'evolution': 9361, 'excellence': 9362, 'exoskeleton': 9363, 'expected': 9364, 'expressionism': 9365, 'extinct': 9366, 'eye': 9367, 'facing': 9368, 'faith': 9369, 'false': 9370, 'faster': 9371, 'featured': 9372, 'feminists': 9373, 'festival': 9374, 'festivals': 9375, 'fibrous': 9376, 'fictional': 9377, 'fight': 9378, 'finale': 9379, 'finished': 9380, 'flags': 9381, 'flattened': 9382, 'flea': 9383, 'flower': 9384, 'flowers': 9385, 'folktale': 9386, 'forebrain': 9387, 'forest': 9388, 'formation': 9389, 'formations': 9390, 'fossa': 9391, 'frequency': 9392, 'frost': 9393, 'fuel': 9394, 'gangliated': 9395, 'gastroenterologist': 9396, 'gastrointestinal': 9397, 'gates': 9398, 'gentlemanly': 9399, 'geothermal': 9400, 'giant': 9401, 'glomerulus': 9402, 'go': 9403, 'governed': 9404, 'granodiorite': 9405, 'gravity': 9406, 'gray': 9407, 'greater': 9408, 'grizzlies': 9409, 'growing': 9410, 'guitarist': 9411, 'gypsy': 9412, 'hair': 9413, 'hall': 9414, 'hallucinations': 9415, 'hamstrings': 9416, 'hand': 9417, 'handkerchiefs': 9418, 'haploid': 9419, 'harbor': 9420, 'harvest': 9421, 'hate': 9422, 'hazardous': 9423, 'heat': 9424, 'heavy': 9425, 'height': 9426, 'heirs': 9427, 'helium': 9428, 'hermaphrodite': 9429, 'heterosexual': 9430, 'highest': 9431, 'highway': 9432, 'hitting': 9433, 'holder': 9434, 'homeowners': 9435, 'hometown': 9436, 'homeward': 9437, 'horizontal': 9438, 'hormone': 9439, 'horse': 9440, 'hotspot': 9441, 'huge': 9442, 'hydrosphere': 9443, 'hyperacusis': 9444, 'iFCP': 9445, 'identified': 9446, 'illness': 9447, 'impression': 9448, 'improve': 9449, 'indie': 9450, 'individual': 9451, 'individuals': 9452, 'informant': 9453, 'informants': 9454, 'informer': 9455, 'infrastructure': 9456, 'inkjet': 9457, 'institution': 9458, 'institutions': 9459, 'instrument': 9460, 'intellectual': 9461, 'interacting': 9462, 'interested': 9463, 'intermembrane': 9464, 'intersex': 9465, 'ion': 9466, 'iron': 9467, 'isoenzyme': 9468, 'jaguar': 9469, 'jargon': 9470, 'jazz': 9471, 'judicial': 9472, 'judiciary': 9473, 'jump': 9474, 'junction': 9475, 'junior': 9476, 'kg': 9477, 'kidney': 9478, 'kidneys': 9479, 'kinds': 9480, 'king': 9481, 'knee': 9482, 'knelt': 9483, 'known': 9484, \"l'Île\": 9485, 'laboratory': 9486, 'lacmus': 9487, 'lagoon': 9488, 'lake': 9489, 'lal': 9490, 'lamb': 9491, 'landfill': 9492, 'landscape': 9493, 'larvae': 9494, 'late-50s': 9495, 'later': 9496, 'leaders': 9497, 'learning': 9498, 'least': 9499, 'left': 9500, 'leg': 9501, 'legal': 9502, 'legislation': 9503, 'liberal': 9504, 'liberty': 9505, 'libéral': 9506, 'license': 9507, 'ligaments': 9508, 'lines': 9509, 'linkage': 9510, 'lipids': 9511, 'lithosphere': 9512, 'liver': 9513, 'load': 9514, 'lobster': 9515, 'local': 9516, 'locations': 9517, 'loin': 9518, 'lotus': 9519, 'lovely': 9520, 'lowest': 9521, 'lucidum': 9522, 'lugal': 9523, 'lung': 9524, 'lymphoma': 9525, 'lymphomas': 9526, 'maggots': 9527, 'mainline': 9528, 'maintaining': 9529, 'make': 9530, 'manœuvrable': 9531, 'market': 9532, 'material': 9533, 'mathematical': 9534, 'matrix': 9535, 'maximum': 9536, 'mean': 9537, 'median': 9538, 'mediated': 9539, 'medical': 9540, 'medusae': 9541, 'megalithic': 9542, 'megalodon': 9543, 'member': 9544, 'mental': 9545, 'meristem': 9546, 'meristems': 9547, 'metro': 9548, 'mi': 9549, 'midbrain': 9550, 'mile': 9551, 'militia': 9552, 'misophonia': 9553, 'missionaries': 9554, 'modified': 9555, 'modulation': 9556, 'molecular': 9557, 'molecules': 9558, 'monarch': 9559, 'monitoring': 9560, 'monosperma': 9561, 'monsoon': 9562, 'month': 9563, 'moodern': 9564, 'mortgage': 9565, 'motor': 9566, 'mount': 9567, 'mountainous': 9568, 'muscat': 9569, 'muscles': 9570, 'mushroom': 9571, 'mythical': 9572, 'mythologies': 9573, 'n': 9574, 'nasone': 9575, 'nation': 9576, 'natriuretic': 9577, 'negative': 9578, 'nephron': 9579, 'neuromuscular': 9580, 'neutrons': 9581, 'night': 9582, 'no': 9583, 'nobility': 9584, 'noble': 9585, 'nondisjunction': 9586, 'notch': 9587, 'nu': 9588, 'nucleotides': 9589, 'numbers': 9590, 'nuns': 9591, 'nut': 9592, 'occasionally': 9593, 'ocelot': 9594, 'offices': 9595, 'older': 9596, 'olive': 9597, 'olmec': 9598, 'once': 9599, 'online': 9600, 'open': 9601, 'operate': 9602, 'operation': 9603, 'opossum': 9604, 'option': 9605, 'orange': 9606, 'order': 9607, 'ores': 9608, 'organization': 9609, 'origin': 9610, 'origins': 9611, 'outside': 9612, 'overlie': 9613, 'overlying': 9614, 'overran': 9615, 'overseas': 9616, 'own': 9617, 'owner': 9618, 'ownership': 9619, 'ozone': 9620, 'parents': 9621, 'parietal': 9622, 'parliamentary': 9623, 'particularly': 9624, 'passengers': 9625, 'passive': 9626, 'pathologist': 9627, 'pattern': 9628, 'pentafluoride': 9629, 'perform': 9630, 'periods': 9631, 'peripheral': 9632, 'permeability': 9633, 'personal': 9634, 'personality': 9635, 'personified': 9636, 'pescatarian': 9637, 'philosophical': 9638, 'phosphorylase': 9639, 'photosphere': 9640, 'physician': 9641, 'physicians': 9642, 'pick': 9643, 'pie': 9644, 'pins': 9645, 'pivot': 9646, 'placarded': 9647, 'planet': 9648, 'planktonic': 9649, 'plantain': 9650, 'plantation': 9651, 'plasma': 9652, 'platform': 9653, 'platforms': 9654, 'plaza': 9655, 'points': 9656, 'policy': 9657, 'polyp': 9658, 'polysaccharides': 9659, 'poor': 9660, 'popliteal': 9661, 'populations': 9662, 'porcupines': 9663, 'pork': 9664, 'positioned': 9665, 'pounds': 9666, 'prepare': 9667, 'prequel': 9668, 'prescription': 9669, 'preseason': 9670, 'presenter': 9671, 'preserver': 9672, 'preyed': 9673, 'priest': 9674, 'priestly': 9675, 'priests': 9676, 'primary': 9677, 'printers': 9678, 'private': 9679, 'privileged': 9680, 'probably': 9681, 'profession': 9682, 'progressive': 9683, 'protecting': 9684, 'provide': 9685, 'provides': 9686, 'provolone': 9687, 'psychiatrist': 9688, 'punctatus': 9689, 'radioactive': 9690, 'ramp': 9691, 'ranked': 9692, 'ranking': 9693, 'rarely': 9694, 'reaches': 9695, 'rear': 9696, 'reasons': 9697, 'recolored': 9698, 'record': 9699, 'redox': 9700, 'reduced': 9701, 'reggae': 9702, 'regime': 9703, 'registries': 9704, 'regular': 9705, 'regulation': 9706, 'reigning': 9707, 'related': 9708, 'release': 9709, 'released': 9710, 'religion': 9711, 'religions': 9712, 'religious': 9713, 'remaining': 9714, 'remember': 9715, 'reporting': 9716, 'representative': 9717, 'reproduction': 9718, 'retrieves': 9719, 'retting': 9720, 'returning': 9721, 'rhyme': 9722, 'rhythm': 9723, 'ribeye': 9724, 'rice': 9725, 'ridges': 9726, 'ringneck': 9727, 'rivers': 9728, 'road': 9729, 'rocksteady': 9730, 'roll': 9731, 'rolled': 9732, 'rollies': 9733, 'rondo': 9734, 'rookie': 9735, 'room': 9736, 'roots': 9737, 'rounds': 9738, 'routing': 9739, 'rticulate': 9740, 'rupiah': 9741, 'russia': 9742, 'saccule': 9743, 'sadness': 9744, 'safeguarding': 9745, 'saguaro': 9746, 'salary': 9747, 'saline': 9748, 'sandstone': 9749, 'sarsen': 9750, 'sat': 9751, 'saturation': 9752, 'savanna': 9753, 'scalar': 9754, 'scene': 9755, 'scholarly': 9756, 'scientists': 9757, 'sea': 9758, 'secrecy': 9759, 'secret': 9760, 'see': 9761, 'seeing': 9762, 'semicircular': 9763, 'semilunar': 9764, 'sequences': 9765, 'serve': 9766, 'sessile': 9767, 'settled': 9768, 'settlers': 9769, 'seventh': 9770, 'shadow': 9771, 'shales': 9772, 'sharp': 9773, 'shepherd': 9774, 'sheriffs': 9775, 'shopping': 9776, 'shoulder': 9777, 'sidekick': 9778, 'signs': 9779, 'simple': 9780, 'sinus': 9781, 'sirloin': 9782, 'site': 9783, 'sixteen': 9784, 'skin': 9785, 'skip': 9786, 'skull': 9787, 'slavery': 9788, 'sliced': 9789, 'slide': 9790, 'slip': 9791, 'snake': 9792, 'snow': 9793, 'society': 9794, 'soft': 9795, 'software': 9796, 'soldiers': 9797, 'solid': 9798, 'somatic': 9799, 'soul': 9800, 'sounding': 9801, 'sovereignty': 9802, 'speaker': 9803, 'spears': 9804, 'special': 9805, 'specified': 9806, 'spinal': 9807, 'spleen': 9808, 'splendid': 9809, 'splendour': 9810, 'sponsor': 9811, 'spontaneous': 9812, 'spoonerism': 9813, 'sq': 9814, 'stables': 9815, 'stacked': 9816, 'star': 9817, 'start': 9818, 'steak': 9819, 'stimulating': 9820, 'stocks': 9821, 'stones': 9822, 'stratified': 9823, 'stratosphere': 9824, 'stratum': 9825, 'strength': 9826, 'strip': 9827, 'stripes': 9828, 'sub': 9829, 'subcontinent': 9830, 'subsidizes': 9831, 'subtitles': 9832, 'such': 9833, 'suicide': 9834, 'sunlight': 9835, 'superior': 9836, 'supermassive': 9837, 'supply': 9838, 'survival': 9839, 'sweat': 9840, 'symptoms': 9841, 'synthase': 9842, 'tangible': 9843, 'taxi': 9844, 'television': 9845, 'telomere': 9846, 'temperature': 9847, 'tenant': 9848, 'tender': 9849, 'tendons': 9850, 'tensions': 9851, 'terminal': 9852, 'testimony': 9853, 'tetrahedron': 9854, 'than': 9855, 'thane': 9856, 'themselves': 9857, 'thermal': 9858, 'these': 9859, 'thiamine': 9860, 'think': 9861, 'thinly': 9862, 'threefold': 9863, 'tie': 9864, 'tied': 9865, 'toll': 9866, 'tonsil': 9867, 'touched': 9868, 'tracks': 9869, 'traders': 9870, 'tradition': 9871, 'traditions': 9872, 'training': 9873, 'transaminase': 9874, 'transfer': 9875, 'transformed': 9876, 'translucent': 9877, 'trematode': 9878, 'trunk': 9879, 'turns': 9880, 'tweaks': 9881, 'twice': 9882, 'under-19': 9883, 'underneath': 9884, 'underwriting': 9885, 'unitary': 9886, 'unless': 9887, 'unlimited': 9888, 'upper': 9889, 'uppermost': 9890, 'ups': 9891, 'utricle': 9892, 'vehicles': 9893, 'veiled': 9894, 'veins': 9895, 'ventricle': 9896, 'vermix': 9897, 'verse': 9898, 'vessels': 9899, 'vestibular': 9900, 'veterans': 9901, 'victory': 9902, 'violence': 9903, 'viruses': 9904, 'vitamin': 9905, 'vocalist': 9906, 'volume': 9907, 'warning': 9908, 'water': 9909, 'waves': 9910, 'weaker': 9911, 'wealth': 9912, 'weapons': 9913, 'wear': 9914, 'web': 9915, 'wheel': 9916, 'while': 9917, 'wildlife': 9918, 'wind': 9919, 'winload.exe': 9920, 'winner': 9921, 'witches': 9922, 'without': 9923, 'withstand': 9924, 'woman': 9925, 'yelling': 9926, 'your': 9927, 'youth': 9928, '·': 9929, 'Édouard': 9930, 'Élodie': 9931, 'Émile': 9932, 'Čech': 9933, 'الحق\\u200e': 9934, 'انضمام': 9935, 'جمهوري': 9936, 'جمهوری': 9937, 'شورا': 9938, 'شوری\\u200e': 9939, 'ملي': 9940, 'ملی\\u200e': 9941, '“': 9942, '”': 9943, '₹1.526': 9944})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGyqmiNlyAiR",
        "outputId": "f0ed1950-2dbd-44e7-85a5-9421219372d5"
      },
      "source": [
        "Sentence.vocab.stoi['new construction'], Label.vocab.stoi['new construction'] "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsycLH0DEzFa"
      },
      "source": [
        "Load our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUo8gH-WEzFa"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fen1DHy6EzFb"
      },
      "source": [
        "We'll also print out an example just to double check they're not reversed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzKhUJo4EzFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946469d8-0525-4f4e-f000-95c37dda11a4"
      },
      "source": [
        "print(vars(train.examples[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sentence': ['Many', 'towns', 'and', 'cities', 'have', 'trash', 'cans', 'where', 'on', 'sidewalks', '?', '\\\\n', '(', 'A', ')', 'hospital', '(', 'B', ')', 'park', '(', 'C', ')', 'corner', '(', 'D', ')', 'motel', '(', 'E', ')', 'office'], 'label': ['corner']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRDnbA09EzFc"
      },
      "source": [
        "Then create our vocabulary, converting all tokens appearing less than twice into `<unk>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2by1IUzDEzFd"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2hiioCLEzFd"
      },
      "source": [
        "Finally, define the `device` and create our iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT4Zg9QrEzFd"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator_de_en, valid_iterator_de_en, test_iterator_de_en = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4davOyYEzFd"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YoFk7hgzQ1h"
      },
      "source": [
        "BATCH_SIZE=16\r\n",
        "train_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train, valid), \r\n",
        "    batch_size = BATCH_SIZE, \r\n",
        "    device = device, sort=False, shuffle=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeK3RzjvzZSg"
      },
      "source": [
        "iter_one = next(iter(train_iterator))"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cid31jyj-cuR",
        "outputId": "18c6a3db-3b31-4051-f191-8e3d2dcbf7fc"
      },
      "source": [
        "iter_one.label.shape"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([53, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NEKYsXD0HK2"
      },
      "source": [
        "for entry in (iter_one.sentence[0][0,:]):\r\n",
        "    print(entry.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqR66FM2EzFj"
      },
      "source": [
        "## Seq2Seq Model\n",
        "\n",
        "Putting the encoder and decoder together, we get:\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq7.png?raw=1)\n",
        "\n",
        "Again, in this implementation we need to ensure the hidden dimensions in both the encoder and the decoder are the same.\n",
        "\n",
        "Briefly going over all of the steps:\n",
        "- the `outputs` tensor is created to hold all predictions, $\\hat{Y}$\n",
        "- the source sequence, $X$, is fed into the encoder to receive a `context` vector\n",
        "- the initial decoder hidden state is set to be the `context` vector, $s_0 = z = h_T$\n",
        "- we use a batch of `<sos>` tokens as the first `input`, $y_1$\n",
        "- we then decode within a loop:\n",
        "  - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and the context vector, $z$, into the decoder\n",
        "  - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n",
        "  - we then decide if we are going to teacher force or not, setting the next input as appropriate (either the ground truth next token in the target sequence or the highest predicted next token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUQMjegvDkpX"
      },
      "source": [
        "## Model Checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEs1lsz62Cbl",
        "outputId": "8711c066-00a6-4b79-b5c4-a23c0df8389c"
      },
      "source": [
        "next(iter(train_iterator_de_en)), iter_one"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\n",
              " [torchtext.data.batch.Batch of size 128 from MULTI30K]\n",
              " \t[.src]:[torch.LongTensor of size 34x128]\n",
              " \t[.trg]:[torch.LongTensor of size 35x128], \n",
              " [torchtext.data.batch.Batch of size 128]\n",
              " \t[.sentence]:('[torch.LongTensor of size 128x64]', '[torch.LongTensor of size 128]')\n",
              " \t[.label]:('[torch.LongTensor of size 128x4]', '[torch.LongTensor of size 128]'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "ox7ymRsn-ggb",
        "outputId": "59c0e070-c953-4801-a6bc-a8aedbfc23a3"
      },
      "source": [
        "iter_one.sentence[1],iter_one.sentence[0][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6fc72cdf7d19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miter_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'iter_one' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX6OvwfE_PEz"
      },
      "source": [
        "text, text_lengths = iter_one.sentence\r\n",
        "embedded = nn.Embedding(len(Sentence.vocab), 256)(text)\r\n",
        "packed_seq = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False) \r\n",
        "outputs, hidden = nn.GRU(256, 512)(packed_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aEzHD7kSPpk"
      },
      "source": [
        "outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\r\n",
        "hidden, hidden_lengths = torch.nn.utils.rnn.pad_packed_sequence(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_uQydwySoFd"
      },
      "source": [
        "hidden, outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeWWqzIRSUZX",
        "outputId": "d1dd886a-6d7c-4ec3-f22e-3991aa6532e4"
      },
      "source": [
        "hidden.shape,dec_embed.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 128, 512]), torch.Size([1, 128, 256]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPdIucFuQw3q"
      },
      "source": [
        "label_text, label_text_lengths = iter_one.label\r\n",
        "label_embedded = nn.Embedding(len(Label.vocab), 256)(label_text)\r\n",
        "label_packed_seq = nn.utils.rnn.pack_padded_sequence(label_embedded, label_text_lengths.cpu(), batch_first=False, enforce_sorted=False) \r\n",
        "label_outputs, label_hidden = nn.GRU(256, 512)(label_packed_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPfuqe19L1b1"
      },
      "source": [
        "embedded.shape, text.shape\r\n",
        "non_padded_output = nn.GRU(256, 512)(embedded)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW1jXbkCPjm5",
        "outputId": "3d985a48-c7cc-44bf-d351-616e8ff80f99"
      },
      "source": [
        "print(non_padded_output[0].shape, non_padded_output[1].shape, outputs.data.shape, hidden.shape)\r\n",
        "print(label_outputs.data.shape, label_hidden.data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 64, 512]) torch.Size([1, 64, 512]) torch.Size([4986, 512]) torch.Size([1, 128, 512])\n",
            "torch.Size([207, 512]) torch.Size([1, 128, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlshEaYtCl_l"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCqHVG8WAa_M",
        "outputId": "ea0c6931-b59d-4cf9-d8ed-7bc18bf085ca"
      },
      "source": [
        "len(Sentence.vocab),len(Label.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4344, 986)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPYVujzqBVlM"
      },
      "source": [
        "iter_de_en = next(iter(train_iterator_de_en))\r\n",
        "embedded = nn.Embedding(len(SRC.vocab), 256)(iter_de_en.src)\r\n",
        "\r\n",
        "output_deen, hidden_deen = nn.GRU(256, 512)(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jVRY76CCjsB",
        "outputId": "083d8891-a253-4332-a02e-0403eb4a4b46"
      },
      "source": [
        "hidden_deen.shape, hidden.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 128, 512]), torch.Size([1, 128, 512]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym1rJxtjDg02"
      },
      "source": [
        "label_text, label_text_lengths =iter_one.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS1L66NMDlC0",
        "outputId": "db17467f-7dd9-4863-b88f-c1da71a9c677"
      },
      "source": [
        "print(label_text.shape, label_text_lengths.shape, iter_de_en.trg.shape)\r\n",
        "label_packed_seq = nn.utils.rnn.pack_padded_sequence(label_text, label_text_lengths.cpu(), batch_first=True, enforce_sorted=False) \r\n",
        "label_packed_seq.sorted_indices.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 4]) torch.Size([128]) torch.Size([31, 128])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QuxMVCDPU91",
        "outputId": "52b07782-cab1-45ad-bb38-2187c043e592"
      },
      "source": [
        "label_text.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOr5l2TfIxg9",
        "outputId": "5ccc3ab7-928f-4cc0-a498-e0a16047c6c9"
      },
      "source": [
        "label_packed_seq.data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([207])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7D36CMHGo6G",
        "outputId": "13c4f456-7878-4916-929b-8506b961e841"
      },
      "source": [
        "iter_de_en.src.shape, iter_de_en.trg.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([36, 128]), torch.Size([31, 128]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i9hoRWSGM31"
      },
      "source": [
        "label_text_reshaped =  label_text.reshape(4, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuQ5G0vJHRIS",
        "outputId": "229d5e15-50d8-419c-a5f4-cfba316cad1f"
      },
      "source": [
        "input_to_decoder.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGxidG4pHAY4"
      },
      "source": [
        "# input_to_decoder = label_text_reshaped[0,:]\r\n",
        "# input_to_decoder = input_to_decoder.unsqueeze(0)\r\n",
        "\r\n",
        "# INPUT_DIM = len(Sentence.vocab)\r\n",
        "# OUTPUT_DIM = len(Label.vocab)\r\n",
        "# ENC_EMB_DIM = 256\r\n",
        "# DEC_EMB_DIM = 256\r\n",
        "# HID_DIM = 512\r\n",
        "# ENC_DROPOUT = 0.5\r\n",
        "# DEC_DROPOUT = 0.5\r\n",
        "\r\n",
        "# enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\r\n",
        "# dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\r\n",
        "\r\n",
        "\r\n",
        "dec_embed = nn.Embedding(len(Label.vocab), 256)(input_to_decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwjR8rsPMgc7"
      },
      "source": [
        "dec_embed.shape\r\n",
        "dec_output, dec_hidden = nn.GRU(256, 512)(dec_embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX-2KkaNNqN_",
        "outputId": "f79c601f-b1dd-4692-a2e0-0422656da0cd"
      },
      "source": [
        "dec_embed.squeeze(0).shape,hidden.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([128, 256]), torch.Size([1, 128, 512]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0b0fGhONhXw",
        "outputId": "17f6412b-1fc0-4908-e60d-01338f146ea8"
      },
      "source": [
        "torch.cat((dec_embed, hidden), dim=2).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX8T2msbNDqu",
        "outputId": "a0a3b080-b3b9-4808-f7be-768cfdc388fa"
      },
      "source": [
        "dec_output.shape, dec_hidden.shape, hidden.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 128, 512]),\n",
              " torch.Size([1, 128, 512]),\n",
              " torch.Size([1, 128, 512]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUTUQnTBFcn9"
      },
      "source": [
        "for entry in label_packed_seq.data:\r\n",
        "    print(Label.vocab.itos[entry.item()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSH6CKKzCw3l"
      },
      "source": [
        "iter_de_en.trg[0,:].shape, iter_one.label[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT00q_JvjdGE",
        "outputId": "f16af9bb-44fb-4412-a7a4-6c317a2d87f4"
      },
      "source": [
        "label_text.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo0QowlGj1LI",
        "outputId": "7c08fd81-7c4c-4715-8840-4f39b92cb7c7"
      },
      "source": [
        "label_text_reshaped[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([196,  36,   3,   1, 393,   1,   1,   1,  35,  82,   1,   1, 971,   1,\n",
              "          1,   1, 888,   1,   1,   1,  28,   1,   1,   1,  17, 300, 551,   1,\n",
              "        791,  21,   1,   1, 605,   1,   1,   1, 215, 304,   1,   1,  58,  11,\n",
              "          5,   1, 428,  52,   1,   1, 108,  64,   1,   1, 705,   1,   1,   1,\n",
              "        549,   1,   1,   1,  51,   3,  16,   1, 148, 701,   1,   1,   6,   1,\n",
              "          1,   1,  44,   1,   1,   1,  12,   1,   1,   1, 410,   1,   1,   1,\n",
              "        795, 772,   1,   1, 688,   1,   1,   1, 611,   1,   1,   1, 592, 719,\n",
              "          1,   1, 188,   1,   1,   1,  34, 200, 497,   1, 563, 857,   1,   1,\n",
              "        601,   1,   1,   1, 771, 951,   1,   1, 665, 974,   1,   1,  25,   1,\n",
              "          1,   1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11CnDhTkEzFd"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        text, text_lengths = src\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # packed_seq = nn.utils.rnn.pack_padded_sequence(embedded, \n",
        "        #                                                text_lengths.cpu(),\n",
        "        #                                                batch_first=False,\n",
        "        #                                                enforce_sorted=False)        \n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded) #no cell state! \n",
        "        ## outputs is a packed sequence but since it is not used we will not \n",
        "        ## unpack it         \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Hb0dXWHLsG"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRA8hkiLEzFh"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded) #no cell state!\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, context):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #context = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        #context = [1, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # print(embedded.shape, hidden.shape, context.shape)\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "            \n",
        "        #emb_con = [1, batch size, emb dim + hid dim]\n",
        "            \n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \n",
        "                           dim = 1)\n",
        "        \n",
        "        #output = [batch size, emb dim + hid dim * 2]\n",
        "        \n",
        "        prediction = self.fc_out(output)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDyNrQ8VEzFk"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is the context\n",
        "        context = self.encoder(src)\n",
        "        \n",
        "        #context also used as the initial hidden state of the decoder\n",
        "        hidden = context\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state and the context state\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIGxiriM4GRC"
      },
      "source": [
        "## Old LSTM Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KimPCskv5Eal"
      },
      "source": [
        "def train_lstm(model, iterator, optimizer, criterion):\r\n",
        "    \r\n",
        "    # initialize every epoch \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    # set the model in training phase\r\n",
        "    model.train()  \r\n",
        "    \r\n",
        "    for batch in iterator:\r\n",
        "        \r\n",
        "        # resets the gradients after every batch\r\n",
        "        optimizer.zero_grad()   \r\n",
        "        \r\n",
        "        # retrieve text and no. of words\r\n",
        "        sentence, sentence_lengths = batch.sentence\r\n",
        "        \r\n",
        "        # convert to 1D tensor\r\n",
        "        predictions = model(sentence, sentence_lengths).squeeze()  \r\n",
        "        #print(predictions)\r\n",
        "        # compute the loss\r\n",
        "        loss = criterion(predictions, batch.label)        \r\n",
        "        \r\n",
        "        # compute the binary accuracy\r\n",
        "        acc = binary_accuracy(predictions, batch.label)   \r\n",
        "        \r\n",
        "        # backpropage the loss and compute the gradients\r\n",
        "        loss.backward()       \r\n",
        "        \r\n",
        "        # update the weights\r\n",
        "        optimizer.step()      \r\n",
        "        \r\n",
        "        # loss and accuracy\r\n",
        "        epoch_loss += loss.item()  \r\n",
        "        epoch_acc += acc.item()    \r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNQqSPyn4nAx"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class classifier(nn.Module):\r\n",
        "    \r\n",
        "    # Define all the layers used in model\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\r\n",
        "        \r\n",
        "        super().__init__()          \r\n",
        "        \r\n",
        "        # Embedding layer\r\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "        \r\n",
        "        # LSTM layer\r\n",
        "        self.encoder = nn.LSTM(embedding_dim, \r\n",
        "                           hidden_dim, \r\n",
        "                           num_layers=n_layers, \r\n",
        "                           dropout=dropout,\r\n",
        "                           batch_first=True)\r\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\r\n",
        "        # try bidirectional and compare their performances\r\n",
        "        \r\n",
        "        # Dense layer\r\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\r\n",
        "        \r\n",
        "    def forward(self, text, text_lengths):\r\n",
        "        \r\n",
        "        # text = [batch size, sent_length]\r\n",
        "        embedded = self.embedding(text)\r\n",
        "        # embedded = [batch size, sent_len, emb dim]\r\n",
        "      \r\n",
        "        # packed sequence\r\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\r\n",
        "        \r\n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\r\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\r\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\r\n",
        "    \r\n",
        "        # Hidden = [batch size, hid dim * num directions]\r\n",
        "        dense_outputs = self.fc(hidden)   \r\n",
        "        \r\n",
        "        # Final activation function softmax\r\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\r\n",
        "            \r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxVMMPUyEzFk"
      },
      "source": [
        "# Training the Seq2Seq Model\n",
        "\n",
        "The rest of this session is very similar to the previous one. \n",
        "\n",
        "We initialise our encoder, decoder and seq2seq model (placing it on the GPU if we have one). As before, the embedding dimensions and the amount of dropout used can be different between the encoder and the decoder, but the hidden dimensions must remain the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDG6jOSuEzFk"
      },
      "source": [
        "# INPUT_DIM = len(SRC.vocab)\n",
        "# OUTPUT_DIM = len(TRG.vocab)\n",
        "\n",
        "INPUT_DIM = len(Sentence.vocab)\n",
        "OUTPUT_DIM = len(Label.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 256\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIc1l6CEzFk"
      },
      "source": [
        "Next, we initialize our parameters. The paper states the parameters are initialized from a normal distribution with a mean of 0 and a standard deviation of 0.01, i.e. $\\mathcal{N}(0, 0.01)$. \n",
        "\n",
        "It also states we should initialize the recurrent parameters to a special initialization, however to keep things simple we'll also initialize them to $\\mathcal{N}(0, 0.01)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgqMqq-oEzFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52802e93-1428-4e4d-8753-19300d06abf1"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(6784, 256)\n",
              "    (rnn): GRU(256, 256)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(9945, 256)\n",
              "    (rnn): GRU(512, 256)\n",
              "    (fc_out): Linear(in_features=768, out_features=9945, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd1QoOsUEzFl"
      },
      "source": [
        "We print out the number of parameters.\n",
        "\n",
        "Even though we only have a single layer RNN for our encoder and decoder we actually have **more** parameters  than the last model. This is due to the increased size of the inputs to the GRU and the linear layer. However, it is not a significant amount of parameters and causes a minimal amount of increase in training time (~3 seconds per epoch extra)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IggCwIBgEzFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ce0561-1a44-44e5-919f-a71677d69fa7"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 12,916,441 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiH54qzHEzFl"
      },
      "source": [
        "We initiaize our optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO1_eoG7EzFl"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR, OneCycleLR, MultiStepLR, CyclicLR, ReduceLROnPlateau\r\n",
        "\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0009893272, weight_decay=0.0001)\r\n",
        "scheduler = OneCycleLR(optimizer, \r\n",
        "                        0.001, \r\n",
        "                        epochs=100, \r\n",
        "                        cycle_momentum=False, \r\n",
        "                        steps_per_epoch=len(train_iterator), \r\n",
        "                        #base_momentum=config.momentum,\r\n",
        "                        #max_momentum=0.95, \r\n",
        "                        pct_start=0.208,\r\n",
        "                        # anneal_strategy=config.anneal_strategy,\r\n",
        "                        div_factor=100,\r\n",
        "                        # final_div_factor=config.final_div_factor\r\n",
        "                        )\r\n",
        "# 0.000981989011942079"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwkV3FKlEzFl"
      },
      "source": [
        "We also initialize the loss function, making sure to ignore the loss on `<pad>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0DAbGbcEzFl"
      },
      "source": [
        "#TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "TRG_PAD_IDX = Label.vocab.stoi[Label.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTay7E9rEzFm"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OYuoFdEzFm"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.sentence\n",
        "        trg = batch.label\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        # print(\"output_dim:\", output.shape)\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        # print(\"output_dim before loss:\", output.shape, trg.shape)\n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEAia1wGvv9K",
        "outputId": "96ef3473-a60b-4568-a088-e9ba0437f088"
      },
      "source": [
        "iter_one.label[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfey_MRivGVO"
      },
      "source": [
        "iter_one = next(iter(train_iterator))\r\n",
        "iter_one.label[0][1:].view(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rfUx5lhEzFm"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw022pw0EzFm"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            # src = batch.src\n",
        "            # trg = batch.trg\n",
        "            src = batch.sentence\n",
        "            trg = batch.label\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E43h8dnQEzFm"
      },
      "source": [
        "We'll also define the function that calculates how long an epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTAmu3-EEzFm"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbCGyO4ZEzFm"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJMgHMkZ71kr"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5fzUqG-nkhn"
      },
      "source": [
        "#optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.001)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjFyRUK9EzFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6731895-1fc7-4f4b-8166-a70629ecb0e0"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "# model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\n",
        "    \n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | LR: {scheduler.get_last_lr()}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 39s\n",
            "\tTrain Loss: 4.327 | Train PPL:  75.738 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 5.967 |  Val. PPL: 390.312\n",
            "Epoch: 02 | Time: 0m 39s\n",
            "\tTrain Loss: 3.858 | Train PPL:  47.367 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 5.818 |  Val. PPL: 336.325\n",
            "Epoch: 03 | Time: 0m 39s\n",
            "\tTrain Loss: 3.411 | Train PPL:  30.299 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.075 |  Val. PPL: 434.819\n",
            "Epoch: 04 | Time: 0m 39s\n",
            "\tTrain Loss: 3.004 | Train PPL:  20.160 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.234 |  Val. PPL: 509.812\n",
            "Epoch: 05 | Time: 0m 39s\n",
            "\tTrain Loss: 2.610 | Train PPL:  13.598 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.272 |  Val. PPL: 529.730\n",
            "Epoch: 06 | Time: 0m 39s\n",
            "\tTrain Loss: 2.228 | Train PPL:   9.278 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.080 |  Val. PPL: 436.870\n",
            "Epoch: 07 | Time: 0m 39s\n",
            "\tTrain Loss: 1.886 | Train PPL:   6.592 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 5.987 |  Val. PPL: 398.151\n",
            "Epoch: 08 | Time: 0m 39s\n",
            "\tTrain Loss: 1.560 | Train PPL:   4.758 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 5.980 |  Val. PPL: 395.321\n",
            "Epoch: 09 | Time: 0m 39s\n",
            "\tTrain Loss: 1.300 | Train PPL:   3.669 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.126 |  Val. PPL: 457.661\n",
            "Epoch: 10 | Time: 0m 39s\n",
            "\tTrain Loss: 1.106 | Train PPL:   3.021 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.110 |  Val. PPL: 450.534\n",
            "Epoch: 11 | Time: 0m 39s\n",
            "\tTrain Loss: 0.963 | Train PPL:   2.619 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.262 |  Val. PPL: 524.233\n",
            "Epoch: 12 | Time: 0m 39s\n",
            "\tTrain Loss: 0.845 | Train PPL:   2.328 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.083 |  Val. PPL: 438.509\n",
            "Epoch: 13 | Time: 0m 39s\n",
            "\tTrain Loss: 0.745 | Train PPL:   2.106 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.057 |  Val. PPL: 427.094\n",
            "Epoch: 14 | Time: 0m 39s\n",
            "\tTrain Loss: 0.676 | Train PPL:   1.965 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.181 |  Val. PPL: 483.498\n",
            "Epoch: 15 | Time: 0m 39s\n",
            "\tTrain Loss: 0.583 | Train PPL:   1.792 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.061 |  Val. PPL: 428.704\n",
            "Epoch: 16 | Time: 0m 39s\n",
            "\tTrain Loss: 0.533 | Train PPL:   1.704 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.181 |  Val. PPL: 483.289\n",
            "Epoch: 17 | Time: 0m 39s\n",
            "\tTrain Loss: 0.494 | Train PPL:   1.639 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.184 |  Val. PPL: 485.158\n",
            "Epoch: 18 | Time: 0m 39s\n",
            "\tTrain Loss: 0.451 | Train PPL:   1.570 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.144 |  Val. PPL: 465.745\n",
            "Epoch: 19 | Time: 0m 39s\n",
            "\tTrain Loss: 0.418 | Train PPL:   1.519 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.185 |  Val. PPL: 485.495\n",
            "Epoch: 20 | Time: 0m 39s\n",
            "\tTrain Loss: 0.403 | Train PPL:   1.496 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 6.191 |  Val. PPL: 488.409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltjZUf7x_PNi",
        "outputId": "7fe0b753-913d-4a57-a71d-6808fed15703"
      },
      "source": [
        "optimizer"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    eps: 1e-08\n",
              "    lr: 0.001\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRfuK1MHIg7v"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.01)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYIRZh9vLH_4",
        "outputId": "55c4730a-aa89-4f05-e6e1-12fa3111ac8d"
      },
      "source": [
        "best_valid_loss"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.818077982740199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXHg1YUjEnXz",
        "outputId": "bbaa6b32-c0f4-43cb-9a58-281b318a889b"
      },
      "source": [
        "N_EPOCHS = 20\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = 5.818077982740199\r\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "print(best_valid_loss)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 39s\n",
            "\tTrain Loss: 3.856 | Train PPL:  47.272\n",
            "\t Val. Loss: 5.698 |  Val. PPL: 298.329\n",
            "Epoch: 02 | Time: 0m 39s\n",
            "\tTrain Loss: 4.366 | Train PPL:  78.757\n",
            "\t Val. Loss: 5.911 |  Val. PPL: 369.151\n",
            "Epoch: 03 | Time: 0m 39s\n",
            "\tTrain Loss: 4.646 | Train PPL: 104.201\n",
            "\t Val. Loss: 5.929 |  Val. PPL: 375.843\n",
            "Epoch: 04 | Time: 0m 39s\n",
            "\tTrain Loss: 4.815 | Train PPL: 123.316\n",
            "\t Val. Loss: 5.853 |  Val. PPL: 348.252\n",
            "Epoch: 05 | Time: 0m 39s\n",
            "\tTrain Loss: 4.922 | Train PPL: 137.241\n",
            "\t Val. Loss: 5.793 |  Val. PPL: 327.893\n",
            "Epoch: 06 | Time: 0m 39s\n",
            "\tTrain Loss: 4.962 | Train PPL: 142.917\n",
            "\t Val. Loss: 5.731 |  Val. PPL: 308.184\n",
            "Epoch: 07 | Time: 0m 39s\n",
            "\tTrain Loss: 4.991 | Train PPL: 147.156\n",
            "\t Val. Loss: 5.781 |  Val. PPL: 324.109\n",
            "Epoch: 08 | Time: 0m 39s\n",
            "\tTrain Loss: 5.010 | Train PPL: 149.879\n",
            "\t Val. Loss: 5.730 |  Val. PPL: 308.008\n",
            "Epoch: 09 | Time: 0m 39s\n",
            "\tTrain Loss: 5.024 | Train PPL: 152.050\n",
            "\t Val. Loss: 5.707 |  Val. PPL: 300.955\n",
            "Epoch: 10 | Time: 0m 39s\n",
            "\tTrain Loss: 5.030 | Train PPL: 152.926\n",
            "\t Val. Loss: 5.791 |  Val. PPL: 327.370\n",
            "Epoch: 11 | Time: 0m 39s\n",
            "\tTrain Loss: 5.035 | Train PPL: 153.651\n",
            "\t Val. Loss: 5.756 |  Val. PPL: 316.128\n",
            "Epoch: 12 | Time: 0m 39s\n",
            "\tTrain Loss: 5.061 | Train PPL: 157.803\n",
            "\t Val. Loss: 5.773 |  Val. PPL: 321.520\n",
            "Epoch: 13 | Time: 0m 39s\n",
            "\tTrain Loss: 5.070 | Train PPL: 159.198\n",
            "\t Val. Loss: 5.798 |  Val. PPL: 329.670\n",
            "Epoch: 14 | Time: 0m 39s\n",
            "\tTrain Loss: 5.074 | Train PPL: 159.750\n",
            "\t Val. Loss: 5.730 |  Val. PPL: 308.001\n",
            "Epoch: 15 | Time: 0m 39s\n",
            "\tTrain Loss: 5.088 | Train PPL: 161.988\n",
            "\t Val. Loss: 5.667 |  Val. PPL: 289.240\n",
            "Epoch: 16 | Time: 0m 39s\n",
            "\tTrain Loss: 5.090 | Train PPL: 162.448\n",
            "\t Val. Loss: 5.695 |  Val. PPL: 297.258\n",
            "Epoch: 17 | Time: 0m 39s\n",
            "\tTrain Loss: 5.088 | Train PPL: 162.035\n",
            "\t Val. Loss: 5.694 |  Val. PPL: 297.133\n",
            "Epoch: 18 | Time: 0m 39s\n",
            "\tTrain Loss: 5.084 | Train PPL: 161.371\n",
            "\t Val. Loss: 5.730 |  Val. PPL: 307.930\n",
            "Epoch: 19 | Time: 0m 39s\n",
            "\tTrain Loss: 5.092 | Train PPL: 162.761\n",
            "\t Val. Loss: 5.746 |  Val. PPL: 312.848\n",
            "Epoch: 20 | Time: 0m 39s\n",
            "\tTrain Loss: 5.094 | Train PPL: 162.972\n",
            "\t Val. Loss: 5.694 |  Val. PPL: 297.090\n",
            "5.6672566900862025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq3-W2A5rVLn",
        "outputId": "041a3c92-7b28-4f9a-e983-d221e163c48c"
      },
      "source": [
        "print(best_valid_loss)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.566523430195261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4plhSU-4qSw"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.000001, weight_decay=0.01)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uGl0qfzjKcs",
        "outputId": "fa5a13ba-7510-4f1c-8618-c92b17ea9898"
      },
      "source": [
        "N_EPOCHS = 20\r\n",
        "CLIP = 1\r\n",
        "print(best_valid_loss)\r\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\r\n",
        "\r\n",
        "best_valid_loss = 5.566523430195261 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.566523430195261\n",
            "Epoch: 01 | Time: 0m 14s\n",
            "\tTrain Loss: 5.269 | Train PPL: 194.203\n",
            "\t Val. Loss: 5.566 |  Val. PPL: 261.344\n",
            "Epoch: 02 | Time: 0m 14s\n",
            "\tTrain Loss: 5.274 | Train PPL: 195.217\n",
            "\t Val. Loss: 5.566 |  Val. PPL: 261.502\n",
            "Epoch: 03 | Time: 0m 14s\n",
            "\tTrain Loss: 5.274 | Train PPL: 195.126\n",
            "\t Val. Loss: 5.566 |  Val. PPL: 261.268\n",
            "Epoch: 04 | Time: 0m 14s\n",
            "\tTrain Loss: 5.273 | Train PPL: 195.009\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.844\n",
            "Epoch: 05 | Time: 0m 14s\n",
            "\tTrain Loss: 5.272 | Train PPL: 194.738\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.865\n",
            "Epoch: 06 | Time: 0m 14s\n",
            "\tTrain Loss: 5.273 | Train PPL: 194.975\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.885\n",
            "Epoch: 07 | Time: 0m 14s\n",
            "\tTrain Loss: 5.273 | Train PPL: 194.943\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.903\n",
            "Epoch: 08 | Time: 0m 14s\n",
            "\tTrain Loss: 5.270 | Train PPL: 194.457\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.942\n",
            "Epoch: 09 | Time: 0m 14s\n",
            "\tTrain Loss: 5.277 | Train PPL: 195.818\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.943\n",
            "Epoch: 10 | Time: 0m 14s\n",
            "\tTrain Loss: 5.287 | Train PPL: 197.838\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.856\n",
            "Epoch: 11 | Time: 0m 14s\n",
            "\tTrain Loss: 5.288 | Train PPL: 198.036\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.799\n",
            "Epoch: 12 | Time: 0m 14s\n",
            "\tTrain Loss: 5.288 | Train PPL: 197.907\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.782\n",
            "Epoch: 13 | Time: 0m 14s\n",
            "\tTrain Loss: 5.290 | Train PPL: 198.377\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.781\n",
            "Epoch: 14 | Time: 0m 14s\n",
            "\tTrain Loss: 5.287 | Train PPL: 197.690\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.786\n",
            "Epoch: 15 | Time: 0m 14s\n",
            "\tTrain Loss: 5.287 | Train PPL: 197.789\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.798\n",
            "Epoch: 16 | Time: 0m 14s\n",
            "\tTrain Loss: 5.290 | Train PPL: 198.310\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.804\n",
            "Epoch: 17 | Time: 0m 14s\n",
            "\tTrain Loss: 5.287 | Train PPL: 197.830\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.832\n",
            "Epoch: 18 | Time: 0m 14s\n",
            "\tTrain Loss: 5.282 | Train PPL: 196.861\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.857\n",
            "Epoch: 19 | Time: 0m 14s\n",
            "\tTrain Loss: 5.286 | Train PPL: 197.602\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.882\n",
            "Epoch: 20 | Time: 0m 14s\n",
            "\tTrain Loss: 5.287 | Train PPL: 197.665\n",
            "\t Val. Loss: 5.568 |  Val. PPL: 261.903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNDTf3JEPsD1",
        "outputId": "683ecc20-a18b-4fb8-84f9-fe3d0d6a1b2d"
      },
      "source": [
        "best_valid_loss"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.654881441846807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-T7eGpdQk7J"
      },
      "source": [
        "#### Final Perplexity = \r\n",
        "Epoch: 16 | Time: 0m 14s\r\n",
        "\tTrain Loss: 5.290 | Train PPL: 198.310\r\n",
        "\t Val. Loss: 5.568 |  Val. PPL: 261.804"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaJo3X9aEzFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7f3a0a-9672-4d3f-b1fc-31318b620a16"
      },
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 5.655 | Test PPL: 285.683 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY7SsC8TEzFn"
      },
      "source": [
        "Just looking at the test loss, we get better performance. This is a pretty good sign that this model architecture is doing something right! Relieving the information compression seems like the way forard, and in the next tutorial we'll expand on this even further with *attention*."
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "END_NoCoDeR_BPETokenize.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1px_H2FL3xEE",
        "i5Pn6pQ1C9fn",
        "hma47DbmC1E1",
        "B5epf-rxhjY0"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "33d763a041d341bc8ed1f57413fdc838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f8eacac26ea14ef68f02e9f80ce314b9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a83996ba92f1468aa2b7e2d3f1afc453",
              "IPY_MODEL_6c7f89c5680a4745bfd473b3dae09d38"
            ]
          }
        },
        "f8eacac26ea14ef68f02e9f80ce314b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a83996ba92f1468aa2b7e2d3f1afc453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e6f3bea487764026aba16708ea84793b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898822,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898822,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a75359b15dd474da1e0c969b3618649"
          }
        },
        "6c7f89c5680a4745bfd473b3dae09d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7b65ae4581d14936b2d2b90459f648ba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:02&lt;00:00, 362kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d69c58244d44b79adb73f4be949c5ec"
          }
        },
        "e6f3bea487764026aba16708ea84793b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a75359b15dd474da1e0c969b3618649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b65ae4581d14936b2d2b90459f648ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d69c58244d44b79adb73f4be949c5ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb899c2f829c4ef3b8eb290233cdd279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1164760fed9e4b44a38ec7dc04b27f3d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_75a441642118440cb778251809266f09",
              "IPY_MODEL_3bd952adcdae4b44bb274372988a0e67"
            ]
          }
        },
        "1164760fed9e4b44a38ec7dc04b27f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75a441642118440cb778251809266f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_95095d77c1b44c96bbe6efacb8eb3227",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd22bf6f5c524a949f0e12c17519f1b8"
          }
        },
        "3bd952adcdae4b44bb274372988a0e67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8b951e8363694b42b3fe67e6aa587698",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 485kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8655d9dfa4f478ba66e3e5c7e6caa64"
          }
        },
        "95095d77c1b44c96bbe6efacb8eb3227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd22bf6f5c524a949f0e12c17519f1b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b951e8363694b42b3fe67e6aa587698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8655d9dfa4f478ba66e3e5c7e6caa64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "79f9557b0b2a4896bc2e306893d03384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a2488c304c054b6f8ddc4678a3384ed4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c9b4a9bd745344cbb0b88d12987d3d64",
              "IPY_MODEL_9c2a833a3e8844d4b298239f035598d5"
            ]
          }
        },
        "a2488c304c054b6f8ddc4678a3384ed4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9b4a9bd745344cbb0b88d12987d3d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aeb0ce3a51a94efb8f7b0d10709b47af",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 150,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 150,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5892578d481d47069e6d78e116635648"
          }
        },
        "9c2a833a3e8844d4b298239f035598d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cd8408197a0c44df8d63b614d96b73cb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 150/150 [00:00&lt;00:00, 203B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_28e481fd2e024629817a35e9993ed3e5"
          }
        },
        "aeb0ce3a51a94efb8f7b0d10709b47af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5892578d481d47069e6d78e116635648": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd8408197a0c44df8d63b614d96b73cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "28e481fd2e024629817a35e9993ed3e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "900b361e9bff41e1b06013abb7ae35d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2bd68e9116f24234aed47b024b46a2a6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b6fa1824364b41268727c9824ab0cd88",
              "IPY_MODEL_b2dcd087c4eb4579a8069a6c648c8827"
            ]
          }
        },
        "2bd68e9116f24234aed47b024b46a2a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b6fa1824364b41268727c9824ab0cd88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b7974e509b334fd1b34545f7873cc55e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 25,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 25,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_daabdf3cadf349428d1b594c5c292c42"
          }
        },
        "b2dcd087c4eb4579a8069a6c648c8827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_af238e4fe8f54139ac997c7faaf5341d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 25.0/25.0 [00:00&lt;00:00, 88.2B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11487e6f38404702a9ccaa2f22e4c2cd"
          }
        },
        "b7974e509b334fd1b34545f7873cc55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "daabdf3cadf349428d1b594c5c292c42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af238e4fe8f54139ac997c7faaf5341d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11487e6f38404702a9ccaa2f22e4c2cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01be97f4600b41b283f03476754704e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_08f21825da2f4dcaa8e6c3a199904c09",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8181535b9d1b4ea1bd0e62c398441970",
              "IPY_MODEL_624e0b080ade4178995e4770a08fadeb"
            ]
          }
        },
        "08f21825da2f4dcaa8e6c3a199904c09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8181535b9d1b4ea1bd0e62c398441970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e34ae2950f1241dea586c18af0ce5827",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 498,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 498,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e02afe2719ba4f6fbcce1300a22a4d27"
          }
        },
        "624e0b080ade4178995e4770a08fadeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_52a3e2128e1c48f9ae7c14c33dd2b186",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 498/498 [00:01&lt;00:00, 370B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_52445c5afaa3425a9eeca9fc00e8c872"
          }
        },
        "e34ae2950f1241dea586c18af0ce5827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e02afe2719ba4f6fbcce1300a22a4d27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52a3e2128e1c48f9ae7c14c33dd2b186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "52445c5afaa3425a9eeca9fc00e8c872": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EVAP2/blob/master/END_NoCoDeR_BPETokenize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOdNYOq_V5C3",
        "outputId": "9a7d3e48-40cb-4c31-e571-03322f474c14"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Mar 13 05:32:14 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4uN4sSQlA7w"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import fileinput\n",
        "import re\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.display import display, HTML\n",
        "#import seaborn as sns\n",
        "import dateutil.parser\n",
        "import datetime\n",
        "#from ipyfilechooser import FileChooser\n",
        "import numpy as np\n",
        "import os\n",
        "import gzip\n",
        "import dateutil.parser\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import glob\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import timedelta\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "import spacy\n",
        "from pprint import pprint\n",
        "import six\n",
        "import sys, token, tokenize\n",
        "import ast\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "#from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm, trange\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzTyfBvwcT50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a2ced9-c72f-4d1e-ba5c-d447de8f902f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRNwGADTvOuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f745a330-bc27-4a0d-d702-fbe51e568457"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 49.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=3f6633259a077428075794322309d23cd828ca547eb20650c811066c97ba6d28\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6XDZbeWK6u_"
      },
      "source": [
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
        "import tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_7b5amJJIQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f6ef837-3b71-433e-f420-9e6afcc9ea14"
      },
      "source": [
        "%%bash\n",
        "python -m spacy download en\n",
        "python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py): started\n",
            "  Building wheel for de-core-news-sm (setup.py): finished with status 'done'\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp37-none-any.whl size=14907057 sha256=02f4971a5a268e1edd40de90dbf8e4c0695bd9946461c3e40f0d68b595720991\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-menevs16/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqKpFyhplfvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d9176d-0446-4760-e3a6-205a9bae9c1a"
      },
      "source": [
        "!wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n",
        "!wget -O gpt2_bpe_encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
        "\n",
        "!unzip /content/python.zip\n",
        "!cp /content/python/final/jsonl/train/python_train_0.jsonl.gz .\n",
        "!gzip -d /content/python_train_0.jsonl.gz\n",
        "\n",
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/english_python_data.txt .\n",
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/english_python_cleaned.txt .\n",
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone.csv ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-13 05:34:57--  https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.114.117\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.114.117|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 940909997 (897M) [application/zip]\n",
            "Saving to: ‘python.zip’\n",
            "\n",
            "python.zip          100%[===================>] 897.32M  46.9MB/s    in 20s     \n",
            "\n",
            "2021-03-13 05:35:17 (45.4 MB/s) - ‘python.zip’ saved [940909997/940909997]\n",
            "\n",
            "--2021-03-13 05:35:17--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [text/plain]\n",
            "Saving to: ‘gpt2_bpe_encoder.json’\n",
            "\n",
            "gpt2_bpe_encoder.js 100%[===================>]   1018K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-03-13 05:35:18 (20.5 MB/s) - ‘gpt2_bpe_encoder.json’ saved [1042301/1042301]\n",
            "\n",
            "Archive:  /content/python.zip\n",
            "   creating: python/\n",
            "   creating: python/final/\n",
            "   creating: python/final/jsonl/\n",
            "   creating: python/final/jsonl/train/\n",
            "  inflating: python/final/jsonl/train/python_train_9.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_12.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_10.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_0.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_6.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_2.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_4.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_8.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_11.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_5.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_13.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_3.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_1.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_7.jsonl.gz  \n",
            "   creating: python/final/jsonl/test/\n",
            "  inflating: python/final/jsonl/test/python_test_0.jsonl.gz  \n",
            "   creating: python/final/jsonl/valid/\n",
            "  inflating: python/final/jsonl/valid/python_valid_0.jsonl.gz  \n",
            "  inflating: python_dedupe_definitions_v2.pkl  \n",
            "  inflating: python_licenses.pkl     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGoF0UiKmLg-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262,
          "referenced_widgets": [
            "33d763a041d341bc8ed1f57413fdc838",
            "f8eacac26ea14ef68f02e9f80ce314b9",
            "a83996ba92f1468aa2b7e2d3f1afc453",
            "6c7f89c5680a4745bfd473b3dae09d38",
            "e6f3bea487764026aba16708ea84793b",
            "3a75359b15dd474da1e0c969b3618649",
            "7b65ae4581d14936b2d2b90459f648ba",
            "6d69c58244d44b79adb73f4be949c5ec",
            "eb899c2f829c4ef3b8eb290233cdd279",
            "1164760fed9e4b44a38ec7dc04b27f3d",
            "75a441642118440cb778251809266f09",
            "3bd952adcdae4b44bb274372988a0e67",
            "95095d77c1b44c96bbe6efacb8eb3227",
            "bd22bf6f5c524a949f0e12c17519f1b8",
            "8b951e8363694b42b3fe67e6aa587698",
            "c8655d9dfa4f478ba66e3e5c7e6caa64",
            "79f9557b0b2a4896bc2e306893d03384",
            "a2488c304c054b6f8ddc4678a3384ed4",
            "c9b4a9bd745344cbb0b88d12987d3d64",
            "9c2a833a3e8844d4b298239f035598d5",
            "aeb0ce3a51a94efb8f7b0d10709b47af",
            "5892578d481d47069e6d78e116635648",
            "cd8408197a0c44df8d63b614d96b73cb",
            "28e481fd2e024629817a35e9993ed3e5",
            "900b361e9bff41e1b06013abb7ae35d4",
            "2bd68e9116f24234aed47b024b46a2a6",
            "b6fa1824364b41268727c9824ab0cd88",
            "b2dcd087c4eb4579a8069a6c648c8827",
            "b7974e509b334fd1b34545f7873cc55e",
            "daabdf3cadf349428d1b594c5c292c42",
            "af238e4fe8f54139ac997c7faaf5341d",
            "11487e6f38404702a9ccaa2f22e4c2cd",
            "01be97f4600b41b283f03476754704e8",
            "08f21825da2f4dcaa8e6c3a199904c09",
            "8181535b9d1b4ea1bd0e62c398441970",
            "624e0b080ade4178995e4770a08fadeb",
            "e34ae2950f1241dea586c18af0ce5827",
            "e02afe2719ba4f6fbcce1300a22a4d27",
            "52a3e2128e1c48f9ae7c14c33dd2b186",
            "52445c5afaa3425a9eeca9fc00e8c872"
          ]
        },
        "outputId": "018f551a-b283-4c8d-9b13-900b550b407c"
      },
      "source": [
        "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "spacy_en = spacy.load('en')\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "#model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "#model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33d763a041d341bc8ed1f57413fdc838",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898822.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb899c2f829c4ef3b8eb290233cdd279",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79f9557b0b2a4896bc2e306893d03384",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=150.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "900b361e9bff41e1b06013abb7ae35d4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=25.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01be97f4600b41b283f03476754704e8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=498.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NPC5Tjn-eHe"
      },
      "source": [
        "\"\"\"\n",
        "    Removes docstrings and comments\n",
        "\"\"\"\n",
        "def remove_docstrings_comments(src_string, doc_string=None, debug=False):\n",
        "    mod = []\n",
        "\n",
        "    prev_toktype = token.INDENT\n",
        "    first_line = None\n",
        "    last_lineno = -1\n",
        "    last_col = 0\n",
        "    try:\n",
        "        #tokgen = tokenize.generate_tokens(source.readline)\n",
        "        tokgen = tokenize.generate_tokens(six.StringIO(src_string.rstrip()).readline)\n",
        "        for toktype, ttext, (slineno, scol), (elineno, ecol), ltext in tokgen:\n",
        "            if 0:   # Change to if 1 to see the tokens fly by.\n",
        "                print(\"%10s %-14s %-20r %r\" % (\n",
        "                    tokenize.tok_name.get(toktype, toktype),\n",
        "                    \"%d.%d-%d.%d\" % (slineno, scol, elineno, ecol),\n",
        "                    ttext, ltext\n",
        "                    ))\n",
        "            if slineno > last_lineno:\n",
        "                last_col = 0\n",
        "            if scol > last_col:\n",
        "                mod.append(\" \" * (scol - last_col))\n",
        "            if toktype == token.STRING and prev_toktype == token.INDENT:\n",
        "                # Docstring\n",
        "                mod.append(\"#--\")\n",
        "            elif toktype == tokenize.COMMENT:\n",
        "                # Comment\n",
        "                mod.append(\"##\")\n",
        "            else:\n",
        "                mod.append(ttext)\n",
        "            prev_toktype = toktype\n",
        "            last_col = ecol\n",
        "            last_lineno = elineno\n",
        "        return \"\".join(mod)\n",
        "    except:\n",
        "        print(doc_string)\n",
        "        print(src_string )\n",
        "        print(sys.exc_info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1px_H2FL3xEE"
      },
      "source": [
        "### CodeSearchNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0T-ZXnm-z3J"
      },
      "source": [
        "columns_long_list = ['repo', 'path', 'url', 'code', \n",
        "                     'code_tokens', 'docstring', 'docstring_tokens', \n",
        "                     'language', 'partition']\n",
        "\n",
        "with open(\"/content/python_train_0.jsonl\",'r') as f:\n",
        "    pd.concat([pd.read_json(f, \n",
        "                            orient='records', \n",
        "                            lines=True)[columns_long_list] \n",
        "                        ], sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4kcq2qc--AE"
      },
      "source": [
        "with open(\"/content/python_train_0.jsonl\",'r') as f:\n",
        "    my_df=pd.read_json(f, \n",
        "                orient='records', \n",
        "                lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhJZMD7y-bev"
      },
      "source": [
        "my_df_copy = my_df.loc[:20000,['docstring', 'code']].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08lnHBCS_CFs",
        "outputId": "ea9954e6-c501-4e8a-b183-f7d14aac53c0"
      },
      "source": [
        "my_df_copy[my_df_copy['cleaned_code_len'] <= 512].count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "docstring           11818\n",
              "code                11818\n",
              "docstring_len       11818\n",
              "code_len            11818\n",
              "cleaned_code        11818\n",
              "cleaned_code_len    11818\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMGDaIM-_L62"
      },
      "source": [
        "my_df_copy['docstring_len'] =my_df_copy['docstring'].apply(lambda x: len(x))\n",
        "my_df_copy['code_len'] = my_df_copy['code'].apply(lambda x: len(x))\n",
        "my_df_copy['cleaned_code'] = my_df_copy.apply(lambda x: remove_docstrings_comments(x.code, x.docstring), axis=1)\n",
        "my_df_copy['cleaned_code_len'] = my_df_copy['cleaned_code'].apply(lambda x: len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwtHz8XxC8ep"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltpMfQiZTFhA"
      },
      "source": [
        "with open(\"gpt2_bpe_encoder.json\") as f:\n",
        "    gpt2_bpe=json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JA2hXmMeGWl"
      },
      "source": [
        "my_df_copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kglj0SUbeIP7"
      },
      "source": [
        "nl_to_pl_df = nl_to_pl_df.append(my_df_copy,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c35HYbYxWBk7"
      },
      "source": [
        "### Create datasets and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nobKWg_DJ5Xp"
      },
      "source": [
        "nl_to_pl_df = pd.read_csv('/content/end_capstone.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prv7EIAX5aOj"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR7nruWfNQNB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "1dca16ac-8bee-41c4-b6ac-2897271f032d"
      },
      "source": [
        "print(nl_to_pl_df['code_len'].max(),nl_to_pl_df['code_len'].min())\n",
        "print(nl_to_pl_df['docstring_len'].max(),nl_to_pl_df['docstring_len'].min())\n",
        "nl_to_pl_df[nl_to_pl_df['code_len'] ==0]\n",
        "nl_to_pl_df[(nl_to_pl_df['code_len'] > 256) & (nl_to_pl_df['code_len'] < 512)] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2443 11\n",
            "313 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>docstring</th>\n",
              "      <th>code</th>\n",
              "      <th>docstring_len</th>\n",
              "      <th>code_len</th>\n",
              "      <th>cleaned_code</th>\n",
              "      <th>cleaned_code_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td># Write a program to check whether a number is...</td>\n",
              "      <td>num = 337\\n\\nif num &gt; 1:\\n    for i in range(2...</td>\n",
              "      <td>60</td>\n",
              "      <td>311</td>\n",
              "      <td>num = 337\\n\\nif num &gt; 1:\\n    for i in range(2...</td>\n",
              "      <td>308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td># Write a program to find the factorial of a n...</td>\n",
              "      <td>num = 13\\nfactorial = 1\\n\\nif num &lt; 0:\\n    pr...</td>\n",
              "      <td>52</td>\n",
              "      <td>265</td>\n",
              "      <td>num = 13\\nfactorial = 1\\n\\nif num &lt; 0:\\n    pr...</td>\n",
              "      <td>262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td># Write a function that takes in height(m) and...</td>\n",
              "      <td>\\ndef bmi(height: \"Meters\", weight: \"Kgs\"):\\n ...</td>\n",
              "      <td>98</td>\n",
              "      <td>450</td>\n",
              "      <td>\\ndef bmi(height: \"Meters\", weight: \"Kgs\"):\\n ...</td>\n",
              "      <td>447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td># write a python program to sort dictionary it...</td>\n",
              "      <td>dict1 = {'car': [7, 6, 3],  \\n        'bike': ...</td>\n",
              "      <td>50</td>\n",
              "      <td>262</td>\n",
              "      <td>dict1 = {'car': [7, 6, 3],  \\n        'bike': ...</td>\n",
              "      <td>260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td># write a python program to Get the maximum an...</td>\n",
              "      <td>\\nmy_dict = {'x':500, 'y':5874, 'z': 560}\\n\\nk...</td>\n",
              "      <td>78</td>\n",
              "      <td>276</td>\n",
              "      <td>\\nmy_dict = {'x':500, 'y':5874, 'z': 560}\\n\\nk...</td>\n",
              "      <td>274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4352</th>\n",
              "      <td># Define a class named Shape and its subclass ...</td>\n",
              "      <td>\\n\\nclass Shape(object):\\n    def __init__(sel...</td>\n",
              "      <td>234</td>\n",
              "      <td>314</td>\n",
              "      <td>\\n\\nclass Shape(object):\\n    def __init__(sel...</td>\n",
              "      <td>312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4354</th>\n",
              "      <td># Write a python program for a binary search f...</td>\n",
              "      <td>import math\\ndef bin_search(li, element):\\n   ...</td>\n",
              "      <td>171</td>\n",
              "      <td>390</td>\n",
              "      <td>import math\\ndef bin_search(li, element):\\n   ...</td>\n",
              "      <td>388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4359</th>\n",
              "      <td># Write a python program to check if a number ...</td>\n",
              "      <td>n = int(input(\"Enter any number: \"))\\nsum1 = 0...</td>\n",
              "      <td>67</td>\n",
              "      <td>261</td>\n",
              "      <td>n = int(input(\"Enter any number: \"))\\nsum1 = 0...</td>\n",
              "      <td>259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4360</th>\n",
              "      <td># Write a python program to Check if a Number ...</td>\n",
              "      <td>sum1 = 0\\nnum = int(input(\"Enter a number:\"))\\...</td>\n",
              "      <td>65</td>\n",
              "      <td>347</td>\n",
              "      <td>sum1 = 0\\nnum = int(input(\"Enter a number:\"))\\...</td>\n",
              "      <td>345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4362</th>\n",
              "      <td># Write python program to find whether-number-...</td>\n",
              "      <td>def is_power_of_two(n):\\n    \"\"\"Return True if...</td>\n",
              "      <td>56</td>\n",
              "      <td>310</td>\n",
              "      <td>def is_power_of_two(n):\\n    #--\\n    if n &lt;= ...</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>613 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              docstring  ... cleaned_code_len\n",
              "5     # Write a program to check whether a number is...  ...              308\n",
              "7     # Write a program to find the factorial of a n...  ...              262\n",
              "53    # Write a function that takes in height(m) and...  ...              447\n",
              "62    # write a python program to sort dictionary it...  ...              260\n",
              "74    # write a python program to Get the maximum an...  ...              274\n",
              "...                                                 ...  ...              ...\n",
              "4352  # Define a class named Shape and its subclass ...  ...              312\n",
              "4354  # Write a python program for a binary search f...  ...              388\n",
              "4359  # Write a python program to check if a number ...  ...              259\n",
              "4360  # Write a python program to Check if a Number ...  ...              345\n",
              "4362  # Write python program to find whether-number-...  ...              270\n",
              "\n",
              "[613 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJUyE3S39KWy"
      },
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    # if n_gpu > 0:\n",
        "    #     torch.cuda.manual_seed_all(seed)\n",
        "set_seed(0x1112233)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nycguq-JCOoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f234db-f029-470f-f161-c00ae3840379"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/glample/fastBPE.git\n",
        "git clone https://github.com/facebookresearch/TransCoder.git\n",
        "pip install /content/fastBPE/.\n",
        "g++ -std=c++11 -pthread -O3 /content/fastBPE/fastBPE/main.cc -IfastBPE -o fast\n",
        "pip install sacrebleu==\"1.2.11\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./fastBPE\n",
            "Building wheels for collected packages: fastBPE\n",
            "  Building wheel for fastBPE (setup.py): started\n",
            "  Building wheel for fastBPE (setup.py): finished with status 'done'\n",
            "  Created wheel for fastBPE: filename=fastBPE-0.1.1-cp37-cp37m-linux_x86_64.whl size=484395 sha256=ad26202b00ed7c05fed94f527589f4d823a1c1816a283c2cbbb01f06a13ada9a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4u2iedmy/wheels/80/4e/ca/17a8ebf3e318c9540d60ef932be4247103012c4ae9d6c77237\n",
            "Successfully built fastBPE\n",
            "Installing collected packages: fastBPE\n",
            "Successfully installed fastBPE-0.1.1\n",
            "Collecting sacrebleu==1.2.11\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/fb/ad7d721fbeeba9e2fe459f489f38a792ca1e5f5b61f09e608f22f400ca66/sacrebleu-1.2.11.tar.gz\n",
            "Collecting typing\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d9/6eebe19d46bd05360c9a9aae822e67a80f9242aabbfc58b641b957546607/typing-3.7.4.3.tar.gz (78kB)\n",
            "Building wheels for collected packages: sacrebleu, typing\n",
            "  Building wheel for sacrebleu (setup.py): started\n",
            "  Building wheel for sacrebleu (setup.py): finished with status 'done'\n",
            "  Created wheel for sacrebleu: filename=sacrebleu-1.2.11-cp37-none-any.whl size=18641 sha256=ffe4a3232f6d22c632faf1fd54840e11f2007fa63ee22970bb8d8be3e6f0cdeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/0f/06/e1c5dcbca58e907c17b59be8e1f10ae4e43bb1fb68197a8d7c\n",
            "  Building wheel for typing (setup.py): started\n",
            "  Building wheel for typing (setup.py): finished with status 'done'\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-cp37-none-any.whl size=26308 sha256=217735eedc0437f8b36420741927bd4b2f3ef793f58313108e5b32b678e88947\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/04/41/8e1836e79581989c22eebac3f4e70aaac9af07b0908da173be\n",
            "Successfully built sacrebleu typing\n",
            "Installing collected packages: typing, sacrebleu\n",
            "Successfully installed sacrebleu-1.2.11 typing-3.7.4.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fastBPE'...\n",
            "Cloning into 'TransCoder'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBDY67cXevhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d477f713-8145-407e-f7ef-f13feab72aab"
      },
      "source": [
        "!cp -rf /content/drive/MyDrive/EVA4/END_Capstone/cubert .\n",
        "!pip install -r /content/cubert/requirements.txt\n",
        "sys.path.append(\"/content/cubert/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/16/0f9376af49c6adcfbaf2470a8f500105a74dd803aa54ac0110af445837b5/bert_tensorflow-1.0.4-py2.py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.3MB/s \n",
            "\u001b[?25hCollecting dopamine-rl==3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/a8/7cf1fe1b5b11a7165c8901bc50091e3fa68f97f48fb4035f31cf6fc675f2/dopamine_rl-3.0.1-py3-none-any.whl (84kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from -r /content/cubert/requirements.txt (line 3)) (2019.12.20)\n",
            "Collecting tensor2tensor\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/7c/9e87d30cefad5cbc390bb7f626efb3ded9b19416b8160f1a1278da81b218/tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 10.8MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 41kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow->-r /content/cubert/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: Pillow>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (7.0.0)\n",
            "Requirement already satisfied: gym>=0.10.5 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (0.17.3)\n",
            "Requirement already satisfied: opencv-python>=3.4.1.15 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (4.1.2.30)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: gin-config>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.10.0)\n",
            "Collecting mesh-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/20/23bbc94034e16bb1ace73e9e7922226e31d6d36b88dcfa257d2c59b3f465/mesh_tensorflow-0.1.18-py3-none-any.whl (361kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 46.8MB/s \n",
            "\u001b[?25hCollecting tensorflow-gan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/2e/62922111d7d50e1900e3030764743ea7735540ce103b3ab30fd5cd2d8a2b/tensorflow_gan-2.0.0-py2.py3-none-any.whl (365kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 48.2MB/s \n",
            "\u001b[?25hCollecting bz2file\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.1.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.12.8)\n",
            "Collecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 46.8MB/s \n",
            "\u001b[?25hCollecting kfac\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/36/06fe2c757044bb51906fef231ac48cc5bf9a277fc9a8c7e1108d7e9e8cfd/kfac-0.2.3-py2.py3-none-any.whl (191kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 55.3MB/s \n",
            "\u001b[?25hCollecting pypng\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 47.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (4.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.4.1)\n",
            "Collecting tensorflow-probability==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/3a/c10b6c22320531c774402ac7186d1b673374e2a9d12502cbc8d811e4601c/tensorflow_probability-0.7.0-py2.py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (4.0.1)\n",
            "Collecting gevent\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/85/df3d1fd2b60a87455475f93012861b76a411d27ba4a0859939adbe2c9dc3/gevent-21.1.2-cp37-cp37m-manylinux2010_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 43.3MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.7.1)\n",
            "Collecting tf-slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 52.6MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (3.12.4)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 30.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (0.36.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (1.1.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (1.32.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.5->dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.5->dopamine-rl==3.0.1->-r /content/cubert/requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gan->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.26.1)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.27.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.17.4)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.7.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.4.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (5.1.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (20.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.28.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (0.1.5)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/42/d8f11eaef844bee267821281fffe445e49cf31b486d72a81821a9d45cd0a/zope.interface-5.2.0-cp37-cp37m-manylinux2010_x86_64.whl (237kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 53.0MB/s \n",
            "\u001b[?25hCollecting greenlet<2.0,>=0.4.17; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/25/f52f0dde4135833c2f85eae30a749d260231065b46942534df8366d7e1ec/greenlet-1.0.0-cp37-cp37m-manylinux2010_x86_64.whl (160kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 45.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (54.0.0)\n",
            "Collecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (3.3.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.1.1)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (20.9)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (1.53.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (4.2.1)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (3.7.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor->-r /content/cubert/requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r /content/cubert/requirements.txt (line 5)) (3.7.4.3)\n",
            "Building wheels for collected packages: bz2file, pypng, gast\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-cp37-none-any.whl size=6884 sha256=a198f9dfe3b302b56f56cc50962f222c2cf30c3b34c1d51616d77d9aeeb7d2f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-cp37-none-any.whl size=67163 sha256=3afe7d516a0378299ae7547a68758ffbc26b06d7fd521e41573edab32c3f9859\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=b6883f4540c81111f1e97647ebb0c6d5791f79841af96902737f930210d7a956\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built bz2file pypng gast\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: bert-tensorflow, dopamine-rl, mesh-tensorflow, tensorflow-probability, tensorflow-gan, bz2file, tensorflow-addons, kfac, pypng, zope.interface, greenlet, zope.event, gevent, gunicorn, tf-slim, tensor2tensor, tensorboard, tensorflow-estimator, gast, keras-applications, tensorflow\n",
            "  Found existing installation: dopamine-rl 1.0.5\n",
            "    Uninstalling dopamine-rl-1.0.5:\n",
            "      Successfully uninstalled dopamine-rl-1.0.5\n",
            "  Found existing installation: tensorflow-probability 0.12.1\n",
            "    Uninstalling tensorflow-probability-0.12.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.12.1\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed bert-tensorflow-1.0.4 bz2file-0.98 dopamine-rl-3.0.1 gast-0.2.2 gevent-21.1.2 greenlet-1.0.0 gunicorn-20.0.4 keras-applications-1.0.8 kfac-0.2.3 mesh-tensorflow-0.1.18 pypng-0.0.20 tensor2tensor-1.15.7 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-addons-0.12.1 tensorflow-estimator-1.15.1 tensorflow-gan-2.0.0 tensorflow-probability-0.7.0 tf-slim-1.1.0 zope.event-4.5.0 zope.interface-5.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5Pn6pQ1C9fn"
      },
      "source": [
        "### Cubert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3k4UAjKgagA"
      },
      "source": [
        "\"\"\"A Python tokenizer subclass of CuBertTokenizer.\"\"\"\n",
        "import keyword\n",
        "import re\n",
        "import tokenize\n",
        "import typing\n",
        "from typing import Any\n",
        "from typing import List\n",
        "from typing import Sequence\n",
        "from typing import Tuple\n",
        "from absl import logging\n",
        "from cubert import cubert_tokenizer\n",
        "from cubert import unified_tokenizer\n",
        "\n",
        "\n",
        "class PythonTokenizer2(cubert_tokenizer.CuBertTokenizer):\n",
        "  \"\"\"Tokenizer that extracts Python's lexical elements preserving strings.\"\"\"\n",
        "  _TOKEN_TYPE_MAP = {\n",
        "      tokenize.COMMENT: unified_tokenizer.TokenKind.COMMENT,\n",
        "      tokenize.DEDENT: unified_tokenizer.TokenKind.KEYWORD,\n",
        "      tokenize.ENDMARKER: unified_tokenizer.TokenKind.EOS,\n",
        "      tokenize.ERRORTOKEN: unified_tokenizer.TokenKind.ERROR,\n",
        "      tokenize.INDENT: unified_tokenizer.TokenKind.KEYWORD,\n",
        "      tokenize.NEWLINE: unified_tokenizer.TokenKind.NEWLINE,\n",
        "      tokenize.NL: unified_tokenizer.TokenKind.PUNCTUATION,\n",
        "      tokenize.NUMBER: unified_tokenizer.TokenKind.NUMBER,\n",
        "      tokenize.OP: unified_tokenizer.TokenKind.PUNCTUATION,\n",
        "      tokenize.STRING: unified_tokenizer.TokenKind.STRING,\n",
        "  }\n",
        "  _REVERSE_TOKEN_MAP = {\n",
        "      cubert_tokenizer.token_from_token_type(tokenize.INDENT):\n",
        "          tokenize.INDENT,\n",
        "      cubert_tokenizer.token_from_token_type(tokenize.DEDENT):\n",
        "          tokenize.DEDENT,\n",
        "      unified_tokenizer.quote_special(unified_tokenizer.TokenKind.EOS.name):\n",
        "          tokenize.ENDMARKER,\n",
        "      unified_tokenizer.quote_special(unified_tokenizer.TokenKind.ERROR.name):\n",
        "          tokenize.ERRORTOKEN,\n",
        "      unified_tokenizer.quote_special(unified_tokenizer.TokenKind.NEWLINE.name):\n",
        "          tokenize.NEWLINE,\n",
        "      cubert_tokenizer.token_from_token_type(tokenize.NL):\n",
        "          tokenize.NL,\n",
        "  }\n",
        "  # Adding the end-of-string anchor \\Z below, since re.fullmatch wasn't\n",
        "  # available in Python2.\n",
        "  _NUMBERS = re.compile('(' + tokenize.Number + r')\\Z')  # pytype: disable=module-attr\n",
        "  _SINGLE_STRINGS = re.compile('(' + tokenize.String + r')\\Z')  # pytype: disable=module-attr\n",
        "  _TRIPLE_STRING_BEGINNINGS = re.compile(tokenize.Triple)  # pytype: disable=module-attr\n",
        "  _COMMENTS = re.compile('(' + tokenize.Comment + r')\\Z')  # pytype: disable=module-attr\n",
        "\n",
        "  _EXACT_TOKEN_TYPES = tokenize.EXACT_TOKEN_TYPES.keys()  # pytype: disable=module-attr\n",
        "\n",
        "  # Token types that CubertTokenizer will tokenize by their type and not\n",
        "  # content.\n",
        "  _TOKEN_TYPES_TO_TOKENIZE_BY_TYPE = [\n",
        "      tokenize.NEWLINE, tokenize.DEDENT, tokenize.NL\n",
        "  ]\n",
        "\n",
        "  def tokenize_and_abstract(\n",
        "      self,\n",
        "      source_code):\n",
        "    \"\"\"Produces a language-agnostic tokenization of the input code.\"\"\"\n",
        "    agnostic_tokens: List[unified_tokenizer.AbstractToken] = []\n",
        "\n",
        "    try:\n",
        "      token_tuples = unified_tokenizer.code_to_tokens(source_code)\n",
        "    except (tokenize.TokenError, IndentationError) as e:\n",
        "      logging.warning('The tokenizer raised exception `%s` while parsing %s', e,\n",
        "                      source_code)\n",
        "\n",
        "      # We don't try to do recovery from errors quite yet. Emit just an\n",
        "      # error and end-of-sequence and return.\n",
        "      agnostic_tokens.append(\n",
        "          unified_tokenizer.AbstractToken(\n",
        "              unified_tokenizer.quote_special(\n",
        "                  unified_tokenizer.TokenKind.ERROR.name),\n",
        "              unified_tokenizer.TokenKind.ERROR,\n",
        "              unified_tokenizer.TokenMetadata(\n",
        "                  start=unified_tokenizer.Position(\n",
        "                      line=0, column=0),\n",
        "                  end=unified_tokenizer.Position(\n",
        "                      line=0, column=0))))\n",
        "      agnostic_tokens.append(\n",
        "          unified_tokenizer.AbstractToken(\n",
        "              unified_tokenizer.quote_special(\n",
        "                  unified_tokenizer.TokenKind.EOS.name),\n",
        "              unified_tokenizer.TokenKind.EOS,\n",
        "              unified_tokenizer.TokenMetadata(\n",
        "                  start=unified_tokenizer.Position(\n",
        "                      line=0, column=0),\n",
        "                  end=unified_tokenizer.Position(\n",
        "                      line=0, column=0))))\n",
        "      return agnostic_tokens\n",
        "\n",
        "    for token_tuple in token_tuples:\n",
        "      spelling = token_tuple.string\n",
        "      kind = token_tuple.type\n",
        "\n",
        "      # We'll adjust the spelling of some tokens, e.g., those that we\n",
        "      # tokenize by their type rather than their original spelling. Indentation\n",
        "      # and dedentation tokens are like that.\n",
        "      adjusted_spelling = spelling\n",
        "      token_kind = unified_tokenizer.TokenKind.NONE\n",
        "      if kind == tokenize.NAME:\n",
        "        # Disambiguate identifiers from keywords.\n",
        "        if keyword.iskeyword(spelling):\n",
        "          token_kind = unified_tokenizer.TokenKind.KEYWORD\n",
        "        else:\n",
        "          token_kind = unified_tokenizer.TokenKind.IDENTIFIER\n",
        "      else:\n",
        "        if kind in PythonTokenizer2._TOKEN_TYPES_TO_TOKENIZE_BY_TYPE:\n",
        "          # Replace spelling with type.\n",
        "          adjusted_spelling = cubert_tokenizer.token_from_token_type(kind)\n",
        "        elif kind is tokenize.INDENT:\n",
        "          # For INDENT, in particular, we also record the actual spelling too.\n",
        "          adjusted_spelling = '{indent}{spelling}'.format(\n",
        "              indent=cubert_tokenizer.token_from_token_type(kind),\n",
        "              spelling=spelling)\n",
        "          #print(adjusted_spelling)\n",
        "        elif kind == tokenize.ENDMARKER:\n",
        "          adjusted_spelling = unified_tokenizer.quote_special(\n",
        "              unified_tokenizer.TokenKind.EOS.name)\n",
        "\n",
        "        # Map everything according to table.\n",
        "        try:\n",
        "          token_kind = PythonTokenizer2._TOKEN_TYPE_MAP[kind]\n",
        "        except KeyError as ke:\n",
        "          # It's possible we're here because of async/await. Those kept being\n",
        "          # turned into keywords and then removed from keywords, so we can't\n",
        "          # rely on knowing which they are. We'll check by spelling.\n",
        "          # See: https://bugs.python.org/issue30406\n",
        "          # and https://bugs.python.org/issue33260\n",
        "          # and https://bugs.python.org/issue35975\n",
        "          if spelling in ('async', 'await'):\n",
        "            token_kind = unified_tokenizer.TokenKind.KEYWORD\n",
        "          else:\n",
        "            raise ValueError('While trying to turn Python token %r into an '\n",
        "                             'agnostic one, raised %r.' %\n",
        "                             ((spelling, kind), ke))\n",
        "\n",
        "      start_line, start_column = token_tuple.start\n",
        "      end_line, end_column = token_tuple.end\n",
        "      # Unlike other languages, NEWLINE tokens are reported as ending on the\n",
        "      # same line as where they started. We adjust that here, to stick to the\n",
        "      # same convention as other tokenizers.\n",
        "      if ((token_kind == unified_tokenizer.TokenKind.NEWLINE) or\n",
        "          (kind == tokenize.NL)):\n",
        "        end_line = start_line + 1\n",
        "        end_column = 0\n",
        "\n",
        "      agnostic_tokens.append(\n",
        "          unified_tokenizer.AbstractToken(\n",
        "              spelling=adjusted_spelling, kind=token_kind,\n",
        "              metadata=unified_tokenizer.TokenMetadata(\n",
        "                  # Python's tokenizer counts lines starting from 1, so we\n",
        "                  # have to offset what we read from the `TokenInfo` tuple.\n",
        "                  start=unified_tokenizer.Position(\n",
        "                      line=start_line - 1, column=start_column),\n",
        "                  end=unified_tokenizer.Position(\n",
        "                      line=end_line - 1, column=end_column))))\n",
        "    #print(agnostic_tokens)\n",
        "    return agnostic_tokens\n",
        "\n",
        "  def untokenize_abstract(self, whole_tokens):\n",
        "    # Reconstruct Python tokenizer tuples, so that Python's untokenize can be\n",
        "    # invoked.\n",
        "    token_tuples: List[Tuple[int, str]] = []\n",
        "\n",
        "    for whole_token in whole_tokens:\n",
        "      if whole_token in PythonTokenizer2._EXACT_TOKEN_TYPES:\n",
        "        token_tuples.append((tokenize.OP, whole_token))\n",
        "      elif cubert_tokenizer.token_from_token_type(\n",
        "          tokenize.INDENT) in whole_token:\n",
        "        # We baked the type and spelling into one token. Break them up.\n",
        "        spelling = whole_token.replace(\n",
        "            cubert_tokenizer.token_from_token_type(tokenize.INDENT), '')\n",
        "        token_tuples.append((tokenize.INDENT, spelling))\n",
        "      elif whole_token in PythonTokenizer2._REVERSE_TOKEN_MAP:\n",
        "        python_kind = PythonTokenizer2._REVERSE_TOKEN_MAP[whole_token]\n",
        "        if python_kind in (tokenize.DEDENT, tokenize.ENDMARKER,\n",
        "                           tokenize.ERRORTOKEN):\n",
        "          spelling = ''\n",
        "        else:  # python_kind in (tokenize.NEWLINE, tokenize.NL)\n",
        "          spelling = '\\n'\n",
        "        token_tuples.append((python_kind, spelling))\n",
        "      elif keyword.iskeyword(whole_token):\n",
        "        token_tuples.append((tokenize.NAME, whole_token))\n",
        "      elif PythonTokenizer2._NUMBERS.match(whole_token):\n",
        "        token_tuples.append((tokenize.NUMBER, whole_token))\n",
        "      elif PythonTokenizer2._SINGLE_STRINGS.match(whole_token):\n",
        "        token_tuples.append((tokenize.STRING, whole_token))\n",
        "      elif PythonTokenizer2._TRIPLE_STRING_BEGINNINGS.match(whole_token):\n",
        "        token_tuples.append((tokenize.STRING, whole_token))\n",
        "      elif PythonTokenizer2._COMMENTS.match(whole_token):\n",
        "        token_tuples.append((tokenize.COMMENT, whole_token))\n",
        "      else:\n",
        "        # Everything else we map back to NAME.\n",
        "        token_tuples.append((tokenize.NAME, whole_token))\n",
        "\n",
        "    reconstructed = tokenize.untokenize(typing.cast(Any, token_tuples))\n",
        "    return reconstructed\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOS_mSj8hIOK"
      },
      "source": [
        "def get_lang_specific_tokens(init_tokenizer, code_snip):\n",
        "    #tokens_complete = init_tokenizer.tokenize(source_code=code_snip)\n",
        "    tokens = init_tokenizer.tokenize_and_abstract(source_code=code_snip )\n",
        "    conditioned = init_tokenizer.condition_full_tokens(tokens)\n",
        "    agnostic_token_lists = unified_tokenizer._agnostic_tokens_to_lists_of_token_lists(conditioned)\n",
        "    with_identifiers_heuristically_split = unified_tokenizer._subtokenize_identifiers_heuristically(\n",
        "        agnostic_token_lists)\n",
        "    with_string_tokens_heuristically_split = unified_tokenizer._subtokenize_strings_heuristically(\n",
        "        with_identifiers_heuristically_split)\n",
        "    shortened_subtokens = unified_tokenizer._shorten_subtokens(with_string_tokens_heuristically_split, 20)\n",
        "    sanitization_mapping = init_tokenizer.get_mappings()\n",
        "    subtoken_lists = unified_tokenizer.sanitize_subtoken_lists(shortened_subtokens,\n",
        "                                            sanitization_mapping,\n",
        "                                            unified_tokenizer.SENTINEL)\n",
        "    #flat_toks =unified_tokenizer.flatten_subtoken_lists(subtoken_lists)\n",
        "    test_spellings = []\n",
        "    test_tok_types = []\n",
        "    for t in subtoken_lists:\n",
        "        #if(len(t.spelling) == 1):\n",
        "        #print(len(t.spellings))\n",
        "        test_spellings.extend(t.spellings)\n",
        "        match=False\n",
        "        for cubert_token in set(init_tokenizer._REVERSE_TOKEN_MAP.keys()):\n",
        "            #print(\"Checking for:\",cubert_token)\n",
        "            if cubert_token in t.spellings[0]:\n",
        "                #print(t.spellings)\n",
        "                selected_token = tokenize.tok_name[init_tokenizer._REVERSE_TOKEN_MAP[cubert_token]]\n",
        "                test_tok_types.extend([selected_token]*len(t.spellings))\n",
        "                match=True          \n",
        "        if match == False:\n",
        "            test_tok_types.extend([t.kind.name]*len(t.spellings))\n",
        "    return test_spellings, test_tok_types\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hma47DbmC1E1"
      },
      "source": [
        "### Cubert Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h03QcbEIhLeg"
      },
      "source": [
        "class NewVectorizer():\n",
        "    def __init__(self, code_piece, tok_type_counter):\n",
        "        self.code_piece = code_piece\n",
        "        self.tok_type_counter = tok_type_counter\n",
        "        self.code_word2idx = {'<s>':0,'</s>':1,'<pad>':2, '<unk>':3}\n",
        "        self.code_idx2word = {v:k for k,v in self.code_word2idx.items()}\n",
        "        self.toktype_word2idx = {'<s>':0,'</s>':1,'<pad>':2, '<unk>':3}\n",
        "        self.toktype_idx2word = {v:k for k,v in self.toktype_word2idx.items()}\n",
        "        self.max_tok_length = len(self.toktype_word2idx)\n",
        "        self.max_code_length = len(self.code_word2idx)\n",
        "        self.UNK_FOR_TOKEN_TYPE = '<unk>'\n",
        "        self.UNK_FOR_CODEPIECE = '<unk>'\n",
        "        self.ID_UNK_TOKEN_TYPE = self.toktype_word2idx[self.UNK_FOR_TOKEN_TYPE]\n",
        "        self.ID_UNK_CODEPIECE = self.code_word2idx[self.UNK_FOR_CODEPIECE]\n",
        "\n",
        "        self.PAD_FOR_CODEPIECE = '<pad>'\n",
        "        self.PAD_FOR_TOKEN_TYPE = '<pad>'\n",
        "        self.ID_PAD_FOR_CODEPIECE = self.code_word2idx['<pad>']\n",
        "        self.ID_PAD_FOR_TOKEN_TYPE = self.toktype_word2idx['<pad>']\n",
        "        \n",
        "        self.SOS_FOR_CODEPIECE = '<s>'\n",
        "        self.SOS_FOR_TOKEN_TYPE = '<s>'\n",
        "        self.ID_SOS_FOR_CODEPIECE = self.code_word2idx['<s>']\n",
        "        self.ID_SOS_FOR_TOKEN_TYPE = self.toktype_word2idx['<s>']\n",
        "\n",
        "        self.EOS_FOR_CODEPIECE = '</s>'\n",
        "        self.EOS_FOR_TOKEN_TYPE = '</s>'\n",
        "        self.ID_EOS_FOR_CODEPIECE = self.code_word2idx['</s>']\n",
        "        self.ID_EOS_FOR_TOKEN_TYPE = self.toktype_word2idx['</s>']\n",
        "        self.build_vocab()\n",
        "\n",
        "    def build_vocab(self):\n",
        "        idx=len(self.code_word2idx.keys())\n",
        "        for k in self.code_piece.keys():\n",
        "            self.code_word2idx[k]=idx\n",
        "            self.code_idx2word[idx]=k\n",
        "            idx += 1\n",
        "        \n",
        "        idx=len(self.toktype_word2idx.keys())\n",
        "        for k in self.tok_type_counter.keys():\n",
        "            self.toktype_word2idx[k]=idx\n",
        "            self.toktype_idx2word[idx]=k\n",
        "            idx += 1\n",
        "\n",
        "        self.max_tok_length = len(self.toktype_word2idx.keys())\n",
        "        self.max_code_length = len(self.code_word2idx.keys())\n",
        "    ### Returns the code piece for a given ID\n",
        "    def convert_id_to_codepiece(self, id_for_code):\n",
        "        if(id_for_code not in list(self.code_idx2word.keys())):\n",
        "            return self.UNK_FOR_CODEPIECE\n",
        "        return self.code_idx2word[id_for_code]\n",
        "    ### Returns the ID for a given code piece\n",
        "    def convert_codepiece_to_id(self, code_piece):\n",
        "        if(code_piece not in list(self.code_word2idx.keys())):\n",
        "            return self.ID_UNK_CODEPIECE\n",
        "        return self.code_word2idx[code_piece]\n",
        "    ### Returns the TOKEN ID for a given TOKEN type\n",
        "    def convert_toktype_to_id(self, tok_piece):\n",
        "        if(tok_piece not in list(self.toktype_word2idx.keys())):\n",
        "            print(\"No match for\",tok_piece)\n",
        "            return self.ID_UNK_TOKEN_TYPE\n",
        "        return self.toktype_word2idx[tok_piece]\n",
        "    ### Returns the TOKEN type for a given TOKEN ID\n",
        "    def convert_id_to_toktype(self, id_for_toktype):\n",
        "        if(id_for_toktype not in list(self.toktype_idx2word.keys())):\n",
        "            return self.UNK_FOR_TOKEN_TYPE\n",
        "        return self.toktype_idx2word[id_for_toktype]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYzMGFIngltr"
      },
      "source": [
        "import cubert\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from tensor2tensor.data_generators import text_encoder\n",
        "import enum\n",
        "import cubert_tokenizer\n",
        "from cubert import code_to_subtokenized_sentences\n",
        "#from cubert import tokenizer_registry\n",
        "from cubert import python_tokenizer\n",
        "import python_tokenizer\n",
        "from tensor2tensor.data_generators import text_encoder_build_subword\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "@enum.unique\n",
        "class TokenizerEnum(enum.Enum):\n",
        "  \"\"\"Enum for Tokenizers.\"\"\"\n",
        "  #PYTHON = python_tokenizer.PythonTokenizer\n",
        "  PYTHON = PythonTokenizer2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7auIaOthNj6"
      },
      "source": [
        "#### Generate and create the Vectorizer instance\n",
        "word_counter=Counter()\n",
        "init_tokenizer=PythonTokenizer2()\n",
        "tok_type_counter = Counter()\n",
        "for code_snip in nl_to_pl_df['cleaned_code']:\n",
        "    toks, tok_types = get_lang_specific_tokens(init_tokenizer, code_snip)\n",
        "    word_counter.update(Counter(toks))\n",
        "    tok_type_counter.update(Counter(tok_types))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a-QqlOuhd3t"
      },
      "source": [
        "code_tok_vectorizer = NewVectorizer(word_counter, tok_type_counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1HaJbq6BwtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3674392-8b3b-4018-dd74-5b5335ed81c1"
      },
      "source": [
        "print(code_tok_vectorizer.max_code_length,code_tok_vectorizer.max_tok_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5813 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfjy6DJUCCp_",
        "outputId": "56d834e2-8aa8-4556-d214-3a551618a1a4"
      },
      "source": [
        "[ code_tok_vectorizer.convert_id_to_toktype(toktype) for toktype in tok_type_ids] == tok_types"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5epf-rxhjY0"
      },
      "source": [
        "### Old NLP DS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k46GU0OzfGeE"
      },
      "source": [
        "tok_ids_list=[]\n",
        "class NLPLSingleEntry(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 code_ids,\n",
        "                 tok_ids,\n",
        "                 code_mask, \n",
        "                 doc_ids,\n",
        "                 doc_mask,\n",
        "                 ):\n",
        "        self.code_ids = code_ids\n",
        "        self.code_mask = code_mask\n",
        "        self.tok_ids = tok_ids\n",
        "        self.doc_ids = doc_ids\n",
        "        self.doc_mask = doc_mask\n",
        "        #self.segment_ids = segment_ids\n",
        "        #self.label_id = label_id\n",
        "class NLPLDataSet():\n",
        "    def __init__(self, \n",
        "                 doc_tokenizer, \n",
        "                 code_tokenizer,\n",
        "                 code_tok_vectorizer):\n",
        "        self.doc_tokenizer = doc_tokenizer\n",
        "        self.code_tokenizer = code_tokenizer\n",
        "        self.code_tok_vectorizer = code_tok_vectorizer\n",
        "\n",
        "    def prepare_tokens(self, \n",
        "                       samples, \n",
        "                       tokenizer, \n",
        "                       max_seq_length=0,\n",
        "                       data_type=None):\n",
        "        \"\"\"\n",
        "            Tokenizes an input sequence, adds padding and SOS+EOS \n",
        "        \"\"\"\n",
        "        toks = tokenizer.tokenize(samples.lower())\n",
        "        # print(data_type)\n",
        "        # print(toks)\n",
        "        if max_seq_length > 2 and len(toks) > max_seq_length - 2:\n",
        "            toks = toks[:max_seq_length -2]\n",
        "        tok_ids =  tokenizer.convert_tokens_to_ids(toks)\n",
        "        ### We use pseudo-BERT process so we will add both CLS and SEP tokens for\n",
        "        ### src and target inputs\n",
        "        tok_ids = [tokenizer.cls_token_id] + tok_ids + [tokenizer.sep_token_id]\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [ 1 ] * len(tok_ids)\n",
        "\n",
        "        if len(tok_ids) < max_seq_length:\n",
        "            padding_length = max_seq_length - len(tok_ids)\n",
        "            tok_ids = tok_ids + ([tokenizer.pad_token_id] * padding_length)\n",
        "            input_mask = input_mask + ([ 0 ] * padding_length) ### Padded tokens are zero-masked\n",
        "        \n",
        "        # print(tok_ids)\n",
        "        return tok_ids, input_mask\n",
        "    def prepare_code_tokens(self, \n",
        "                       samples, \n",
        "                       tokenizer, \n",
        "                       max_seq_length=0,\n",
        "                       data_type=None):\n",
        "        \"\"\"\n",
        "            Tokenizes an input sequence, adds padding and SOS+EOS \n",
        "        \"\"\"\n",
        "        _toks, _tok_types = get_lang_specific_tokens(self.code_tokenizer, samples)\n",
        "        #print(_tok_types)\n",
        "        # print(data_type)\n",
        "        # print(toks)\n",
        "        if max_seq_length > 2 and len(_toks) > max_seq_length - 2:\n",
        "            _toks = _toks[:max_seq_length -2]\n",
        "            _tok_types = _tok_types[:max_seq_length -2]\n",
        "        #tok_ids =  tokenizer.convert_tokens_to_ids(toks)\n",
        "        tok_ids = [ self.code_tok_vectorizer.convert_codepiece_to_id(code) for code in _toks]\n",
        "        tok_types = [ self.code_tok_vectorizer.convert_toktype_to_id(toktype) for toktype in _tok_types]\n",
        "        \n",
        "        ### We use pseudo-BERT process so we will add both CLS and SEP tokens for\n",
        "        ### src and target inputs\n",
        "        tok_ids = [self.code_tok_vectorizer.ID_SOS_FOR_CODEPIECE] + tok_ids + [self.code_tok_vectorizer.ID_EOS_FOR_CODEPIECE]\n",
        "        tok_types = [self.code_tok_vectorizer.ID_SOS_FOR_TOKEN_TYPE] + tok_types + [self.code_tok_vectorizer.ID_EOS_FOR_TOKEN_TYPE]\n",
        "        #print(len(tok_ids), len(tok_types))\n",
        "        assert(len(tok_ids) == len(tok_types))\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [ 1 ] * len(tok_ids)\n",
        "\n",
        "        if len(tok_ids) < max_seq_length:\n",
        "            padding_length = max_seq_length - len(tok_ids)\n",
        "            tok_ids = tok_ids + ([self.code_tok_vectorizer.ID_PAD_FOR_CODEPIECE] * padding_length)\n",
        "            tok_types = tok_types + ([self.code_tok_vectorizer.ID_PAD_FOR_TOKEN_TYPE] * padding_length)\n",
        "            input_mask = input_mask + ([ 0 ] * padding_length) ### Padded tokens are zero-masked\n",
        "        \n",
        "        # print(tok_ids)\n",
        "        return tok_ids, tok_types, input_mask\n",
        "\n",
        "    def create_dataset(self,\n",
        "                    nl_to_pl_df,\n",
        "                    final_ds,\n",
        "                    sample_count=10000,\n",
        "                    max_doc_len=50,\n",
        "                    max_code_len=0):\n",
        "        \"\"\"\n",
        "            Reads from a dataframe, tokenizes and numericalizes both docstrings \n",
        "            and code. \n",
        "\n",
        "        \"\"\"\n",
        "        #final_ds = []\n",
        "        for idx in nl_to_pl_df.itertuples():\n",
        "            ## For SOS and EOS tokens 2 positions are left\n",
        "            if not idx.cleaned_code:\n",
        "                print(\"Invalid entry, No code found for:\", idx.docstring)\n",
        "            \n",
        "            doc_toks, doc_mask = self.prepare_tokens(idx.docstring,\n",
        "                                                      self.doc_tokenizer,\n",
        "                                                      max_doc_len,\n",
        "                                                      \"docs\")\n",
        "            code_ids, tok_ids, code_mask = self.prepare_code_tokens(idx.cleaned_code,\n",
        "                                                      self.code_tokenizer,\n",
        "                                                      max_code_len,\n",
        "                                                      \"code\")\n",
        "            #code_toks = None\n",
        "            ### Skip over current iteration if no valid code found\n",
        "\n",
        "            # print(code_toks)\n",
        "            # print(code_mask)\n",
        "            # print(doc_toks)\n",
        "            # print(doc_mask)\n",
        "            final_entry = NLPLSingleEntry(code_ids,\n",
        "                                          tok_ids,\n",
        "                                          code_mask, \n",
        "                                          doc_toks, \n",
        "                                          doc_mask)\n",
        "            # print(final_entry.code_ids)\n",
        "            # print(final_entry.code_mask)\n",
        "            # print(final_entry.doc_ids)\n",
        "            # print(final_entry.doc_mask)\n",
        "            final_ds.append(final_entry)\n",
        "        #print(len(final_ds))\n",
        "        return final_ds\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u517c9pfDKAE"
      },
      "source": [
        "### TransCoder Tokenizer + FastBPE + TransCoder DataSet Creator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UUgBUPXDy1a",
        "outputId": "abbbc12e-64df-4115-8c5c-32cb1bdcba04"
      },
      "source": [
        "!pip install clang"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting clang\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/5e/25e882fa2b1f27c44a831331929d1d0d054db0a86fa2991d336f84c50d6c/clang-11.0-py3-none-any.whl\n",
            "Installing collected packages: clang\n",
            "Successfully installed clang-11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q094eIh0D9Yz"
      },
      "source": [
        "sys.path.append(\"/content/TransCoder\")\n",
        "import clang\n",
        "from clang.cindex import TokenKind\n",
        "\n",
        "from preprocessing.src.timeout import timeout, TimeoutError\n",
        "import preprocessing.src.javalang_tokenizer as javalang_tok\n",
        "#from preprocessing.src.code_tokenizer import tokenize_python, detokenize_python\n",
        "clang.cindex.Config.set_library_path('/usr/lib/llvm-6/lib/')\n",
        "clang.cindex.Config.set_library_file(\"/usr/lib/llvm-6.0/lib/libclang-6.0.so.1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU3HnH05DdcR"
      },
      "source": [
        "# sys.path.append(\"/content/TransCoder\")\n",
        "# import clang\n",
        "# from clang.cindex import TokenKind\n",
        "\n",
        "# from preprocessing.src.timeout import timeout, TimeoutError\n",
        "# import preprocessing.src.javalang_tokenizer as javalang_tok\n",
        "# #from preprocessing.src.code_tokenizer import tokenize_python, detokenize_python\n",
        "# clang.cindex.Config.set_library_path('/usr/lib/llvm-6/lib/')\n",
        "# clang.cindex.Config.set_library_file(\"/usr/lib/llvm-6.0/lib/libclang-6.0.so.1\")\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import tokenize\n",
        "from io import BytesIO\n",
        "\n",
        "import preprocessing.src.javalang_tokenizer as javalang_tok\n",
        "from clang.cindex import TokenKind\n",
        "from preprocessing.src.timeout import timeout, TimeoutError\n",
        "from sacrebleu import tokenize_v14_international\n",
        "\n",
        "TOK_NO_SPACE_BEFORE = {',', ';'}\n",
        "#clang.cindex.Config.set_library_path('/usr/lib/llvm-7/lib/')\n",
        "STRINGS_AND_COMMENTS_TOKEN_KINDS = {TokenKind.LITERAL, TokenKind.COMMENT}\n",
        "logging.basicConfig(\n",
        "    filename='timeout_cpp_tokenizer_examples.log', level=logging.DEBUG)\n",
        "\n",
        "idx = clang.cindex.Index.create()\n",
        "\n",
        "JAVA_TOKEN2CHAR = {'STOKEN0': \"//\",\n",
        "                   'STOKEN1': \"/*\",\n",
        "                   'STOKEN2': \"*/\",\n",
        "                   'STOKEN3': \"/**\",\n",
        "                   'STOKEN4': \"**/\",\n",
        "                   'STOKEN5': '\"\"\"',\n",
        "                   'STOKEN6': '\\\\n'\n",
        "                   }\n",
        "JAVA_CHAR2TOKEN = {\"//\": ' STOKEN0 ',\n",
        "                   \"/*\": ' STOKEN1 ',\n",
        "                   \"*/\": ' STOKEN2 ',\n",
        "                   \"/**\": ' STOKEN3 ',\n",
        "                   \"**/\": ' STOKEN4 ',\n",
        "                   '\"\"\"': ' STOKEN5 ',\n",
        "                   '\\\\n': ' STOKEN6 '\n",
        "                   }\n",
        "\n",
        "CPP_TOKEN2CHAR = JAVA_TOKEN2CHAR.copy()\n",
        "CPP_CHAR2TOKEN = JAVA_CHAR2TOKEN.copy()\n",
        "\n",
        "PYTHON_TOKEN2CHAR = {'STOKEN0': '#',\n",
        "                     'STOKEN1': \"\\\\n\",\n",
        "                     'STOKEN2': '\"\"\"',\n",
        "                     'STOKEN3': \"'''\"\n",
        "                     }\n",
        "\n",
        "PYTHON_CHAR2TOKEN = {'#': ' STOKEN0 ',\n",
        "                     \"\\\\n\": ' STOKEN1 ',\n",
        "                     '\"\"\"': ' STOKEN2 ',\n",
        "                     \"'''\": ' STOKEN3 '\n",
        "                     }\n",
        "\n",
        "\n",
        "class ind_iter(object):\n",
        "    def __init__(self, len):\n",
        "        self.i = 0\n",
        "        self.len = len\n",
        "\n",
        "    def next(self):\n",
        "        self.i += 1\n",
        "        if self.i > (self.len - 1):\n",
        "            raise StopIteration\n",
        "\n",
        "    def prev(self):\n",
        "        self.i -= 1\n",
        "        if self.i < 0:\n",
        "            raise StopIteration\n",
        "\n",
        "\n",
        "def process_string(tok, char2tok, tok2char, is_comment):\n",
        "    if is_comment:\n",
        "        tok = re.sub(' +', ' ', tok)\n",
        "        tok = re.sub(r\"(.)\\1\\1\\1\\1+\", r\"\\1\\1\\1\\1\\1\", tok)\n",
        "        if len(re.sub(r'\\W', '', tok)) < 2:\n",
        "            return ''\n",
        "    tok = tok.replace(' ', ' ▁ ')\n",
        "    for char, special_token in char2tok.items():\n",
        "        tok = tok.replace(char, special_token)\n",
        "    if tok.startswith(' STOKEN0'):\n",
        "        if tok.endswith('\\n'):\n",
        "            tok = tok[:-1]\n",
        "        tok += ' ENDCOM'\n",
        "    tok = tok.replace('\\n', ' STRNEWLINE ')\n",
        "    tok = tok.replace('\\t', ' TABSYMBOL ')\n",
        "    tok = re.sub(' +', ' ', tok)\n",
        "    tok = tokenize_v14_international(tok)\n",
        "    tok = re.sub(' +', ' ', tok)\n",
        "    for special_token, char in tok2char.items():\n",
        "        tok = tok.replace(special_token, char)\n",
        "    tok = tok.replace('\\r', '')\n",
        "\n",
        "    return tok\n",
        "\n",
        "\n",
        "def tokenize_python(s, keep_comments=False):\n",
        "    try:\n",
        "        assert isinstance(s, str)\n",
        "        s = s.replace(r'\\r', '')\n",
        "        tokens = []\n",
        "\n",
        "        try:\n",
        "            iterator = tokenize.tokenize(BytesIO(s.encode('utf-8')).readline)\n",
        "        except SyntaxError as excep:\n",
        "            raise SyntaxError(excep)\n",
        "\n",
        "        removed_docstr = 0\n",
        "        while True:\n",
        "            try:\n",
        "                toktype, tok, _, _, line = next(iterator)\n",
        "            except (tokenize.TokenError, IndentationError, SyntaxError, UnicodeDecodeError):\n",
        "                raise Exception(\n",
        "                    f\"Impossible to parse tokens because icorrect source code \\\"{s[0:30]}\\\" ...\")\n",
        "            except StopIteration:\n",
        "                raise Exception(f\"End of iterator before ENDMARKER token.\")\n",
        "\n",
        "            if toktype == tokenize.ENCODING or toktype == tokenize.NL:\n",
        "                continue\n",
        "\n",
        "            elif toktype == tokenize.NEWLINE:\n",
        "                if removed_docstr == 1:\n",
        "                    removed_docstr = 0\n",
        "                    continue\n",
        "                tokens.append('NEW_LINE')\n",
        "\n",
        "            elif toktype == tokenize.COMMENT:\n",
        "                if keep_comments:\n",
        "                    com = process_string(\n",
        "                        tok, PYTHON_CHAR2TOKEN, PYTHON_TOKEN2CHAR, True)\n",
        "                    if len(com) > 0:\n",
        "                        tokens.append(com)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            elif toktype == tokenize.STRING:\n",
        "                if tok == line.strip():  # docstring\n",
        "                    if not keep_comments:\n",
        "                        removed_docstr = 1\n",
        "                        continue\n",
        "                    else:\n",
        "                        coms = process_string(\n",
        "                            tok, PYTHON_CHAR2TOKEN, PYTHON_TOKEN2CHAR, True)\n",
        "                        if len(coms) > 0:\n",
        "                            tokens.append(coms)\n",
        "                        else:\n",
        "                            removed_docstr = 1\n",
        "                else:\n",
        "                    tokens.append(process_string(\n",
        "                        tok, PYTHON_CHAR2TOKEN, PYTHON_TOKEN2CHAR, False))\n",
        "\n",
        "            elif toktype == tokenize.INDENT:\n",
        "                tokens.append('INDENT')\n",
        "\n",
        "            elif toktype == tokenize.DEDENT:\n",
        "                # empty block\n",
        "                if tokens[-1] == 'INDENT':\n",
        "                    tokens = tokens[:-1]\n",
        "                else:\n",
        "                    tokens.append('DEDENT')\n",
        "\n",
        "            elif toktype == tokenize.ENDMARKER:\n",
        "                tokens.append('ENDMARKER')\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                tokens.append(tok)\n",
        "\n",
        "        assert (tokens[-1] == 'ENDMARKER'), \"Error, no end marker\"\n",
        "        return tokens[:-1]\n",
        "    except KeyboardInterrupt:\n",
        "        raise\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "\n",
        "def detokenize_python(s):\n",
        "    try:\n",
        "        assert isinstance(s, str) or isinstance(s, list)\n",
        "        if isinstance(s, list):\n",
        "            s = ' '.join(s)\n",
        "        s = s.replace('ENDCOM', 'NEW_LINE')\n",
        "        s = s.replace('▁', 'SPACETOKEN')\n",
        "\n",
        "        lines = s.split('NEW_LINE')\n",
        "        tabs = ''\n",
        "        for i, line in enumerate(lines):\n",
        "            line = line.strip()\n",
        "            if line.startswith('INDENT '):\n",
        "                tabs += '    '\n",
        "                line = line.replace('INDENT ', tabs)\n",
        "            elif line.startswith('DEDENT'):\n",
        "                number_dedent = line.count('DEDENT')\n",
        "                tabs = tabs[4 * number_dedent:]\n",
        "                line = line.replace(\"DEDENT\", '')\n",
        "                line = line.strip()\n",
        "                line = tabs + line\n",
        "            elif line == 'DEDENT':\n",
        "                line = ''\n",
        "            else:\n",
        "                line = tabs + line\n",
        "            lines[i] = line\n",
        "        untok_s = '\\n'.join(lines)\n",
        "\n",
        "        # find string and comment with parser and detokenize string correctly\n",
        "        try:\n",
        "            for toktype, tok, _, _, line in tokenize.tokenize(BytesIO(untok_s.encode('utf-8')).readline):\n",
        "                if toktype == tokenize.STRING or toktype == tokenize.COMMENT:\n",
        "                    tok_ = tok.replace('STRNEWLINE', '\\n').replace(\n",
        "                        'TABSYMBOL', '\\t').replace(' ', '').replace('SPACETOKEN', ' ')\n",
        "                    untok_s = untok_s.replace(tok, tok_)\n",
        "        except KeyboardInterrupt:\n",
        "            raise\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # detokenize imports\n",
        "        untok_s = untok_s.replace('. ', '.').replace(' .', '.').replace(\n",
        "            'import.', 'import .').replace('from.', 'from .')\n",
        "        untok_s = untok_s.replace('> >', '>>').replace('< <', '<<')\n",
        "        return untok_s\n",
        "    except KeyboardInterrupt:\n",
        "        raise\n",
        "    except:\n",
        "        return ''\n",
        "\n",
        "\n",
        "def extract_functions_python_with_docstring(function):\n",
        "    ds = re.findall(\n",
        "        \"[:][ ][N][E][W][_][L][I][N][E][ ][I][N][D][E][N][T][ ]['][']['].*?['][']['][ ][N][E][W][_][L][I][N][E]|[:][ ][N][E][W][_][L][I][N][E][ ][I][N][D][E][N][T][ ][\\\"][\\\"][\\\"].*?[\\\"][\\\"][\\\"][ ][N][E][W][_][L][I][N][E]\",\n",
        "        function, re.DOTALL)\n",
        "    if len(ds) > 0:\n",
        "        for d in ds:\n",
        "            function = function.replace(d[18:-9], '')\n",
        "        coms = ' '.join([d[18:-9] for d in ds])\n",
        "        inline_coms = re.findall('[#].*?[E][N][D][C][O][M]', function)\n",
        "        for inline_com in inline_coms:\n",
        "            function = function.replace(inline_com, '')\n",
        "            coms += ' <INLINE> '\n",
        "            coms += inline_com\n",
        "        if len(re.sub(r'\\W', '', coms.replace('<INLINE>', '').replace('ENDCOM', ''))) < 5:\n",
        "            return '', ''\n",
        "        else:\n",
        "            return re.sub('\\s+', ' ', function), coms\n",
        "    else:\n",
        "        return '', ''\n",
        "\n",
        "\n",
        "def extract_functions_python(s):\n",
        "    tokens = iter(s.split())\n",
        "    functions_standalone = []\n",
        "    functions_class = []\n",
        "    number_indent = 0\n",
        "    try:\n",
        "        token = next(tokens)\n",
        "    except StopIteration:\n",
        "        return [], []\n",
        "    while True:\n",
        "        try:\n",
        "            if token == 'def':\n",
        "                function = ['def']\n",
        "                while not (token == 'DEDENT' and number_indent == 0):\n",
        "                    token = next(tokens)\n",
        "                    if token == 'INDENT':\n",
        "                        number_indent += 1\n",
        "                    elif token == 'DEDENT':\n",
        "                        number_indent -= 1\n",
        "                    function.append(token)\n",
        "                try:\n",
        "                    if function[function.index('(') + 1] == 'self':\n",
        "                        function = filter_functions_python_2_3(\n",
        "                            ' '.join(function))\n",
        "                        if function is not None:\n",
        "                            functions_class.append(function)\n",
        "                    else:\n",
        "                        function = filter_functions_python_2_3(\n",
        "                            ' '.join(function))\n",
        "                        if function is not None:\n",
        "                            functions_standalone.append(function)\n",
        "                except:\n",
        "                    print(function)\n",
        "                    token = next(tokens)\n",
        "            else:\n",
        "                token = next(tokens)\n",
        "        except StopIteration:\n",
        "            break\n",
        "    return functions_standalone, functions_class\n",
        "\n",
        "\n",
        "def filter_functions_python_2_3(function):\n",
        "    if (re.search(\"print [^(]\", function) is None and\n",
        "            re.search(\"raise \\w+ ,\", function) is None and\n",
        "            re.search(\"except \\w+ ,\", function) is None and\n",
        "            re.search(\"[^ ]+ = \\d+ L\", function) is None and\n",
        "            \". iterkeys ( )\" not in function and\n",
        "            \". itervalues ( )\" not in function and\n",
        "            \". iteritems ( )\" not in function and\n",
        "            \"xrange (\" not in function and\n",
        "            \"imap (\" not in function):\n",
        "        return function\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_function_name_python(s):\n",
        "    assert isinstance(s, str) or isinstance(s, list)\n",
        "    if isinstance(s, str):\n",
        "        s = s.split()\n",
        "    return s[s.index('def') + 1]\n",
        "\n",
        "\n",
        "@timeout(10)\n",
        "def get_cpp_tokens_and_types(s):\n",
        "    tokens = []\n",
        "    assert isinstance(s, str)\n",
        "    s = s.replace(r'\\r', '')\n",
        "    hash = str(random.getrandbits(128))\n",
        "    parsed_code = idx.parse(\n",
        "        hash + '_tmp.cpp', args=['-std=c++11'], unsaved_files=[(hash + '_tmp.cpp', s)], options=0)\n",
        "    for tok in parsed_code.get_tokens(extent=parsed_code.cursor.extent):\n",
        "        tokens.append((tok.spelling, tok.kind))\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def tokenize_cpp(s, keep_comments=False):\n",
        "    tokens = []\n",
        "    assert isinstance(s, str)\n",
        "    try:\n",
        "        tokens_and_types = get_cpp_tokens_and_types(s)\n",
        "        for tok, typ in tokens_and_types:\n",
        "            if not keep_comments and typ == TokenKind.COMMENT:\n",
        "                continue\n",
        "            if typ in STRINGS_AND_COMMENTS_TOKEN_KINDS:\n",
        "                if typ == TokenKind.COMMENT:\n",
        "                    com = process_string(\n",
        "                        tok, CPP_CHAR2TOKEN, CPP_TOKEN2CHAR, True)\n",
        "                    if len(com) > 0:\n",
        "                        tokens.append(com)\n",
        "                else:\n",
        "                    tokens.append(process_string(\n",
        "                        tok, CPP_CHAR2TOKEN, CPP_TOKEN2CHAR, False))\n",
        "            else:\n",
        "                tokens.append(tok)\n",
        "        return tokens\n",
        "    except KeyboardInterrupt:\n",
        "        raise\n",
        "    except TimeoutError:\n",
        "        print(f'TimeOut Error')\n",
        "        logging.info('*' * 20)\n",
        "        logging.info(f'TimeOut Error for string {s}')\n",
        "        return []\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "\n",
        "def tokenize_java(s, keep_comments=False):\n",
        "    try:\n",
        "        tokens = []\n",
        "        assert isinstance(s, str)\n",
        "        s = s.replace(r'\\r', '')\n",
        "        tokens_generator = javalang_tok.tokenize(\n",
        "            s, keep_comments=keep_comments)\n",
        "        for token in tokens_generator:\n",
        "            if isinstance(token, javalang_tok.String):\n",
        "                tokens.append(process_string(\n",
        "                    token.value, JAVA_CHAR2TOKEN, JAVA_TOKEN2CHAR, False))\n",
        "            elif isinstance(token, javalang_tok.Comment):\n",
        "                com = process_string(\n",
        "                    token.value, JAVA_CHAR2TOKEN, JAVA_TOKEN2CHAR, True)\n",
        "                if len(com) > 0:\n",
        "                    tokens.append(com)\n",
        "            else:\n",
        "                tokens.append(token.value)\n",
        "        return tokens\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "\n",
        "def detokenize_cpp(s):\n",
        "    assert isinstance(s, str) or isinstance(s, list)\n",
        "    if isinstance(s, list):\n",
        "        s = ' '.join(s)\n",
        "    # the ▁ character created bugs in the cpp tokenizer\n",
        "    s = s.replace('ENDCOM', '\\n').replace('▁', ' SPACETOKEN ')\n",
        "    try:\n",
        "        tokens_and_types = get_cpp_tokens_and_types(s)\n",
        "    except:\n",
        "        return ''\n",
        "    new_tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens_and_types):\n",
        "        token, type = tokens_and_types[i]\n",
        "        if type in STRINGS_AND_COMMENTS_TOKEN_KINDS:\n",
        "            new_tokens.append(token.replace('STRNEWLINE', '\\n').replace(\n",
        "                'TABSYMBOL', '\\t').replace(' ', '').replace('SPACETOKEN', ' '))\n",
        "            if type == TokenKind.COMMENT:\n",
        "                new_tokens.append('NEW_LINE')\n",
        "        elif token == '}':\n",
        "            if i < len(tokens_and_types) - 1 and tokens_and_types[i + 1][0] == ';':\n",
        "                new_tokens += ['CB_COLON', 'NEW_LINE']\n",
        "                i += 2\n",
        "                continue\n",
        "            if i < len(tokens_and_types) - 1 and tokens_and_types[i + 1][0] == ',':\n",
        "                new_tokens += ['CB_COMA', 'NEW_LINE']\n",
        "                i += 2\n",
        "                continue\n",
        "            new_tokens += ['CB_', 'NEW_LINE']\n",
        "        elif token == '{':\n",
        "            new_tokens += ['OB_', 'NEW_LINE']\n",
        "        elif token == '*/':\n",
        "            new_tokens += ['*/', 'NEW_LINE']\n",
        "        elif token == ';':\n",
        "            new_tokens += [';', 'NEW_LINE']\n",
        "        else:\n",
        "            new_tokens.append(token)\n",
        "\n",
        "        if i < len(tokens_and_types) - 1 and tokens_and_types[i + 1][0] in TOK_NO_SPACE_BEFORE:\n",
        "            next_token = tokens_and_types[i + 1][0]\n",
        "            new_tokens[len(new_tokens) - 1] += next_token\n",
        "            if next_token == ';':\n",
        "                new_tokens.append('NEW_LINE')\n",
        "            i += 2\n",
        "            continue\n",
        "        i += 1\n",
        "\n",
        "    lines = re.split('NEW_LINE', ' '.join(new_tokens))\n",
        "\n",
        "    untok_s = indent_lines(lines)\n",
        "    untok_s = untok_s.replace('CB_COLON', '};').replace(\n",
        "        'CB_COMA', '},').replace('CB_', '}').replace('OB_', '{')\n",
        "    return untok_s\n",
        "\n",
        "\n",
        "def indent_lines(lines):\n",
        "    prefix = ''\n",
        "    for i, line in enumerate(lines):\n",
        "        line = line.strip()\n",
        "        if re.match('CB_COLON|CB_COMA|CB_', line):\n",
        "            prefix = prefix[2:]\n",
        "            line = prefix + line\n",
        "        elif line.endswith('OB_'):\n",
        "            line = prefix + line\n",
        "            prefix += '  '\n",
        "        else:\n",
        "            line = prefix + line\n",
        "        lines[i] = line\n",
        "    untok_s = '\\n'.join(lines)\n",
        "    return untok_s\n",
        "\n",
        "\n",
        "def detokenize_java(s):\n",
        "    assert isinstance(s, str) or isinstance(s, list)\n",
        "    if isinstance(s, list):\n",
        "        s = ' '.join(s)\n",
        "    s = s.replace('ENDCOM', 'NEW_LINE')\n",
        "    s = s.replace('▁', 'SPACETOKEN')\n",
        "\n",
        "    s = s.replace('} \"', 'CB_ \"')\n",
        "    s = s.replace('\" {', '\" OB_')\n",
        "    s = s.replace('*/ ', '*/ NEW_LINE')\n",
        "    s = s.replace('} ;', 'CB_COLON NEW_LINE')\n",
        "    s = s.replace('} ,', 'CB_COMA')\n",
        "    s = s.replace('}', 'CB_ NEW_LINE')\n",
        "    s = s.replace('{', 'OB_ NEW_LINE')\n",
        "    s = s.replace(';', '; NEW_LINE')\n",
        "    lines = re.split('NEW_LINE', s)\n",
        "\n",
        "    untok_s = indent_lines(lines)\n",
        "    untok_s = untok_s.replace('CB_COLON', '};').replace(\n",
        "        'CB_COMA', '},').replace('CB_', '}').replace('OB_', '{')\n",
        "    untok_s = untok_s.replace('> > >', '>>>').replace('<< <', '<<<')\n",
        "    untok_s = untok_s.replace('> >', '>>').replace('< <', '<<')\n",
        "\n",
        "    try:\n",
        "        # call parser of the tokenizer to find comments and string and detokenize them correctly\n",
        "        tokens_generator = javalang_tok.tokenize(untok_s, keep_comments=True)\n",
        "        for token in tokens_generator:\n",
        "            if isinstance(token, javalang_tok.String) or isinstance(token, javalang_tok.Comment):\n",
        "                token_ = token.value.replace('STRNEWLINE', '\\n').replace('TABSYMBOL', '\\t').replace(' ', '').replace(\n",
        "                    'SPACETOKEN', ' ')\n",
        "                untok_s = untok_s.replace(token.value, token_)\n",
        "    except KeyboardInterrupt:\n",
        "        raise\n",
        "    except:\n",
        "        pass\n",
        "    return untok_s\n",
        "\n",
        "\n",
        "def extract_functions_java(s):\n",
        "    tokens = s.split()\n",
        "    i = ind_iter(len(tokens))\n",
        "    functions_standalone = []\n",
        "    functions_class = []\n",
        "    try:\n",
        "        token = tokens[i.i]\n",
        "    except KeyboardInterrupt:\n",
        "        raise\n",
        "    except:\n",
        "        return [], []\n",
        "    while True:\n",
        "        try:\n",
        "            # detect function\n",
        "            if token == ')' and (tokens[i.i + 1] == '{' or (tokens[i.i + 1] == 'throws' and tokens[i.i + 3] == '{')):\n",
        "                # go previous until the start of function\n",
        "                while token not in [';', '}', '{', '*/', 'ENDCOM']:\n",
        "                    i.prev()\n",
        "                    token = tokens[i.i]\n",
        "\n",
        "                if token == '*/':\n",
        "                    while token != '/*':\n",
        "                        i.prev()\n",
        "                        token = tokens[i.i]\n",
        "                    function = [token]\n",
        "                    while token != '*/':\n",
        "                        i.next()\n",
        "                        token = tokens[i.i]\n",
        "                        function.append(token)\n",
        "                elif token == 'ENDCOM':\n",
        "                    while token != '//':\n",
        "                        i.prev()\n",
        "                        token = tokens[i.i]\n",
        "                    function = [token]\n",
        "                    while token != 'ENDCOM':\n",
        "                        i.next()\n",
        "                        token = tokens[i.i]\n",
        "                        function.append(token)\n",
        "                else:\n",
        "                    i.next()\n",
        "                    token = tokens[i.i]\n",
        "                    function = [token]\n",
        "\n",
        "                while token != '{':\n",
        "                    i.next()\n",
        "                    token = tokens[i.i]\n",
        "                    function.append(token)\n",
        "                if token == '{':\n",
        "                    number_indent = 1\n",
        "                    while not (token == '}' and number_indent == 0):\n",
        "                        try:\n",
        "                            i.next()\n",
        "                            token = tokens[i.i]\n",
        "                            if token == '{':\n",
        "                                number_indent += 1\n",
        "                            elif token == '}':\n",
        "                                number_indent -= 1\n",
        "                            function.append(token)\n",
        "                        except StopIteration:\n",
        "                            break\n",
        "                    if 'static' in function[0:function.index('{')]:\n",
        "                        functions_standalone.append(\n",
        "                            remove_java_annotation(' '.join(function)))\n",
        "                    else:\n",
        "                        functions_class.append(\n",
        "                            remove_java_annotation(' '.join(function)))\n",
        "            i.next()\n",
        "            token = tokens[i.i]\n",
        "        except KeyboardInterrupt:\n",
        "            raise\n",
        "        except:\n",
        "            break\n",
        "    return functions_standalone, functions_class\n",
        "\n",
        "\n",
        "def extract_functions_java_with_docstring(function):\n",
        "    ds = re.findall('[/][*].*?[*][/][ ]', function, re.DOTALL)\n",
        "    if len(ds) > 0:\n",
        "        for d in ds:\n",
        "            function = function.replace(d, '')\n",
        "        coms = ' '.join([d[:-1] for d in ds])\n",
        "        inline_coms = re.findall('[/][/].*?[E][N][D][C][O][M]', function)\n",
        "        for inline_com in inline_coms:\n",
        "            function = function.replace(inline_com, '')\n",
        "            coms += ' <INLINE> '\n",
        "            coms += inline_com\n",
        "        if len(re.sub(r'\\W', '', coms.replace('<INLINE>', '').replace('ENDCOM', ''))) < 5:\n",
        "            return '', ''\n",
        "        else:\n",
        "            return re.sub('\\s+', ' ', function), coms\n",
        "    else:\n",
        "        return '', ''\n",
        "\n",
        "\n",
        "def clean_hashtags_functions_cpp(function):\n",
        "    function = re.sub('[#][ ][i][n][c][l][u][d][e][ ][\"].*?[\"]', \"\", function)\n",
        "    function = re.sub('[#][ ][i][n][c][l][u][d][e][ ][<].*?[>]', \"\", function)\n",
        "    function = re.sub('[#][ ][i][f][n][d][e][f][ ][^ ]*', \"\", function)\n",
        "    function = re.sub('[#][ ][i][f][d][e][f][ ][^ ]*', \"\", function)\n",
        "    function = re.sub(\n",
        "        '[#][ ][d][e][f][i][n][e][ ][^ ]*[ ][(][ ].*?[ ][)][ ][(][ ].*[ ][)]', \"\", function)\n",
        "    function = re.sub(\n",
        "        '[#][ ][d][e][f][i][n][e][ ][^ ]*[ ][(][ ].*?[ ][)][ ][{][ ].*[ ][}]', \"\", function)\n",
        "    function = re.sub(\n",
        "        '[#][ ][d][e][f][i][n][e][ ][^ ]*[ ]([(][ ])?[\"].*?[\"]([ ][)])?', \"\", function)\n",
        "    function = re.sub(\n",
        "        '[#][ ][d][e][f][i][n][e][ ][^ ]*[ ]([(][ ])?\\d*\\.?\\d*([ ][+-/*][ ]?\\d*\\.?\\d*)?([ ][)])?', \"\", function)\n",
        "    function = re.sub('[#][ ][d][e][f][i][n][e][ ][^ ]', \"\", function)\n",
        "    function = re.sub(\n",
        "        '[#][ ][i][f][ ][d][e][f][i][n][e][d][ ][(][ ].*?[ ][)]', \"\", function)\n",
        "    function = re.sub('[#][ ][i][f][ ][^ ]*', \"\", function)\n",
        "    function = function.replace('# else', '')\n",
        "    function = function.replace('# endif', '')\n",
        "    function = function.strip()\n",
        "    return function\n",
        "\n",
        "\n",
        "def extract_functions_cpp(s):\n",
        "    try:\n",
        "        s = clean_hashtags_functions_cpp(s)\n",
        "        s = s.replace('ENDCOM', '\\n').replace('▁', 'SPACETOKEN')\n",
        "        tokens = get_cpp_tokens_and_types(s)\n",
        "    except:\n",
        "        return [], []\n",
        "    i = ind_iter(len(tokens))\n",
        "    functions_standalone = []\n",
        "    functions_class = []\n",
        "    try:\n",
        "        token, token_type = tokens[i.i]\n",
        "    except:\n",
        "        return [], []\n",
        "    while True:\n",
        "        try:\n",
        "            # detect function\n",
        "            if token == ')' and ((tokens[i.i + 1][0] == '{' and tokens[i.i + 2][0] != '}') or (\n",
        "                    tokens[i.i + 1][0] == 'throw' and tokens[i.i + 4][0] == '{' and tokens[i.i + 5][0] == '}')):\n",
        "                # go previous until the start of function\n",
        "                while token not in {';', '}', '{'}:\n",
        "                    try:\n",
        "                        i.prev()\n",
        "                    except StopIteration:\n",
        "                        break\n",
        "                    token = tokens[i.i][0]\n",
        "\n",
        "                i.next()\n",
        "                token, token_type = tokens[i.i]\n",
        "                if token_type == TokenKind.COMMENT:\n",
        "                    token = token.strip()\n",
        "                    token += \" ENDCOM\"\n",
        "                function = [token]\n",
        "                token_types = [token_type]\n",
        "                while token != '{':\n",
        "                    i.next()\n",
        "                    token, token_type = tokens[i.i]\n",
        "                    if token_type == TokenKind.COMMENT:\n",
        "                        token = token.strip()\n",
        "                        token += \" ENDCOM\"\n",
        "                    function.append(token)\n",
        "                    token_types.append(token_type)\n",
        "\n",
        "                if token_types[function.index('(') - 1] != TokenKind.IDENTIFIER:\n",
        "                    continue\n",
        "                if token == '{':\n",
        "                    number_indent = 1\n",
        "                    while not (token == '}' and number_indent == 0):\n",
        "                        try:\n",
        "                            i.next()\n",
        "                            token, token_type = tokens[i.i]\n",
        "                            if token == '{':\n",
        "                                number_indent += 1\n",
        "                            elif token == '}':\n",
        "                                number_indent -= 1\n",
        "                            if token_type == TokenKind.COMMENT:\n",
        "                                token = token.strip()\n",
        "                                token += \" ENDCOM\"\n",
        "                            function.append(token)\n",
        "                        except StopIteration:\n",
        "                            break\n",
        "                    if 'static' in function[0:function.index('{')] or '::' not in function[0:function.index('(')]:\n",
        "                        function = ' '.join(function)\n",
        "                        function = re.sub(\n",
        "                            \"[<][ ][D][O][C][U][M][E][N][T].*?[>] \", \"\", function)\n",
        "                        function = clean_hashtags_functions_cpp(function)\n",
        "                        function = function.strip()\n",
        "                        function = function.replace(\n",
        "                            '\\n', 'ENDCOM').replace('SPACETOKEN', '▁')\n",
        "                        if not re.sub('[^ ]*[ ][(][ ]\\w*([ ][,][ ]\\w*)*[ ][)]', \"\", function[:function.index('{')]).strip().startswith('{') and not function.startswith('#'):\n",
        "                            functions_standalone.append(function)\n",
        "                    else:\n",
        "                        function = ' '.join(function)\n",
        "                        function = re.sub(\n",
        "                            \"[<][ ][D][O][C][U][M][E][N][T].*?[>] \", \"\", function)\n",
        "                        function = clean_hashtags_functions_cpp(function)\n",
        "                        function = function.strip()\n",
        "                        function = function.replace(\n",
        "                            '\\n', 'ENDCOM').replace('SPACETOKEN', '▁')\n",
        "                        if not re.sub('[^ ]*[ ][(][ ]\\w*([ ][,][ ]\\w*)*[ ][)]', \"\", function[:function.index('{')]).strip().startswith('{') and not function.startswith('#'):\n",
        "                            functions_class.append(function)\n",
        "            i.next()\n",
        "            token = tokens[i.i][0]\n",
        "        except:\n",
        "            break\n",
        "    return functions_standalone, functions_class\n",
        "\n",
        "\n",
        "def extract_functions_cpp_with_docstring(function):\n",
        "    function = re.sub(\"[<][ ][D][O][C][U][M][E][N][T].*?[>] \", \"\", function)\n",
        "    ds = re.findall('[/][*].*?[*][/][ ]', function, re.DOTALL)\n",
        "    if len(ds) > 0:\n",
        "        for d in ds:\n",
        "            function = function.replace(d, '')\n",
        "        coms = ' '.join([d[:-1] for d in ds])\n",
        "        inline_coms = re.findall('[/][/].*?[E][N][D][C][O][M]', function)\n",
        "        for inline_com in inline_coms:\n",
        "            function = function.replace(inline_com, '')\n",
        "            coms += ' <INLINE> '\n",
        "            coms += inline_com\n",
        "        if len(re.sub(r'\\W', '', coms.replace('<INLINE>', '').replace('ENDCOM', ''))) < 5:\n",
        "            return '', ''\n",
        "        else:\n",
        "            return re.sub('\\s+', ' ', function), coms\n",
        "    else:\n",
        "        return '', ''\n",
        "\n",
        "\n",
        "def remove_java_annotation(function):\n",
        "    return re.sub('^(@ (Override|Deprecated|SuppressWarnings) (\\( .* \\) )?)*', '', function)\n",
        "\n",
        "\n",
        "def get_first_token_before_first_parenthesis(s):\n",
        "    assert isinstance(s, str) or isinstance(s, list)\n",
        "    if isinstance(s, str):\n",
        "        s = s.split()\n",
        "    return s[s.index('(') - 1]\n",
        "\n",
        "\n",
        "def get_function_name_java(s):\n",
        "    return get_first_token_before_first_parenthesis(s)\n",
        "\n",
        "\n",
        "def get_function_name_cpp(s):\n",
        "    return get_first_token_before_first_parenthesis(s)\n",
        "\n",
        "\n",
        "def extract_arguments_java(f):\n",
        "    return extract_arguments_java_using_parentheses(f)\n",
        "\n",
        "\n",
        "def extract_arguments_cpp(f):\n",
        "    return extract_arguments_java_using_parentheses(f)\n",
        "\n",
        "\n",
        "def extract_arguments_java_using_parentheses(f):\n",
        "    f = f.split(' ')\n",
        "    types = []\n",
        "    names = []\n",
        "    par = 0\n",
        "    arguments = []\n",
        "    f = f[f.index('('):]\n",
        "    for tok in f:\n",
        "        if tok == '(':\n",
        "            par += 1\n",
        "        elif tok == ')':\n",
        "            par -= 1\n",
        "        arguments.append(tok)\n",
        "        if par == 0:\n",
        "            break\n",
        "    arguments = ' '.join(arguments[1:-1])\n",
        "    if arguments == '':\n",
        "        return ['None'], ['None']\n",
        "    arguments = arguments.split(',')\n",
        "    for arg in arguments:\n",
        "        bracks = re.findall('\\[ \\]', arg)\n",
        "        bracks = ' '.join(bracks)\n",
        "        arg = arg.replace(bracks, '')\n",
        "        arg = arg.strip()\n",
        "        arg = re.sub(' +', ' ', arg)\n",
        "        t = ' '.join(arg.split(' ')[:-1] + [bracks])\n",
        "        n = arg.split(' ')[-1]\n",
        "        types.append(t)\n",
        "        names.append(n)\n",
        "    return types, names\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fioHzVQTESZJ"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from logging import getLogger\n",
        "\n",
        "\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "PAD_WORD = '<pad>'\n",
        "UNK_WORD = '<unk>'\n",
        "\n",
        "SPECIAL_WORD = '<special%i>'\n",
        "SPECIAL_WORDS = 10\n",
        "\n",
        "SEP_WORD = SPECIAL_WORD % 0\n",
        "MASK_WORD = SPECIAL_WORD % 1\n",
        "\n",
        "\n",
        "class Dictionary(object):\n",
        "\n",
        "    def __init__(self, id2word, word2id, counts):\n",
        "        assert len(id2word) == len(word2id) == len(counts)\n",
        "        self.id2word = id2word\n",
        "        self.word2id = word2id\n",
        "        self.counts = counts\n",
        "        self.bos_index = word2id[BOS_WORD]\n",
        "        self.eos_index = word2id[EOS_WORD]\n",
        "        self.pad_index = word2id[PAD_WORD]\n",
        "        self.unk_index = word2id[UNK_WORD]\n",
        "        self.check_valid()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of words in the dictionary.\n",
        "        \"\"\"\n",
        "        return len(self.id2word)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"\n",
        "        Returns the word of the specified index.\n",
        "        \"\"\"\n",
        "        return self.id2word[i]\n",
        "\n",
        "    def __contains__(self, w):\n",
        "        \"\"\"\n",
        "        Returns whether a word is in the dictionary.\n",
        "        \"\"\"\n",
        "        return w in self.word2id\n",
        "\n",
        "    def __eq__(self, y):\n",
        "        \"\"\"\n",
        "        Compare this dictionary with another one.\n",
        "        \"\"\"\n",
        "        self.check_valid()\n",
        "        y.check_valid()\n",
        "        if len(self.id2word) != len(y):\n",
        "            return False\n",
        "        return all(self.id2word[i] == y[i] for i in range(len(y)))\n",
        "\n",
        "    def check_valid(self):\n",
        "        \"\"\"\n",
        "        Check that the dictionary is valid.\n",
        "        \"\"\"\n",
        "        assert self.bos_index == 0\n",
        "        assert self.eos_index == 1\n",
        "        assert self.pad_index == 2\n",
        "        assert self.unk_index == 3\n",
        "        assert all(self.id2word[4 + i] == SPECIAL_WORD %\n",
        "                   i for i in range(SPECIAL_WORDS))\n",
        "        assert len(self.id2word) == len(self.word2id) == len(self.counts)\n",
        "        assert set(self.word2id.keys()) == set(self.counts.keys())\n",
        "        for i in range(len(self.id2word)):\n",
        "            assert self.word2id[self.id2word[i]] == i\n",
        "        last_count = 1e18\n",
        "        for i in range(4 + SPECIAL_WORDS, len(self.id2word) - 1):\n",
        "            count = self.counts[self.id2word[i]]\n",
        "            assert count <= last_count\n",
        "            last_count = count\n",
        "\n",
        "    def index(self, word, no_unk=False):\n",
        "        \"\"\"\n",
        "        Returns the index of the specified word.\n",
        "        \"\"\"\n",
        "        if no_unk:\n",
        "            return self.word2id[word]\n",
        "        else:\n",
        "            return self.word2id.get(word, self.unk_index)\n",
        "\n",
        "    def max_vocab(self, max_vocab):\n",
        "        \"\"\"\n",
        "        Limit the vocabulary size.\n",
        "        \"\"\"\n",
        "        assert max_vocab >= 1\n",
        "        init_size = len(self)\n",
        "        self.id2word = {k: v for k, v in self.id2word.items() if k < max_vocab}\n",
        "        self.word2id = {v: k for k, v in self.id2word.items()}\n",
        "        self.counts = {k: v for k, v in self.counts.items()\n",
        "                       if k in self.word2id}\n",
        "        self.check_valid()\n",
        "        logger.info(\"Maximum vocabulary size: %i. Dictionary size: %i -> %i (removed %i words).\"\n",
        "                    % (max_vocab, init_size, len(self), init_size - len(self)))\n",
        "\n",
        "    def min_count(self, min_count):\n",
        "        \"\"\"\n",
        "        Threshold on the word frequency counts.\n",
        "        \"\"\"\n",
        "        assert min_count >= 0\n",
        "        init_size = len(self)\n",
        "        self.id2word = {k: v for k, v in self.id2word.items(\n",
        "        ) if self.counts[self.id2word[k]] >= min_count or k < 4 + SPECIAL_WORDS}\n",
        "        self.word2id = {v: k for k, v in self.id2word.items()}\n",
        "        self.counts = {k: v for k, v in self.counts.items()\n",
        "                       if k in self.word2id}\n",
        "        self.check_valid()\n",
        "        logger.info(\"Minimum frequency count: %i. Dictionary size: %i -> %i (removed %i words).\"\n",
        "                    % (min_count, init_size, len(self), init_size - len(self)))\n",
        "\n",
        "    @staticmethod\n",
        "    def read_vocab(vocab_path):\n",
        "        \"\"\"\n",
        "        Create a dictionary from a vocabulary file.\n",
        "        \"\"\"\n",
        "        skipped = 0\n",
        "        assert os.path.isfile(vocab_path), vocab_path\n",
        "        word2id = {BOS_WORD: 0, EOS_WORD: 1, PAD_WORD: 2, UNK_WORD: 3}\n",
        "        for i in range(SPECIAL_WORDS):\n",
        "            word2id[SPECIAL_WORD % i] = 4 + i\n",
        "        counts = {k: 0 for k in word2id.keys()}\n",
        "        f = open(vocab_path, 'r', encoding='utf-8')\n",
        "        for i, line in enumerate(f):\n",
        "            if '\\u2028' in line:\n",
        "                skipped += 1\n",
        "                continue\n",
        "            line = line.rstrip().split()\n",
        "            if len(line) != 2:\n",
        "                skipped += 1\n",
        "                continue\n",
        "            assert len(line) == 2, (i, line)\n",
        "            # assert line[0] not in word2id and line[1].isdigit(), (i, line)\n",
        "            assert line[1].isdigit(), (i, line)\n",
        "            if line[0] in word2id:\n",
        "                skipped += 1\n",
        "                print('%s already in vocab' % line[0])\n",
        "                continue\n",
        "            if not line[1].isdigit():\n",
        "                skipped += 1\n",
        "                print('Empty word at line %s with count %s' % (i, line))\n",
        "                continue\n",
        "            word2id[line[0]] = 4 + SPECIAL_WORDS + i - \\\n",
        "                skipped  # shift because of extra words\n",
        "            counts[line[0]] = int(line[1])\n",
        "        f.close()\n",
        "        id2word = {v: k for k, v in word2id.items()}\n",
        "        dico = Dictionary(id2word, word2id, counts)\n",
        "        logger.info(\"Read %i words from the vocabulary file.\" % len(dico))\n",
        "        if skipped > 0:\n",
        "            logger.warning(\"Skipped %i empty lines!\" % skipped)\n",
        "        return dico\n",
        "\n",
        "    @staticmethod\n",
        "    def index_data(path, bin_path, dico):\n",
        "        \"\"\"\n",
        "        Index sentences with a dictionary.\n",
        "        \"\"\"\n",
        "        if bin_path is not None and os.path.isfile(bin_path):\n",
        "            print(\"Loading data from %s ...\" % bin_path)\n",
        "            data = torch.load(bin_path)\n",
        "            assert dico == data['dico']\n",
        "            return data\n",
        "\n",
        "        positions = []\n",
        "        sentences = []\n",
        "        unk_words = {}\n",
        "\n",
        "        # index sentences\n",
        "        f = open(path, 'r', encoding='utf-8')\n",
        "        for i, line in enumerate(f):\n",
        "            if i % 1000000 == 0 and i > 0:\n",
        "                print(i)\n",
        "            s = line.rstrip().split()\n",
        "            # skip empty sentences\n",
        "            if len(s) == 0:\n",
        "                print(\"Empty sentence in line %i.\" % i)\n",
        "            # index sentence words\n",
        "            count_unk = 0\n",
        "            indexed = []\n",
        "            for w in s:\n",
        "                word_id = dico.index(w, no_unk=False)\n",
        "                # if we find a special word which is not an unknown word, skip the sentence\n",
        "                if 0 <= word_id < 4 + SPECIAL_WORDS and word_id != 3:\n",
        "                    logger.warning(\n",
        "                        'Found unexpected special word \"%s\" (%i)!!' % (w, word_id))\n",
        "                    continue\n",
        "                assert word_id >= 0\n",
        "                indexed.append(word_id)\n",
        "                if word_id == dico.unk_index:\n",
        "                    unk_words[w] = unk_words.get(w, 0) + 1\n",
        "                    count_unk += 1\n",
        "            # add sentence\n",
        "            positions.append([len(sentences), len(sentences) + len(indexed)])\n",
        "            sentences.extend(indexed)\n",
        "            sentences.append(1)  # EOS index\n",
        "        f.close()\n",
        "\n",
        "        # tensorize data\n",
        "        positions = np.int64(positions)\n",
        "        if len(dico) < 1 << 16:\n",
        "            sentences = np.uint16(sentences)\n",
        "        elif len(dico) < 1 << 31:\n",
        "            sentences = np.int32(sentences)\n",
        "        else:\n",
        "            raise Exception(\"Dictionary is too big.\")\n",
        "        assert sentences.min() >= 0\n",
        "        data = {\n",
        "            'dico': dico,\n",
        "            'positions': positions,\n",
        "            'sentences': sentences,\n",
        "            'unk_words': unk_words,\n",
        "        }\n",
        "        if bin_path is not None:\n",
        "            print(\"Saving the data to %s ...\" % bin_path)\n",
        "            torch.save(data, bin_path, pickle_protocol=4)\n",
        "\n",
        "        return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhxmrULGFhum"
      },
      "source": [
        "### Generating Code Vocab files with fastBPE "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRGYoDVGFHwt"
      },
      "source": [
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/TransCoder/* ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYXEeBAfFpXe"
      },
      "source": [
        "nl_pl_df = pd.read_csv(\"/content/end_capstone.csv\")\n",
        "# nl_pl_df['docstring'].to_csv(\"/content/end_capstone_docstring.txt\",index=False)\n",
        "# nl_pl_df['code'].to_csv(\"/content/end_capstone_code.txt\",index=False)\n",
        "with open(\"enc_code_full.txt\",'w') as f:\n",
        "    for code_piece in nl_pl_df.loc[:, 'code']:\n",
        "        toks = tokenize_python(code_piece, keep_comments=False)\n",
        "        f.write(' '.join(toks))\n",
        "        f.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nTl3G16Ffgu",
        "outputId": "4471a783-a042-4b0d-d4f0-2619ff4a30e5"
      },
      "source": [
        "%%bash\n",
        "./fast learnbpe 10000 /content/enc_code_full.txt > python_codes_full.txt\n",
        "./fast getvocab /content/enc_code_full.txt > vocab_full.txt\n",
        "./fast applybpe python_code_applybpe_full.txt /content/enc_code_full.txt python_codes_full.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading vocabulary from /content/enc_code_full.txt ...\n",
            "Read 276934 words (5686 unique) from text file.\n",
            "tcmalloc: large alloc 12000002048 bytes == 0x561cca8c2000 @  0x7fe73d35d887 0x561cc81548f3 0x561cc814978f 0x7fe73c798bf7 0x561cc8149a1a\n",
            "Loading vocabulary from /content/enc_code_full.txt ...\n",
            "Read 276934 words (5686 unique) from text file.\n",
            "Loading codes from python_codes_full.txt ...\n",
            "Read 10000 codes from the codes file.\n",
            "Loading vocabulary from /content/enc_code_full.txt ...\n",
            "Read 276934 words (5686 unique) from text file.\n",
            "Applying BPE to /content/enc_code_full.txt ...\n",
            "Modified 276934 words from text file.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlGmVYBhFe97"
      },
      "source": [
        "### Creating the vocab\n",
        "my_dict = Dictionary.read_vocab(\"/content/vocab_full.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80LVVVV-FMb3",
        "outputId": "423df77b-29a4-4f06-f335-1fb912acdfcf"
      },
      "source": [
        "complete_data = Dictionary.index_data('/content/enc_code_full.txt', '/content/enc_code_full.txt.pth', my_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data from /content/enc_code_full.txt.pth ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP4FDsCrJEbY"
      },
      "source": [
        "for index in data['positions'][:10]:\n",
        "    print(index[0],index[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRSPC2zVJgBJ"
      },
      "source": [
        "list(data['sentences'][0:27])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG_5Uv0kGNkV"
      },
      "source": [
        "for index in data['positions'][:10]:\n",
        "    start,end = index[0], index[1]\n",
        "    [self.code_tok_vectorizer.bos_index] + tok_ids + [self.code_tok_vectorizer.eos_index]\n",
        "    #print(' '.join([ my_dict.id2word[idx] for idx in data['sentences'][start:end]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCOWxVmsKqfi",
        "outputId": "5030716c-4785-4abe-fccf-5b3e9d0be78d"
      },
      "source": [
        "my_dict.pad_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTR0cQhHhmWC"
      },
      "source": [
        "### New NLP DS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVD-3TjxY1SD"
      },
      "source": [
        "tok_ids_list=[]\n",
        "class NLPLSingleEntry(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 code_ids,\n",
        "                 code_mask, \n",
        "                 doc_ids,\n",
        "                 doc_mask,\n",
        "                 ):\n",
        "        self.code_ids = code_ids\n",
        "        self.code_mask = code_mask\n",
        "        #self.tok_ids = tok_ids\n",
        "        self.doc_ids = doc_ids\n",
        "        self.doc_mask = doc_mask\n",
        "        #self.segment_ids = segment_ids\n",
        "        #self.label_id = label_id\n",
        "class NLPLDataSet():\n",
        "    def __init__(self, \n",
        "                 doc_tokenizer, \n",
        "                 code_tokenizer,\n",
        "                 code_tok_vectorizer):\n",
        "        self.doc_tokenizer = doc_tokenizer\n",
        "        self.code_tokenizer = code_tokenizer\n",
        "        self.code_tok_vectorizer = code_tok_vectorizer\n",
        "\n",
        "    def prepare_tokens(self, \n",
        "                       samples, \n",
        "                       tokenizer, \n",
        "                       max_seq_length=0,\n",
        "                       data_type=None):\n",
        "        \"\"\"\n",
        "            Tokenizes an input sequence, adds padding and SOS+EOS \n",
        "        \"\"\"\n",
        "        toks = tokenizer.tokenize(samples.lower())\n",
        "        # print(data_type)\n",
        "        # print(toks)\n",
        "        if max_seq_length > 2 and len(toks) > max_seq_length - 2:\n",
        "            toks = toks[:max_seq_length -2]\n",
        "        tok_ids =  tokenizer.convert_tokens_to_ids(toks)\n",
        "        ### We use pseudo-BERT process so we will add both CLS and SEP tokens for\n",
        "        ### src and target inputs\n",
        "        tok_ids = [tokenizer.cls_token_id] + tok_ids + [tokenizer.sep_token_id]\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [ 1 ] * len(tok_ids)\n",
        "\n",
        "        if len(tok_ids) < max_seq_length:\n",
        "            padding_length = max_seq_length - len(tok_ids)\n",
        "            tok_ids = tok_ids + ([tokenizer.pad_token_id] * padding_length)\n",
        "            input_mask = input_mask + ([ 0 ] * padding_length) ### Padded tokens are zero-masked\n",
        "        \n",
        "        # print(tok_ids)\n",
        "        return tok_ids, input_mask\n",
        "    def prepare_code_tokens(self, \n",
        "                       samples, \n",
        "                       tokenizer, \n",
        "                       max_seq_length=0,\n",
        "                       data_type=None):\n",
        "        \"\"\"\n",
        "            Tokenizes an input sequence, adds padding and SOS+EOS \n",
        "        \"\"\"\n",
        "        #_toks, _tok_types = get_lang_specific_tokens(self.code_tokenizer, samples)\n",
        "        tok_ids = samples\n",
        "        #print(_tok_types)\n",
        "        # print(data_type)\n",
        "        # print(toks)\n",
        "        if max_seq_length > 2 and len(tok_ids) > max_seq_length - 2:\n",
        "            tok_ids = tok_ids[:max_seq_length -2]\n",
        "            #_tok_types = _tok_types[:max_seq_length -2]\n",
        "        #tok_ids =  tokenizer.convert_tokens_to_ids(toks)\n",
        "        #tok_ids = [ self.code_tok_vectorizer.convert_codepiece_to_id(code) for code in _toks]\n",
        "        #tok_types = [ self.code_tok_vectorizer.convert_toktype_to_id(toktype) for toktype in _tok_types]\n",
        "        \n",
        "        ### We use pseudo-BERT process so we will add both CLS and SEP tokens for\n",
        "        ### src and target inputs\n",
        "        tok_ids = [self.code_tok_vectorizer.bos_index] + tok_ids + [self.code_tok_vectorizer.eos_index]\n",
        "        #tok_types = [self.code_tok_vectorizer.ID_SOS_FOR_TOKEN_TYPE] + tok_types + [self.code_tok_vectorizer.ID_EOS_FOR_TOKEN_TYPE]\n",
        "        #print(len(tok_ids), len(tok_types))\n",
        "        #assert(len(tok_ids) == len(tok_types))\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [ 1 ] * len(tok_ids)\n",
        "\n",
        "        if len(tok_ids) < max_seq_length:\n",
        "            padding_length = max_seq_length - len(tok_ids)\n",
        "            tok_ids = tok_ids + ([self.code_tok_vectorizer.pad_index] * padding_length)\n",
        "            #tok_types = tok_types + ([self.code_tok_vectorizer.ID_PAD_FOR_TOKEN_TYPE] * padding_length)\n",
        "            input_mask = input_mask + ([ 0 ] * padding_length) ### Padded tokens are zero-masked\n",
        "        \n",
        "        # print(tok_ids)\n",
        "        #return tok_ids, tok_types, input_mask\n",
        "        return tok_ids, input_mask\n",
        "\n",
        "    def create_dataset(self,\n",
        "                    nl_to_pl_df,\n",
        "                    code_data_array,\n",
        "                    final_ds,\n",
        "                    sample_count=10000,\n",
        "                    max_doc_len=50,\n",
        "                    max_code_len=0):\n",
        "        \"\"\"\n",
        "            Reads from a dataframe, tokenizes and numericalizes both docstrings \n",
        "            and code. \n",
        "\n",
        "        \"\"\"\n",
        "        #final_ds = []\n",
        "        for idx in nl_to_pl_df.itertuples():\n",
        "            ## For SOS and EOS tokens 2 positions are left\n",
        "            if not idx.cleaned_code:\n",
        "                print(\"Invalid entry, No code found for:\", idx.docstring)\n",
        "            \n",
        "            doc_toks, doc_mask = self.prepare_tokens(idx.docstring,\n",
        "                                                      self.doc_tokenizer,\n",
        "                                                      max_doc_len,\n",
        "                                                      \"docs\")\n",
        "        for index in code_data_array['positions']:\n",
        "            start,end = index[0], index[1]\n",
        "            code_array = list(code_data_array['sentences'][start:end])\n",
        "            code_ids, code_mask = self.prepare_code_tokens(code_array,\n",
        "                                                            None,\n",
        "                                                            max_code_len,\n",
        "                                                            \"code\")\n",
        "            #code_toks = None\n",
        "            ### Skip over current iteration if no valid code found\n",
        "\n",
        "            # print(code_toks)\n",
        "            # print(code_mask)\n",
        "            # print(doc_toks)\n",
        "            # print(doc_mask)\n",
        "            final_entry = NLPLSingleEntry(code_ids,\n",
        "                                          code_mask, \n",
        "                                          doc_toks, \n",
        "                                          doc_mask)\n",
        "            # print(final_entry.code_ids)\n",
        "            # print(final_entry.code_mask)\n",
        "            # print(final_entry.doc_ids)\n",
        "            # print(final_entry.doc_mask)\n",
        "            final_ds.append(final_entry)\n",
        "        #print(len(final_ds))\n",
        "        return final_ds\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V71S9kbEyhv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06746d53-1552-4c85-9029-8df6dba3d485"
      },
      "source": [
        "### We will use the same tokenizer for both docstrings and code\n",
        "final_ds = []\n",
        "MAX_LENGTH=512\n",
        "#selected_elems = nl_to_pl_df[nl_to_pl_df['cleaned_code_len'] <= MAX_LENGTH]\n",
        "#selected_elems = my_df_copy[my_df_copy['code_len'] <= MAX_LENGTH]\n",
        "MAX_VOCAB_LENGTH=128\n",
        "assert(MAX_VOCAB_LENGTH <= MAX_LENGTH)\n",
        "#selected_elems = nl_to_pl_df[nl_to_pl_df['cleaned_code_len'] <= MAX_LENGTH]\n",
        "#selected_elems = my_df_copy[my_df_copy['code_len'] <= MAX_LENGTH]\n",
        "#init_tokenizer=PythonTokenizer2()\n",
        "selected_elems = nl_to_pl_df[nl_to_pl_df['cleaned_code_len'] <= MAX_VOCAB_LENGTH]\n",
        "selected_elems_file_path = '/content/enc_code_local.txt'\n",
        "selected_elems_saved_dict = selected_elems_file_path+'.pth'\n",
        "my_dict = Dictionary.read_vocab(\"/content/vocab_full.txt\")\n",
        "\n",
        "with open(selected_elems_file_path,'w') as f:\n",
        "    for code_piece in selected_elems.loc[:, 'code']:\n",
        "        toks = tokenize_python(code_piece, keep_comments=False)\n",
        "        f.write(' '.join(toks))\n",
        "        f.write('\\n')\n",
        "complete_data = Dictionary.index_data(selected_elems_file_path, \n",
        "                                      selected_elems_saved_dict, \n",
        "                                      my_dict)\n",
        "\n",
        "my_nlpl_ds = NLPLDataSet(auto_tokenizer, None, my_dict).create_dataset(selected_elems,\n",
        "                                                                        complete_data,\n",
        "                                                                        final_ds, \n",
        "                                                                        max_doc_len=MAX_VOCAB_LENGTH, \n",
        "                                                                        max_code_len=MAX_VOCAB_LENGTH)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving the data to /content/enc_code_local.txt.pth ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtGV7HK2OJZK"
      },
      "source": [
        "my_nlpl_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShA8ITXD9wfe"
      },
      "source": [
        "all_code_ids = torch.tensor([f.code_ids for f in my_nlpl_ds], dtype=torch.long)\n",
        "all_code_mask = torch.tensor([f.code_mask for f in my_nlpl_ds], dtype=torch.long)\n",
        "all_doc_ids = torch.tensor([f.doc_ids for f in my_nlpl_ds], dtype=torch.long)\n",
        "all_doc_mask = torch.tensor([f.doc_mask for f in my_nlpl_ds], dtype=torch.long)\n",
        "#all_tok_ids = torch.tensor([f.tok_ids for f in my_nlpl_ds], dtype=torch.long)\n",
        "# if output_mode == \"classification\":\n",
        "#     all_label_ids = torch.tensor([f.label_id for f in my_nlpl_ds], dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(all_code_ids,\n",
        "                              all_code_mask, \n",
        "                              all_doc_ids, \n",
        "                              all_doc_mask)\n",
        " #                             all_tok_ids)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyAPl3XWOR1f",
        "outputId": "e5037830-ac00-4083-8708-e03eb73d8f30"
      },
      "source": [
        "len(my_dict.id2word.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5700"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKJG-nAJl3i9"
      },
      "source": [
        "dataset_size = len(train_dataset)\n",
        "dataset_indices = list(range(dataset_size))\n",
        "np.random.shuffle(dataset_indices)\n",
        "val_split_index = int(np.floor(0.2 * dataset_size))\n",
        "\n",
        "train_idx, val_idx = dataset_indices[val_split_index:], dataset_indices[:val_split_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grRIIkyMl4fY"
      },
      "source": [
        "BATCH_SIZE=32\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "\n",
        "#train_sampler = RandomSampler(train_dataset,) #if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_dataloader = DataLoader(train_dataset, sampler=val_sampler, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "#train_sampler = RandomSampler(train_dataset) #if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSmbw44v__OU"
      },
      "source": [
        "iter_one = next(iter(train_dataloader))\n",
        "iter_one_val = next(iter(val_dataloader))\n",
        "\n",
        "#iter_one[0]=all_code_ids, \n",
        "#iter_one[1]=all_code_mask, \n",
        "#iter_one[2]=all_doc_ids, \n",
        "#iter_one[3]=all_doc_mask\n",
        "#mask_reshape.shape\n",
        "# trg = iter_one[0]\n",
        "# trg_len = trg.shape[1]\n",
        "# trg_pad_mask = iter_one[1].unsqueeze(1).unsqueeze(2) \n",
        "# trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len))).bool()\n",
        "\n",
        "# #trg_sub_mask = [trg len, trg len]\n",
        "    \n",
        "# trg_mask = trg_pad_mask & trg_sub_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbgAhdfWAIOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3da7f01-62d5-4c64-defd-a1b6d84711a6"
      },
      "source": [
        "iter_one[-1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scH0xbQHKvS3"
      },
      "source": [
        "### Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2wiHHFBuXxi"
      },
      "source": [
        "class TransEncoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = MAX_LENGTH):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([TransEncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_PH9wz_KzuM"
      },
      "source": [
        "class TransEncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OodBNJ46LCQo"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK2KNXZSLEAz"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH18KwEnLFhF"
      },
      "source": [
        "class TransDecoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 1000,\n",
        "                 tok_type_dim=62):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        #self.tok_type_embedding = nn.Embedding(tok_type_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([TransDecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        #self.fc_out_tok = nn.Linear(hid_dim, tok_type_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask, src_tok_types=None):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "            \n",
        "        # trg = self.dropout((self.tok_embedding(trg) * self.scale) + \n",
        "        #                    self.pos_embedding(pos) + \n",
        "        #                    self.tok_type_embedding(src_tok_types))\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + \n",
        "                           self.pos_embedding(pos) )\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        #output_tok = self.fc_out_tok(trg)\n",
        "        #output =F.softmax(output, dim=2)  \n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        #return output, output_tok, attention\n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IChtMK8QLHOP"
      },
      "source": [
        "class TransDecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        # query, key, value\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swxv3Y0SLJmE"
      },
      "source": [
        "class TransSeq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = src_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg, trg_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        trg_pad_mask = trg_mask.unsqueeze(1).unsqueeze(2) \n",
        "        \"\"\"\n",
        "            A boolean tensor of shape [batch size, 1, 1, trg len]\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, src_mask, trg, trg_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src_mask)\n",
        "        trg_mask = self.make_trg_mask(trg, trg_mask)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, \n",
        "                                         enc_src, \n",
        "                                         trg_mask, \n",
        "                                         src_mask, \n",
        "                                         None)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucVQ_58__yks",
        "outputId": "3be2232e-21e0-4609-ccc5-4a4b4f293967"
      },
      "source": [
        "len(my_dict.id2word.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5700"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cJUUj5QLvbw"
      },
      "source": [
        "INPUT_DIM = auto_tokenizer.vocab_size\n",
        "OUTPUT_DIM = len(my_dict.id2word.keys())\n",
        "TOK_TYPE_OUTPUT_DIM = 62#code_tok_vectorizer.max_tok_length\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 16\n",
        "DEC_HEADS = 16\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = TransEncoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device,\n",
        "              max_length=MAX_LENGTH)\n",
        "\n",
        "dec = TransDecoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device,\n",
        "              max_length=MAX_LENGTH,\n",
        "              tok_type_dim=TOK_TYPE_OUTPUT_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzSK7caBygSu",
        "outputId": "0ace597e-29dc-4f2b-ace8-bc2ec1ecce74"
      },
      "source": [
        "code_tok_vectorizer.max_tok_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mtBeOIl5uWg",
        "outputId": "892119f5-4dee-4026-91c7-5bb6ee7c9ac2"
      },
      "source": [
        "my_dict.pad_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7yKcgxLPKiv"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olwlAq_B3boJ"
      },
      "source": [
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_baseline_128.pt ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lexhxG9rL8pQ"
      },
      "source": [
        "SRC_PAD_IDX = auto_tokenizer.pad_token_id #SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = my_dict.pad_index#code_tok_vectorizer.ID_PAD_FOR_TOKEN_TYPE #TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "model = TransSeq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw9b7bQ-MIWM"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-yLC9A_Mmuo",
        "outputId": "5ef82a62-4333-4e98-8661-801c34dfcf46"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 20,007,748 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdFwqR18PwFg"
      },
      "source": [
        "#del model\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhrFXeouMQEb"
      },
      "source": [
        "model.apply(initialize_weights);\n",
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqHShEuoNQlw"
      },
      "source": [
        "iter_one = next(iter(train_dataloader))\n",
        "batch = tuple(t.to(device) for t in iter_one)\n",
        "# inputs = {'input_ids': batch[0],\n",
        "#             'attention_mask': batch[1],\n",
        "#             'token_type_ids': batch[2] ,\n",
        "#             # XLM don't use segment_ids\n",
        "#             'labels': batch[3]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cWIuwNUMQXs"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip, device,double_loss=False):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_crit_loss = 0\n",
        "    epoch_tok_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        trg = batch[0].to(device)\n",
        "        trg_mask = batch[1].to(device)\n",
        "        src = batch[2].to(device)\n",
        "        src_mask = batch[3].to(device)\n",
        "        #src_tok_type = batch[4].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, src_mask, trg[:,:-1], trg_mask[:,:-1])\n",
        "        #output, tok_op, _ = model(src, src_mask, trg[:,:-1], trg_mask[:,:-1], src_tok_type[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "        if(double_loss == True):            \n",
        "            tok_op_output_dim = tok_op.shape[-1]            \n",
        "            tok_op = tok_op.contiguous().view(-1, tok_op_output_dim)\n",
        "            src_tok_type = src_tok_type[:,1:].contiguous().view(-1)\n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]            \n",
        "            loss,crit_loss, tok_loss = criterion(output, \n",
        "                                                 trg,\n",
        "                                                 tok_op,\n",
        "                                                 src_tok_type)\n",
        "            epoch_crit_loss += crit_loss.item()\n",
        "            epoch_tok_loss += tok_loss.item()\n",
        "        else:\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "        loss.backward()        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()        \n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    if(double_loss == True):  \n",
        "        print(f'Train\\tCrit Loss: {epoch_crit_loss/(len(iterator)):.3f} | Token Loss: {epoch_tok_loss/(len(iterator)):.3f}')\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26yD4gOnMr9U"
      },
      "source": [
        "def evaluate(model, iterator, criterion, device,double_loss=False):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_crit_loss = 0\n",
        "    epoch_tok_loss = 0\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            trg = batch[0].to(device)\n",
        "            trg_mask = batch[1].to(device)\n",
        "            src = batch[2].to(device)\n",
        "            src_mask = batch[3].to(device)\n",
        "            #src_tok_type = batch[4].to(device)\n",
        "\n",
        "            output, _ = model(src, src_mask, trg[:,:-1], trg_mask[:,:-1])\n",
        "            #output, tok_op, _ = model(src, src_mask, trg[:,:-1], trg_mask[:,:-1], src_tok_type[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            if(double_loss == True):            \n",
        "                tok_op_output_dim = tok_op.shape[-1]            \n",
        "                tok_op = tok_op.contiguous().view(-1, tok_op_output_dim)\n",
        "                src_tok_type = src_tok_type[:,1:].contiguous().view(-1)\n",
        "                #output = [batch size * trg len - 1, output dim]\n",
        "                #trg = [batch size * trg len - 1]            \n",
        "                loss,crit_loss, tok_loss = criterion(output, \n",
        "                                                     trg,\n",
        "                                                     tok_op,\n",
        "                                                     src_tok_type)\n",
        "                epoch_crit_loss += crit_loss.item()\n",
        "                epoch_tok_loss += tok_loss.item()\n",
        "            else:\n",
        "                loss = criterion(output, trg)\n",
        "            \n",
        "\n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            # loss = criterion(output, \n",
        "            #                  trg,\n",
        "            #                  tok_op,\n",
        "            #                  src_tok_type)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    if(double_loss == True):  \n",
        "        print(f'Val\\tCrit Loss: {epoch_crit_loss/(len(iterator)):.3f} | Token Loss: {epoch_tok_loss/(len(iterator)):.3f}')  \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGz-LB82MvFT"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKiDCTjObPkX",
        "outputId": "47012f4b-223c-4719-9c0c-1976ee383d84"
      },
      "source": [
        "TRG_PAD_IDX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHcNK9jmPC7U"
      },
      "source": [
        "# my_torch_weights = torch.ones(auto_tokenizer.vocab_size)\n",
        "# my_torch_weights[1437] = 2\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX, weight=my_torch_weights.to(device) )\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "\n",
        "class WeightedCrossEntropy(nn.Module):\n",
        "    def __init__(self,\n",
        "                 code_weights=None,\n",
        "                 code_ignore_idx=None,\n",
        "                 tok_type_weights=None,\n",
        "                 tok_type_ignore_idx=None,\n",
        "                 mix_ratio=0.5):\n",
        "        \n",
        "        super(WeightedCrossEntropy, self).__init__()\n",
        "        self.code_weights=code_weights\n",
        "        self.code_ignore_idx=code_ignore_idx\n",
        "        self.tok_type_weights=tok_type_weights\n",
        "        self.tok_type_ignore_idx=tok_type_ignore_idx\n",
        "        self.mix_ratio = mix_ratio\n",
        "    \n",
        "    def forward(self, \n",
        "                code_output, \n",
        "                code_trg,\n",
        "                tok_type_output,\n",
        "                tok_type_trg):\n",
        "        \n",
        "        # code_criterion = nn.CrossEntropyLoss(ignore_index = self.code_ignore_idx,\n",
        "        #                                      weight=self.code_weights)\n",
        "        # toktype_criterion = nn.CrossEntropyLoss(ignore_index = self.tok_type_ignore_idx,\n",
        "        #                                 weight=self.tok_type_weights)\n",
        "\n",
        "        code_criterion = F.cross_entropy(code_output, \n",
        "                                         code_trg, \n",
        "                                         weight=self.code_weights,\n",
        "                                         ignore_index = self.code_ignore_idx)\n",
        "        toktype_criterion = F.cross_entropy(tok_type_output, \n",
        "                                         tok_type_trg, \n",
        "                                         weight=self.tok_type_weights,\n",
        "                                         ignore_index = self.tok_type_ignore_idx)\n",
        "        \n",
        "        total_loss = self.mix_ratio * code_criterion + (1-self.mix_ratio )*toktype_criterion\n",
        "        return total_loss, code_criterion, toktype_criterion\n",
        "\n",
        "# criterion = WeightedCrossEntropy(code_ignore_idx=TRG_PAD_IDX, \n",
        "#                                  tok_type_ignore_idx=TRG_PAD_IDX,\n",
        "#                                  mix_ratio=0.9999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HwsjJd3PiGf",
        "outputId": "fc029374-ad05-44e6-c07d-610f255adb4a"
      },
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "trg = batch[0].to(device)\n",
        "trg_mask = batch[1].to(device)\n",
        "src = batch[2].to(device)\n",
        "src_mask = batch[3].to(device)\n",
        "#src_tok_types = batch[4].to(device)\n",
        "print(trg.shape, trg_mask.shape, src.shape, src_mask.shape)#, src_tok_types.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 128]) torch.Size([16, 128]) torch.Size([16, 128]) torch.Size([16, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaK3e2iMP66R"
      },
      "source": [
        "model.train()\n",
        "#with torch.no_grad():\n",
        "output,attend_val = model(src, src_mask, trg[:,:-1], trg_mask[:,:-1])#, src_tok_types[:,:-1])\n",
        "    # output_dim = output.shape[-1]\n",
        "    \n",
        "    # output = output.contiguous().view(-1, output_dim)\n",
        "    # trg = trg[:,1:].contiguous().view(-1)\n",
        "    \n",
        "    #output = [batch size * trg len - 1, output dim]\n",
        "    #trg = [batch size * trg len - 1]\n",
        "    \n",
        "    #loss = criterion(output, trg)\n",
        "    #print(loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNgUyTyZ_k0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde684d5-c985-4538-d29d-a2817a554bbc"
      },
      "source": [
        "trg.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2032])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxDsVPtGYvcd"
      },
      "source": [
        "output_dim = output.shape[-1]\n",
        "\n",
        "output = output.contiguous().view(-1, output_dim)\n",
        "trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "#tok_output_dim = tok_op.shape[-1]\n",
        "#tok_op = tok_op.contiguous().view(-1, tok_output_dim)\n",
        "#src_tok_types = src_tok_types[:,1:].contiguous().view(-1)\n",
        "    \n",
        "#output = [batch size * trg len - 1, output dim]\n",
        "#trg = [batch size * trg len - 1]\n",
        "\n",
        "#loss = criterion(output, trg)\n",
        "#print(loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKqoz_GQBkWE"
      },
      "source": [
        "output.shape, trg.shape, tok_op.shape, src_tok_types.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBG82-UVaCab"
      },
      "source": [
        "loss = criterion(output, trg)#, tok_op, src_tok_types)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBiqn6yYvgs7",
        "outputId": "b0c6ccb7-e782-4cee-a3c1-0824e6d6a451"
      },
      "source": [
        "loss.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.71438217163086"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hrlBV-kAM4h"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def run_train_eval_loop(model, \n",
        "                        train_dataloader,\n",
        "                        val_dataloader,\n",
        "                        optimizer,\n",
        "                        criterion,\n",
        "                        device,\n",
        "                        epochs=20,\n",
        "                        clip=1,\n",
        "                        best_valid_loss=float('inf'),\n",
        "                        file_path='end_capstone_baseline_128.pt',\n",
        "                        double_loss=False,\n",
        "                        scheduler=None,\n",
        "                        mix_ratio=0.5):\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "\n",
        "    if double_loss == True:    \n",
        "        criterion = WeightedCrossEntropy(code_ignore_idx=TRG_PAD_IDX, \n",
        "                                        tok_type_ignore_idx=TRG_PAD_IDX,\n",
        "                                        mix_ratio=mix_ratio)            \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "    \n",
        "        start_time = time.time()\n",
        "        \n",
        "        train_loss = train(model, train_dataloader, optimizer, criterion, clip, device,double_loss=double_loss)\n",
        "        valid_loss = evaluate(model, val_dataloader, criterion, device,double_loss=double_loss)\n",
        "        \n",
        "        if(scheduler is not None):\n",
        "            scheduler.step(valid_loss)\n",
        "        end_time = time.time()\n",
        "        \n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save({\"model\":model.state_dict(),\n",
        "                \"optimizer\":optimizer.state_dict(),\n",
        "                \"loss\":valid_loss,\n",
        "                },file_path)\n",
        "        \n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE1Xa6EJwk2F"
      },
      "source": [
        "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
        "    \n",
        "    assert n_rows * n_cols == n_heads\n",
        "    \n",
        "    fig = plt.figure(figsize=(15,25))\n",
        "    \n",
        "    for i in range(n_heads):\n",
        "        \n",
        "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
        "        \n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                           rotation=45)\n",
        "        ax.set_yticklabels(['']+translation)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjSwMRObaXsl",
        "outputId": "d5562bda-a179-42fb-f13c-f671cf2ba4d8"
      },
      "source": [
        "len(train_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2191"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiwj5jwOA_Lf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70c8e822-54ad-4b78-8f69-e237ef2e35c5"
      },
      "source": [
        "file_path='end_capstone_bpe3.pt'\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "run_train_eval_loop(model,\n",
        "                    train_dataloader,\n",
        "                    val_dataloader,\n",
        "                    optimizer,\n",
        "                    criterion,\n",
        "                    device,\n",
        "                    epochs=30,\n",
        "                    clip=1.4,\n",
        "                    best_valid_loss=float('inf'),\n",
        "                    file_path=file_path,\n",
        "                    double_loss=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 6s\n",
            "\tTrain Loss: 7.737 | Train PPL: 2292.302\n",
            "\t Val. Loss: 6.329 |  Val. PPL: 560.559\n",
            "Epoch: 02 | Time: 0m 6s\n",
            "\tTrain Loss: 6.001 | Train PPL: 403.914\n",
            "\t Val. Loss: 4.994 |  Val. PPL: 147.546\n",
            "Epoch: 03 | Time: 0m 6s\n",
            "\tTrain Loss: 4.928 | Train PPL: 138.056\n",
            "\t Val. Loss: 4.721 |  Val. PPL: 112.245\n",
            "Epoch: 04 | Time: 0m 6s\n",
            "\tTrain Loss: 4.690 | Train PPL: 108.806\n",
            "\t Val. Loss: 4.715 |  Val. PPL: 111.562\n",
            "Epoch: 05 | Time: 0m 6s\n",
            "\tTrain Loss: 4.645 | Train PPL: 104.108\n",
            "\t Val. Loss: 4.628 |  Val. PPL: 102.346\n",
            "Epoch: 06 | Time: 0m 6s\n",
            "\tTrain Loss: 4.550 | Train PPL:  94.600\n",
            "\t Val. Loss: 4.346 |  Val. PPL:  77.192\n",
            "Epoch: 07 | Time: 0m 6s\n",
            "\tTrain Loss: 4.376 | Train PPL:  79.515\n",
            "\t Val. Loss: 4.081 |  Val. PPL:  59.206\n",
            "Epoch: 08 | Time: 0m 6s\n",
            "\tTrain Loss: 4.140 | Train PPL:  62.790\n",
            "\t Val. Loss: 3.838 |  Val. PPL:  46.433\n",
            "Epoch: 09 | Time: 0m 7s\n",
            "\tTrain Loss: 3.928 | Train PPL:  50.801\n",
            "\t Val. Loss: 3.682 |  Val. PPL:  39.717\n",
            "Epoch: 10 | Time: 0m 7s\n",
            "\tTrain Loss: 3.760 | Train PPL:  42.941\n",
            "\t Val. Loss: 3.560 |  Val. PPL:  35.148\n",
            "Epoch: 11 | Time: 0m 7s\n",
            "\tTrain Loss: 3.626 | Train PPL:  37.546\n",
            "\t Val. Loss: 3.457 |  Val. PPL:  31.734\n",
            "Epoch: 12 | Time: 0m 7s\n",
            "\tTrain Loss: 3.513 | Train PPL:  33.541\n",
            "\t Val. Loss: 3.354 |  Val. PPL:  28.612\n",
            "Epoch: 13 | Time: 0m 7s\n",
            "\tTrain Loss: 3.412 | Train PPL:  30.333\n",
            "\t Val. Loss: 3.270 |  Val. PPL:  26.319\n",
            "Epoch: 14 | Time: 0m 7s\n",
            "\tTrain Loss: 3.330 | Train PPL:  27.950\n",
            "\t Val. Loss: 3.191 |  Val. PPL:  24.306\n",
            "Epoch: 15 | Time: 0m 7s\n",
            "\tTrain Loss: 3.256 | Train PPL:  25.944\n",
            "\t Val. Loss: 3.134 |  Val. PPL:  22.964\n",
            "Epoch: 16 | Time: 0m 7s\n",
            "\tTrain Loss: 3.184 | Train PPL:  24.146\n",
            "\t Val. Loss: 3.042 |  Val. PPL:  20.939\n",
            "Epoch: 17 | Time: 0m 7s\n",
            "\tTrain Loss: 3.121 | Train PPL:  22.678\n",
            "\t Val. Loss: 2.992 |  Val. PPL:  19.934\n",
            "Epoch: 18 | Time: 0m 7s\n",
            "\tTrain Loss: 3.065 | Train PPL:  21.431\n",
            "\t Val. Loss: 2.955 |  Val. PPL:  19.201\n",
            "Epoch: 19 | Time: 0m 7s\n",
            "\tTrain Loss: 3.008 | Train PPL:  20.238\n",
            "\t Val. Loss: 2.901 |  Val. PPL:  18.200\n",
            "Epoch: 20 | Time: 0m 7s\n",
            "\tTrain Loss: 2.958 | Train PPL:  19.251\n",
            "\t Val. Loss: 2.865 |  Val. PPL:  17.554\n",
            "Epoch: 21 | Time: 0m 7s\n",
            "\tTrain Loss: 2.915 | Train PPL:  18.453\n",
            "\t Val. Loss: 2.830 |  Val. PPL:  16.937\n",
            "Epoch: 22 | Time: 0m 7s\n",
            "\tTrain Loss: 2.871 | Train PPL:  17.648\n",
            "\t Val. Loss: 2.798 |  Val. PPL:  16.415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-081dafba75c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mbest_valid_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     double_loss=False)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-e4ce168cb8c0>\u001b[0m in \u001b[0;36mrun_train_eval_loop\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, criterion, device, epochs, clip, best_valid_loss, file_path, double_loss, scheduler, mix_ratio)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdouble_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdouble_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdouble_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdouble_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-259c75273c4a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip, device, double_loss)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t6vRJqGvOe4"
      },
      "source": [
        "torch.save({\"model\":model.state_dict(),\n",
        "    \"optimizer\":optimizer.state_dict(),\n",
        "    \"loss\":1.360,\n",
        "    },\"/content/end_capstone_self_encode_sizeCor_lowerCase_128_2.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP8TCASrKRB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a902ea42-1b7b-469d-e32d-9140f7e35214"
      },
      "source": [
        "file_path='/content/end_capstone_bpe2.pt'\n",
        "\n",
        "chkpt = torch.load(file_path)\n",
        "print(chkpt['loss'])\n",
        "model.load_state_dict(chkpt['model'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4893418295042857\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H35YKslnmSWT",
        "outputId": "233d9acf-313e-447b-9d99-0ecf7271fe05"
      },
      "source": [
        "model.load_state_dict(chkpt['model'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xbt0QojKy6M"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "optimizer.load_state_dict(chkpt['optimizer'])\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-8,verbose=True)\n",
        "run_train_eval_loop(model,\n",
        "                    train_dataloader,\n",
        "                    val_dataloader,\n",
        "                    optimizer,\n",
        "                    criterion,\n",
        "                    device,\n",
        "                    epochs=120,\n",
        "                    clip=1.4,\n",
        "                    best_valid_loss=chkpt['loss'],\n",
        "                    file_path=file_path,\n",
        "                    double_loss=False,\n",
        "                    scheduler=scheduler,\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KrWgIHbvMVM"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "optimizer.load_state_dict(chkpt['optimizer'])\n",
        "scheduler = ReduceLROnPlateau(optimizer, patience=5, min_lr=1e-9,verbose=True)\n",
        "run_train_eval_loop(model,\n",
        "                    train_dataloader,\n",
        "                    val_dataloader,\n",
        "                    optimizer,\n",
        "                    criterion,\n",
        "                    device,\n",
        "                    epochs=50,\n",
        "                    clip=1,\n",
        "                    best_valid_loss=chkpt['loss'],\n",
        "                    file_path=file_path,\n",
        "                    double_loss=True,\n",
        "                    scheduler=scheduler,\n",
        "                    mix_ratio=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI7LRWH_rKu0"
      },
      "source": [
        "!cp /content/end_capstone_bpe2.pt /content/drive/MyDrive/EVA4/END_Capstone/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRBviYJnKOUT"
      },
      "source": [
        "file_path='/content/drive/MyDrive/EVA4/END_Capstone/end_capstone_self_encode_sizeCor_stage2_128wrn_2.pt'\n",
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "run_train_eval_loop(model,\n",
        "                    train_dataloader,\n",
        "                    val_dataloader,\n",
        "                    optimizer,\n",
        "                    criterion,\n",
        "                    device,\n",
        "                    epochs=20,\n",
        "                    clip=1.4,\n",
        "                    best_valid_loss=5.050,\n",
        "                    file_path=file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2husK3UdJv5"
      },
      "source": [
        "run_train_eval_loop(model,\n",
        "                    train_dataloader,\n",
        "                    val_dataloader,\n",
        "                    optimizer,\n",
        "                    criterion,\n",
        "                    device,\n",
        "                    epochs=20,\n",
        "                    clip=1,\n",
        "                    best_valid_loss=chkpt['loss'],\n",
        "                    file_path=file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvyYB8qwfCN1"
      },
      "source": [
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_256_mod2.pt /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZU107Z7y4IF"
      },
      "source": [
        "!cp tut6-model.pt /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_256_mod2.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFGCa94292bR"
      },
      "source": [
        "src_mask = torch.LongTensor(src_mask).unsqueeze(0)#.to(device)\n",
        "    \n",
        "src_mask = model.make_src_mask(src_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghm2RaIG-CHn"
      },
      "source": [
        "!cp /content/end_capstone_codesearch_256_seqLen512.pt /content/drive/MyDrive/EVA4/END_Capstone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kvI-A00R3Bp"
      },
      "source": [
        "!cp /content/drive/MyDrive/EVA4/END_Capstone/end_capstone_codesearch_256_seqLen512.pt ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imWxpZVvhqAD"
      },
      "source": [
        "def get_code(sentence,\n",
        "             tokenizer,\n",
        "             model, \n",
        "             device, \n",
        "             max_len = 100):\n",
        "    \n",
        "    model.eval()\n",
        "    dataset_handler = NLPLDataSet(tokenizer, tokenizer)\n",
        "    src_indexes, src_mask =  dataset_handler.prepare_tokens(sentence, dataset_handler.doc_tokenizer)\n",
        "    # if isinstance(sentence, str):\n",
        "    #     nlp = spacy.load('de')\n",
        "    #     tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    # else:\n",
        "    #     tokens = [token.lower() for token in sentence]\n",
        "    #dataset_handler.prepare_tokens()\n",
        "\n",
        "    #tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
        "        \n",
        "    #src_indexes = [tokenizer.convert_tokens_to_ids[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    src_mask = torch.LongTensor(src_mask).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_mask)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [tokenizer.cls_token_id]\n",
        "    #trg_mask = [1]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        base_mask = torch.LongTensor([1]*(i+1)).unsqueeze(0).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor, base_mask)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == tokenizer.sep_token_id:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [tokenizer.convert_ids_to_tokens(i) for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfXb0fs3TF5I",
        "outputId": "3b22ffe1-7f80-4162-f428-7917f0bb0a67"
      },
      "source": [
        "my_dict.bos_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDo3e4HVKMxI"
      },
      "source": [
        "def get_code(sentence,\n",
        "             doc_tokenizer,\n",
        "             code_tokenizer,\n",
        "             code_tok_vectorizer,\n",
        "             model, \n",
        "             device, \n",
        "             max_len = 100):\n",
        "    \n",
        "    model.eval()\n",
        "    print(sentence)\n",
        "    dataset_handler = NLPLDataSet(doc_tokenizer, code_tokenizer, code_tok_vectorizer)\n",
        "    src_indexes, src_mask =  dataset_handler.prepare_tokens(sentence, dataset_handler.doc_tokenizer)\n",
        "    # if isinstance(sentence, str):\n",
        "    #     nlp = spacy.load('de')\n",
        "    #     tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    # else:\n",
        "    #     tokens = [token.lower() for token in sentence]\n",
        "    #dataset_handler.prepare_tokens()\n",
        "\n",
        "    #tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
        "        \n",
        "    #src_indexes = [tokenizer.convert_tokens_to_ids[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    src_mask = torch.LongTensor(src_mask).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_mask)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [code_tok_vectorizer.bos_index]\n",
        "    #trg_tok_indexes = [code_tok_vectorizer.ID_SOS_FOR_TOKEN_TYPE]\n",
        "    #trg_mask = [1]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "        #trg_tok_tensor = torch.LongTensor(trg_tok_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        base_mask = torch.LongTensor([1]*(i+1)).unsqueeze(0).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor, base_mask)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            #output, tok_output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask, trg_tok_tensor)\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        #pred_tok_type = tok_output.argmax(2)[:,-1].item()\n",
        "        #trg_tok_indexes.append(pred_tok_type)\n",
        "\n",
        "        if (pred_token == code_tok_vectorizer.eos_index):\n",
        "         #or pred_token == code_tok_vectorizer.code_word2idx['___EOS___']):\n",
        "            break\n",
        "\n",
        "    trg_tokens = [code_tok_vectorizer.id2word[i] for i in trg_indexes]\n",
        "    #trg_token_types = [code_tok_vectorizer.convert_id_to_toktype(i) for i in trg_tok_indexes]\n",
        "    \n",
        "    #return trg_tokens[1:], trg_token_types[1:], attention\n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAyHpFJRTm07"
      },
      "source": [
        "my_dict.id2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynb0w-4Yuu0c"
      },
      "source": [
        "!cp /content/end_capstone_self_encode_corr1024.pt /content/drive/MyDrive/EVA4/END_Capstone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ammem8M7qAL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab0d346-ef92-48d9-b181-4581bbeb8b4f"
      },
      "source": [
        "input_text = \"program to print upper case of a string\"\n",
        "cleaned_string = input_text#.lower().rstrip('\\n').lstrip('#')\n",
        "splitted_text = auto_tokenizer.tokenize(input_text)\n",
        "mycode, attention_val = get_code(cleaned_string,\n",
        "                                 auto_tokenizer, \n",
        "                                 None,\n",
        "                                 my_dict,\n",
        "                                 model, \n",
        "                                 device,\n",
        "                                 max_len=MAX_LENGTH)\n",
        "\n",
        "print(detokenize_python(mycode))\n",
        "#output = auto_tokenizer.convert_tokens_to_string(mycode[:-1])\n",
        "#print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "program to print upper case of a string\n",
            "def def ( n ) :\n",
            "    return ( n )\n",
            "</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h9ZJCi3ZlAY"
      },
      "source": [
        "mycode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnR2E5nHUCeX"
      },
      "source": [
        "my_dict.e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayl995l96d6L"
      },
      "source": [
        "import matplotlib.ticker as ticker\n",
        "\n",
        "#display_attention(splitted_text, mycode, attention_val[:,2,:,:].unsqueeze(1), n_heads=1, n_rows=1, n_cols=1)\n",
        "n_heads=4\n",
        "n_rows=2\n",
        "n_cols=n_heads/n_rows\n",
        "display_attention(splitted_text, \n",
        "                  mycode, \n",
        "                  attention_val[:,:8,:,:], \n",
        "                  n_heads=n_heads, \n",
        "                  n_rows=n_rows, \n",
        "                  n_cols=n_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMF9Ypkxhz4e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_Arp5Z30Hds"
      },
      "source": [
        "all_strings = nl_to_pl_df['docstring'].values\n",
        "for one_string in all_strings[2000:2049]:\n",
        "    #splitted_text = auto_tokenizer.tokenize(input_text)\n",
        "    #input_text = \"Write a Python function to add two numbers\"\n",
        "    cleaned_string = one_string#.lower().rstrip('\\n').lstrip('#')\n",
        "    #splitted_text = auto_tokenizer.tokenize(one_string)\n",
        "    mycode, attention_val = get_code(cleaned_string,\n",
        "                                    auto_tokenizer, \n",
        "                                    None,\n",
        "                                    my_dict,\n",
        "                                    model, \n",
        "                                    device,\n",
        "                                    max_len=200)\n",
        "    print(cleaned_string)\n",
        "    print(detokenize_python(mycode))\n",
        "    \n",
        "    #print(init_tokenizer.untokenize(mycode))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEeqtSb8OGIr"
      },
      "source": [
        "nl_to_pl_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAVE6pwqdNxD"
      },
      "source": [
        "all_strings = nl_to_pl_df['docstring'].values\n",
        "for one_string in all_strings[100:149]:\n",
        "    cleaned_string = one_string.rstrip('\\n').lstrip('#')\n",
        "    mycode, attention_val = get_code(cleaned_string,auto_tokenizer, model, device, max_len=512)\n",
        "    print(cleaned_string)\n",
        "    print(auto_tokenizer.convert_tokens_to_string(mycode[:-1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVWgi6c5dyPC"
      },
      "source": [
        "def even_odd_num(num):\n",
        "    max = 0\n",
        "    for num in num:\n",
        "         if num % 10 == 0:\n",
        "                 maxnum = num\n",
        "         return maxnum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G7TU847WEVV"
      },
      "source": [
        "even_odd_num(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzNkBY3S7owj"
      },
      "source": [
        "torch.save({\"model\":model.state_dict(),\n",
        "            \"optimizer\":optimizer.state_dict(),\n",
        "            \"loss\":1.373\n",
        "            },'end_capstone_baseline_128.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLGgHyMv8CTy"
      },
      "source": [
        "n = int(input(\"How many terms? \"))\n",
        "\n",
        "n1 = 0\n",
        "\n",
        "for i in range(n+1):\n",
        "    result = 0\n",
        "    for i in range(n2, n+1):\n",
        "        result = result + result*n2\n",
        "    print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT1a37CF7tD4"
      },
      "source": [
        "!cp /content/end_capstone_baseline_128.pt /content/drive/MyDrive/EVA4/END_Capstone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fGMACkcqByC"
      },
      "source": [
        "a = [1, 2, 3, 4, 5]\n",
        "b = [5, 6, 7, 8]\n",
        "a.update(b)\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTPUPBz4qJki"
      },
      "source": [
        "a = {1, 2, 3}\n",
        "b = {3, 4, 5, 6}\n",
        "a = {1, 2, 2, 3, 4}\n",
        "x = a[i]*b for (a, b) in zip(a, b) )\n",
        "print(f\"{a}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUiEo5VBx1K6"
      },
      "source": [
        "def printSubArrays(arr, start, end):\n",
        "    if end == len(arr):\n",
        "        return\n",
        "    elif start > end:\n",
        "        return printSubArrays(arr, 0, end + 1)\n",
        "    else:\n",
        "            print(arr[start:end + 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6swZArv2CvL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ENDS7_4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tp5IzBGsPGHs",
        "XJ6o_79ISSVb",
        "AKdllP3FST4N",
        "1AbsQwqkVyAy",
        "eXajorf5Xz7t",
        "LZgzB0ZkHVTI",
        "a5aeKuNCRGip",
        "pgSSIb9UCfE6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EVAP2/blob/master/ENDS7_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYiRsFGD6iUC"
      },
      "source": [
        "# 0 TorchText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4Gh1NBBPgms",
        "outputId": "5c9458e2-c050-4364-8c2d-7f51809e9700"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Dec 10 15:22:35 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOIbi5WzO5OU"
      },
      "source": [
        "### Random Swap\n",
        "The random swap augmentation takes a sentence and then swaps words within it n times, with each iteration working on the previously swapped sentence. Here we sample two random numbers based on the length of the sentence, and then just keep swapping until we hit n."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnkbG15HO3Yj"
      },
      "source": [
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "599NpwfMR5Vm"
      },
      "source": [
        "For more on this please go through this [paper](https://arxiv.org/pdf/1901.11196.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5aeKuNCRGip"
      },
      "source": [
        "### Back Translation\n",
        "\n",
        "Another popular approach for augmenting text datasets is back translation. This involves translating a sentence from our target language into one or more other languages and then translating all of them back to the original language. We can use the Python library googletrans for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exHthzSl7Toc",
        "outputId": "f688ce36-59ca-4abb-b75f-aae6ca632bd0"
      },
      "source": [
        "#!pip install googletrans==4.0.0-rc1\r\n",
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.11.8)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hCollecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/3c/cdeaf9ab0404853e77c45d9e8021d0d2c01f70a1bb26e460090926fe2a5e/hstspreload-2020.11.21-py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 7.3MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Collecting contextvars>=2.1; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Collecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.8MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
            "\u001b[?25hCollecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 6.1MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans, contextvars\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp36-none-any.whl size=16369 sha256=9a5ffd9aeadf84a170a944267fd129c62f46c220d765cf6234a8b0ebf441e734\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=085cc3c54fe7336d20c47ea29745d3e64aa38fd9ff46c6c36e60479fd1bf3f65\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built googletrans contextvars\n",
            "Installing collected packages: immutables, contextvars, sniffio, hpack, hyperframe, h2, h11, httpcore, hstspreload, rfc3986, httpx, googletrans\n",
            "Successfully installed contextvars-2.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.11.21 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 immutables-0.14 rfc3986-1.4.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHhNBbYrRXNy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7f690616-3161-43f4-b657-eb4e6b16b1d9"
      },
      "source": [
        "import random\n",
        "import googletrans\n",
        "#import googletrans.Translator as Translator\n",
        "\n",
        "translator = googletrans.Translator()\n",
        "sentence = ['The dog slept on the rug', 'ran lazily']\n",
        "\n",
        "available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "trans_lang = random.choice(available_langs) \n",
        "print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n",
        "\n",
        "trans_lang"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translating to sundanese\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'su'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSWT3GfLXQFP",
        "outputId": "9d090e85-97ff-4ce0-9565-16774e17ba14"
      },
      "source": [
        "sentence = ['The dog slept on the rug']\r\n",
        "available_langs = list(googletrans.LANGUAGES.keys()) \r\n",
        "trans_lang = random.choice(available_langs) \r\n",
        "print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translating to sundanese\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ-w4wrZXbkv",
        "outputId": "a6c91a82-3fed-4d5a-d3a3-6ca0525bec7b"
      },
      "source": [
        "translations = translator.translate(sentence, dest=trans_lang) \r\n",
        "t_text = [t.text for t in translations]\r\n",
        "print(t_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Anjing bobo dina karpét']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Olh1nRRFaXAN",
        "outputId": "9f4f854b-ec43-460a-bfa9-7287c425c782"
      },
      "source": [
        "translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \r\n",
        "en_text = [t.text for t in translations_en_random]\r\n",
        "print(en_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The dog is lying on the carpet']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjo1GRTJvxrb"
      },
      "source": [
        "translator = googletrans.Translator()\r\n",
        "\r\n",
        "def back_translate(sentence):\r\n",
        "    available_langs = list(googletrans.LANGUAGES.keys()) \r\n",
        "    trans_lang = random.choice(available_langs) \r\n",
        "    translations = translator.translate(sentence, dest=trans_lang) \r\n",
        "    t_text = [t.text for t in translations]\r\n",
        "    translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \r\n",
        "    en_text = [t.text for t in translations_en_random]\r\n",
        "    return en_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TGV-LnOwSwY",
        "outputId": "4f5fc350-2569-4939-da0b-77bc2a3f7a98"
      },
      "source": [
        "back_translate([my_new_df.iloc[0].sentence])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['s performance reaffirms her']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMnhfzXLxCOK"
      },
      "source": [
        "empty_series = { col:[] for col in my_new_df.columns.values}\r\n",
        "simpler_series_df = pd.DataFrame(data=empty_series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je4VZYcpyASC"
      },
      "source": [
        "my_new_df[\"sentence_index\"] = my_new_df[\"sentence_index\"].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54tCxi31yQ8Y"
      },
      "source": [
        "my_new_df[\"Phrase_Id\"] = my_new_df[\"Phrase_Id\"].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WObhkay6DVNw"
      },
      "source": [
        "my_new_df =pd.read_csv(\"/content/StanJoinedRetrans.csv\", sep=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKjJZZBVymDu",
        "outputId": "f2152bf1-5f00-4f65-bb8a-65f41e1c15d6"
      },
      "source": [
        "simpler_series_df,my_new_df[\"sentence_index\"].max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Empty DataFrame\n",
              " Columns: [sentence_index, sentence, Phrase_Id, Sentiment_Score, label]\n",
              " Index: [], 56998)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_-KfgtROJKh",
        "outputId": "ee1dcb9a-f6fc-4159-9a4a-1a1fabbc1feb"
      },
      "source": [
        "simpler_series_df[\"sentence_index\"].max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58998.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IturNNnAvo2t"
      },
      "source": [
        "#simpler_series_df = my_new_df[:100]\r\n",
        "max_sentence_ids=simpler_series_df[\"sentence_index\"].max()\r\n",
        "for idx in my_new_df[3000:5000].itertuples():\r\n",
        "    #parts = line[:-1].split('\\t')\r\n",
        "    #label = parts[0]\r\n",
        "    sentence = idx.sentence\r\n",
        "    aug_sentence = back_translate([sentence])[0]\r\n",
        "    #aug_sentences = eda(sentence, alpha_sr=alpha_sr, alpha_ri=alpha_ri, alpha_rs=alpha_rs, p_rd=alpha_rd, num_aug=num_aug)\r\n",
        "    #print(back_translate(sentence))\r\n",
        "    # for aug_sentence in aug_sentences:\r\n",
        "    max_sentence_ids += 1\r\n",
        "    simpler_series_df.loc[len(simpler_series_df)] = [max_sentence_ids,aug_sentence, np.int(idx.Phrase_Id), idx.Sentiment_Score, idx.label ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnZBNc4sJf8a"
      },
      "source": [
        "simpler_series_df.to_csv(\"StanGoogTranslate.csv\", index=False, sep=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dtTSj-_WdpV",
        "outputId": "a1a1b5ca-a481-49a5-e6f6-d47b4afaba4b"
      },
      "source": [
        "#!pip install google_trans_new\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google_trans_new\n",
            "  Downloading https://files.pythonhosted.org/packages/f9/7b/9f136106dc5824dc98185c97991d3cd9b53e70a197154dd49f7b899128f6/google_trans_new-1.1.9-py3-none-any.whl\n",
            "Installing collected packages: google-trans-new\n",
            "Successfully installed google-trans-new-1.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hySEELDSWz-E",
        "outputId": "fb2b9097-02f2-4fd0-85b9-0d244602e690"
      },
      "source": [
        "len(available_langs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgSSIb9UCfE6"
      },
      "source": [
        "### Stanford Sentiment Analysis TreeBank\r\n",
        "\r\n",
        "\r\n",
        "The original dataset includes:\r\n",
        "1. original_rt_snippets.txt contains 10,605 processed snippets from the original pool of Rotten Tomatoes HTML files. Please note that some snippet may contain multiple sentences.\r\n",
        "\r\n",
        "2. dictionary.txt contains all phrases and their IDs, separated by a vertical line |\r\n",
        "\r\n",
        "3. sentiment_labels.txt contains all phrase ids and the corresponding sentiment labels, separated by a vertical line.\r\n",
        "Note that you can recover the 5 classes by mapping the positivity probability using the following cut-offs:\r\n",
        "[0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], (0.8, 1.0]\r\n",
        "for very negative, negative, neutral, positive, very positive, respectively.\r\n",
        "Please note that phrase ids and sentence ids are not the same.\r\n",
        "\r\n",
        "4. SOStr.txt and STree.txt encode the structure of the parse trees. \r\n",
        "STree encodes the trees in a parent pointer format. Each line corresponds to each sentence in the datasetSentences.txt file. The Matlab code of this paper will show you how to read this format if you are not familiar with it.\r\n",
        "\r\n",
        "5. datasetSentences.txt contains the sentence index, followed by the sentence string separated by a tab. These are the sentences of the train/dev/test sets.\r\n",
        "\r\n",
        "6. datasetSplit.txt contains the sentence index (corresponding to the index in datasetSentences.txt file) followed by the set label separated by a comma:\r\n",
        "\t1 = train\r\n",
        "\t2 = test\r\n",
        "\t3 = dev\r\n",
        "\r\n",
        "Please note that the datasetSentences.txt file has more sentences/lines than the original_rt_snippet.txt. \r\n",
        "Each row in the latter represents a snippet as shown on RT, whereas the former is each sub sentence as determined by the Stanford parser.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU05pf0cX46N"
      },
      "source": [
        "## Data Handling\r\n",
        "\r\n",
        "For the purpose of this notebook, we do the following:\r\n",
        "1. From datasentences.txt, we take each sentence and find the corresponding \"Phrase_Id\" in dictionary.txt\r\n",
        "2. Using the \"Phrase_Id\" we retrieve the \"sentiment values\" from sentiment_labels.txt\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbAG463onuka"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F9f_foJVovP"
      },
      "source": [
        "datasetSentences_df = pd.read_csv(\"/content/datasetSentences.txt\", sep=\"\\t\")\r\n",
        "dictionary_df = pd.read_csv(\"/content/dictionary.txt\", sep=\"|\", names=[\"Phrase\", \"Phrase_Id\"])\r\n",
        "sentiment_labels_df =  pd.read_csv(\"/content/sentiment_labels.txt\", sep=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KooNHbPA8Shk"
      },
      "source": [
        "datasetSentences_df[\"Phrase_Id\"] = 0\r\n",
        "datasetSentences_df[\"Sentiment_Score\"] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "YJUA_5D05mQu",
        "outputId": "4ac63975-b926-4d37-c77a-48d35fd31632"
      },
      "source": [
        "common_sentences = datasetSentences_df[datasetSentences_df[\"sentence\"].isin(dictionary_df[\"Phrase\"])]\r\n",
        "common_sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>Phrase_Id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>224044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>224044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>224044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>224044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>224044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11850</th>\n",
              "      <td>11851</td>\n",
              "      <td>A real snooze .</td>\n",
              "      <td>224044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11851</th>\n",
              "      <td>11852</td>\n",
              "      <td>No surprises .</td>\n",
              "      <td>224044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11852</th>\n",
              "      <td>11853</td>\n",
              "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
              "      <td>224044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11853</th>\n",
              "      <td>11854</td>\n",
              "      <td>Her fans walked out muttering words like `` ho...</td>\n",
              "      <td>224044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11854</th>\n",
              "      <td>11855</td>\n",
              "      <td>In this case zero .</td>\n",
              "      <td>224044</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11286 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_index  ... Phrase_Id\n",
              "0                   1  ...    224044\n",
              "1                   2  ...    224044\n",
              "2                   3  ...    224044\n",
              "3                   4  ...    224044\n",
              "4                   5  ...    224044\n",
              "...               ...  ...       ...\n",
              "11850           11851  ...    224044\n",
              "11851           11852  ...    224044\n",
              "11852           11853  ...    224044\n",
              "11853           11854  ...    224044\n",
              "11854           11855  ...    224044\n",
              "\n",
              "[11286 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa_kvxGJCsfC"
      },
      "source": [
        "### Create Dataframe to have sentiment scores and Full sentences together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2Ce5A511in-"
      },
      "source": [
        "def sentence_exists(input_sentence):\r\n",
        "    #print(input_sentence)\r\n",
        "    matching_phrase = dictionary_df[dictionary_df[\"Phrase\"] == input_sentence][\"Phrase_Id\"].values\r\n",
        "    default_series = pd.Series({\"Phrase_Id\":-1000, \"Sentiment_Score\": -1000}) ## For cases where we dont find a full sentence match\r\n",
        "    if(len(matching_phrase)) > 0:\r\n",
        "        phrase_id = np.int(matching_phrase[0])\r\n",
        "        sentiment_value = sentiment_labels_df[sentiment_labels_df[\"phrase ids\"] == matching_phrase[0]][\"sentiment values\"].values[0]\r\n",
        "        default_series = pd.Series({\"Phrase_Id\":phrase_id, \"Sentiment_Score\": sentiment_value})\r\n",
        "\r\n",
        "    return default_series\r\n",
        "\r\n",
        "\r\n",
        "datasetSentences_df.loc[:, [\"Phrase_Id\", \"Sentiment_Score\"]] =  datasetSentences_df.loc[:,\"sentence\"].apply(sentence_exists)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW-XYN6z_JoR"
      },
      "source": [
        "datasetSentences_df[\"Phrase_Id\"] = datasetSentences_df[\"Phrase_Id\"].astype('int')\r\n",
        "datasetSentences_df = datasetSentences_df[datasetSentences_df[\"Phrase_Id\"] > 0].iloc[:,:-1]\r\n",
        "datasetSentences_df.to_csv(\"StanfordNLP.csv\",index=False, sep=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oiowfq8qEzut"
      },
      "source": [
        "new_df = pd.read_csv(\"/content/StanfordNLP.csv\",sep=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "gGJm_8A1E7M5",
        "outputId": "8da00d10-65ff-4b59-8ff2-e2ef8ea01e67"
      },
      "source": [
        "new_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>Phrase_Id</th>\n",
              "      <th>Sentiment_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>226166</td>\n",
              "      <td>0.69444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>226300</td>\n",
              "      <td>0.83333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>13995</td>\n",
              "      <td>0.51389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>14123</td>\n",
              "      <td>0.73611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>13999</td>\n",
              "      <td>0.86111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11281</th>\n",
              "      <td>11851</td>\n",
              "      <td>A real snooze .</td>\n",
              "      <td>222071</td>\n",
              "      <td>0.11111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11282</th>\n",
              "      <td>11852</td>\n",
              "      <td>No surprises .</td>\n",
              "      <td>225165</td>\n",
              "      <td>0.22222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11283</th>\n",
              "      <td>11853</td>\n",
              "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
              "      <td>226985</td>\n",
              "      <td>0.75000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11284</th>\n",
              "      <td>11854</td>\n",
              "      <td>Her fans walked out muttering words like `` ho...</td>\n",
              "      <td>223632</td>\n",
              "      <td>0.13889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11285</th>\n",
              "      <td>11855</td>\n",
              "      <td>In this case zero .</td>\n",
              "      <td>224044</td>\n",
              "      <td>0.34722</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11286 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_index  ... Sentiment_Score\n",
              "0                   1  ...         0.69444\n",
              "1                   2  ...         0.83333\n",
              "2                   3  ...         0.51389\n",
              "3                   4  ...         0.73611\n",
              "4                   5  ...         0.86111\n",
              "...               ...  ...             ...\n",
              "11281           11851  ...         0.11111\n",
              "11282           11852  ...         0.22222\n",
              "11283           11853  ...         0.75000\n",
              "11284           11854  ...         0.13889\n",
              "11285           11855  ...         0.34722\n",
              "\n",
              "[11286 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRftRimeYdY2"
      },
      "source": [
        "## Training and test dataset\r\n",
        "After applying above techiniques we get a combined DataFrame that has the following fields:\r\n",
        "\r\n",
        "1. **sentence_index** : Original sentence_index in the datasetSentences.txt\r\n",
        "2. **sentence**: The actual sentence\r\n",
        "3. **Phrase_Id**: The Phrase_Id used for mapping/retrieving sentiment score\r\n",
        "4. **Sentiment_Score**: Actual Sentiment score for the sentence\r\n",
        "\r\n",
        "Now we split the DataFrame into Training and Test Dataframes. Note that we are not using the recommended train/ dev /test splits from the dataset. \r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezrr6qPzw_zf"
      },
      "source": [
        "train_df = new_df.sample(frac = 0.8) \r\n",
        "test_df = new_df[~new_df.sentence_index.isin(train_df.sentence_index)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_k53AizE9ve"
      },
      "source": [
        "train_df.to_csv(\"StanTrain.csv\",index=False, sep=\"|\")\r\n",
        "test_df.to_csv(\"StanTest.csv\", index=False, sep=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwyrjV-LZoDJ"
      },
      "source": [
        "## Data Augementation\r\n",
        "We use the EDA techniques referred in this [code](https://github.com/jasonwei20/eda_nlp)\r\n",
        "Our parameters for Augmentation are as follows:\r\n",
        "1. Num_aug = 5 i.e 5 augmented sentences\r\n",
        "2. alpha_rs=0.2 i.e 20% percent of words in each sentence to be swapped\r\n",
        "3. alpha_rd=0.2 i.e 20% percent of words in each sentence to be dropped"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BxXNtloJ3sX"
      },
      "source": [
        "from eda import *\r\n",
        "\r\n",
        "simpler_series_df = train_df.copy()\r\n",
        "empty_series = { col:[] for col in train_df.columns.values}\r\n",
        "my_new_df = pd.DataFrame(data=empty_series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qq5m2PuJqSC"
      },
      "source": [
        "simpler_series_df = train_df#simpler_series_df[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egdMkVnrGYXc"
      },
      "source": [
        "max_sentence_ids = simpler_series_df.sentence_index.max()\r\n",
        "alpha_sr = 0.0\r\n",
        "alpha_ri=0.0\r\n",
        "alpha_rs=0.2\r\n",
        "alpha_rd=0.2\r\n",
        "num_aug=5\r\n",
        "\r\n",
        "for idx in simpler_series_df.itertuples():\r\n",
        "    #parts = line[:-1].split('\\t')\r\n",
        "    #label = parts[0]\r\n",
        "    sentence = idx.sentence\r\n",
        "    aug_sentences = eda(sentence, alpha_sr=alpha_sr, alpha_ri=alpha_ri, alpha_rs=alpha_rs, p_rd=alpha_rd, num_aug=num_aug)\r\n",
        "    for aug_sentence in aug_sentences:\r\n",
        "        max_sentence_ids += 1\r\n",
        "        my_new_df.loc[len(my_new_df)] = [max_sentence_ids,aug_sentence, np.int(idx.Phrase_Id), idx.Sentiment_Score ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "X4O4P3uMLLxu",
        "outputId": "7edd4865-816c-4282-8754-1bfc2dac11f6"
      },
      "source": [
        "my_new_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>Phrase_Id</th>\n",
              "      <th>Sentiment_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11855.0</td>\n",
              "      <td>and amazing is excessively strained contrived</td>\n",
              "      <td>151215.0</td>\n",
              "      <td>0.44444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11856.0</td>\n",
              "      <td>amazing and neither unhappily is lovely excess...</td>\n",
              "      <td>151215.0</td>\n",
              "      <td>0.44444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11857.0</td>\n",
              "      <td>neither and amazing contrived is lovely excess...</td>\n",
              "      <td>151215.0</td>\n",
              "      <td>0.44444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11858.0</td>\n",
              "      <td>lovely and unhappily is neither strained and c...</td>\n",
              "      <td>151215.0</td>\n",
              "      <td>0.44444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11859.0</td>\n",
              "      <td>lovely and amazing unhappily is neither excess...</td>\n",
              "      <td>151215.0</td>\n",
              "      <td>0.44444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45140</th>\n",
              "      <td>56995.0</td>\n",
              "      <td>not absolutely mention to refreshed</td>\n",
              "      <td>225210.0</td>\n",
              "      <td>0.80556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45141</th>\n",
              "      <td>56996.0</td>\n",
              "      <td>absolutely to mention not refreshed</td>\n",
              "      <td>225210.0</td>\n",
              "      <td>0.80556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45142</th>\n",
              "      <td>56997.0</td>\n",
              "      <td>not mention absolutely refreshed</td>\n",
              "      <td>225210.0</td>\n",
              "      <td>0.80556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45143</th>\n",
              "      <td>56998.0</td>\n",
              "      <td>not mention absolutely refreshed</td>\n",
              "      <td>225210.0</td>\n",
              "      <td>0.80556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45144</th>\n",
              "      <td>56999.0</td>\n",
              "      <td>not to mention absolutely refreshed</td>\n",
              "      <td>225210.0</td>\n",
              "      <td>0.80556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45145 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_index  ... Sentiment_Score\n",
              "0             11855.0  ...         0.44444\n",
              "1             11856.0  ...         0.44444\n",
              "2             11857.0  ...         0.44444\n",
              "3             11858.0  ...         0.44444\n",
              "4             11859.0  ...         0.44444\n",
              "...               ...  ...             ...\n",
              "45140         56995.0  ...         0.80556\n",
              "45141         56996.0  ...         0.80556\n",
              "45142         56997.0  ...         0.80556\n",
              "45143         56998.0  ...         0.80556\n",
              "45144         56999.0  ...         0.80556\n",
              "\n",
              "[45145 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axiiXzNcLXLc"
      },
      "source": [
        "my_new_df = my_new_df.sample(frac = 1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiTF2d7lND6X"
      },
      "source": [
        "my_new_df.to_csv(\"StanAugmented.csv\", index=False, sep=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "B_-jxoBBNfdo",
        "outputId": "bfa84d85-7f02-493a-b821-81f1ff81738c"
      },
      "source": [
        "my_new_df = my_new_df.append(train_df,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>Phrase_Id</th>\n",
              "      <th>Sentiment_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34766.0</td>\n",
              "      <td>s performance confirms her once again</td>\n",
              "      <td>44733.0</td>\n",
              "      <td>0.88889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18024.0</td>\n",
              "      <td>what s the russian word for wow</td>\n",
              "      <td>227385.0</td>\n",
              "      <td>0.77778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>23140.0</td>\n",
              "      <td>verve s stylishly directed with it</td>\n",
              "      <td>66669.0</td>\n",
              "      <td>0.75000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50084.0</td>\n",
              "      <td>without shakespeare s eloquent language the up...</td>\n",
              "      <td>111221.0</td>\n",
              "      <td>0.15278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28678.0</td>\n",
              "      <td>a de of cinema</td>\n",
              "      <td>103960.0</td>\n",
              "      <td>0.63889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54169</th>\n",
              "      <td>3075.0</td>\n",
              "      <td>This fascinating experiment plays as more of a...</td>\n",
              "      <td>70193.0</td>\n",
              "      <td>0.66667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54170</th>\n",
              "      <td>2645.0</td>\n",
              "      <td>Remove Spider-Man the movie from its red herri...</td>\n",
              "      <td>68393.0</td>\n",
              "      <td>0.66667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54171</th>\n",
              "      <td>1940.0</td>\n",
              "      <td>High Crimes knows the mistakes that bad movies...</td>\n",
              "      <td>45489.0</td>\n",
              "      <td>0.58333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54172</th>\n",
              "      <td>2642.0</td>\n",
              "      <td>The Bourne Identity should n't be half as ente...</td>\n",
              "      <td>69263.0</td>\n",
              "      <td>0.77778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54173</th>\n",
              "      <td>4930.0</td>\n",
              "      <td>Not to mention absolutely refreshed .</td>\n",
              "      <td>225210.0</td>\n",
              "      <td>0.80556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>54174 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_index  ... Sentiment_Score\n",
              "0             34766.0  ...         0.88889\n",
              "1             18024.0  ...         0.77778\n",
              "2             23140.0  ...         0.75000\n",
              "3             50084.0  ...         0.15278\n",
              "4             28678.0  ...         0.63889\n",
              "...               ...  ...             ...\n",
              "54169          3075.0  ...         0.66667\n",
              "54170          2645.0  ...         0.66667\n",
              "54171          1940.0  ...         0.58333\n",
              "54172          2642.0  ...         0.77778\n",
              "54173          4930.0  ...         0.80556\n",
              "\n",
              "[54174 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vc_BJZwekpE"
      },
      "source": [
        "## Augmenting with Back_Translate\r\n",
        "\r\n",
        "In this method we make a translate from source English sentence to a sentence in random language and then back into English. We create a dataframe of around 4000 samples and append it back to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQKAxlBNfLdU"
      },
      "source": [
        "#!pip install googletrans==4.0.0-rc1\r\n",
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52ghG_DxejRB"
      },
      "source": [
        "translator = googletrans.Translator()\r\n",
        "\r\n",
        "def back_translate(sentence):\r\n",
        "    available_langs = list(googletrans.LANGUAGES.keys()) \r\n",
        "    trans_lang = random.choice(available_langs) \r\n",
        "    translations = translator.translate(sentence, dest=trans_lang) \r\n",
        "    t_text = [t.text for t in translations]\r\n",
        "    translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \r\n",
        "    en_text = [t.text for t in translations_en_random]\r\n",
        "    return en_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2MIs1_9ejRC"
      },
      "source": [
        "#simpler_series_df = my_new_df[:100]\r\n",
        "max_sentence_ids=simpler_series_df[\"sentence_index\"].max()\r\n",
        "for idx in my_new_df[3000:5000].itertuples():\r\n",
        "    #parts = line[:-1].split('\\t')\r\n",
        "    #label = parts[0]\r\n",
        "    sentence = idx.sentence\r\n",
        "    aug_sentence = back_translate([sentence])[0]\r\n",
        "    #aug_sentences = eda(sentence, alpha_sr=alpha_sr, alpha_ri=alpha_ri, alpha_rs=alpha_rs, p_rd=alpha_rd, num_aug=num_aug)\r\n",
        "    #print(back_translate(sentence))\r\n",
        "    # for aug_sentence in aug_sentences:\r\n",
        "    max_sentence_ids += 1\r\n",
        "    simpler_series_df.loc[len(simpler_series_df)] = [max_sentence_ids,aug_sentence, np.int(idx.Phrase_Id), idx.Sentiment_Score, idx.label ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNHa9Ha9fXp3"
      },
      "source": [
        "simpler_series_df.to_csv(\"StanGoogTranslate.csv\", index=False, sep=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-22-FX3Nq8X"
      },
      "source": [
        "my_new_df = my_new_df.append(train_df,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V32aKSY3gDzG"
      },
      "source": [
        "### Range expansion\r\n",
        "The sentiment scores are floats between [0,1] so we convert this to equivalent integers between 0 and 24(i.e 25 different classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ2v-NhkN5Un"
      },
      "source": [
        "my_new_df[\"label\"] = 0\r\n",
        "# def quantize_predictions(score_val):\r\n",
        "#     if(score_val >=0 and score_val <=0.2):\r\n",
        "#         return 0\r\n",
        "#     if(score_val > 0.2 and score_val <=0.4):\r\n",
        "#         return 1\r\n",
        "#     if(score_val > 0.4 and score_val <=0.6):\r\n",
        "#         return 2\r\n",
        "#     if(score_val > 0.6 and score_val <=0.8):\r\n",
        "#         return 3\r\n",
        "#     if(score_val > 0.8 and score_val <=1):\r\n",
        "#         return 4\r\n",
        "def quantize_predictions(score_val):\r\n",
        "    return np.floor(score_val * 24)\r\n",
        "\r\n",
        "\r\n",
        "my_new_df.loc[:,\"label\"] = my_new_df.loc[:,\"Sentiment_Score\"].apply(quantize_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOHPi0fgv74m"
      },
      "source": [
        "my_new_df[\"sentence_index\"] = my_new_df[\"sentence_index\"].astype('int')\r\n",
        "my_new_df[\"Phrase_Id\"] = my_new_df[\"Phrase_Id\"].astype('int')\r\n",
        "my_new_df[\"label\"] = my_new_df[\"label\"].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "2YKQoWrUfg-o",
        "outputId": "817c9704-4e44-43b2-ddbe-9c48b0d01341"
      },
      "source": [
        "my_new_df#.Sentiment_Score.sort_values(ascending=True).values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>Phrase_Id</th>\n",
              "      <th>Sentiment_Score</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34766</td>\n",
              "      <td>s performance confirms her once again</td>\n",
              "      <td>44733</td>\n",
              "      <td>0.88889</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18024</td>\n",
              "      <td>what s the russian word for wow</td>\n",
              "      <td>227385</td>\n",
              "      <td>0.77778</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>23140</td>\n",
              "      <td>verve s stylishly directed with it</td>\n",
              "      <td>66669</td>\n",
              "      <td>0.75000</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50084</td>\n",
              "      <td>without shakespeare s eloquent language the up...</td>\n",
              "      <td>111221</td>\n",
              "      <td>0.15278</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28678</td>\n",
              "      <td>a de of cinema</td>\n",
              "      <td>103960</td>\n",
              "      <td>0.63889</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58169</th>\n",
              "      <td>3075</td>\n",
              "      <td>This fascinating experiment plays as more of a...</td>\n",
              "      <td>70193</td>\n",
              "      <td>0.66667</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58170</th>\n",
              "      <td>2645</td>\n",
              "      <td>Remove Spider-Man the movie from its red herri...</td>\n",
              "      <td>68393</td>\n",
              "      <td>0.66667</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58171</th>\n",
              "      <td>1940</td>\n",
              "      <td>High Crimes knows the mistakes that bad movies...</td>\n",
              "      <td>45489</td>\n",
              "      <td>0.58333</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58172</th>\n",
              "      <td>2642</td>\n",
              "      <td>The Bourne Identity should n't be half as ente...</td>\n",
              "      <td>69263</td>\n",
              "      <td>0.77778</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58173</th>\n",
              "      <td>4930</td>\n",
              "      <td>Not to mention absolutely refreshed .</td>\n",
              "      <td>225210</td>\n",
              "      <td>0.80556</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>58174 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_index  ... label\n",
              "0               34766  ...    21\n",
              "1               18024  ...    18\n",
              "2               23140  ...    18\n",
              "3               50084  ...     3\n",
              "4               28678  ...    15\n",
              "...               ...  ...   ...\n",
              "58169            3075  ...    16\n",
              "58170            2645  ...    16\n",
              "58171            1940  ...    13\n",
              "58172            2642  ...    18\n",
              "58173            4930  ...    19\n",
              "\n",
              "[58174 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cisw5RndXCX"
      },
      "source": [
        "## Training on Sentiment Tree Bank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOFwnI6UOp_H",
        "outputId": "590a23e6-8613-40b9-9c5d-03789c78191b"
      },
      "source": [
        "# Import Library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext import data \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3b1b615b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksdskX85vzOw"
      },
      "source": [
        "my_new_df = pd.read_csv(\"/content/StanFullTrain.csv\", sep=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "ag-b59PziJRz",
        "outputId": "4ece0004-d815-41f6-8160-bc4bdbeb863d"
      },
      "source": [
        "my_new_df.Sentiment_Score\r\n",
        "num_bins = 25\r\n",
        "   \r\n",
        "# n, bins, patches = plt.hist( my_new_df.Sentiment_Score.sort_values(ascending=True).values,num_bins,\r\n",
        "#                             density = 1,  \r\n",
        "#                             color ='green', \r\n",
        "#                             alpha = 0.7) \r\n",
        "colors = ['green']#, 'blue', 'lime'] \r\n",
        "plt.hist(my_new_df.label, density=False, bins=25,histtype ='bar',color = colors, \r\n",
        "         label = colors)\r\n",
        "#plt.hist(my_new_df.Sentiment_Score, density=False, bins=25,histtype ='bar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 349., 1241., 1119., 1704., 3842., 2341., 3840., 4484., 1782.,\n",
              "        2714., 3121., 1126., 2571., 2685., 1617., 2337., 4409., 2528.,\n",
              "        4015., 4882., 1575., 2322., 1190.,  209.,  171.]),\n",
              " array([ 0.  ,  0.96,  1.92,  2.88,  3.84,  4.8 ,  5.76,  6.72,  7.68,\n",
              "         8.64,  9.6 , 10.56, 11.52, 12.48, 13.44, 14.4 , 15.36, 16.32,\n",
              "        17.28, 18.24, 19.2 , 20.16, 21.12, 22.08, 23.04, 24.  ]),\n",
              " <a list of 25 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1AYCNg5oJ5z",
        "outputId": "b5211f20-268a-4c35-e867-c1b6de7a582a"
      },
      "source": [
        "my_new_df.label.min(),my_new_df.label.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 24)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKOVv6XSm-tY",
        "outputId": "66e7e7f6-e14d-442b-c219-5720f7a2e3c9"
      },
      "source": [
        "len(my_new_df.label.unique())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNMObsMAOp_H"
      },
      "source": [
        "Sentence = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkx_qhtSOp_I"
      },
      "source": [
        "fields = [('sentence', Sentence),('label',Label)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYOesmXLOp_I"
      },
      "source": [
        "#example = [data.Example.fromlist([df.tweets[i],df.labels[i]], fields) for i in range(df.shape[0])] \r\n",
        "\r\n",
        "\r\n",
        "example = [data.Example.fromlist([my_new_df.sentence[i],my_new_df.label[i]], fields) for i in range(my_new_df.shape[0])] \r\n",
        "# Creating dataset\r\n",
        "#twitterDataset = data.TabularDataset(path=\"tweets.csv\", format=\"CSV\", fields=fields, skip_header=True)\r\n",
        "\r\n",
        "stanTreeDataset = data.Dataset(example, fields)\r\n",
        "(train, valid) = stanTreeDataset.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))\r\n",
        "Sentence.build_vocab(train)\r\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62b7mQk8nGEl",
        "outputId": "37a3723e-4b44-46f4-92f4-c52bf6dddbfe"
      },
      "source": [
        "my_new_df.label.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZc56d-UiSjm",
        "outputId": "77558e64-5329-4703-ea2d-a1e27f8fcfbf"
      },
      "source": [
        "(len(train), len(valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49448, 8726)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nql-x9R8iSjn",
        "outputId": "aa7f5c69-5ee3-4d58-b9bc-f7efdfdcf097"
      },
      "source": [
        "print('Size of input vocab : ', len(Sentence.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  19829\n",
            "Size of label vocab :  25\n",
            "Top 10 words appreared repeatedly : [('the', 38365), ('a', 27657), ('and', 23762), ('of', 23635), ('to', 16449), ('is', 13853), ('it', 12608), ('s', 11477), ('in', 10427), ('that', 10222)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7fa8567d41e0>, {19: 0, 7: 1, 16: 2, 18: 3, 6: 4, 4: 5, 10: 6, 9: 7, 13: 8, 12: 9, 17: 10, 15: 11, 5: 12, 21: 13, 8: 14, 3: 15, 14: 16, 20: 17, 1: 18, 22: 19, 11: 20, 2: 21, 0: 22, 23: 23, 24: 24})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90pG8UVeOp_K"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0jhdBaPOp_K"
      },
      "source": [
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid), batch_size = 32, \n",
        "                                                            sort_key = lambda x: len(x.sentence),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhFphxXlVpOA",
        "outputId": "f7665803-0ddd-4285-9355-b04f38fc9228"
      },
      "source": [
        "next(iter(train_iterator))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.data.batch.Batch of size 32]\n",
              "\t[.sentence]:('[torch.cuda.LongTensor of size 32x16 (GPU 0)]', '[torch.cuda.LongTensor of size 32 (GPU 0)]')\n",
              "\t[.label]:[torch.cuda.LongTensor of size 32 (GPU 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c329Bb1Op_K"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Sentence.vocab.stoi, tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AbsQwqkVyAy"
      },
      "source": [
        "## Defining Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PED4HJWH4t"
      },
      "source": [
        "We use the Embedding and LSTM modules in PyTorch to build a simple model for classifying tweets.\n",
        "\n",
        "In this model we create three layers. \n",
        "1. First, the words in our tweets are pushed into an Embedding layer, which we have established as a 300-dimensional vector embedding. \n",
        "2. That’s then fed into a 2 stacked-LSTMs with 100 hidden features (again, we’re compressing down from the 300-dimensional input like we did with images). We are using 2 LSTMs for using the dropout.\n",
        "3. Finally, the output of the LSTM (the final hidden state after processing the incoming tweet) is pushed through a standard fully connected layer with three outputs to correspond to our three possible classes (negative, positive, or neutral)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUuCywRNUSKv"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhvHOeOcUSK3"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Sentence.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 100\n",
        "num_output_nodes = len(my_new_df.label.unique())\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXLJjEfyUSK3",
        "outputId": "fd140556-e101-4560-b509-207dd3d683c6"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(15552, 300)\n",
            "  (encoder): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (fc): Linear(in_features=100, out_features=25, bias=True)\n",
            ")\n",
            "The model has 4,909,725 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwdsJdHVUSK4"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXajorf5Xz7t"
      },
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrE9RpMtZ1Vs"
      },
      "source": [
        "First define the optimizer and loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzcEdCXdUSK4"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        sentence, sentence_lengths = batch.sentence\n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(sentence, sentence_lengths).squeeze()  \n",
        "        #print(predictions)\n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.label)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.label)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL-t0228USK5"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            sentence, sentence_lengths = batch.sentence\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(sentence, sentence_lengths).squeeze()\n",
        "            #print(predictions)\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWMDOAfslFK9",
        "outputId": "1fba0097-087d-4c84-9aed-64b36c34a4aa"
      },
      "source": [
        "N_EPOCHS = 100\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    #train_loss = train(model, train_iterator, optimizer, criterion)\n",
        "\n",
        "\n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    #valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tEpoch: {epoch} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tEpoch: 0 | Train Loss: 3.179 | Train Acc: 10.40%\n",
            "\t Val. Loss: 3.151 |  Val. Acc: 13.91% \n",
            "\n",
            "\tEpoch: 1 | Train Loss: 3.107 | Train Acc: 18.68%\n",
            "\t Val. Loss: 3.091 |  Val. Acc: 20.56% \n",
            "\n",
            "\tEpoch: 2 | Train Loss: 3.038 | Train Acc: 26.47%\n",
            "\t Val. Loss: 3.027 |  Val. Acc: 27.81% \n",
            "\n",
            "\tEpoch: 3 | Train Loss: 2.972 | Train Acc: 33.28%\n",
            "\t Val. Loss: 2.983 |  Val. Acc: 32.03% \n",
            "\n",
            "\tEpoch: 4 | Train Loss: 2.926 | Train Acc: 37.35%\n",
            "\t Val. Loss: 2.955 |  Val. Acc: 34.29% \n",
            "\n",
            "\tEpoch: 5 | Train Loss: 2.897 | Train Acc: 39.86%\n",
            "\t Val. Loss: 2.929 |  Val. Acc: 36.66% \n",
            "\n",
            "\tEpoch: 6 | Train Loss: 2.876 | Train Acc: 41.70%\n",
            "\t Val. Loss: 2.909 |  Val. Acc: 38.50% \n",
            "\n",
            "\tEpoch: 7 | Train Loss: 2.855 | Train Acc: 44.05%\n",
            "\t Val. Loss: 2.891 |  Val. Acc: 40.78% \n",
            "\n",
            "\tEpoch: 8 | Train Loss: 2.828 | Train Acc: 46.98%\n",
            "\t Val. Loss: 2.867 |  Val. Acc: 43.41% \n",
            "\n",
            "\tEpoch: 9 | Train Loss: 2.804 | Train Acc: 49.28%\n",
            "\t Val. Loss: 2.854 |  Val. Acc: 44.51% \n",
            "\n",
            "\tEpoch: 10 | Train Loss: 2.785 | Train Acc: 50.96%\n",
            "\t Val. Loss: 2.847 |  Val. Acc: 45.12% \n",
            "\n",
            "\tEpoch: 11 | Train Loss: 2.768 | Train Acc: 52.47%\n",
            "\t Val. Loss: 2.822 |  Val. Acc: 47.59% \n",
            "\n",
            "\tEpoch: 12 | Train Loss: 2.754 | Train Acc: 53.63%\n",
            "\t Val. Loss: 2.809 |  Val. Acc: 48.69% \n",
            "\n",
            "\tEpoch: 13 | Train Loss: 2.742 | Train Acc: 54.79%\n",
            "\t Val. Loss: 2.799 |  Val. Acc: 49.69% \n",
            "\n",
            "\tEpoch: 14 | Train Loss: 2.727 | Train Acc: 56.37%\n",
            "\t Val. Loss: 2.785 |  Val. Acc: 51.17% \n",
            "\n",
            "\tEpoch: 15 | Train Loss: 2.703 | Train Acc: 59.01%\n",
            "\t Val. Loss: 2.769 |  Val. Acc: 53.31% \n",
            "\n",
            "\tEpoch: 16 | Train Loss: 2.678 | Train Acc: 61.73%\n",
            "\t Val. Loss: 2.746 |  Val. Acc: 55.70% \n",
            "\n",
            "\tEpoch: 17 | Train Loss: 2.653 | Train Acc: 64.24%\n",
            "\t Val. Loss: 2.731 |  Val. Acc: 57.10% \n",
            "\n",
            "\tEpoch: 18 | Train Loss: 2.629 | Train Acc: 66.68%\n",
            "\t Val. Loss: 2.718 |  Val. Acc: 58.68% \n",
            "\n",
            "\tEpoch: 19 | Train Loss: 2.607 | Train Acc: 68.80%\n",
            "\t Val. Loss: 2.700 |  Val. Acc: 60.32% \n",
            "\n",
            "\tEpoch: 20 | Train Loss: 2.586 | Train Acc: 71.06%\n",
            "\t Val. Loss: 2.686 |  Val. Acc: 62.08% \n",
            "\n",
            "\tEpoch: 21 | Train Loss: 2.567 | Train Acc: 72.73%\n",
            "\t Val. Loss: 2.667 |  Val. Acc: 63.47% \n",
            "\n",
            "\tEpoch: 22 | Train Loss: 2.553 | Train Acc: 74.00%\n",
            "\t Val. Loss: 2.654 |  Val. Acc: 64.68% \n",
            "\n",
            "\tEpoch: 23 | Train Loss: 2.540 | Train Acc: 75.21%\n",
            "\t Val. Loss: 2.643 |  Val. Acc: 65.96% \n",
            "\n",
            "\tEpoch: 24 | Train Loss: 2.529 | Train Acc: 76.12%\n",
            "\t Val. Loss: 2.636 |  Val. Acc: 66.35% \n",
            "\n",
            "\tEpoch: 25 | Train Loss: 2.520 | Train Acc: 76.95%\n",
            "\t Val. Loss: 2.629 |  Val. Acc: 66.96% \n",
            "\n",
            "\tEpoch: 26 | Train Loss: 2.511 | Train Acc: 77.74%\n",
            "\t Val. Loss: 2.616 |  Val. Acc: 68.21% \n",
            "\n",
            "\tEpoch: 27 | Train Loss: 2.502 | Train Acc: 78.59%\n",
            "\t Val. Loss: 2.610 |  Val. Acc: 68.92% \n",
            "\n",
            "\tEpoch: 28 | Train Loss: 2.493 | Train Acc: 79.51%\n",
            "\t Val. Loss: 2.598 |  Val. Acc: 70.04% \n",
            "\n",
            "\tEpoch: 29 | Train Loss: 2.486 | Train Acc: 80.20%\n",
            "\t Val. Loss: 2.591 |  Val. Acc: 70.55% \n",
            "\n",
            "\tEpoch: 30 | Train Loss: 2.479 | Train Acc: 80.85%\n",
            "\t Val. Loss: 2.587 |  Val. Acc: 71.05% \n",
            "\n",
            "\tEpoch: 31 | Train Loss: 2.473 | Train Acc: 81.41%\n",
            "\t Val. Loss: 2.580 |  Val. Acc: 71.76% \n",
            "\n",
            "\tEpoch: 32 | Train Loss: 2.467 | Train Acc: 82.05%\n",
            "\t Val. Loss: 2.575 |  Val. Acc: 72.09% \n",
            "\n",
            "\tEpoch: 33 | Train Loss: 2.460 | Train Acc: 82.68%\n",
            "\t Val. Loss: 2.571 |  Val. Acc: 72.57% \n",
            "\n",
            "\tEpoch: 34 | Train Loss: 2.454 | Train Acc: 83.29%\n",
            "\t Val. Loss: 2.565 |  Val. Acc: 73.24% \n",
            "\n",
            "\tEpoch: 35 | Train Loss: 2.448 | Train Acc: 83.79%\n",
            "\t Val. Loss: 2.557 |  Val. Acc: 74.08% \n",
            "\n",
            "\tEpoch: 36 | Train Loss: 2.441 | Train Acc: 84.56%\n",
            "\t Val. Loss: 2.550 |  Val. Acc: 74.78% \n",
            "\n",
            "\tEpoch: 37 | Train Loss: 2.433 | Train Acc: 85.53%\n",
            "\t Val. Loss: 2.545 |  Val. Acc: 75.25% \n",
            "\n",
            "\tEpoch: 38 | Train Loss: 2.424 | Train Acc: 86.40%\n",
            "\t Val. Loss: 2.537 |  Val. Acc: 76.01% \n",
            "\n",
            "\tEpoch: 39 | Train Loss: 2.415 | Train Acc: 87.26%\n",
            "\t Val. Loss: 2.526 |  Val. Acc: 77.11% \n",
            "\n",
            "\tEpoch: 40 | Train Loss: 2.408 | Train Acc: 87.99%\n",
            "\t Val. Loss: 2.523 |  Val. Acc: 77.28% \n",
            "\n",
            "\tEpoch: 41 | Train Loss: 2.401 | Train Acc: 88.59%\n",
            "\t Val. Loss: 2.518 |  Val. Acc: 77.97% \n",
            "\n",
            "\tEpoch: 42 | Train Loss: 2.397 | Train Acc: 89.00%\n",
            "\t Val. Loss: 2.518 |  Val. Acc: 77.85% \n",
            "\n",
            "\tEpoch: 43 | Train Loss: 2.391 | Train Acc: 89.59%\n",
            "\t Val. Loss: 2.506 |  Val. Acc: 79.17% \n",
            "\n",
            "\tEpoch: 44 | Train Loss: 2.386 | Train Acc: 90.02%\n",
            "\t Val. Loss: 2.500 |  Val. Acc: 79.68% \n",
            "\n",
            "\tEpoch: 45 | Train Loss: 2.382 | Train Acc: 90.44%\n",
            "\t Val. Loss: 2.494 |  Val. Acc: 80.24% \n",
            "\n",
            "\tEpoch: 46 | Train Loss: 2.378 | Train Acc: 90.83%\n",
            "\t Val. Loss: 2.495 |  Val. Acc: 80.03% \n",
            "\n",
            "\tEpoch: 47 | Train Loss: 2.373 | Train Acc: 91.30%\n",
            "\t Val. Loss: 2.492 |  Val. Acc: 80.35% \n",
            "\n",
            "\tEpoch: 48 | Train Loss: 2.370 | Train Acc: 91.66%\n",
            "\t Val. Loss: 2.477 |  Val. Acc: 81.89% \n",
            "\n",
            "\tEpoch: 49 | Train Loss: 2.366 | Train Acc: 92.05%\n",
            "\t Val. Loss: 2.477 |  Val. Acc: 81.99% \n",
            "\n",
            "\tEpoch: 50 | Train Loss: 2.362 | Train Acc: 92.39%\n",
            "\t Val. Loss: 2.472 |  Val. Acc: 82.33% \n",
            "\n",
            "\tEpoch: 51 | Train Loss: 2.359 | Train Acc: 92.68%\n",
            "\t Val. Loss: 2.471 |  Val. Acc: 82.63% \n",
            "\n",
            "\tEpoch: 52 | Train Loss: 2.356 | Train Acc: 92.99%\n",
            "\t Val. Loss: 2.465 |  Val. Acc: 83.11% \n",
            "\n",
            "\tEpoch: 53 | Train Loss: 2.354 | Train Acc: 93.20%\n",
            "\t Val. Loss: 2.464 |  Val. Acc: 83.19% \n",
            "\n",
            "\tEpoch: 54 | Train Loss: 2.351 | Train Acc: 93.41%\n",
            "\t Val. Loss: 2.459 |  Val. Acc: 83.75% \n",
            "\n",
            "\tEpoch: 55 | Train Loss: 2.350 | Train Acc: 93.59%\n",
            "\t Val. Loss: 2.456 |  Val. Acc: 83.91% \n",
            "\n",
            "\tEpoch: 56 | Train Loss: 2.349 | Train Acc: 93.69%\n",
            "\t Val. Loss: 2.452 |  Val. Acc: 84.23% \n",
            "\n",
            "\tEpoch: 57 | Train Loss: 2.347 | Train Acc: 93.84%\n",
            "\t Val. Loss: 2.455 |  Val. Acc: 84.00% \n",
            "\n",
            "\tEpoch: 58 | Train Loss: 2.345 | Train Acc: 94.00%\n",
            "\t Val. Loss: 2.450 |  Val. Acc: 84.50% \n",
            "\n",
            "\tEpoch: 59 | Train Loss: 2.345 | Train Acc: 94.08%\n",
            "\t Val. Loss: 2.445 |  Val. Acc: 84.96% \n",
            "\n",
            "\tEpoch: 60 | Train Loss: 2.343 | Train Acc: 94.22%\n",
            "\t Val. Loss: 2.448 |  Val. Acc: 84.54% \n",
            "\n",
            "\tEpoch: 61 | Train Loss: 2.342 | Train Acc: 94.31%\n",
            "\t Val. Loss: 2.441 |  Val. Acc: 85.33% \n",
            "\n",
            "\tEpoch: 62 | Train Loss: 2.341 | Train Acc: 94.39%\n",
            "\t Val. Loss: 2.443 |  Val. Acc: 85.23% \n",
            "\n",
            "\tEpoch: 63 | Train Loss: 2.340 | Train Acc: 94.50%\n",
            "\t Val. Loss: 2.438 |  Val. Acc: 85.57% \n",
            "\n",
            "\tEpoch: 64 | Train Loss: 2.339 | Train Acc: 94.59%\n",
            "\t Val. Loss: 2.439 |  Val. Acc: 85.48% \n",
            "\n",
            "\tEpoch: 65 | Train Loss: 2.338 | Train Acc: 94.66%\n",
            "\t Val. Loss: 2.441 |  Val. Acc: 85.31% \n",
            "\n",
            "\tEpoch: 66 | Train Loss: 2.338 | Train Acc: 94.70%\n",
            "\t Val. Loss: 2.435 |  Val. Acc: 85.80% \n",
            "\n",
            "\tEpoch: 67 | Train Loss: 2.337 | Train Acc: 94.77%\n",
            "\t Val. Loss: 2.435 |  Val. Acc: 85.72% \n",
            "\n",
            "\tEpoch: 68 | Train Loss: 2.337 | Train Acc: 94.83%\n",
            "\t Val. Loss: 2.439 |  Val. Acc: 85.29% \n",
            "\n",
            "\tEpoch: 69 | Train Loss: 2.336 | Train Acc: 94.87%\n",
            "\t Val. Loss: 2.432 |  Val. Acc: 86.07% \n",
            "\n",
            "\tEpoch: 70 | Train Loss: 2.335 | Train Acc: 94.94%\n",
            "\t Val. Loss: 2.435 |  Val. Acc: 85.79% \n",
            "\n",
            "\tEpoch: 71 | Train Loss: 2.336 | Train Acc: 94.94%\n",
            "\t Val. Loss: 2.433 |  Val. Acc: 85.99% \n",
            "\n",
            "\tEpoch: 72 | Train Loss: 2.335 | Train Acc: 95.04%\n",
            "\t Val. Loss: 2.430 |  Val. Acc: 86.36% \n",
            "\n",
            "\tEpoch: 73 | Train Loss: 2.334 | Train Acc: 95.08%\n",
            "\t Val. Loss: 2.429 |  Val. Acc: 86.12% \n",
            "\n",
            "\tEpoch: 74 | Train Loss: 2.333 | Train Acc: 95.16%\n",
            "\t Val. Loss: 2.426 |  Val. Acc: 86.56% \n",
            "\n",
            "\tEpoch: 75 | Train Loss: 2.333 | Train Acc: 95.22%\n",
            "\t Val. Loss: 2.429 |  Val. Acc: 86.26% \n",
            "\n",
            "\tEpoch: 76 | Train Loss: 2.332 | Train Acc: 95.29%\n",
            "\t Val. Loss: 2.427 |  Val. Acc: 86.36% \n",
            "\n",
            "\tEpoch: 77 | Train Loss: 2.332 | Train Acc: 95.33%\n",
            "\t Val. Loss: 2.427 |  Val. Acc: 86.47% \n",
            "\n",
            "\tEpoch: 78 | Train Loss: 2.331 | Train Acc: 95.41%\n",
            "\t Val. Loss: 2.422 |  Val. Acc: 87.00% \n",
            "\n",
            "\tEpoch: 79 | Train Loss: 2.330 | Train Acc: 95.46%\n",
            "\t Val. Loss: 2.420 |  Val. Acc: 87.15% \n",
            "\n",
            "\tEpoch: 80 | Train Loss: 2.330 | Train Acc: 95.52%\n",
            "\t Val. Loss: 2.425 |  Val. Acc: 86.68% \n",
            "\n",
            "\tEpoch: 81 | Train Loss: 2.329 | Train Acc: 95.61%\n",
            "\t Val. Loss: 2.422 |  Val. Acc: 86.90% \n",
            "\n",
            "\tEpoch: 82 | Train Loss: 2.328 | Train Acc: 95.66%\n",
            "\t Val. Loss: 2.422 |  Val. Acc: 86.93% \n",
            "\n",
            "\tEpoch: 83 | Train Loss: 2.328 | Train Acc: 95.73%\n",
            "\t Val. Loss: 2.426 |  Val. Acc: 86.60% \n",
            "\n",
            "\tEpoch: 84 | Train Loss: 2.327 | Train Acc: 95.79%\n",
            "\t Val. Loss: 2.421 |  Val. Acc: 86.91% \n",
            "\n",
            "\tEpoch: 85 | Train Loss: 2.327 | Train Acc: 95.84%\n",
            "\t Val. Loss: 2.423 |  Val. Acc: 86.79% \n",
            "\n",
            "\tEpoch: 86 | Train Loss: 2.326 | Train Acc: 95.93%\n",
            "\t Val. Loss: 2.420 |  Val. Acc: 87.11% \n",
            "\n",
            "\tEpoch: 87 | Train Loss: 2.325 | Train Acc: 96.01%\n",
            "\t Val. Loss: 2.415 |  Val. Acc: 87.47% \n",
            "\n",
            "\tEpoch: 88 | Train Loss: 2.324 | Train Acc: 96.10%\n",
            "\t Val. Loss: 2.420 |  Val. Acc: 87.10% \n",
            "\n",
            "\tEpoch: 89 | Train Loss: 2.323 | Train Acc: 96.17%\n",
            "\t Val. Loss: 2.419 |  Val. Acc: 87.29% \n",
            "\n",
            "\tEpoch: 90 | Train Loss: 2.323 | Train Acc: 96.19%\n",
            "\t Val. Loss: 2.421 |  Val. Acc: 86.97% \n",
            "\n",
            "\tEpoch: 91 | Train Loss: 2.322 | Train Acc: 96.26%\n",
            "\t Val. Loss: 2.414 |  Val. Acc: 87.70% \n",
            "\n",
            "\tEpoch: 92 | Train Loss: 2.322 | Train Acc: 96.30%\n",
            "\t Val. Loss: 2.410 |  Val. Acc: 87.93% \n",
            "\n",
            "\tEpoch: 93 | Train Loss: 2.322 | Train Acc: 96.33%\n",
            "\t Val. Loss: 2.412 |  Val. Acc: 87.79% \n",
            "\n",
            "\tEpoch: 94 | Train Loss: 2.321 | Train Acc: 96.36%\n",
            "\t Val. Loss: 2.412 |  Val. Acc: 87.86% \n",
            "\n",
            "\tEpoch: 95 | Train Loss: 2.321 | Train Acc: 96.41%\n",
            "\t Val. Loss: 2.416 |  Val. Acc: 87.69% \n",
            "\n",
            "\tEpoch: 96 | Train Loss: 2.320 | Train Acc: 96.45%\n",
            "\t Val. Loss: 2.413 |  Val. Acc: 87.67% \n",
            "\n",
            "\tEpoch: 97 | Train Loss: 2.320 | Train Acc: 96.50%\n",
            "\t Val. Loss: 2.413 |  Val. Acc: 87.81% \n",
            "\n",
            "\tEpoch: 98 | Train Loss: 2.319 | Train Acc: 96.54%\n",
            "\t Val. Loss: 2.410 |  Val. Acc: 88.10% \n",
            "\n",
            "\tEpoch: 99 | Train Loss: 2.319 | Train Acc: 96.58%\n",
            "\t Val. Loss: 2.410 |  Val. Acc: 88.10% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix9ofsGvhleM",
        "outputId": "b48fe788-554f-43d5-9aee-3ac12d8780d6"
      },
      "source": [
        "N_EPOCHS = 100\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "     \r\n",
        "    # train the model\r\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\r\n",
        "    #train_loss = train(model, train_iterator, optimizer, criterion)\r\n",
        "\r\n",
        "\r\n",
        "    # evaluate the model\r\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\r\n",
        "    #valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    # save the best model\r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\r\n",
        "    \r\n",
        "    print(f'\\tEpoch: {epoch} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tEpoch: 0 | Train Loss: 2.319 | Train Acc: 96.61%\n",
            "\t Val. Loss: 2.406 |  Val. Acc: 88.51% \n",
            "\n",
            "\tEpoch: 1 | Train Loss: 2.318 | Train Acc: 96.64%\n",
            "\t Val. Loss: 2.407 |  Val. Acc: 88.38% \n",
            "\n",
            "\tEpoch: 2 | Train Loss: 2.318 | Train Acc: 96.66%\n",
            "\t Val. Loss: 2.410 |  Val. Acc: 88.05% \n",
            "\n",
            "\tEpoch: 3 | Train Loss: 2.318 | Train Acc: 96.70%\n",
            "\t Val. Loss: 2.411 |  Val. Acc: 87.94% \n",
            "\n",
            "\tEpoch: 4 | Train Loss: 2.318 | Train Acc: 96.72%\n",
            "\t Val. Loss: 2.411 |  Val. Acc: 87.99% \n",
            "\n",
            "\tEpoch: 5 | Train Loss: 2.317 | Train Acc: 96.75%\n",
            "\t Val. Loss: 2.407 |  Val. Acc: 88.26% \n",
            "\n",
            "\tEpoch: 6 | Train Loss: 2.317 | Train Acc: 96.77%\n",
            "\t Val. Loss: 2.407 |  Val. Acc: 88.40% \n",
            "\n",
            "\tEpoch: 7 | Train Loss: 2.317 | Train Acc: 96.80%\n",
            "\t Val. Loss: 2.408 |  Val. Acc: 88.25% \n",
            "\n",
            "\tEpoch: 8 | Train Loss: 2.317 | Train Acc: 96.84%\n",
            "\t Val. Loss: 2.406 |  Val. Acc: 88.28% \n",
            "\n",
            "\tEpoch: 9 | Train Loss: 2.316 | Train Acc: 96.85%\n",
            "\t Val. Loss: 2.404 |  Val. Acc: 88.46% \n",
            "\n",
            "\tEpoch: 10 | Train Loss: 2.316 | Train Acc: 96.88%\n",
            "\t Val. Loss: 2.401 |  Val. Acc: 89.01% \n",
            "\n",
            "\tEpoch: 11 | Train Loss: 2.316 | Train Acc: 96.90%\n",
            "\t Val. Loss: 2.401 |  Val. Acc: 88.88% \n",
            "\n",
            "\tEpoch: 12 | Train Loss: 2.316 | Train Acc: 96.93%\n",
            "\t Val. Loss: 2.400 |  Val. Acc: 89.04% \n",
            "\n",
            "\tEpoch: 13 | Train Loss: 2.316 | Train Acc: 96.91%\n",
            "\t Val. Loss: 2.404 |  Val. Acc: 88.80% \n",
            "\n",
            "\tEpoch: 14 | Train Loss: 2.316 | Train Acc: 96.94%\n",
            "\t Val. Loss: 2.401 |  Val. Acc: 88.97% \n",
            "\n",
            "\tEpoch: 15 | Train Loss: 2.315 | Train Acc: 96.98%\n",
            "\t Val. Loss: 2.404 |  Val. Acc: 88.61% \n",
            "\n",
            "\tEpoch: 16 | Train Loss: 2.315 | Train Acc: 97.02%\n",
            "\t Val. Loss: 2.401 |  Val. Acc: 89.02% \n",
            "\n",
            "\tEpoch: 17 | Train Loss: 2.314 | Train Acc: 97.06%\n",
            "\t Val. Loss: 2.398 |  Val. Acc: 89.09% \n",
            "\n",
            "\tEpoch: 18 | Train Loss: 2.314 | Train Acc: 97.11%\n",
            "\t Val. Loss: 2.397 |  Val. Acc: 89.43% \n",
            "\n",
            "\tEpoch: 19 | Train Loss: 2.313 | Train Acc: 97.15%\n",
            "\t Val. Loss: 2.395 |  Val. Acc: 89.58% \n",
            "\n",
            "\tEpoch: 20 | Train Loss: 2.313 | Train Acc: 97.19%\n",
            "\t Val. Loss: 2.397 |  Val. Acc: 89.32% \n",
            "\n",
            "\tEpoch: 21 | Train Loss: 2.313 | Train Acc: 97.20%\n",
            "\t Val. Loss: 2.401 |  Val. Acc: 89.08% \n",
            "\n",
            "\tEpoch: 22 | Train Loss: 2.313 | Train Acc: 97.24%\n",
            "\t Val. Loss: 2.394 |  Val. Acc: 89.52% \n",
            "\n",
            "\tEpoch: 23 | Train Loss: 2.312 | Train Acc: 97.27%\n",
            "\t Val. Loss: 2.396 |  Val. Acc: 89.45% \n",
            "\n",
            "\tEpoch: 24 | Train Loss: 2.312 | Train Acc: 97.30%\n",
            "\t Val. Loss: 2.401 |  Val. Acc: 88.92% \n",
            "\n",
            "\tEpoch: 25 | Train Loss: 2.312 | Train Acc: 97.33%\n",
            "\t Val. Loss: 2.398 |  Val. Acc: 89.21% \n",
            "\n",
            "\tEpoch: 26 | Train Loss: 2.311 | Train Acc: 97.37%\n",
            "\t Val. Loss: 2.401 |  Val. Acc: 88.88% \n",
            "\n",
            "\tEpoch: 27 | Train Loss: 2.311 | Train Acc: 97.41%\n",
            "\t Val. Loss: 2.399 |  Val. Acc: 89.12% \n",
            "\n",
            "\tEpoch: 28 | Train Loss: 2.311 | Train Acc: 97.42%\n",
            "\t Val. Loss: 2.398 |  Val. Acc: 89.12% \n",
            "\n",
            "\tEpoch: 29 | Train Loss: 2.310 | Train Acc: 97.46%\n",
            "\t Val. Loss: 2.400 |  Val. Acc: 88.93% \n",
            "\n",
            "\tEpoch: 30 | Train Loss: 2.310 | Train Acc: 97.50%\n",
            "\t Val. Loss: 2.396 |  Val. Acc: 89.44% \n",
            "\n",
            "\tEpoch: 31 | Train Loss: 2.310 | Train Acc: 97.51%\n",
            "\t Val. Loss: 2.396 |  Val. Acc: 89.52% \n",
            "\n",
            "\tEpoch: 32 | Train Loss: 2.310 | Train Acc: 97.53%\n",
            "\t Val. Loss: 2.395 |  Val. Acc: 89.72% \n",
            "\n",
            "\tEpoch: 33 | Train Loss: 2.309 | Train Acc: 97.58%\n",
            "\t Val. Loss: 2.396 |  Val. Acc: 89.39% \n",
            "\n",
            "\tEpoch: 34 | Train Loss: 2.309 | Train Acc: 97.61%\n",
            "\t Val. Loss: 2.396 |  Val. Acc: 89.43% \n",
            "\n",
            "\tEpoch: 35 | Train Loss: 2.309 | Train Acc: 97.63%\n",
            "\t Val. Loss: 2.401 |  Val. Acc: 89.12% \n",
            "\n",
            "\tEpoch: 36 | Train Loss: 2.308 | Train Acc: 97.66%\n",
            "\t Val. Loss: 2.395 |  Val. Acc: 89.63% \n",
            "\n",
            "\tEpoch: 37 | Train Loss: 2.308 | Train Acc: 97.67%\n",
            "\t Val. Loss: 2.393 |  Val. Acc: 89.69% \n",
            "\n",
            "\tEpoch: 38 | Train Loss: 2.308 | Train Acc: 97.70%\n",
            "\t Val. Loss: 2.392 |  Val. Acc: 89.87% \n",
            "\n",
            "\tEpoch: 39 | Train Loss: 2.308 | Train Acc: 97.70%\n",
            "\t Val. Loss: 2.390 |  Val. Acc: 89.99% \n",
            "\n",
            "\tEpoch: 40 | Train Loss: 2.308 | Train Acc: 97.72%\n",
            "\t Val. Loss: 2.393 |  Val. Acc: 89.69% \n",
            "\n",
            "\tEpoch: 41 | Train Loss: 2.308 | Train Acc: 97.73%\n",
            "\t Val. Loss: 2.393 |  Val. Acc: 89.75% \n",
            "\n",
            "\tEpoch: 42 | Train Loss: 2.307 | Train Acc: 97.75%\n",
            "\t Val. Loss: 2.391 |  Val. Acc: 89.87% \n",
            "\n",
            "\tEpoch: 43 | Train Loss: 2.307 | Train Acc: 97.77%\n",
            "\t Val. Loss: 2.393 |  Val. Acc: 89.83% \n",
            "\n",
            "\tEpoch: 44 | Train Loss: 2.307 | Train Acc: 97.79%\n",
            "\t Val. Loss: 2.390 |  Val. Acc: 89.99% \n",
            "\n",
            "\tEpoch: 45 | Train Loss: 2.307 | Train Acc: 97.80%\n",
            "\t Val. Loss: 2.393 |  Val. Acc: 89.53% \n",
            "\n",
            "\tEpoch: 46 | Train Loss: 2.307 | Train Acc: 97.83%\n",
            "\t Val. Loss: 2.390 |  Val. Acc: 89.92% \n",
            "\n",
            "\tEpoch: 47 | Train Loss: 2.307 | Train Acc: 97.84%\n",
            "\t Val. Loss: 2.394 |  Val. Acc: 89.47% \n",
            "\n",
            "\tEpoch: 48 | Train Loss: 2.306 | Train Acc: 97.87%\n",
            "\t Val. Loss: 2.395 |  Val. Acc: 89.46% \n",
            "\n",
            "\tEpoch: 49 | Train Loss: 2.306 | Train Acc: 97.89%\n",
            "\t Val. Loss: 2.392 |  Val. Acc: 89.71% \n",
            "\n",
            "\tEpoch: 50 | Train Loss: 2.306 | Train Acc: 97.89%\n",
            "\t Val. Loss: 2.391 |  Val. Acc: 89.83% \n",
            "\n",
            "\tEpoch: 51 | Train Loss: 2.306 | Train Acc: 97.90%\n",
            "\t Val. Loss: 2.391 |  Val. Acc: 89.77% \n",
            "\n",
            "\tEpoch: 52 | Train Loss: 2.306 | Train Acc: 97.94%\n",
            "\t Val. Loss: 2.388 |  Val. Acc: 90.23% \n",
            "\n",
            "\tEpoch: 53 | Train Loss: 2.306 | Train Acc: 97.94%\n",
            "\t Val. Loss: 2.388 |  Val. Acc: 90.12% \n",
            "\n",
            "\tEpoch: 54 | Train Loss: 2.305 | Train Acc: 97.96%\n",
            "\t Val. Loss: 2.388 |  Val. Acc: 90.24% \n",
            "\n",
            "\tEpoch: 55 | Train Loss: 2.305 | Train Acc: 97.97%\n",
            "\t Val. Loss: 2.387 |  Val. Acc: 90.36% \n",
            "\n",
            "\tEpoch: 56 | Train Loss: 2.305 | Train Acc: 97.98%\n",
            "\t Val. Loss: 2.389 |  Val. Acc: 90.07% \n",
            "\n",
            "\tEpoch: 57 | Train Loss: 2.305 | Train Acc: 97.99%\n",
            "\t Val. Loss: 2.388 |  Val. Acc: 90.21% \n",
            "\n",
            "\tEpoch: 58 | Train Loss: 2.305 | Train Acc: 98.00%\n",
            "\t Val. Loss: 2.386 |  Val. Acc: 90.45% \n",
            "\n",
            "\tEpoch: 59 | Train Loss: 2.305 | Train Acc: 98.00%\n",
            "\t Val. Loss: 2.385 |  Val. Acc: 90.49% \n",
            "\n",
            "\tEpoch: 60 | Train Loss: 2.305 | Train Acc: 98.01%\n",
            "\t Val. Loss: 2.385 |  Val. Acc: 90.67% \n",
            "\n",
            "\tEpoch: 61 | Train Loss: 2.305 | Train Acc: 98.03%\n",
            "\t Val. Loss: 2.384 |  Val. Acc: 90.60% \n",
            "\n",
            "\tEpoch: 62 | Train Loss: 2.305 | Train Acc: 98.04%\n",
            "\t Val. Loss: 2.387 |  Val. Acc: 90.30% \n",
            "\n",
            "\tEpoch: 63 | Train Loss: 2.304 | Train Acc: 98.06%\n",
            "\t Val. Loss: 2.384 |  Val. Acc: 90.32% \n",
            "\n",
            "\tEpoch: 64 | Train Loss: 2.304 | Train Acc: 98.07%\n",
            "\t Val. Loss: 2.384 |  Val. Acc: 90.38% \n",
            "\n",
            "\tEpoch: 65 | Train Loss: 2.304 | Train Acc: 98.07%\n",
            "\t Val. Loss: 2.386 |  Val. Acc: 90.24% \n",
            "\n",
            "\tEpoch: 66 | Train Loss: 2.304 | Train Acc: 98.09%\n",
            "\t Val. Loss: 2.388 |  Val. Acc: 90.23% \n",
            "\n",
            "\tEpoch: 67 | Train Loss: 2.304 | Train Acc: 98.10%\n",
            "\t Val. Loss: 2.384 |  Val. Acc: 90.46% \n",
            "\n",
            "\tEpoch: 68 | Train Loss: 2.304 | Train Acc: 98.10%\n",
            "\t Val. Loss: 2.383 |  Val. Acc: 90.52% \n",
            "\n",
            "\tEpoch: 69 | Train Loss: 2.304 | Train Acc: 98.11%\n",
            "\t Val. Loss: 2.383 |  Val. Acc: 90.70% \n",
            "\n",
            "\tEpoch: 70 | Train Loss: 2.304 | Train Acc: 98.11%\n",
            "\t Val. Loss: 2.386 |  Val. Acc: 90.35% \n",
            "\n",
            "\tEpoch: 71 | Train Loss: 2.304 | Train Acc: 98.13%\n",
            "\t Val. Loss: 2.384 |  Val. Acc: 90.54% \n",
            "\n",
            "\tEpoch: 72 | Train Loss: 2.304 | Train Acc: 98.13%\n",
            "\t Val. Loss: 2.386 |  Val. Acc: 90.24% \n",
            "\n",
            "\tEpoch: 73 | Train Loss: 2.304 | Train Acc: 98.15%\n",
            "\t Val. Loss: 2.387 |  Val. Acc: 90.27% \n",
            "\n",
            "\tEpoch: 74 | Train Loss: 2.303 | Train Acc: 98.16%\n",
            "\t Val. Loss: 2.385 |  Val. Acc: 90.51% \n",
            "\n",
            "\tEpoch: 75 | Train Loss: 2.303 | Train Acc: 98.17%\n",
            "\t Val. Loss: 2.384 |  Val. Acc: 90.48% \n",
            "\n",
            "\tEpoch: 76 | Train Loss: 2.303 | Train Acc: 98.19%\n",
            "\t Val. Loss: 2.384 |  Val. Acc: 90.58% \n",
            "\n",
            "\tEpoch: 77 | Train Loss: 2.303 | Train Acc: 98.21%\n",
            "\t Val. Loss: 2.385 |  Val. Acc: 90.39% \n",
            "\n",
            "\tEpoch: 78 | Train Loss: 2.303 | Train Acc: 98.24%\n",
            "\t Val. Loss: 2.388 |  Val. Acc: 89.98% \n",
            "\n",
            "\tEpoch: 79 | Train Loss: 2.302 | Train Acc: 98.27%\n",
            "\t Val. Loss: 2.387 |  Val. Acc: 90.25% \n",
            "\n",
            "\tEpoch: 80 | Train Loss: 2.302 | Train Acc: 98.29%\n",
            "\t Val. Loss: 2.386 |  Val. Acc: 90.35% \n",
            "\n",
            "\tEpoch: 81 | Train Loss: 2.302 | Train Acc: 98.29%\n",
            "\t Val. Loss: 2.388 |  Val. Acc: 90.00% \n",
            "\n",
            "\tEpoch: 82 | Train Loss: 2.302 | Train Acc: 98.30%\n",
            "\t Val. Loss: 2.388 |  Val. Acc: 90.20% \n",
            "\n",
            "\tEpoch: 83 | Train Loss: 2.302 | Train Acc: 98.31%\n",
            "\t Val. Loss: 2.387 |  Val. Acc: 90.26% \n",
            "\n",
            "\tEpoch: 84 | Train Loss: 2.302 | Train Acc: 98.31%\n",
            "\t Val. Loss: 2.385 |  Val. Acc: 90.31% \n",
            "\n",
            "\tEpoch: 85 | Train Loss: 2.302 | Train Acc: 98.34%\n",
            "\t Val. Loss: 2.384 |  Val. Acc: 90.45% \n",
            "\n",
            "\tEpoch: 86 | Train Loss: 2.301 | Train Acc: 98.36%\n",
            "\t Val. Loss: 2.382 |  Val. Acc: 90.54% \n",
            "\n",
            "\tEpoch: 87 | Train Loss: 2.301 | Train Acc: 98.37%\n",
            "\t Val. Loss: 2.384 |  Val. Acc: 90.34% \n",
            "\n",
            "\tEpoch: 88 | Train Loss: 2.301 | Train Acc: 98.37%\n",
            "\t Val. Loss: 2.385 |  Val. Acc: 90.35% \n",
            "\n",
            "\tEpoch: 89 | Train Loss: 2.301 | Train Acc: 98.39%\n",
            "\t Val. Loss: 2.382 |  Val. Acc: 90.77% \n",
            "\n",
            "\tEpoch: 90 | Train Loss: 2.301 | Train Acc: 98.41%\n",
            "\t Val. Loss: 2.381 |  Val. Acc: 90.79% \n",
            "\n",
            "\tEpoch: 91 | Train Loss: 2.301 | Train Acc: 98.42%\n",
            "\t Val. Loss: 2.380 |  Val. Acc: 90.85% \n",
            "\n",
            "\tEpoch: 92 | Train Loss: 2.301 | Train Acc: 98.43%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.99% \n",
            "\n",
            "\tEpoch: 93 | Train Loss: 2.301 | Train Acc: 98.43%\n",
            "\t Val. Loss: 2.380 |  Val. Acc: 90.90% \n",
            "\n",
            "\tEpoch: 94 | Train Loss: 2.301 | Train Acc: 98.44%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.92% \n",
            "\n",
            "\tEpoch: 95 | Train Loss: 2.301 | Train Acc: 98.44%\n",
            "\t Val. Loss: 2.380 |  Val. Acc: 90.82% \n",
            "\n",
            "\tEpoch: 96 | Train Loss: 2.301 | Train Acc: 98.46%\n",
            "\t Val. Loss: 2.380 |  Val. Acc: 90.79% \n",
            "\n",
            "\tEpoch: 97 | Train Loss: 2.300 | Train Acc: 98.46%\n",
            "\t Val. Loss: 2.378 |  Val. Acc: 91.00% \n",
            "\n",
            "\tEpoch: 98 | Train Loss: 2.300 | Train Acc: 98.46%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.80% \n",
            "\n",
            "\tEpoch: 99 | Train Loss: 2.300 | Train Acc: 98.48%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.99% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGvP09N5k97M",
        "outputId": "e57cb44d-f61c-47bb-dde2-ccc2cd0c8f72"
      },
      "source": [
        "N_EPOCHS = 100\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "     \r\n",
        "    # train the model\r\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\r\n",
        "    #train_loss = train(model, train_iterator, optimizer, criterion)\r\n",
        "\r\n",
        "\r\n",
        "    # evaluate the model\r\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\r\n",
        "    #valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    # save the best model\r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\r\n",
        "    \r\n",
        "    print(f'\\tEpoch: {epoch} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tEpoch: 0 | Train Loss: 2.300 | Train Acc: 98.48%\n",
            "\t Val. Loss: 2.384 |  Val. Acc: 90.43% \n",
            "\n",
            "\tEpoch: 1 | Train Loss: 2.300 | Train Acc: 98.50%\n",
            "\t Val. Loss: 2.384 |  Val. Acc: 90.35% \n",
            "\n",
            "\tEpoch: 2 | Train Loss: 2.300 | Train Acc: 98.52%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.97% \n",
            "\n",
            "\tEpoch: 3 | Train Loss: 2.300 | Train Acc: 98.52%\n",
            "\t Val. Loss: 2.384 |  Val. Acc: 90.48% \n",
            "\n",
            "\tEpoch: 4 | Train Loss: 2.300 | Train Acc: 98.52%\n",
            "\t Val. Loss: 2.383 |  Val. Acc: 90.54% \n",
            "\n",
            "\tEpoch: 5 | Train Loss: 2.300 | Train Acc: 98.53%\n",
            "\t Val. Loss: 2.381 |  Val. Acc: 90.84% \n",
            "\n",
            "\tEpoch: 6 | Train Loss: 2.300 | Train Acc: 98.55%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.84% \n",
            "\n",
            "\tEpoch: 7 | Train Loss: 2.300 | Train Acc: 98.55%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.94% \n",
            "\n",
            "\tEpoch: 8 | Train Loss: 2.299 | Train Acc: 98.56%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.94% \n",
            "\n",
            "\tEpoch: 9 | Train Loss: 2.299 | Train Acc: 98.56%\n",
            "\t Val. Loss: 2.378 |  Val. Acc: 91.03% \n",
            "\n",
            "\tEpoch: 10 | Train Loss: 2.299 | Train Acc: 98.57%\n",
            "\t Val. Loss: 2.378 |  Val. Acc: 91.00% \n",
            "\n",
            "\tEpoch: 11 | Train Loss: 2.299 | Train Acc: 98.57%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.97% \n",
            "\n",
            "\tEpoch: 12 | Train Loss: 2.299 | Train Acc: 98.59%\n",
            "\t Val. Loss: 2.378 |  Val. Acc: 91.05% \n",
            "\n",
            "\tEpoch: 13 | Train Loss: 2.299 | Train Acc: 98.59%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.16% \n",
            "\n",
            "\tEpoch: 14 | Train Loss: 2.299 | Train Acc: 98.60%\n",
            "\t Val. Loss: 2.376 |  Val. Acc: 91.25% \n",
            "\n",
            "\tEpoch: 15 | Train Loss: 2.299 | Train Acc: 98.60%\n",
            "\t Val. Loss: 2.376 |  Val. Acc: 91.34% \n",
            "\n",
            "\tEpoch: 16 | Train Loss: 2.299 | Train Acc: 98.60%\n",
            "\t Val. Loss: 2.381 |  Val. Acc: 90.92% \n",
            "\n",
            "\tEpoch: 17 | Train Loss: 2.299 | Train Acc: 98.60%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.93% \n",
            "\n",
            "\tEpoch: 18 | Train Loss: 2.299 | Train Acc: 98.61%\n",
            "\t Val. Loss: 2.376 |  Val. Acc: 91.24% \n",
            "\n",
            "\tEpoch: 19 | Train Loss: 2.299 | Train Acc: 98.61%\n",
            "\t Val. Loss: 2.376 |  Val. Acc: 91.27% \n",
            "\n",
            "\tEpoch: 20 | Train Loss: 2.299 | Train Acc: 98.62%\n",
            "\t Val. Loss: 2.382 |  Val. Acc: 90.78% \n",
            "\n",
            "\tEpoch: 21 | Train Loss: 2.299 | Train Acc: 98.61%\n",
            "\t Val. Loss: 2.378 |  Val. Acc: 91.03% \n",
            "\n",
            "\tEpoch: 22 | Train Loss: 2.299 | Train Acc: 98.62%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.04% \n",
            "\n",
            "\tEpoch: 23 | Train Loss: 2.299 | Train Acc: 98.62%\n",
            "\t Val. Loss: 2.376 |  Val. Acc: 91.14% \n",
            "\n",
            "\tEpoch: 24 | Train Loss: 2.299 | Train Acc: 98.62%\n",
            "\t Val. Loss: 2.375 |  Val. Acc: 91.35% \n",
            "\n",
            "\tEpoch: 25 | Train Loss: 2.299 | Train Acc: 98.62%\n",
            "\t Val. Loss: 2.375 |  Val. Acc: 91.28% \n",
            "\n",
            "\tEpoch: 26 | Train Loss: 2.299 | Train Acc: 98.62%\n",
            "\t Val. Loss: 2.375 |  Val. Acc: 91.34% \n",
            "\n",
            "\tEpoch: 27 | Train Loss: 2.299 | Train Acc: 98.64%\n",
            "\t Val. Loss: 2.380 |  Val. Acc: 90.90% \n",
            "\n",
            "\tEpoch: 28 | Train Loss: 2.299 | Train Acc: 98.65%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.90% \n",
            "\n",
            "\tEpoch: 29 | Train Loss: 2.298 | Train Acc: 98.66%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.07% \n",
            "\n",
            "\tEpoch: 30 | Train Loss: 2.298 | Train Acc: 98.67%\n",
            "\t Val. Loss: 2.376 |  Val. Acc: 91.14% \n",
            "\n",
            "\tEpoch: 31 | Train Loss: 2.298 | Train Acc: 98.66%\n",
            "\t Val. Loss: 2.375 |  Val. Acc: 91.36% \n",
            "\n",
            "\tEpoch: 32 | Train Loss: 2.298 | Train Acc: 98.66%\n",
            "\t Val. Loss: 2.376 |  Val. Acc: 91.26% \n",
            "\n",
            "\tEpoch: 33 | Train Loss: 2.298 | Train Acc: 98.68%\n",
            "\t Val. Loss: 2.376 |  Val. Acc: 91.31% \n",
            "\n",
            "\tEpoch: 34 | Train Loss: 2.298 | Train Acc: 98.67%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.20% \n",
            "\n",
            "\tEpoch: 35 | Train Loss: 2.298 | Train Acc: 98.69%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.15% \n",
            "\n",
            "\tEpoch: 36 | Train Loss: 2.298 | Train Acc: 98.69%\n",
            "\t Val. Loss: 2.375 |  Val. Acc: 91.59% \n",
            "\n",
            "\tEpoch: 37 | Train Loss: 2.298 | Train Acc: 98.69%\n",
            "\t Val. Loss: 2.374 |  Val. Acc: 91.62% \n",
            "\n",
            "\tEpoch: 38 | Train Loss: 2.298 | Train Acc: 98.69%\n",
            "\t Val. Loss: 2.378 |  Val. Acc: 91.04% \n",
            "\n",
            "\tEpoch: 39 | Train Loss: 2.298 | Train Acc: 98.71%\n",
            "\t Val. Loss: 2.378 |  Val. Acc: 91.12% \n",
            "\n",
            "\tEpoch: 40 | Train Loss: 2.298 | Train Acc: 98.71%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.26% \n",
            "\n",
            "\tEpoch: 41 | Train Loss: 2.298 | Train Acc: 98.71%\n",
            "\t Val. Loss: 2.380 |  Val. Acc: 90.77% \n",
            "\n",
            "\tEpoch: 42 | Train Loss: 2.298 | Train Acc: 98.71%\n",
            "\t Val. Loss: 2.378 |  Val. Acc: 90.98% \n",
            "\n",
            "\tEpoch: 43 | Train Loss: 2.298 | Train Acc: 98.71%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.84% \n",
            "\n",
            "\tEpoch: 44 | Train Loss: 2.298 | Train Acc: 98.73%\n",
            "\t Val. Loss: 2.382 |  Val. Acc: 90.64% \n",
            "\n",
            "\tEpoch: 45 | Train Loss: 2.298 | Train Acc: 98.73%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.98% \n",
            "\n",
            "\tEpoch: 46 | Train Loss: 2.298 | Train Acc: 98.73%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.96% \n",
            "\n",
            "\tEpoch: 47 | Train Loss: 2.298 | Train Acc: 98.73%\n",
            "\t Val. Loss: 2.378 |  Val. Acc: 90.99% \n",
            "\n",
            "\tEpoch: 48 | Train Loss: 2.298 | Train Acc: 98.73%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.22% \n",
            "\n",
            "\tEpoch: 49 | Train Loss: 2.298 | Train Acc: 98.74%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.05% \n",
            "\n",
            "\tEpoch: 50 | Train Loss: 2.298 | Train Acc: 98.74%\n",
            "\t Val. Loss: 2.375 |  Val. Acc: 91.36% \n",
            "\n",
            "\tEpoch: 51 | Train Loss: 2.298 | Train Acc: 98.74%\n",
            "\t Val. Loss: 2.375 |  Val. Acc: 91.30% \n",
            "\n",
            "\tEpoch: 52 | Train Loss: 2.298 | Train Acc: 98.74%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.90% \n",
            "\n",
            "\tEpoch: 53 | Train Loss: 2.298 | Train Acc: 98.75%\n",
            "\t Val. Loss: 2.380 |  Val. Acc: 90.82% \n",
            "\n",
            "\tEpoch: 54 | Train Loss: 2.298 | Train Acc: 98.75%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.86% \n",
            "\n",
            "\tEpoch: 55 | Train Loss: 2.297 | Train Acc: 98.76%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.06% \n",
            "\n",
            "\tEpoch: 56 | Train Loss: 2.297 | Train Acc: 98.77%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.02% \n",
            "\n",
            "\tEpoch: 57 | Train Loss: 2.297 | Train Acc: 98.77%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 90.98% \n",
            "\n",
            "\tEpoch: 58 | Train Loss: 2.297 | Train Acc: 98.78%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.14% \n",
            "\n",
            "\tEpoch: 59 | Train Loss: 2.297 | Train Acc: 98.78%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.06% \n",
            "\n",
            "\tEpoch: 60 | Train Loss: 2.297 | Train Acc: 98.77%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.03% \n",
            "\n",
            "\tEpoch: 61 | Train Loss: 2.297 | Train Acc: 98.77%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.28% \n",
            "\n",
            "\tEpoch: 62 | Train Loss: 2.297 | Train Acc: 98.78%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.34% \n",
            "\n",
            "\tEpoch: 63 | Train Loss: 2.297 | Train Acc: 98.79%\n",
            "\t Val. Loss: 2.375 |  Val. Acc: 91.35% \n",
            "\n",
            "\tEpoch: 64 | Train Loss: 2.297 | Train Acc: 98.79%\n",
            "\t Val. Loss: 2.374 |  Val. Acc: 91.47% \n",
            "\n",
            "\tEpoch: 65 | Train Loss: 2.297 | Train Acc: 98.79%\n",
            "\t Val. Loss: 2.376 |  Val. Acc: 91.14% \n",
            "\n",
            "\tEpoch: 66 | Train Loss: 2.297 | Train Acc: 98.79%\n",
            "\t Val. Loss: 2.375 |  Val. Acc: 91.28% \n",
            "\n",
            "\tEpoch: 67 | Train Loss: 2.297 | Train Acc: 98.79%\n",
            "\t Val. Loss: 2.374 |  Val. Acc: 91.37% \n",
            "\n",
            "\tEpoch: 68 | Train Loss: 2.297 | Train Acc: 98.79%\n",
            "\t Val. Loss: 2.373 |  Val. Acc: 91.38% \n",
            "\n",
            "\tEpoch: 69 | Train Loss: 2.297 | Train Acc: 98.79%\n",
            "\t Val. Loss: 2.373 |  Val. Acc: 91.51% \n",
            "\n",
            "\tEpoch: 70 | Train Loss: 2.297 | Train Acc: 98.80%\n",
            "\t Val. Loss: 2.372 |  Val. Acc: 91.58% \n",
            "\n",
            "\tEpoch: 71 | Train Loss: 2.297 | Train Acc: 98.81%\n",
            "\t Val. Loss: 2.378 |  Val. Acc: 91.04% \n",
            "\n",
            "\tEpoch: 72 | Train Loss: 2.297 | Train Acc: 98.82%\n",
            "\t Val. Loss: 2.376 |  Val. Acc: 91.17% \n",
            "\n",
            "\tEpoch: 73 | Train Loss: 2.297 | Train Acc: 98.83%\n",
            "\t Val. Loss: 2.375 |  Val. Acc: 91.35% \n",
            "\n",
            "\tEpoch: 74 | Train Loss: 2.297 | Train Acc: 98.83%\n",
            "\t Val. Loss: 2.374 |  Val. Acc: 91.41% \n",
            "\n",
            "\tEpoch: 75 | Train Loss: 2.297 | Train Acc: 98.83%\n",
            "\t Val. Loss: 2.374 |  Val. Acc: 91.46% \n",
            "\n",
            "\tEpoch: 76 | Train Loss: 2.297 | Train Acc: 98.82%\n",
            "\t Val. Loss: 2.373 |  Val. Acc: 91.54% \n",
            "\n",
            "\tEpoch: 77 | Train Loss: 2.297 | Train Acc: 98.82%\n",
            "\t Val. Loss: 2.380 |  Val. Acc: 90.83% \n",
            "\n",
            "\tEpoch: 78 | Train Loss: 2.297 | Train Acc: 98.84%\n",
            "\t Val. Loss: 2.377 |  Val. Acc: 91.11% \n",
            "\n",
            "\tEpoch: 79 | Train Loss: 2.297 | Train Acc: 98.84%\n",
            "\t Val. Loss: 2.376 |  Val. Acc: 91.30% \n",
            "\n",
            "\tEpoch: 80 | Train Loss: 2.297 | Train Acc: 98.84%\n",
            "\t Val. Loss: 2.375 |  Val. Acc: 91.28% \n",
            "\n",
            "\tEpoch: 81 | Train Loss: 2.297 | Train Acc: 98.85%\n",
            "\t Val. Loss: 2.374 |  Val. Acc: 91.38% \n",
            "\n",
            "\tEpoch: 82 | Train Loss: 2.297 | Train Acc: 98.85%\n",
            "\t Val. Loss: 2.373 |  Val. Acc: 91.48% \n",
            "\n",
            "\tEpoch: 83 | Train Loss: 2.297 | Train Acc: 98.86%\n",
            "\t Val. Loss: 2.373 |  Val. Acc: 91.54% \n",
            "\n",
            "\tEpoch: 84 | Train Loss: 2.297 | Train Acc: 98.85%\n",
            "\t Val. Loss: 2.374 |  Val. Acc: 91.40% \n",
            "\n",
            "\tEpoch: 85 | Train Loss: 2.297 | Train Acc: 98.86%\n",
            "\t Val. Loss: 2.376 |  Val. Acc: 91.31% \n",
            "\n",
            "\tEpoch: 86 | Train Loss: 2.297 | Train Acc: 98.86%\n",
            "\t Val. Loss: 2.379 |  Val. Acc: 90.90% \n",
            "\n",
            "\tEpoch: 87 | Train Loss: 2.296 | Train Acc: 98.86%\n",
            "\t Val. Loss: 2.373 |  Val. Acc: 91.62% \n",
            "\n",
            "\tEpoch: 88 | Train Loss: 2.296 | Train Acc: 98.87%\n",
            "\t Val. Loss: 2.373 |  Val. Acc: 91.60% \n",
            "\n",
            "\tEpoch: 89 | Train Loss: 2.296 | Train Acc: 98.87%\n",
            "\t Val. Loss: 2.373 |  Val. Acc: 91.69% \n",
            "\n",
            "\tEpoch: 90 | Train Loss: 2.296 | Train Acc: 98.87%\n",
            "\t Val. Loss: 2.372 |  Val. Acc: 91.72% \n",
            "\n",
            "\tEpoch: 91 | Train Loss: 2.296 | Train Acc: 98.87%\n",
            "\t Val. Loss: 2.372 |  Val. Acc: 91.66% \n",
            "\n",
            "\tEpoch: 92 | Train Loss: 2.296 | Train Acc: 98.87%\n",
            "\t Val. Loss: 2.371 |  Val. Acc: 91.72% \n",
            "\n",
            "\tEpoch: 93 | Train Loss: 2.296 | Train Acc: 98.87%\n",
            "\t Val. Loss: 2.373 |  Val. Acc: 91.66% \n",
            "\n",
            "\tEpoch: 94 | Train Loss: 2.296 | Train Acc: 98.88%\n",
            "\t Val. Loss: 2.372 |  Val. Acc: 91.61% \n",
            "\n",
            "\tEpoch: 95 | Train Loss: 2.296 | Train Acc: 98.88%\n",
            "\t Val. Loss: 2.374 |  Val. Acc: 91.38% \n",
            "\n",
            "\tEpoch: 96 | Train Loss: 2.296 | Train Acc: 98.88%\n",
            "\t Val. Loss: 2.373 |  Val. Acc: 91.60% \n",
            "\n",
            "\tEpoch: 97 | Train Loss: 2.296 | Train Acc: 98.88%\n",
            "\t Val. Loss: 2.373 |  Val. Acc: 91.60% \n",
            "\n",
            "\tEpoch: 98 | Train Loss: 2.296 | Train Acc: 98.89%\n",
            "\t Val. Loss: 2.372 |  Val. Acc: 91.65% \n",
            "\n",
            "\tEpoch: 99 | Train Loss: 2.296 | Train Acc: 98.89%\n",
            "\t Val. Loss: 2.372 |  Val. Acc: 91.62% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlJqaA2-rXtR"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    #categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    #return categories[pred.item()]\n",
        "    return pred.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k5MaYDFq5dI",
        "outputId": "3b14b57e-88cc-4284-b874-afdad1f59050"
      },
      "source": [
        "classify_tweet(\"A valid explanation for why Trump won't let women on the golf course.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dlQRdeFrTDD",
        "outputId": "856d7525-97c0-4ec4-e100-6b38bfd061f4"
      },
      "source": [
        "classify_tweet(\"The movie had zilch character, nada screenplay, zero action scenes, almost negligible thougts\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4Nl5W2UrmGu",
        "outputId": "8e51e1df-43dc-4fb9-9b72-3ed7f9a84a9f"
      },
      "source": [
        "classify_tweet(\"5 minutes into the movie, you feel you want to runaway but eventually the movie catches on and does a great job\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    }
  ]
}
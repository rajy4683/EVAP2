{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "ENDS9_TF_commonqa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EVAP2/blob/master/ENDS9_TF_commonqa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzxOJwN7EzFO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1YC7Z0guYGX",
        "outputId": "6141f4bf-a883-43e4-eca9-4a7604c6bfc1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jan  4 07:46:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUnMLdevEzFT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og-MDQc4hKL7",
        "outputId": "7fd4306c-5bef-441c-e717-10a53ac0a461"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtaBLMJ3j-ZT"
      },
      "source": [
        "!cp /content/drive/MyDrive/EVA4/ENDS9/CommonQA.zip ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL8N5uepkCD4",
        "outputId": "56b21640-928d-4367-c6ef-6a1aae5b471a"
      },
      "source": [
        "!unzip /content/CommonQA.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/CommonQA.zip\n",
            "   creating: CommonQA/\n",
            "  inflating: CommonQA/dev_rand_split.jsonl  \n",
            "  inflating: CommonQA/test_rand_split_no_answers.jsonl  \n",
            "  inflating: CommonQA/train_rand_split.jsonl  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzErHAsEzFU"
      },
      "source": [
        "Then set a random seed for deterministic results/reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_wP4J4LEzFX"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-7us2LRkpCh"
      },
      "source": [
        "read_file(\"/content/CommonQA/train_rand_split.jsonl\", \"dev\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Vn-t6Elk1u"
      },
      "source": [
        "sentence = []\r\n",
        "label = []\r\n",
        "with open('/content/CommonQA/train_rand_split.jsonl') as h:\r\n",
        "    for line in h:\r\n",
        "        example = json.loads(line)\r\n",
        "        scores = []\r\n",
        "        merged_choices = ' A: '.join([choice['text'] for choice in example['question']['choices']])\r\n",
        "        input = 'Q: ' + example['question']['stem'] + ' A: ' + merged_choices\r\n",
        "        correct_answer = [ choice['text'] for choice in example['question']['choices'] if choice['label'] == example['answerKey'] ][0]\r\n",
        "        sentence.append(input)\r\n",
        "        label.append(correct_answer)\r\n",
        "        #print(input, correct_answer)\r\n",
        "dataset_df = pd.DataFrame({'sentence':sentence, 'label':label})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "DphX8AeVXmCz",
        "outputId": "e831b382-0584-40aa-ea1e-760566f10c9f"
      },
      "source": [
        "dataset_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q: The sanctions against the school were a pun...</td>\n",
              "      <td>ignore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q: Sammy wanted to go to where the people were...</td>\n",
              "      <td>populated areas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q: To locate a choker not located in a jewelry...</td>\n",
              "      <td>jewelry store</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q: Google Maps and other highway and street GP...</td>\n",
              "      <td>atlas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q: The fox walked from the city into the fores...</td>\n",
              "      <td>natural habitat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9736</th>\n",
              "      <td>Q: What would someone need to do if he or she ...</td>\n",
              "      <td>telling all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9737</th>\n",
              "      <td>Q: Where might you find a chair at an office? ...</td>\n",
              "      <td>cubicle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9738</th>\n",
              "      <td>Q: Where would you buy jeans in a place with a...</td>\n",
              "      <td>shopping mall</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9739</th>\n",
              "      <td>Q: John fell down the well.  he couldn't belie...</td>\n",
              "      <td>fairytale</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9740</th>\n",
              "      <td>Q: I forgot to pay the electricity bill, now w...</td>\n",
              "      <td>produce heat</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9741 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence            label\n",
              "0     Q: The sanctions against the school were a pun...           ignore\n",
              "1     Q: Sammy wanted to go to where the people were...  populated areas\n",
              "2     Q: To locate a choker not located in a jewelry...    jewelry store\n",
              "3     Q: Google Maps and other highway and street GP...            atlas\n",
              "4     Q: The fox walked from the city into the fores...  natural habitat\n",
              "...                                                 ...              ...\n",
              "9736  Q: What would someone need to do if he or she ...      telling all\n",
              "9737  Q: Where might you find a chair at an office? ...          cubicle\n",
              "9738  Q: Where would you buy jeans in a place with a...    shopping mall\n",
              "9739  Q: John fell down the well.  he couldn't belie...        fairytale\n",
              "9740  Q: I forgot to pay the electricity bill, now w...     produce heat\n",
              "\n",
              "[9741 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGYiDLywFypD"
      },
      "source": [
        "#dataset = pd.read_csv(\"/content/dev.tsv\", sep='\\t', header=None)\r\n",
        "dataset = pd.read_csv(\"/content/qasc/train.tsv\", sep='\\t', header=None)\r\n",
        "dataset.columns = [\"sentence\", \"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-7zLo-ZyqwS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "d6fdbc27-ba7b-4590-d59d-f5a443412d64"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What type of water formation is formed by clou...</td>\n",
              "      <td>beads</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Where do beads of water come from? \\n (A) Too ...</td>\n",
              "      <td>Vapor turning into a liquid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What forms beads of water?  \\n (A) Necklaces. ...</td>\n",
              "      <td>Steam.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what kind of beads are formed from vapor conde...</td>\n",
              "      <td>h2o</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what kind of beads are formed by their vapor c...</td>\n",
              "      <td>h2o</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8129</th>\n",
              "      <td>Chitin can be used for protection by whom? \\n ...</td>\n",
              "      <td>Fish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8130</th>\n",
              "      <td>Which type of animal uses plates for protectio...</td>\n",
              "      <td>reptiles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8131</th>\n",
              "      <td>What are used for protection by fish? \\n (A) s...</td>\n",
              "      <td>scales</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8132</th>\n",
              "      <td>What are pangolins covered in? \\n (A) tunicate...</td>\n",
              "      <td>protection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8133</th>\n",
              "      <td>What are covered with protection? \\n (A) apple...</td>\n",
              "      <td>fish</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8134 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence                        label\n",
              "0     What type of water formation is formed by clou...                        beads\n",
              "1     Where do beads of water come from? \\n (A) Too ...  Vapor turning into a liquid\n",
              "2     What forms beads of water?  \\n (A) Necklaces. ...                       Steam.\n",
              "3     what kind of beads are formed from vapor conde...                          h2o\n",
              "4     what kind of beads are formed by their vapor c...                          h2o\n",
              "...                                                 ...                          ...\n",
              "8129  Chitin can be used for protection by whom? \\n ...                         Fish\n",
              "8130  Which type of animal uses plates for protectio...                     reptiles\n",
              "8131  What are used for protection by fish? \\n (A) s...                       scales\n",
              "8132  What are pangolins covered in? \\n (A) tunicate...                   protection\n",
              "8133  What are covered with protection? \\n (A) apple...                         fish\n",
              "\n",
              "[8134 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hawianoFEzFY"
      },
      "source": [
        "Instantiate our German and English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLSPj13L2yAr",
        "outputId": "c941595e-8203-4fe4-d033-fcb918664d1e"
      },
      "source": [
        "!python3 -m spacy download de_core_news_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 26.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907057 sha256=159fc9a1398616216f21ed4d1d5c7f9bac1ae3aba32ea1f5a513c4f3886181ab\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_7ehpzcl/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVVbBjSb3MO1"
      },
      "source": [
        "import de_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BP3YSvJEzFY"
      },
      "source": [
        "spacy_de =  de_core_news_sm.load()\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bEkyPt5EzFY"
      },
      "source": [
        "Previously we reversed the source (German) sentence, however in the paper we are implementing they don't do this, so neither will we."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KaGEZ45EzFZ"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7rbQLYHEzFZ"
      },
      "source": [
        "Create our fields to process our data. This will append the \"start of sentence\" and \"end of sentence\" tokens as well as converting all words to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sytzu5pTnSCa"
      },
      "source": [
        "from torchtext import data \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "Sentence = data.Field(sequential = True, tokenize = 'spacy', batch_first =False, include_lengths=False, lower=False)\r\n",
        "Label = data.Field(sequential =True, tokenize ='spacy', is_target=False, batch_first =False, include_lengths=False, lower=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDuSMFrQwrXC"
      },
      "source": [
        "dataset = dataset_df\r\n",
        "fields = [('sentence', Sentence),('label',Label)]\r\n",
        "example = [data.Example.fromlist([dataset.sentence[i],dataset.label[i]], fields) for i in range(dataset.shape[0])] \r\n",
        "commonqa_ds = data.Dataset(example, fields)\r\n",
        "(train, valid) = commonqa_ds.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97K5lZwYx0BB"
      },
      "source": [
        "Sentence.build_vocab(train)\r\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZlXYwYyyz46",
        "outputId": "78e6904b-5cac-4589-83ec-723c5014dbf4"
      },
      "source": [
        "print('Size of input vocab : ', len(Sentence.vocab))\r\n",
        "print('Size of label vocab : ', len(Label.vocab))\r\n",
        "print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\r\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  11736\n",
            "Size of label vocab :  3515\n",
            "Top 10 words appreared repeatedly : [(':', 49680), ('A', 41641), ('Q', 8280), ('?', 8228), ('to', 5004), ('a', 4761), ('the', 3593), ('what', 3290), (',', 3103), ('of', 2335)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7f340adfbea0>, {'<unk>': 0, '<pad>': 1, 'store': 2, 'of': 3, 'house': 4, 'to': 5, 'office': 6, \"'s\": 7, 'room': 8, 'city': 9, 'get': 10, 'money': 11, 'building': 12, 'school': 13, 'go': 14, 'have': 15, 'new': 16, 'in': 17, 'music': 18, 'water': 19, 'home': 20, 'down': 21, 'food': 22, 'feel': 23, 'being': 24, 'park': 25, 'area': 26, 'restaurant': 27, 'shop': 28, 'good': 29, 'fun': 30, 'game': 31, 'out': 32, 'kitchen': 33, 'own': 34, 'cabinet': 35, 'countryside': 36, 'death': 37, 'car': 38, 'getting': 39, 'make': 40, 'people': 41, 'work': 42, 'play': 43, 'table': 44, 'up': 45, 'band': 46, 'desk': 47, 'feeling': 48, 'for': 49, 'market': 50, 'michigan': 51, 'station': 52, 'better': 53, 'ground': 54, 'hotel': 55, 'pain': 56, 'show': 57, 'tired': 58, 'town': 59, 'apartment': 60, 'box': 61, 'knowledge': 62, 'supermarket': 63, 'cupboard': 64, 'drawer': 65, 'eat': 66, 'learn': 67, 'mall': 68, 'street': 69, 'bedroom': 70, 'classroom': 71, 'ocean': 72, 'refrigerator': 73, 'space': 74, 'sporting': 75, 'television': 76, 'bus': 77, 'fall': 78, 'public': 79, 'rest': 80, 'time': 81, 'airport': 82, 'at': 83, 'basement': 84, 'closet': 85, 'drink': 86, 'friend': 87, 'great': 88, 'happiness': 89, 'take': 90, 'toy': 91, 'book': 92, 'church': 93, 'falling': 94, 'going': 95, 'grocery': 96, 'jail': 97, 'large': 98, 'life': 99, 'meeting': 100, 'more': 101, 'movie': 102, 'neighbor': 103, 'state': 104, 'york': 105, 'become': 106, 'die': 107, 'earth': 108, 'gain': 109, 'hospital': 110, 'place': 111, 'states': 112, 'train': 113, 'america': 114, 'and': 115, 'bathroom': 116, 'front': 117, 'garage': 118, 'museum': 119, 'other': 120, 'person': 121, 'pocket': 122, 'shopping': 123, 'shuttle': 124, 'sleep': 125, 'theater': 126, 'things': 127, 'weight': 128, 'yard': 129, 'asleep': 130, 'big': 131, 'children': 132, 'country': 133, 'full': 134, 'high': 135, 'jar': 136, 'library': 137, 'outside': 138, 'shelf': 139, 'south': 140, 'stress': 141, 'united': 142, 'use': 143, 'about': 144, 'bad': 145, 'body': 146, 'california': 147, 'center': 148, 'department': 149, 'each': 150, 'england': 151, 'garden': 152, 'hard': 153, 'having': 154, 'living': 155, 'off': 156, 'pleasure': 157, 'stop': 158, 'with': 159, 'back': 160, 'bar': 161, 'bed': 162, 'cold': 163, 'goods': 164, 'health': 165, 'information': 166, 'line': 167, 'loss': 168, 'love': 169, 'not': 170, 'on': 171, 'open': 172, 'orchestra': 173, 'satisfaction': 174, 'sex': 175, 'think': 176, 'tree': 177, 'war': 178, 'wet': 179, 'art': 180, 'away': 181, 'backpack': 182, 'desktop': 183, 'disneyland': 184, 'door': 185, 'fear': 186, 'hockey': 187, 'north': 188, 'relaxation': 189, 'soccer': 190, 'surface': 191, 'theatre': 192, 'themselves': 193, 'alley': 194, 'anger': 195, 'bank': 196, 'boat': 197, 'class': 198, 'club': 199, 'coffee': 200, 'do': 201, 'experience': 202, 'forest': 203, 'gas': 204, 'happy': 205, 'hardware': 206, 'human': 207, 'injury': 208, 'medicine': 209, 'mexico': 210, 'military': 211, 'old': 212, 'outdoors': 213, 'see': 214, 'somewhere': 215, 'supply': 216, 'talk': 217, 'together': 218, 'university': 219, 'books': 220, 'canada': 221, 'computer': 222, 'concert': 223, 'event': 224, 'fast': 225, 'feelings': 226, 'fire': 227, 'fish': 228, 'frustration': 229, 'hands': 230, 'heart': 231, 'himself': 232, 'increased': 233, 'laboratory': 234, 'mail': 235, 'meet': 236, 'or': 237, 'pantry': 238, 'pass': 239, 'pool': 240, 'sea': 241, 'study': 242, 'suitcase': 243, 'wait': 244, 'yourself': 245, 'zoo': 246, 'ask': 247, 'buy': 248, 'cellar': 249, 'communication': 250, 'container': 251, 'corner': 252, 'depot': 253, 'dinner': 254, 'downtown': 255, 'energy': 256, 'fight': 257, 'floor': 258, 'furniture': 259, 'head': 260, 'legs': 261, 'listen': 262, 'live': 263, 'mouth': 264, 'opera': 265, 'pay': 266, 'produce': 267, 'race': 268, 'sky': 269, 'smile': 270, 'still': 271, 'two': 272, 'warm': 273, 'write': 274, 'you': 275, 'air': 276, 'base': 277, 'bowling': 278, 'case': 279, 'college': 280, 'comfortable': 281, 'control': 282, 'demonstration': 283, 'exhaustion': 284, 'fairgrounds': 285, 'fatigue': 286, 'field': 287, 'fountain': 288, 'free': 289, 'friends': 290, 'headaches': 291, 'lake': 292, 'learning': 293, 'losing': 294, 'machine': 295, 'many': 296, 'movies': 297, 'problems': 298, 'purse': 299, 'read': 300, 'river': 301, 'satisfied': 302, 'say': 303, 'ship': 304, 'stomach': 305, 'swimming': 306, 'very': 307, 'working': 308, 'american': 309, 'anxiety': 310, 'beach': 311, 'breath': 312, 'bridge': 313, 'business': 314, 'cafe': 315, 'clear': 316, 'clothes': 317, 'conference': 318, 'construction': 319, 'cool': 320, 'cup': 321, 'doctor': 322, 'drug': 323, 'early': 324, 'express': 325, 'eyes': 326, 'family': 327, 'farmer': 328, 'fridge': 329, 'grow': 330, 'hall': 331, 'joy': 332, 'lie': 333, 'lower': 334, 'motel': 335, 'mountains': 336, 'need': 337, 'newspaper': 338, 'post': 339, 'roof': 340, 'stadium': 341, 'system': 342, 'texas': 343, 'track': 344, 'turn': 345, 'universe': 346, 'urban': 347, 'wall': 348, 'well': 349, 'amusement': 350, 'backyard': 351, 'bag': 352, 'board': 353, 'breathe': 354, 'brown': 355, 'change': 356, 'community': 357, 'court': 358, 'cross': 359, 'deep': 360, 'enjoy': 361, 'enjoyment': 362, 'entertainment': 363, 'every': 364, 'everything': 365, 'excitement': 366, 'feet': 367, 'fly': 368, 'freedom': 369, 'from': 370, 'fruit': 371, 'headache': 372, 'injuries': 373, 'inspiration': 374, 'internet': 375, 'light': 376, 'livingroom': 377, 'lose': 378, 'lot': 379, 'making': 380, 'muscles': 381, 'no': 382, 'one': 383, 'paper': 384, 'pet': 385, 'pizza': 386, 'plate': 387, 'porch': 388, 'pride': 389, 'prison': 390, 'problem': 391, 'punishment': 392, 'questions': 393, 'repair': 394, 'rock': 395, 'rug': 396, 'rural': 397, 'sentence': 398, 'set': 399, 'sit': 400, 'solar': 401, 'sports': 402, 'stay': 403, 'sweating': 404, 'symphony': 405, 'synagogue': 406, 'tool': 407, 'trouble': 408, 'trunk': 409, 'understand': 410, 'washington': 411, 'will': 412, 'win': 413, \"'re\": 414, 'africa': 415, 'airplane': 416, 'atlantic': 417, 'atlas': 418, 'attic': 419, 'auditorium': 420, 'becoming': 421, 'bones': 422, 'bookstore': 423, 'bored': 424, 'broken': 425, 'care': 426, 'carnival': 427, 'carolina': 428, 'carpet': 429, 'cause': 430, 'chess': 431, 'chest': 432, 'commercial': 433, 'compete': 434, 'depression': 435, 'detroit': 436, 'dishes': 437, 'dream': 438, 'dry': 439, 'east': 440, 'end': 441, 'entertained': 442, 'eye': 443, 'fishing': 444, 'form': 445, 'freeway': 446, 'french': 447, 'georgia': 448, 'god': 449, 'guilt': 450, 'guilty': 451, 'hair': 452, 'hand': 453, 'hear': 454, 'help': 455, 'hold': 456, 'houses': 457, 'hurt': 458, 'ideas': 459, 'laugh': 460, 'leave': 461, 'like': 462, 'london': 463, 'louisiana': 464, 'manhattan': 465, 'marching': 466, 'middle': 467, 'montana': 468, 'mountain': 469, 'move': 470, 'moving': 471, 'natural': 472, 'nature': 473, 'noise': 474, 'northern': 475, 'over': 476, 'panic': 477, 'party': 478, 'peace': 479, 'physical': 480, 'pile': 481, 'plants': 482, 'question': 483, 'radio': 484, 'relax': 485, 'road': 486, 'run': 487, 'safe': 488, 'san': 489, 'sing': 490, 'site': 491, 'sitting': 492, 'skill': 493, 'skin': 494, 'smoke': 495, 'sorrow': 496, 'speak': 497, 'spend': 498, 'stage': 499, 'stand': 500, 'strip': 501, 'suffering': 502, 'sun': 503, 'sweat': 504, 'tall': 505, 'thinking': 506, 'travel': 507, 'watch': 508, 'wear': 509, 'windowsill': 510, 'accomplishment': 511, 'aircraft': 512, 'all': 513, 'angry': 514, 'antique': 515, 'appear': 516, 'arrested': 517, 'attacks': 518, 'attention': 519, 'australia': 520, 'basket': 521, 'battlefield': 522, 'blood': 523, 'boredom': 524, 'bread': 525, 'catch': 526, 'circus': 527, 'cities': 528, 'clean': 529, 'cleanliness': 530, 'come': 531, 'complete': 532, 'complex': 533, 'concentrate': 534, 'contact': 535, 'cost': 536, 'crime': 537, 'dance': 538, 'dirty': 539, 'dog': 540, 'drugstore': 541, 'education': 542, 'enemy': 543, 'enlightenment': 544, 'expensive': 545, 'fabric': 546, 'factory': 547, 'fat': 548, 'find': 549, 'flower': 550, 'flowers': 551, 'four': 552, 'further': 553, 'gaining': 554, 'hang': 555, 'healthy': 556, 'heat': 557, 'history': 558, 'interesting': 559, 'keep': 560, 'lab': 561, 'late': 562, 'lay': 563, 'let': 564, 'listening': 565, 'look': 566, 'maine': 567, 'meat': 568, 'might': 569, 'motion': 570, 'national': 571, 'nausea': 572, 'neighborhood': 573, 'nice': 574, 'night': 575, 'nothing': 576, 'ohio': 577, 'oven': 578, 'pacific': 579, 'parking': 580, 'pharmacy': 581, 'phone': 582, 'plant': 583, 'pot': 584, 'practice': 585, 'prepare': 586, 'pressure': 587, 'rain': 588, 'raise': 589, 'remorse': 590, 'reproduce': 591, 'roadblock': 592, 'save': 593, 'shock': 594, 'sink': 595, 'small': 596, 'sore': 597, 'spending': 598, 'story': 599, 'studio': 600, 'subway': 601, 'suffer': 602, 'summer': 603, 'surf': 604, 'talking': 605, 'throw': 606, 'ticket': 607, 'tiredness': 608, 'transportation': 609, 'truth': 610, 'understanding': 611, 'utah': 612, 'victory': 613, 'voice': 614, 'wine': 615, 'world': 616, 'wrong': 617, 'able': 618, 'accident': 619, 'age': 620, 'ahead': 621, 'animal': 622, 'another': 623, 'answer': 624, 'anything': 625, 'anywhere': 626, 'areas': 627, 'arena': 628, 'arm': 629, 'asia': 630, 'attempt': 631, 'awake': 632, 'bankruptcy': 633, 'battle': 634, 'bay': 635, 'beer': 636, 'believe': 637, 'birth': 638, 'booth': 639, 'bowl': 640, 'brain': 641, 'breathing': 642, 'bring': 643, 'can': 644, 'carry': 645, 'cars': 646, 'celebrate': 647, 'centre': 648, 'chair': 649, 'close': 650, 'clothing': 651, 'commit': 652, 'company': 653, 'continent': 654, 'controversy': 655, 'corn': 656, 'course': 657, 'cow': 658, 'crash': 659, 'create': 660, 'dangerous': 661, 'day': 662, 'dentist': 663, 'different': 664, 'direct': 665, 'discomfort': 666, 'dress': 667, 'drive': 668, 'drop': 669, 'electric': 670, 'embarrassment': 671, 'emergency': 672, 'europe': 673, 'exercise': 674, 'facts': 675, 'fairy': 676, 'farm': 677, 'francisco': 678, 'freezer': 679, 'games': 680, 'give': 681, 'glass': 682, 'government': 683, 'grade': 684, 'group': 685, 'gymnasium': 686, 'hallway': 687, 'hampshire': 688, 'heartburn': 689, 'heavy': 690, 'higher': 691, 'hot': 692, 'hunger': 693, 'hungry': 694, 'ice': 695, 'illness': 696, 'important': 697, 'instrument': 698, 'intelligence': 699, 'interest': 700, 'japan': 701, 'kentucky': 702, 'kiss': 703, 'kit': 704, 'know': 705, 'laughter': 706, 'law': 707, 'legal': 708, 'less': 709, 'linen': 710, 'lobby': 711, 'locker': 712, 'magazine': 713, 'marry': 714, 'math': 715, 'meadow': 716, 'mind': 717, 'mood': 718, 'most': 719, 'much': 720, 'net': 721, 'nightclub': 722, 'notebook': 723, 'order': 724, 'piano': 725, 'pick': 726, 'places': 727, 'pond': 728, 'populated': 729, 'pregnancy': 730, 'press': 731, 'prices': 732, 'put': 733, 'rate': 734, 'relief': 735, 'salon': 736, 'science': 737, 'serve': 738, 'shake': 739, 'share': 740, 'shed': 741, 'shower': 742, 'slow': 743, 'smiling': 744, 'soap': 745, 'social': 746, 'socialize': 747, 'society': 748, 'soft': 749, 'sound': 750, 'soup': 751, 'spare': 752, 'strength': 753, 'sunny': 754, 'teach': 755, 'telephone': 756, 'tension': 757, 'traffic': 758, 'tropical': 759, 'truck': 760, 'unpredictable': 761, 'valley': 762, 'video': 763, 'virginia': 764, 'walk': 765, 'wash': 766, 'washing': 767, 'waterfall': 768, 'wedding': 769, 'welcome': 770, 'were': 771, 'west': 772, 'western': 773, 'white': 774, 'wild': 775, 'winning': 776, 'yellow': 777, 'accidents': 778, 'accomplish': 779, 'act': 780, 'adequate': 781, 'against': 782, 'aggression': 783, 'alive': 784, 'amount': 785, 'ancient': 786, 'answers': 787, 'army': 788, 'around': 789, 'arrest': 790, 'arrive': 791, 'arthritis': 792, 'atmosphere': 793, 'barn': 794, 'basin': 795, 'basketball': 796, 'be': 797, 'bedside': 798, 'begin': 799, 'bills': 800, 'bleeding': 801, 'blisters': 802, 'blue': 803, 'bottle': 804, 'branch': 805, 'briefcase': 806, 'bright': 807, 'cage': 808, 'calm': 809, 'campus': 810, 'cancer': 811, 'candy': 812, 'capital': 813, 'captivity': 814, 'card': 815, 'cards': 816, 'carpal': 817, 'cash': 818, 'casino': 819, 'cast': 820, 'central': 821, 'chemistry': 822, 'chicken': 823, 'china': 824, 'choice': 825, 'christmas': 826, 'climate': 827, 'climb': 828, 'collection': 829, 'colorado': 830, 'common': 831, 'communicate': 832, 'competition': 833, 'competitiveness': 834, 'compost': 835, 'conclusion': 836, 'confession': 837, 'confusion': 838, 'contempt': 839, 'contract': 840, 'conversation': 841, 'cook': 842, 'cooler': 843, 'coral': 844, 'cough': 845, 'countries': 846, 'cramps': 847, 'creation': 848, 'deck': 849, 'delicious': 850, 'deny': 851, 'deposit': 852, 'depressed': 853, 'desert': 854, 'destroy': 855, 'dictionary': 856, 'dining': 857, 'disease': 858, 'dishwasher': 859, 'distress': 860, 'district': 861, 'done': 862, 'doors': 863, 'doorway': 864, 'drinking': 865, 'driving': 866, 'drunk': 867, 'drunkenness': 868, 'earn': 869, 'ears': 870, 'eating': 871, 'effort': 872, 'email': 873, 'emotion': 874, 'encyclopedia': 875, 'erections': 876, 'exchange': 877, 'excited': 878, 'exhibits': 879, 'expect': 880, 'expectations': 881, 'expense': 882, 'experiences': 883, 'faces': 884, 'fair': 885, 'film': 886, 'flat': 887, 'florida': 888, 'football': 889, 'foreign': 890, 'forgetfulness': 891, 'france': 892, 'fresh': 893, 'friendly': 894, 'friendships': 895, 'gardens': 896, 'germany': 897, 'gifts': 898, 'goodbye': 899, 'grass': 900, 'green': 901, 'growth': 902, 'gulf': 903, 'habitat': 904, 'hangover': 905, 'heaven': 906, 'hell': 907, 'hemisphere': 908, 'homework': 909, 'hurry': 910, 'idaho': 911, 'ignore': 912, 'imagination': 913, 'imprisoned': 914, 'increase': 915, 'into': 916, 'intoxication': 917, 'irritation': 918, 'jazz': 919, 'jersey': 920, 'job': 921, 'judge': 922, 'justice': 923, 'king': 924, 'known': 925, 'labyrinth': 926, 'laid': 927, 'land': 928, 'landfill': 929, 'language': 930, 'las': 931, 'laundromat': 932, 'lawn': 933, 'laziness': 934, 'left': 935, 'leg': 936, 'letter': 937, 'longer': 938, 'looking': 939, 'loud': 940, 'main': 941, 'man': 942, 'manipulate': 943, 'mansion': 944, 'map': 945, 'mass': 946, 'may': 947, 'medical': 948, 'meditate': 949, 'minnesota': 950, 'mistakes': 951, 'misunderstanding': 952, 'misunderstandings': 953, 'mountainous': 954, 'musical': 955, 'nest': 956, 'news': 957, 'nose': 958, 'notes': 959, 'nursery': 960, 'nursing': 961, 'obesity': 962, 'offices': 963, 'older': 964, 'opening': 965, 'opinions': 966, 'orbit': 967, 'orchard': 968, 'orgasm': 969, 'others': 970, 'pages': 971, 'painting': 972, 'palace': 973, 'part': 974, 'patience': 975, 'pennsylvania': 976, 'petting': 977, 'plan': 978, 'planet': 979, 'playground': 980, 'playing': 981, 'police': 982, 'poor': 983, 'power': 984, 'pretty': 985, 'progress': 986, 'property': 987, 'rates': 988, 'receive': 989, 'record': 990, 'red': 991, 'reef': 992, 'register': 993, 'regret': 994, 'reject': 995, 'relationship': 996, 'relaxing': 997, 'report': 998, 'research': 999, 'resentment': 1000, 'residential': 1001, 'resort': 1002, 'respect': 1003, 'retail': 1004, 'retribution': 1005, 'ride': 1006, 'rome': 1007, 'row': 1008, 'rush': 1009, 'sadness': 1010, 'salad': 1011, 'salt': 1012, 'sand': 1013, 'sense': 1014, 'separate': 1015, 'service': 1016, 'settle': 1017, 'sewer': 1018, 'sexual': 1019, 'shadow': 1020, 'shape': 1021, 'shelter': 1022, 'shirt': 1023, 'shoes': 1024, 'side': 1025, 'sleeping': 1026, 'smiles': 1027, 'southern': 1028, 'special': 1029, 'stairs': 1030, 'standing': 1031, 'stopping': 1032, 'stories': 1033, 'straight': 1034, 'stream': 1035, 'strong': 1036, 'stupidity': 1037, 'subdivision': 1038, 'submarine': 1039, 'suburb': 1040, 'suburbs': 1041, 'suicide': 1042, 'sunshine': 1043, 'syndrome': 1044, 'tale': 1045, 'tennessee': 1046, 'terrible': 1047, 'there': 1048, 'thin': 1049, 'thought': 1050, 'too': 1051, 'treasure': 1052, 'trial': 1053, 'tunnel': 1054, 'tv': 1055, 'underground': 1056, 'understood': 1057, 'underwater': 1058, 'upright': 1059, 'using': 1060, 'vegas': 1061, 'vegetable': 1062, 'vegetables': 1063, 'vomiting': 1064, 'walking': 1065, 'wallet': 1066, 'warehouse': 1067, 'wave': 1068, 'weakness': 1069, 'wealth': 1070, 'weather': 1071, 'web': 1072, 'wheel': 1073, 'wilderness': 1074, 'wings': 1075, 'wonder': 1076, 'woods': 1077, 'words': 1078, 'workplace': 1079, 'written': 1080, 'years': 1081, \"'ll\": 1082, 'abandoned': 1083, 'accelerate': 1084, 'accidental': 1085, 'accomplishing': 1086, 'acquire': 1087, 'acting': 1088, 'action': 1089, 'active': 1090, 'added': 1091, 'addiction': 1092, 'admiration': 1093, 'admire': 1094, 'afternoon': 1095, 'aggravation': 1096, 'agreement': 1097, 'aid': 1098, 'alcohol': 1099, 'allergies': 1100, 'alone': 1101, 'along': 1102, 'amazon': 1103, 'animals': 1104, 'ante': 1105, 'apart': 1106, 'appliance': 1107, 'apply': 1108, 'aquarium': 1109, 'arcade': 1110, 'arctic': 1111, 'argument': 1112, 'arguments': 1113, 'arkansas': 1114, 'arms': 1115, 'arrangement': 1116, 'arriving': 1117, 'article': 1118, 'articulate': 1119, 'artifacts': 1120, 'artificial': 1121, 'artist': 1122, 'ashtray': 1123, 'asthma': 1124, 'attractive': 1125, 'authority': 1126, 'automobile': 1127, 'babies': 1128, 'backache': 1129, 'baking': 1130, 'ball': 1131, 'ballpark': 1132, 'barbeque': 1133, 'barber': 1134, 'bathe': 1135, 'battleship': 1136, 'beautiful': 1137, 'behavior': 1138, 'best': 1139, 'betrayal': 1140, 'bible': 1141, 'bicycle': 1142, 'black': 1143, 'blaring': 1144, 'bloom': 1145, 'boardroom': 1146, 'bodies': 1147, 'botanical': 1148, 'brass': 1149, 'break': 1150, 'breakfast': 1151, 'breathlessness': 1152, 'british': 1153, 'broadcast': 1154, 'broom': 1155, 'brother': 1156, 'buildings': 1157, 'burn': 1158, 'bush': 1159, 'busy': 1160, 'buying': 1161, 'cafeteria': 1162, 'calluses': 1163, 'camp': 1164, 'canon': 1165, 'cape': 1166, 'careful': 1167, 'castle': 1168, 'cat': 1169, 'catching': 1170, 'cathedral': 1171, 'cave': 1172, 'cease': 1173, 'celebrating': 1174, 'ceremony': 1175, 'charming': 1176, 'chicago': 1177, 'child': 1178, 'chinatown': 1179, 'clap': 1180, 'clerk': 1181, 'clock': 1182, 'closed': 1183, 'closing': 1184, 'cloudy': 1185, 'code': 1186, 'colors': 1187, 'columbia': 1188, 'coma': 1189, 'comfort': 1190, 'coming': 1191, 'communicating': 1192, 'compare': 1193, 'comparison': 1194, 'compassion': 1195, 'composted': 1196, 'congress': 1197, 'connecticut': 1198, 'conquer': 1199, 'conscience': 1200, 'construct': 1201, 'continue': 1202, 'cooking': 1203, 'coop': 1204, 'correct': 1205, 'county': 1206, 'cover': 1207, 'criminal': 1208, 'crops': 1209, 'crowd': 1210, 'crown': 1211, 'cry': 1212, 'dairy': 1213, 'dakota': 1214, 'dancing': 1215, 'dead': 1216, 'deceive': 1217, 'decisions': 1218, 'decomposition': 1219, 'deflation': 1220, 'demand': 1221, 'dense': 1222, 'desire': 1223, 'despair': 1224, 'destruction': 1225, 'detestable': 1226, 'diego': 1227, 'disaster': 1228, 'discount': 1229, 'discovery': 1230, 'disorientation': 1231, 'distance': 1232, 'ditch': 1233, 'diverse': 1234, 'doing': 1235, 'doze': 1236, 'draw': 1237, 'dreams': 1238, 'dressing': 1239, 'driveway': 1240, 'drugs': 1241, 'dump': 1242, 'dwarf': 1243, 'dying': 1244, 'eastern': 1245, 'easy': 1246, 'economic': 1247, 'eden': 1248, 'efficiency': 1249, 'egg': 1250, 'eggs': 1251, 'elevator': 1252, 'embarassment': 1253, 'embrace': 1254, 'emotional': 1255, 'engine': 1256, 'english': 1257, 'enjoyable': 1258, 'enough': 1259, 'enter': 1260, 'entertain': 1261, 'entertaining': 1262, 'estate': 1263, 'euphoria': 1264, 'exceptional': 1265, 'exchanging': 1266, 'exertion': 1267, 'exist': 1268, 'expected': 1269, 'expression': 1270, 'failure': 1271, 'fairytale': 1272, 'faithful': 1273, 'far': 1274, 'fart': 1275, 'favorite': 1276, 'female': 1277, 'fields': 1278, 'fights': 1279, 'filing': 1280, 'firm': 1281, 'first': 1282, 'fitness': 1283, 'fitting': 1284, 'flatulence': 1285, 'fleas': 1286, 'floral': 1287, 'flowing': 1288, 'follow': 1289, 'foot': 1290, 'forbid': 1291, 'forget': 1292, 'forrest': 1293, 'forward': 1294, 'found': 1295, 'fragile': 1296, 'frame': 1297, 'fraternity': 1298, 'freight': 1299, 'friendship': 1300, 'fright': 1301, 'frightened': 1302, 'frustrated': 1303, 'fullness': 1304, 'funding': 1305, 'funeral': 1306, 'funny': 1307, 'galaxy': 1308, 'gap': 1309, 'garbage': 1310, 'gather': 1311, 'gathering': 1312, 'gear': 1313, 'generous': 1314, 'geometry': 1315, 'glee': 1316, 'golf': 1317, 'grades': 1318, 'guard': 1319, 'guitar': 1320, 'gym': 1321, 'hallucination': 1322, 'happen': 1323, 'harmful': 1324, 'harsh': 1325, 'heard': 1326, 'helicopter': 1327, 'hello': 1328, 'herself': 1329, 'hole': 1330, 'holster': 1331, 'homeschool': 1332, 'honor': 1333, 'horrible': 1334, 'horseradish': 1335, 'horses': 1336, 'hour': 1337, 'humdrum': 1338, 'hunt': 1339, 'hunting': 1340, 'illinois': 1341, 'illiterate': 1342, 'improve': 1343, 'improved': 1344, 'incompetent': 1345, 'inhale': 1346, 'injured': 1347, 'ink': 1348, 'innocence': 1349, 'insight': 1350, 'insights': 1351, 'insomnia': 1352, 'inspiring': 1353, 'investigation': 1354, 'istanbul': 1355, 'italy': 1356, 'jealousy': 1357, 'jeans': 1358, 'jerusalem': 1359, 'jewelry': 1360, 'jewish': 1361, 'join': 1362, 'jump': 1363, 'jungle': 1364, 'k': 1365, 'kansas': 1366, 'karma': 1367, 'kill': 1368, 'killing': 1369, 'lack': 1370, 'landscape': 1371, 'larger': 1372, 'last': 1373, 'latin': 1374, 'laughing': 1375, 'laundry': 1376, 'lawyer': 1377, 'lessons': 1378, 'lies': 1379, 'lights': 1380, 'liquid': 1381, 'liquor': 1382, 'little': 1383, 'lives': 1384, 'lodge': 1385, 'loft': 1386, 'lonely': 1387, 'long': 1388, 'loose': 1389, 'loved': 1390, 'low': 1391, 'lunch': 1392, 'magazines': 1393, 'major': 1394, 'males': 1395, 'mammals': 1396, 'mandatory': 1397, 'maritime': 1398, 'mart': 1399, 'mean': 1400, 'meditation': 1401, 'memories': 1402, 'memory': 1403, 'men': 1404, 'meow': 1405, 'metro': 1406, 'miracles': 1407, 'misery': 1408, 'mississippi': 1409, 'mix': 1410, 'moon': 1411, 'motorbike': 1412, 'motorboat': 1413, 'motorcycle': 1414, 'movement': 1415, 'mulberry': 1416, 'names': 1417, 'near': 1418, 'neatness': 1419, 'needle': 1420, 'netherlands': 1421, 'nevada': 1422, 'nightmares': 1423, 'novel': 1424, 'object': 1425, 'objects': 1426, 'offer': 1427, 'oklahoma': 1428, 'online': 1429, 'opponent': 1430, 'orders': 1431, 'ore': 1432, 'oregon': 1433, 'organization': 1434, 'orleans': 1435, 'outrage': 1436, 'overspending': 1437, 'oxygen': 1438, 'paid': 1439, 'palpitations': 1440, 'panel': 1441, 'paris': 1442, 'past': 1443, 'patient': 1444, 'paying': 1445, 'payment': 1446, 'pencil': 1447, 'perspiration': 1448, 'picture': 1449, 'pictures': 1450, 'piggy': 1451, 'plain': 1452, 'plane': 1453, 'player': 1454, 'pleasant': 1455, 'plug': 1456, 'point': 1457, 'poisonous': 1458, 'poker': 1459, 'polite': 1460, 'pollution': 1461, 'popularity': 1462, 'port': 1463, 'possessing': 1464, 'pregnant': 1465, 'prepared': 1466, 'pretend': 1467, 'procreate': 1468, 'program': 1469, 'promise': 1470, 'putting': 1471, 'puzzle': 1472, 'quartet': 1473, 'quickly': 1474, 'rainforest': 1475, 'rainy': 1476, 'ranch': 1477, 'range': 1478, 'reach': 1479, 'real': 1480, 'realization': 1481, 'reason': 1482, 'recorded': 1483, 'recording': 1484, 'recycled': 1485, 'refineries': 1486, 'refreshed': 1487, 'region': 1488, 'release': 1489, 'religion': 1490, 'remember': 1491, 'reproduction': 1492, 'response': 1493, 'responsibility': 1494, 'restaurants': 1495, 'restroom': 1496, 'retaliation': 1497, 'reverse': 1498, 'revolution': 1499, 'rice': 1500, 'rich': 1501, 'rink': 1502, 'riots': 1503, 'rivalry': 1504, 'roadsides': 1505, 'rocky': 1506, 'rod': 1507, 'rope': 1508, 'running': 1509, 'russia': 1510, 'sad': 1511, 'satisfy': 1512, 'schools': 1513, 'score': 1514, 'sears': 1515, 'seattle': 1516, 'secret': 1517, 'seeing': 1518, 'sell': 1519, 'selling': 1520, 'sensible': 1521, 'serving': 1522, 'settlement': 1523, 'severe': 1524, 'sharing': 1525, 'shops': 1526, 'shore': 1527, 'shortness': 1528, 'shout': 1529, 'sick': 1530, 'sickness': 1531, 'sign': 1532, 'sincere': 1533, 'single': 1534, 'situations': 1535, 'skiing': 1536, 'skinny': 1537, 'skyscraper': 1538, 'slapped': 1539, 'slavery': 1540, 'sleepiness': 1541, 'sleepy': 1542, 'slender': 1543, 'slim': 1544, 'slot': 1545, 'smart': 1546, 'smell': 1547, 'soda': 1548, 'solitude': 1549, 'solution': 1550, 'solve': 1551, 'son': 1552, 'song': 1553, 'songs': 1554, 'sores': 1555, 'soundly': 1556, 'southwest': 1557, 'spain': 1558, 'sparse': 1559, 'speaking': 1560, 'speed': 1561, 'speeding': 1562, 'spot': 1563, 'spouse': 1564, 'spread': 1565, 'spring': 1566, 'staggering': 1567, 'stairwell': 1568, 'staying': 1569, 'steam': 1570, 'step': 1571, 'stew': 1572, 'stock': 1573, 'stores': 1574, 'strain': 1575, 'stretch': 1576, 'string': 1577, 'student': 1578, 'students': 1579, 'stumbling': 1580, 'stupid': 1581, 'suburbia': 1582, 'success': 1583, 'suddenly': 1584, 'suit': 1585, 'swim': 1586, 'switch': 1587, 'switzerland': 1588, 'taste': 1589, 'tasty': 1590, 'team': 1591, 'tears': 1592, 'tell': 1593, 'temperate': 1594, 'tennis': 1595, 'tensions': 1596, 'tent': 1597, 'terrarium': 1598, 'text': 1599, 'thank': 1600, 'then': 1601, 'thick': 1602, 'throwing': 1603, 'thumb': 1604, 'tidal': 1605, 'tomb': 1606, 'tops': 1607, 'torso': 1608, 'tow': 1609, 'towns': 1610, 'toys': 1611, 'trade': 1612, 'trap': 1613, 'trash': 1614, 'trashcan': 1615, 'treated': 1616, 'trees': 1617, 'trip': 1618, 'triple': 1619, 'tropics': 1620, 'trust': 1621, 'try': 1622, 'trying': 1623, 'unconventional': 1624, 'union': 1625, 'unique': 1626, 'unobservant': 1627, 'upset': 1628, 'used': 1629, 'useless': 1630, 'utility': 1631, 'vengeance': 1632, 'vessel': 1633, 'village': 1634, 'visual': 1635, 'wage': 1636, 'waiting': 1637, 'wake': 1638, 'want': 1639, 'warmth': 1640, 'waste': 1641, 'weapons': 1642, 'wellness': 1643, 'whisper': 1644, 'wimbledon': 1645, 'winch': 1646, 'window': 1647, 'winery': 1648, 'winter': 1649, 'wishing': 1650, 'wood': 1651, 'worthy': 1652, 'wyoming': 1653, 'zero': 1654, \"'m\": 1655, '1': 1656, '100': 1657, 'abandon': 1658, 'abdominal': 1659, 'ability': 1660, 'above': 1661, 'absolution': 1662, 'abundant': 1663, 'acceptance': 1664, 'account': 1665, 'ache': 1666, 'aches': 1667, 'achromatic': 1668, 'activity': 1669, 'adore': 1670, 'adrenaline': 1671, 'adults': 1672, 'advance': 1673, 'advantageous': 1674, 'adverse': 1675, 'advertisement': 1676, 'advisory': 1677, 'advocate': 1678, 'aeroplane': 1679, 'affection': 1680, 'affluent': 1681, 'afghanistan': 1682, 'african': 1683, 'aged': 1684, 'agency': 1685, 'ages': 1686, 'agility': 1687, 'agitated': 1688, 'agitation': 1689, 'agony': 1690, 'agree': 1691, 'agreeable': 1692, 'aground': 1693, 'aids': 1694, 'airports': 1695, 'albums': 1696, 'alcatraz': 1697, 'alcoholism': 1698, 'alien': 1699, 'alienate': 1700, 'alimentary': 1701, 'almost': 1702, 'alpenstock': 1703, 'already': 1704, 'altruistic': 1705, 'am': 1706, 'amazement': 1707, 'ambulance': 1708, 'amnesia': 1709, 'amok': 1710, 'amsterdam': 1711, 'angeles': 1712, 'animated': 1713, 'ankle': 1714, 'annoy': 1715, 'annoyance': 1716, 'answering': 1717, 'anterior': 1718, 'antidote': 1719, 'antipathy': 1720, 'anxious': 1721, 'apathy': 1722, 'apiary': 1723, 'apparatus': 1724, 'appearance': 1725, 'applaud': 1726, 'applause': 1727, 'apple': 1728, 'applied': 1729, 'appreciate': 1730, 'appreciated': 1731, 'approval': 1732, 'architect': 1733, 'architecture': 1734, 'ardor': 1735, 'argentina': 1736, 'arithmetic': 1737, 'arizona': 1738, 'arlington': 1739, 'armed': 1740, 'armies': 1741, 'armor': 1742, 'armored': 1743, 'armpits': 1744, 'array': 1745, 'artillery': 1746, 'arts': 1747, 'ascending': 1748, 'asexually': 1749, 'ashamed': 1750, 'asking': 1751, 'aspect': 1752, 'assembly': 1753, 'assistance': 1754, 'atheism': 1755, 'atheist': 1756, 'attached': 1757, 'attack': 1758, 'attacked': 1759, 'attempting': 1760, 'attend': 1761, 'attraction': 1762, 'aunt': 1763, 'aunty': 1764, 'austral': 1765, 'australian': 1766, 'authentic': 1767, 'available': 1768, 'aviary': 1769, 'avoid': 1770, 'awards': 1771, 'awe': 1772, 'awesome': 1773, 'baby': 1774, 'backward': 1775, 'backwards': 1776, 'backyards': 1777, 'baggage': 1778, 'bags': 1779, 'bake': 1780, 'bakery': 1781, 'balance': 1782, 'balloon': 1783, 'bamboo': 1784, 'banjo': 1785, 'banking': 1786, 'banned': 1787, 'barbers': 1788, 'bark': 1789, 'barrel': 1790, 'barrier': 1791, 'baseball': 1792, 'basic': 1793, 'bass': 1794, 'bate': 1795, 'bath': 1796, 'beans': 1797, 'beast': 1798, 'beautifull': 1799, 'bedsores': 1800, 'bee': 1801, 'beef': 1802, 'beehive': 1803, 'before': 1804, 'beginning': 1805, 'behind': 1806, 'belief': 1807, 'believed': 1808, 'belittle': 1809, 'belligerent': 1810, 'bells': 1811, 'belong': 1812, 'belts': 1813, 'benefit': 1814, 'bermuda': 1815, 'berries': 1816, 'between': 1817, 'bigger': 1818, 'bind': 1819, 'binder': 1820, 'biology': 1821, 'bird': 1822, 'birthday': 1823, 'bit': 1824, 'bite': 1825, 'bitter': 1826, 'bitterness': 1827, 'bladders': 1828, 'bland': 1829, 'blank': 1830, 'bless': 1831, 'bliss': 1832, 'blizzard': 1833, 'block': 1834, 'bloodshot': 1835, 'blooms': 1836, 'blossom': 1837, 'blotter': 1838, 'blow': 1839, 'blowing': 1840, 'blues': 1841, 'blunt': 1842, 'boats': 1843, 'boiling': 1844, 'bomb': 1845, 'bonding': 1846, 'boom': 1847, 'booze': 1848, 'born': 1849, 'boss': 1850, 'botanic': 1851, 'bottom': 1852, 'bound': 1853, 'bouquet': 1854, 'boutique': 1855, 'boxes': 1856, 'boy': 1857, 'brainless': 1858, 'brains': 1859, 'brainy': 1860, 'brassiere': 1861, 'brave': 1862, 'brazil': 1863, 'breaking': 1864, 'bribe': 1865, 'britain': 1866, 'broke': 1867, 'brook': 1868, 'bruise': 1869, 'brush': 1870, 'buffet': 1871, 'build': 1872, 'bulgaria': 1873, 'bundle': 1874, 'burger': 1875, 'burlap': 1876, 'burns': 1877, 'burp': 1878, 'burrow': 1879, 'bushes': 1880, 'businesses': 1881, 'butcher': 1882, 'butt': 1883, 'buzz': 1884, 'ca': 1885, 'cab': 1886, 'cabin': 1887, 'cactus': 1888, 'cakewalk': 1889, 'calcium': 1890, 'calculus': 1891, 'calmness': 1892, 'cambodia': 1893, 'camera': 1894, 'canal': 1895, 'candies': 1896, 'candle': 1897, 'canopy': 1898, 'canteen': 1899, 'canter': 1900, 'capillaries': 1901, 'capsule': 1902, 'carefree': 1903, 'carefully': 1904, 'cargo': 1905, 'caribbean': 1906, 'carpenter': 1907, 'carpeting': 1908, 'carriage': 1909, 'carrier': 1910, 'cart': 1911, 'cartoon': 1912, 'cartridges': 1913, 'carved': 1914, 'casing': 1915, 'cassettes': 1916, 'catalogue': 1917, 'cattle': 1918, 'caught': 1919, 'causing': 1920, 'cd': 1921, 'cedar': 1922, 'ceiling': 1923, 'celebration': 1924, 'cell': 1925, 'cells': 1926, 'cemetary': 1927, 'challenge': 1928, 'challenged': 1929, 'changing': 1930, 'chaos': 1931, 'charge': 1932, 'charges': 1933, 'cheap': 1934, 'checker': 1935, 'checkers': 1936, 'checks': 1937, 'cheers': 1938, 'chemise': 1939, 'chemist': 1940, 'chemotherapy': 1941, 'cherish': 1942, 'cherry': 1943, 'chesapeake': 1944, 'chewing': 1945, 'chills': 1946, 'chinese': 1947, 'choke': 1948, 'choking': 1949, 'choose': 1950, 'chopped': 1951, 'chuck': 1952, 'churchyard': 1953, 'cigarettes': 1954, 'cinema': 1955, 'circumstances': 1956, 'civil': 1957, 'civilisation': 1958, 'civilization': 1959, 'claim': 1960, 'clam': 1961, 'clay': 1962, 'cleaning': 1963, 'clearly': 1964, 'cleverest': 1965, 'clients': 1966, 'cliff': 1967, 'climates': 1968, 'clinic': 1969, 'closeness': 1970, 'closer': 1971, 'cloud': 1972, 'cloy': 1973, 'cluster': 1974, 'clutter': 1975, 'coastal': 1976, 'coaster': 1977, 'coat': 1978, 'cockpit': 1979, 'cocktail': 1980, 'cod': 1981, 'coffeepot': 1982, 'coin': 1983, 'collapse': 1984, 'colloquial': 1985, 'color': 1986, 'coloring': 1987, 'colours': 1988, 'comb': 1989, 'combustion': 1990, 'comfortably': 1991, 'comic': 1992, 'communism': 1993, 'companionable': 1994, 'compartment': 1995, 'compensation': 1996, 'competence': 1997, 'competency': 1998, 'competent': 1999, 'competitive': 2000, 'compile': 2001, 'complaining': 2002, 'complications': 2003, 'complimentary': 2004, 'compliments': 2005, 'composed': 2006, 'comprehension': 2007, 'compromises': 2008, 'compulsive': 2009, 'concentrating': 2010, 'conclave': 2011, 'concordance': 2012, 'conformity': 2013, 'confuse': 2014, 'confusing': 2015, 'congratulated': 2016, 'conscious': 2017, 'consciousness': 2018, 'conservatory': 2019, 'considerable': 2020, 'consideration': 2021, 'constant': 2022, 'constantly': 2023, 'constellation': 2024, 'consumed': 2025, 'consuming': 2026, 'consumption': 2027, 'contagious': 2028, 'contentment': 2029, 'contrast': 2030, 'contribution': 2031, 'contumely': 2032, 'convenient': 2033, 'convention': 2034, 'conventional': 2035, 'converse': 2036, 'cooked': 2037, 'cooling': 2038, 'cooties': 2039, 'copulate': 2040, 'cordoba': 2041, 'corps': 2042, 'corral': 2043, 'correctly': 2044, 'costa': 2045, 'costly': 2046, 'cotton': 2047, 'coughing': 2048, 'could': 2049, 'council': 2050, 'count': 2051, 'couple': 2052, 'courtroom': 2053, 'coverage': 2054, 'covers': 2055, 'crawl': 2056, 'crazy': 2057, 'cream': 2058, 'creatively': 2059, 'creativity': 2060, 'credit': 2061, 'creek': 2062, 'creeks': 2063, 'cremated': 2064, 'crimes': 2065, 'crisp': 2066, 'critically': 2067, 'criticism': 2068, 'crowding': 2069, 'crucial': 2070, 'cruel': 2071, 'cruet': 2072, 'crumpled': 2073, 'crushed': 2074, 'crying': 2075, 'crystals': 2076, 'cub': 2077, 'cuba': 2078, 'cube': 2079, 'cubes': 2080, 'cubicle': 2081, 'cultural': 2082, 'cum': 2083, 'cumbersome': 2084, 'cure': 2085, 'curiosity': 2086, 'current': 2087, 'curry': 2088, 'curved': 2089, 'cut': 2090, 'd.c': 2091, 'dale': 2092, 'dam': 2093, 'damnation': 2094, 'danger': 2095, 'dark': 2096, 'darkness': 2097, 'dashboard': 2098, 'dauntless': 2099, 'dawn': 2100, 'daydreaming': 2101, 'days': 2102, 'daytime': 2103, 'dc': 2104, 'de': 2105, 'deadly': 2106, 'deaf': 2107, 'deafness': 2108, 'dealership': 2109, 'deaths': 2110, 'deceitful': 2111, 'deception': 2112, 'deceptive': 2113, 'deciding': 2114, 'decision': 2115, 'deeply': 2116, 'defeat': 2117, 'defects': 2118, 'defend': 2119, 'definitely': 2120, 'delay': 2121, 'delays': 2122, 'delta': 2123, 'demean': 2124, 'democracy': 2125, 'democratic': 2126, 'demotion': 2127, 'den': 2128, 'denmark': 2129, 'dental': 2130, 'derby': 2131, 'derivative': 2132, 'description': 2133, 'deserts': 2134, 'design': 2135, 'desination': 2136, 'desirable': 2137, 'destroying': 2138, 'detachment': 2139, 'details': 2140, 'detention': 2141, 'determination': 2142, 'developed': 2143, 'device': 2144, 'devices': 2145, 'devil': 2146, 'dexterity': 2147, 'diaper': 2148, 'diarrhea': 2149, 'diary': 2150, 'dictatorship': 2151, 'did': 2152, 'dies': 2153, 'differently': 2154, 'digestion': 2155, 'dim': 2156, 'diminish': 2157, 'diminishment': 2158, 'diner': 2159, 'dirt': 2160, 'disagree': 2161, 'disappointed': 2162, 'disappointment': 2163, 'disapproval': 2164, 'disastrous': 2165, 'discipline': 2166, 'discover': 2167, 'discreet': 2168, 'discussion': 2169, 'disgust': 2170, 'dish': 2171, 'dishearten': 2172, 'disinterest': 2173, 'disobey': 2174, 'disparagement': 2175, 'dispenser': 2176, 'disperse': 2177, 'display': 2178, 'disposed': 2179, 'dispute': 2180, 'disputes': 2181, 'disregard': 2182, 'diss': 2183, 'dissatisfaction': 2184, 'dissipate': 2185, 'distances': 2186, 'distraction': 2187, 'distraught': 2188, 'distributed': 2189, 'divided': 2190, 'dock': 2191, 'documentary': 2192, 'domestic': 2193, 'donut': 2194, 'dorm': 2195, 'dormitory': 2196, 'doubt': 2197, 'dreaming': 2198, 'drenching': 2199, 'dribble': 2200, 'drill': 2201, 'drinks': 2202, 'dropping': 2203, 'drum': 2204, 'dryer': 2205, 'dumpster': 2206, 'duty': 2207, 'dweller': 2208, 'dwelling': 2209, 'earning': 2210, 'earnings': 2211, 'easily': 2212, 'easter': 2213, 'ecosphere': 2214, 'edge': 2215, 'edible': 2216, 'edinburgh': 2217, 'educated': 2218, 'educational': 2219, 'effectively': 2220, 'effectiveness': 2221, 'egypt': 2222, 'elapsed': 2223, 'elbow': 2224, 'elderly': 2225, 'elections': 2226, 'electricity': 2227, 'electronic': 2228, 'embarrassed': 2229, 'emblem': 2230, 'emotions': 2231, 'employability': 2232, 'employee': 2233, 'employees': 2234, 'employment': 2235, 'empowering': 2236, 'encourage': 2237, 'encouraging': 2238, 'endanger': 2239, 'enema': 2240, 'engage': 2241, 'engineering': 2242, 'engines': 2243, 'engraving': 2244, 'enjoying': 2245, 'enthusiastic': 2246, 'entrance': 2247, 'envelope': 2248, 'equal': 2249, 'equation': 2250, 'erase': 2251, 'errors': 2252, 'escape': 2253, 'eternal': 2254, 'european': 2255, 'evaluate': 2256, 'evaluating': 2257, 'evening': 2258, 'events': 2259, 'everglades': 2260, 'everywhere': 2261, 'evidence': 2262, 'evil': 2263, 'exaggerate': 2264, 'examples': 2265, 'excavations': 2266, 'exception': 2267, 'excrete': 2268, 'exhaust': 2269, 'exhausted': 2270, 'exhilaration': 2271, 'existing': 2272, 'exists': 2273, 'exit': 2274, 'exotic': 2275, 'expectation': 2276, 'expend': 2277, 'experiment': 2278, 'expiration': 2279, 'explosion': 2280, 'exposure': 2281, 'expressive': 2282, 'extra': 2283, 'face': 2284, 'facing': 2285, 'fail': 2286, 'fairness': 2287, 'faith': 2288, 'families': 2289, 'farmers': 2290, 'farming': 2291, 'farmyard': 2292, 'farts': 2293, 'fashion': 2294, 'favourite': 2295, 'fearful': 2296, 'fearless': 2297, 'feather': 2298, 'feathers': 2299, 'feces': 2300, 'feeble': 2301, 'feels': 2302, 'fees': 2303, 'feild': 2304, 'festival': 2305, 'fever': 2306, 'fiction': 2307, 'fidelity': 2308, 'fieldhouse': 2309, 'fifties': 2310, 'fighting': 2311, 'figure': 2312, 'file': 2313, 'files': 2314, 'fill': 2315, 'financial': 2316, 'finger': 2317, 'fingernails': 2318, 'finish': 2319, 'finite': 2320, 'firearm': 2321, 'fired': 2322, 'fireworks': 2323, 'fists': 2324, 'fitter': 2325, 'flashlight': 2326, 'flats': 2327, 'flea': 2328, 'flesh': 2329, 'flight': 2330, 'flirt': 2331, 'floors': 2332, 'flour': 2333, 'flowerpot': 2334, 'fluidity': 2335, 'flush': 2336, 'flustered': 2337, 'folder': 2338, 'force': 2339, 'forceful': 2340, 'forces': 2341, 'foreigner': 2342, 'forests': 2343, 'forgetting': 2344, 'fork': 2345, 'formed': 2346, 'forms': 2347, 'fort': 2348, 'forth': 2349, 'freezing': 2350, 'frequent': 2351, 'fresher': 2352, 'freshwater': 2353, 'fries': 2354, 'frigate': 2355, 'frightening': 2356, 'frivolous': 2357, 'frying': 2358, 'fucking': 2359, 'fuel': 2360, 'fulfilling': 2361, 'fulfillment': 2362, 'fundamental': 2363, 'fur': 2364, 'fuse': 2365, 'gallbladder': 2366, 'garder': 2367, 'garish': 2368, 'garments': 2369, 'gate': 2370, 'geek': 2371, 'generation': 2372, 'geography': 2373, 'ghetto': 2374, 'gilded': 2375, 'gills': 2376, 'girl': 2377, 'globular': 2378, 'glove': 2379, 'goals': 2380, 'gods': 2381, 'gorge': 2382, 'gorgeous': 2383, 'got': 2384, 'graduate': 2385, 'grand': 2386, 'grandmother': 2387, 'graph': 2388, 'gratification': 2389, 'gratifying': 2390, 'gravesite': 2391, 'graveyard': 2392, 'gravity': 2393, 'greece': 2394, 'greeting': 2395, 'grief': 2396, 'grieve': 2397, 'groceries': 2398, 'grotesque': 2399, 'grove': 2400, 'growing': 2401, 'grown': 2402, 'guest': 2403, 'guests': 2404, 'guillotine': 2405, 'gum': 2406, 'gun': 2407, 'habitual': 2408, 'had': 2409, 'haircut': 2410, 'ham': 2411, 'hamper': 2412, 'handbag': 2413, 'hanger': 2414, 'hanging': 2415, 'happened': 2416, 'harbor': 2417, 'hardship': 2418, 'harm': 2419, 'harmed': 2420, 'harmony': 2421, 'harvesting': 2422, 'haste': 2423, 'hat': 2424, 'hate': 2425, 'hats': 2426, 'haunted': 2427, 'hawaiian': 2428, 'heading': 2429, 'heal': 2430, 'healthier': 2431, 'healthiness': 2432, 'hearing': 2433, 'heartache': 2434, 'heavens': 2435, 'hedgerow': 2436, 'heights': 2437, 'helpful': 2438, 'hen': 2439, 'henhouse': 2440, 'herb': 2441, 'here': 2442, 'heretical': 2443, 'herpes': 2444, 'hesitation': 2445, 'hi': 2446, 'hiccups': 2447, 'hide': 2448, 'hiding': 2449, 'highway': 2450, 'hiking': 2451, 'hill': 2452, 'hinder': 2453, 'hired': 2454, 'historical': 2455, 'hive': 2456, 'hoarse': 2457, 'hock': 2458, 'holding': 2459, 'hollow': 2460, 'hollowness': 2461, 'hollywood': 2462, 'homeless': 2463, 'homely': 2464, 'homes': 2465, 'honest': 2466, 'hop': 2467, 'hope': 2468, 'horrendous': 2469, 'hostility': 2470, 'hotels': 2471, 'houseplant': 2472, 'how': 2473, 'hug': 2474, 'humane': 2475, 'humidifier': 2476, 'humiliated': 2477, 'humiliation': 2478, 'humor': 2479, 'hundredweight': 2480, 'hurtful': 2481, 'hydration': 2482, 'hydroelectric': 2483, 'hyperbolic': 2484, 'hysteria': 2485, 'hysterical': 2486, 'i': 2487, 'identification': 2488, 'idols': 2489, 'ignition': 2490, 'ignorable': 2491, 'ignorance': 2492, 'ignorant': 2493, 'ignoring': 2494, 'ill': 2495, 'illuminate': 2496, 'illumination': 2497, 'illustrate': 2498, 'images': 2499, 'imagine': 2500, 'imagining': 2501, 'immune': 2502, 'impatience': 2503, 'impossibility': 2504, 'impudence': 2505, 'inaccurate': 2506, 'inappropriate': 2507, 'incarceration': 2508, 'include': 2509, 'income': 2510, 'incomplete': 2511, 'incorrect': 2512, 'india': 2513, 'indian': 2514, 'indiana': 2515, 'indifference': 2516, 'indifferent': 2517, 'individual': 2518, 'indoor': 2519, 'industrial': 2520, 'industrialized': 2521, 'inebriation': 2522, 'ineffectual': 2523, 'inelegant': 2524, 'infect': 2525, 'infinite': 2526, 'inform': 2527, 'informal': 2528, 'inhaling': 2529, 'injustices': 2530, 'input': 2531, 'inside': 2532, 'inspires': 2533, 'instability': 2534, 'instructions': 2535, 'instruments': 2536, 'insubstantial': 2537, 'intangible': 2538, 'integrate': 2539, 'intelligent': 2540, 'interested': 2541, 'interests': 2542, 'international': 2543, 'introduce': 2544, 'introduced': 2545, 'introducing': 2546, 'invent': 2547, 'invigorating': 2548, 'invisible': 2549, 'iowa': 2550, 'ipod': 2551, 'ireland': 2552, 'irish': 2553, 'irreverence': 2554, 'irritability': 2555, 'island': 2556, 'isolation': 2557, 'italian': 2558, 'itches': 2559, 'itself': 2560, 'jamb': 2561, 'janeiro': 2562, 'japanese': 2563, 'jerseys': 2564, 'jet': 2565, 'jewellers': 2566, 'jigsaw': 2567, 'jog': 2568, 'joining': 2569, 'joke': 2570, 'joyful': 2571, 'juice': 2572, 'junk': 2573, 'junkyard': 2574, 'jury': 2575, 'just': 2576, 'kalahari': 2577, 'keg': 2578, 'kenne': 2579, 'kennedy': 2580, 'key': 2581, 'keys': 2582, 'killed': 2583, 'kind': 2584, 'kindergarten': 2585, 'kindness': 2586, 'kinds': 2587, 'kingdom': 2588, 'kisses': 2589, 'kitchens': 2590, 'knee': 2591, 'kneel': 2592, 'knife': 2593, 'knit': 2594, 'knocking': 2595, 'knowingly': 2596, 'knowledgable': 2597, 'la': 2598, 'lacerations': 2599, 'lady': 2600, 'lag': 2601, 'lamb': 2602, 'lamp': 2603, 'landmine': 2604, 'landowner': 2605, 'landscaping': 2606, 'lap': 2607, 'lark': 2608, 'latency': 2609, 'lawbook': 2610, 'laws': 2611, 'lazy': 2612, 'lead': 2613, 'leadership': 2614, 'leash': 2615, 'lens': 2616, 'letters': 2617, 'level': 2618, 'liberal': 2619, 'lighting': 2620, 'lightly': 2621, 'lightning': 2622, 'lightweight': 2623, 'liked': 2624, 'lines': 2625, 'liquidated': 2626, 'lists': 2627, 'literature': 2628, 'loading': 2629, 'loaf': 2630, 'lobster': 2631, 'location': 2632, 'lock': 2633, 'locomotion': 2634, 'log': 2635, 'logging': 2636, 'logic': 2637, 'longing': 2638, 'loosely': 2639, 'loosing': 2640, 'loquacious': 2641, 'los': 2642, 'lost': 2643, 'loving': 2644, 'lowering': 2645, 'loyal': 2646, 'lubricate': 2647, 'lucky': 2648, 'lumberyard': 2649, 'lung': 2650, 'lute': 2651, 'machines': 2652, 'mad': 2653, 'madagascar': 2654, 'made': 2655, 'maintain': 2656, 'malaise': 2657, 'male': 2658, 'malls': 2659, 'managed': 2660, 'manual': 2661, 'maps': 2662, 'marine': 2663, 'mark': 2664, 'marriage': 2665, 'married': 2666, 'masturbation': 2667, 'match': 2668, 'mate': 2669, 'material': 2670, 'maternity': 2671, 'matter': 2672, 'maze': 2673, 'meal': 2674, 'meals': 2675, 'measureing': 2676, 'mediterranean': 2677, 'melt': 2678, 'melting': 2679, 'memorize': 2680, 'mentally': 2681, 'menu': 2682, 'mess': 2683, 'metal': 2684, 'metaphor': 2685, 'meter': 2686, 'metric': 2687, 'metropolis': 2688, 'microphone': 2689, 'milk': 2690, 'milky': 2691, 'millions': 2692, 'millpond': 2693, 'milwaukee': 2694, 'mining': 2695, 'minor': 2696, 'misfire': 2697, 'misfortune': 2698, 'misrepresent': 2699, 'misshapen': 2700, 'missing': 2701, 'missouri': 2702, 'mistrial': 2703, 'mistrust': 2704, 'mit': 2705, 'mixed': 2706, 'mixer': 2707, 'modern': 2708, 'molehill': 2709, 'momentum': 2710, 'monarchy': 2711, 'monastic': 2712, 'monitor': 2713, 'monochromatic': 2714, 'montreal': 2715, 'mop': 2716, 'moral': 2717, 'morning': 2718, 'mortal': 2719, 'mortuary': 2720, 'mother': 2721, 'motor': 2722, 'mouse': 2723, 'moved': 2724, 'multiple': 2725, 'multiplied': 2726, 'murdered': 2727, 'muscle': 2728, 'mute': 2729, 'muttering': 2730, 'naked': 2731, 'name': 2732, 'nanotechnology': 2733, 'nap': 2734, 'napster': 2735, 'nasa': 2736, 'native': 2737, 'nearest': 2738, 'neat': 2739, 'nebraska': 2740, 'necessary': 2741, 'neck': 2742, 'negative': 2743, 'neglect': 2744, 'negligence': 2745, 'negligible': 2746, 'nepal': 2747, 'network': 2748, 'networked': 2749, 'neutral': 2750, 'niece': 2751, 'nightgown': 2752, 'nodding': 2753, 'noises': 2754, 'noisy': 2755, 'nonsense': 2756, 'nonstandard': 2757, 'normal': 2758, 'northeast': 2759, 'northwest': 2760, 'nostalgia': 2761, 'notice': 2762, 'notoriety': 2763, 'nt': 2764, 'nuclear': 2765, 'nudity': 2766, 'number': 2767, 'nurse': 2768, 'nuts': 2769, 'oath': 2770, 'obligatory': 2771, 'obscure': 2772, 'obsessive': 2773, 'obstruct': 2774, 'obtaining': 2775, 'occasionally': 2776, 'oceans': 2777, 'odors': 2778, 'oil': 2779, 'ontario': 2780, 'opaque': 2781, 'openness': 2782, 'operation': 2783, 'opinion': 2784, 'opportunity': 2785, 'optical': 2786, 'oral': 2787, 'organ': 2788, 'original': 2789, 'orphanage': 2790, 'ostracism': 2791, 'otherwise': 2792, 'outback': 2793, 'outer': 2794, 'overconfidence': 2795, 'overeating': 2796, 'overpopulation': 2797, 'overweight': 2798, 'owning': 2799, 'pack': 2800, 'paddy': 2801, 'page': 2802, 'painful': 2803, 'painless': 2804, 'pair': 2805, 'palm': 2806, 'pan': 2807, 'paragraph': 2808, 'parcel': 2809, 'parent': 2810, 'parents': 2811, 'parliament': 2812, 'parlor': 2813, 'partial': 2814, 'parties': 2815, 'partner': 2816, 'parts': 2817, 'passenger': 2818, 'passing': 2819, 'passionate': 2820, 'paste': 2821, 'patch': 2822, 'patiently': 2823, 'paws': 2824, 'pc': 2825, 'peaceful': 2826, 'peacefulness': 2827, 'peck': 2828, 'peculiar': 2829, 'pedaling': 2830, 'pee': 2831, 'peel': 2832, 'perform': 2833, 'perfume': 2834, 'perfumery': 2835, 'period': 2836, 'peripheral': 2837, 'permanent': 2838, 'permitted': 2839, 'persistent': 2840, 'personality': 2841, 'perspectives': 2842, 'persuaded': 2843, 'petals': 2844, 'petrify': 2845, 'pets': 2846, 'petty': 2847, 'philosophy': 2848, 'phoenix': 2849, 'phonebook': 2850, 'phoning': 2851, 'photo': 2852, 'photograph': 2853, 'photosynthesis': 2854, 'picnic': 2855, 'pills': 2856, 'pilot': 2857, 'pine': 2858, 'pipe': 2859, 'pirate': 2860, 'plains': 2861, 'planetarium': 2862, 'plantation': 2863, 'plastic': 2864, 'players': 2865, 'playful': 2866, 'playfulness': 2867, 'playroom': 2868, 'please': 2869, 'pleasing': 2870, 'pleasurable': 2871, 'plebeian': 2872, 'pleistocene': 2873, 'plentitude': 2874, 'plethora': 2875, 'plumbing': 2876, 'pockets': 2877, 'poems': 2878, 'pointed': 2879, 'points': 2880, 'poisoning': 2881, 'poland': 2882, 'pole': 2883, 'polished': 2884, 'poop': 2885, 'pop': 2886, 'popcorn': 2887, 'popular': 2888, 'pornography': 2889, 'position': 2890, 'positive': 2891, 'possibility': 2892, 'potato': 2893, 'potence': 2894, 'pots': 2895, 'poverty': 2896, 'praising': 2897, 'precious': 2898, 'predetermination': 2899, 'predictable': 2900, 'prehistoric': 2901, 'prehistory': 2902, 'prejudice': 2903, 'preschool': 2904, 'presence': 2905, 'present': 2906, 'presentation': 2907, 'presents': 2908, 'preserve': 2909, 'president': 2910, 'pressing': 2911, 'price': 2912, 'prime': 2913, 'printed': 2914, 'printer': 2915, 'prior': 2916, 'prisoner': 2917, 'processed': 2918, 'procrastination': 2919, 'products': 2920, 'profit': 2921, 'programs': 2922, 'project': 2923, 'promotion': 2924, 'promotions': 2925, 'pronoun': 2926, 'propeller': 2927, 'propitious': 2928, 'proposals': 2929, 'prosecution': 2930, 'prostitute': 2931, 'protection': 2932, 'protein': 2933, 'protests': 2934, 'province': 2935, 'proximity': 2936, 'pta': 2937, 'pub': 2938, 'pull': 2939, 'pump': 2940, 'punch': 2941, 'punctured': 2942, 'purchase': 2943, 'purpose': 2944, 'purr': 2945, 'push': 2946, 'qualification': 2947, 'quark': 2948, 'quarry': 2949, 'quebec': 2950, 'queensland': 2951, 'queue': 2952, 'quiet': 2953, 'quietness': 2954, 'quintuplets': 2955, 'quitting': 2956, 'rabbit': 2957, 'racetrack': 2958, 'racket': 2959, 'radical': 2960, 'railway': 2961, 'rainbow': 2962, 'randomness': 2963, 'rapport': 2964, 'rare': 2965, 'rattle': 2966, 'razor': 2967, 'reaction': 2968, 'reading': 2969, 'ready': 2970, 'reality': 2971, 'ream': 2972, 'reasonable': 2973, 'rebel': 2974, 'rebirth': 2975, 'rebound': 2976, 'receiving': 2977, 'recession': 2978, 'reckless': 2979, 'reconciled': 2980, 'recorder': 2981, 'recreational': 2982, 'recur': 2983, 'recyclable': 2984, 'recycling': 2985, 'redemption': 2986, 'reduce': 2987, 'reduced': 2988, 'reflecting': 2989, 'reflection': 2990, 'refreshment': 2991, 'refuge': 2992, 'refund': 2993, 'refuse': 2994, 'regressive': 2995, 'regular': 2996, 'rejection': 2997, 'relatives': 2998, 'relaxed': 2999, 'relieved': 3000, 'religious': 3001, 'remake': 3002, 'remarkable': 3003, 'remote': 3004, 'renewal': 3005, 'repeat': 3006, 'representative': 3007, 'reproducing': 3008, 'republican': 3009, 'reputable': 3010, 'require': 3011, 'resent': 3012, 'reservations': 3013, 'reserve': 3014, 'residence': 3015, 'resist': 3016, 'resistant': 3017, 'respiration': 3018, 'respond': 3019, 'responsibilities': 3020, 'restlessness': 3021, 'restrained': 3022, 'restricted': 3023, 'result': 3024, 'results': 3025, 'resuscitate': 3026, 'retina': 3027, 'retire': 3028, 'retirement': 3029, 'retract': 3030, 'reunion': 3031, 'revenge': 3032, 'reverence': 3033, 'review': 3034, 'revised': 3035, 'reviving': 3036, 'reward': 3037, 'rhode': 3038, 'rhyme': 3039, 'rhythm': 3040, 'ribs': 3041, 'rica': 3042, 'richer': 3043, 'rid': 3044, 'ridiculous': 3045, 'right': 3046, 'ring': 3047, 'rio': 3048, 'riot': 3049, 'rise': 3050, 'rising': 3051, 'roads': 3052, 'roast': 3053, 'rocks': 3054, 'roll': 3055, 'roman': 3056, 'rooms': 3057, 'rooster': 3058, 'root': 3059, 'rose': 3060, 'roulette': 3061, 'round': 3062, 'ruin': 3063, 'ruined': 3064, 'ruling': 3065, 'rumors': 3066, 'runway': 3067, 'sack': 3068, 'sail': 3069, 'sailing': 3070, 'saltwater': 3071, 'salty': 3072, 'sanctuary': 3073, 'sandwich': 3074, 'sandy': 3075, 'saturated': 3076, 'saucepan': 3077, 'saving': 3078, 'savings': 3079, 'scare': 3080, 'scars': 3081, 'scattering': 3082, 'scene': 3083, 'scheme': 3084, 'schoolyard': 3085, 'scientific': 3086, 'scotland': 3087, 'screen': 3088, 'screened': 3089, 'searching': 3090, 'seas': 3091, 'season': 3092, 'seasons': 3093, 'seat': 3094, 'seats': 3095, 'secondary': 3096, 'secrets': 3097, 'secure': 3098, 'seed': 3099, 'seeds': 3100, 'seizure': 3101, 'selfless': 3102, 'seminary': 3103, 'send': 3104, 'sentences': 3105, 'sequence': 3106, 'serious': 3107, 'setting': 3108, 'sever': 3109, 'several': 3110, 'sewers': 3111, 'sewing': 3112, 'seychelles': 3113, 'shack': 3114, 'shady': 3115, 'shaking': 3116, 'shame': 3117, 'shameful': 3118, 'sharp': 3119, 'shaving': 3120, 'sheath': 3121, 'sheet': 3122, 'shift': 3123, 'shipyard': 3124, 'shiver': 3125, 'shivering': 3126, 'shoe': 3127, 'shoelace': 3128, 'short': 3129, 'shorter': 3130, 'should': 3131, 'shovelling': 3132, 'shrinkage': 3133, 'shrinking': 3134, 'shut': 3135, 'shyness': 3136, 'sides': 3137, 'sierra': 3138, 'sigh': 3139, 'sight': 3140, 'sights': 3141, 'signed': 3142, 'silence': 3143, 'silent': 3144, 'silently': 3145, 'sill': 3146, 'silliness': 3147, 'silo': 3148, 'simplicity': 3149, 'singing': 3150, 'sites': 3151, 'situation': 3152, 'skateboard': 3153, 'skating': 3154, 'skeleton': 3155, 'skepticism': 3156, 'ski': 3157, 'skills': 3158, 'skip': 3159, 'skirt': 3160, 'slack': 3161, 'slaughter': 3162, 'slaughterhouse': 3163, 'sleepwalking': 3164, 'slight': 3165, 'slip': 3166, 'sloth': 3167, 'slowly': 3168, 'slurred': 3169, 'smells': 3170, 'smooth': 3171, 'sneezing': 3172, 'snoring': 3173, 'snow': 3174, 'snowball': 3175, 'so': 3176, 'sock': 3177, 'socks': 3178, 'softness': 3179, 'software': 3180, 'soil': 3181, 'solutions': 3182, 'solving': 3183, 'somber': 3184, 'someone': 3185, 'someplace': 3186, 'sorry': 3187, 'sort': 3188, 'sounds': 3189, 'sour': 3190, 'spacecraft': 3191, 'spanish': 3192, 'sparks': 3193, 'speaker': 3194, 'specialty': 3195, 'specific': 3196, 'spectacles': 3197, 'speech': 3198, 'speechless': 3199, 'spell': 3200, 'spillage': 3201, 'spills': 3202, 'sping': 3203, 'spinning': 3204, 'spiritual': 3205, 'spit': 3206, 'spontaneous': 3207, 'sprain': 3208, 'spreadsheet': 3209, 'square': 3210, 'stable': 3211, 'stagnant': 3212, 'stammering': 3213, 'standard': 3214, 'standards': 3215, 'stanza': 3216, 'stapler': 3217, 'star': 3218, 'stares': 3219, 'stars': 3220, 'start': 3221, 'starting': 3222, 'startled': 3223, 'starvation': 3224, 'starve': 3225, 'static': 3226, 'stationery': 3227, 'steak': 3228, 'stems': 3229, 'steppe': 3230, 'stink': 3231, 'stitches': 3232, 'stocking': 3233, 'stockings': 3234, 'stockmarket': 3235, 'stockpile': 3236, 'stoic': 3237, 'stolen': 3238, 'stone': 3239, 'storage': 3240, 'storm': 3241, 'storybook': 3242, 'straighten': 3243, 'straightforward': 3244, 'stressed': 3245, 'strikes': 3246, 'stringed': 3247, 'stripes': 3248, 'stroked': 3249, 'stronger': 3250, 'studios': 3251, 'stumble': 3252, 'submarines': 3253, 'subordinate': 3254, 'suburban': 3255, 'succeeding': 3256, 'successful': 3257, 'suck': 3258, 'sucking': 3259, 'sufficient': 3260, 'suite': 3261, 'suits': 3262, 'sum': 3263, 'sump': 3264, 'sundown': 3265, 'sunlight': 3266, 'super': 3267, 'superior': 3268, 'surprise': 3269, 'surprised': 3270, 'survive': 3271, 'sushi': 3272, 'suspension': 3273, 'sustaining': 3274, 'swallow': 3275, 'swamp': 3276, 'sweater': 3277, 'sweet': 3278, 'swing': 3279, 'sword': 3280, 'syllable': 3281, 'tag': 3282, 'tahoe': 3283, 'tail': 3284, 'tailor': 3285, 'tails': 3286, 'taking': 3287, 'talent': 3288, 'tank': 3289, 'tape': 3290, 'tardiness': 3291, 'taxes': 3292, 'taxi': 3293, 'tea': 3294, 'teachers': 3295, 'teams': 3296, 'tear': 3297, 'teeth': 3298, 'telegraph': 3299, 'telescope': 3300, 'telling': 3301, 'temperature': 3302, 'temple': 3303, 'tended': 3304, 'tentative': 3305, 'terminal': 3306, 'terrain': 3307, 'terrorism': 3308, 'test': 3309, 'testify': 3310, 'tetrahedron': 3311, 'than': 3312, 'that': 3313, 'theaters': 3314, 'thing': 3315, 'thirst': 3316, 'thorough': 3317, 'though': 3318, 'thoughtful': 3319, 'thoughts': 3320, 'three': 3321, 'thrift': 3322, 'thunderstorm': 3323, 'tickets': 3324, 'tickle': 3325, 'tide': 3326, 'tidepools': 3327, 'tie': 3328, 'times': 3329, 'timing': 3330, 'toenails': 3331, 'toilet': 3332, 'tokio': 3333, 'tomales': 3334, 'toolbox': 3335, 'toolkit': 3336, 'tools': 3337, 'tornado': 3338, 'tornadoes': 3339, 'torture': 3340, 'toxic': 3341, 'toxicity': 3342, 'tradesmen': 3343, 'tradition': 3344, 'trailer': 3345, 'tramp': 3346, 'transistor': 3347, 'transit': 3348, 'transmission': 3349, 'trapped': 3350, 'trauma': 3351, 'treetops': 3352, 'trek': 3353, 'trenches': 3354, 'tricks': 3355, 'trigonometry': 3356, 'trips': 3357, 'tromsø': 3358, 'trophy': 3359, 'troubles': 3360, 'trouser': 3361, 'true': 3362, 'trunks': 3363, 'tsunami': 3364, 'tube': 3365, 'tugboat': 3366, 'tumor': 3367, 'tundra': 3368, 'turbine': 3369, 'turmoil': 3370, 'turning': 3371, 'tuxedo': 3372, 'twins': 3373, 'typewriter': 3374, 'u.s': 3375, 'ugliness': 3376, 'unbreakable': 3377, 'uncertainty': 3378, 'uncomfortable': 3379, 'uncommon': 3380, 'undecided': 3381, 'underage': 3382, 'underbrush': 3383, 'undergrowth': 3384, 'underneath': 3385, 'underpants': 3386, 'unemployed': 3387, 'unfeeling': 3388, 'unfinished': 3389, 'ungulate': 3390, 'unhappiness': 3391, 'unhappy': 3392, 'unimportant': 3393, 'unite': 3394, 'universal': 3395, 'unknowable': 3396, 'unpleasant': 3397, 'unwanted': 3398, 'ups': 3399, 'upstairs': 3400, 'upstream': 3401, 'urinate': 3402, 'us': 3403, 'vacation': 3404, 'vacationgoer': 3405, 'vacuuming': 3406, 'vagina': 3407, 'value': 3408, 'valve': 3409, 'various': 3410, 'vase': 3411, 'vault': 3412, 'vehicle': 3413, 'vein': 3414, 'venereal': 3415, 'venezuela': 3416, 'venice': 3417, 'venue': 3418, 'verbal': 3419, 'verbose': 3420, 'verdict': 3421, 'versailles': 3422, 'vessels': 3423, 'victorian': 3424, 'villa': 3425, 'ville': 3426, 'vinegar': 3427, 'vineyard': 3428, 'violence': 3429, 'violent': 3430, 'violin': 3431, 'virtuous': 3432, 'vision': 3433, 'visit': 3434, 'visiting': 3435, 'vocation': 3436, 'void': 3437, 'volunteer': 3438, 'vomit': 3439, 'vote': 3440, 'wade': 3441, 'waist': 3442, 'wakefulness': 3443, 'wallflower': 3444, 'walmart': 3445, 'wanting': 3446, 'ward': 3447, 'warren': 3448, 'wastebasket': 3449, 'wasted': 3450, 'wasteland': 3451, 'wasting': 3452, 'watching': 3453, 'watering': 3454, 'waters': 3455, 'waving': 3456, 'way': 3457, 'ways': 3458, 'weak': 3459, 'weigh': 3460, 'weird': 3461, 'westerly': 3462, 'wetness': 3463, 'what': 3464, 'wheeze': 3465, 'wherever': 3466, 'whiskers': 3467, 'whistle': 3468, 'whistles': 3469, 'whitehouse': 3470, 'whorehouse': 3471, 'wide': 3472, 'wildlife': 3473, 'wind': 3474, 'windowless': 3475, 'winner': 3476, 'wins': 3477, 'wisconsin': 3478, 'witness': 3479, 'woman': 3480, 'women': 3481, 'wonderful': 3482, 'wooded': 3483, 'wooden': 3484, 'woodlands': 3485, 'wool': 3486, 'word': 3487, 'worked': 3488, 'workload': 3489, 'workmanship': 3490, 'works': 3491, 'worrying': 3492, 'worship': 3493, 'worthlessness': 3494, 'would': 3495, 'wounds': 3496, 'wrap': 3497, 'wreak': 3498, 'wreck': 3499, 'wrinkled': 3500, 'wrinkles': 3501, 'wristwatch': 3502, 'writing': 3503, 'yawn': 3504, 'year': 3505, 'yeast': 3506, 'yell': 3507, 'ymca': 3508, 'zealand': 3509, 'zip': 3510, 'zone': 3511, 'zones': 3512, 'zoological': 3513, 'zoos': 3514})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGyqmiNlyAiR",
        "outputId": "c8c3b86e-69b7-453b-945b-019dd762b08a"
      },
      "source": [
        "Sentence.vocab.stoi['new construction'], Label.vocab.stoi['new construction'] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsycLH0DEzFa"
      },
      "source": [
        "Load our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRDnbA09EzFc"
      },
      "source": [
        "Then create our vocabulary, converting all tokens appearing less than twice into `<unk>` tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2hiioCLEzFd"
      },
      "source": [
        "Finally, define the `device` and create our iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4davOyYEzFd"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT4Zg9QrEzFd"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator_de_en, valid_iterator_de_en, test_iterator_de_en = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YoFk7hgzQ1h"
      },
      "source": [
        "BATCH_SIZE=128\r\n",
        "train_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train, valid), \r\n",
        "    batch_size = BATCH_SIZE, \r\n",
        "    device = device, sort=False, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeK3RzjvzZSg"
      },
      "source": [
        "iter_one = next(iter(train_iterator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cid31jyj-cuR",
        "outputId": "a8d7f350-c4b5-4811-c511-8ee7b8a72e43"
      },
      "source": [
        "iter_one.label.shape, "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 128]),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NEKYsXD0HK2"
      },
      "source": [
        "for entry in (iter_one.sentence[0][0,:]):\r\n",
        "    print(entry.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqR66FM2EzFj"
      },
      "source": [
        "## Seq2Seq Model\n",
        "\n",
        "Putting the encoder and decoder together, we get:\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq7.png?raw=1)\n",
        "\n",
        "Again, in this implementation we need to ensure the hidden dimensions in both the encoder and the decoder are the same.\n",
        "\n",
        "Briefly going over all of the steps:\n",
        "- the `outputs` tensor is created to hold all predictions, $\\hat{Y}$\n",
        "- the source sequence, $X$, is fed into the encoder to receive a `context` vector\n",
        "- the initial decoder hidden state is set to be the `context` vector, $s_0 = z = h_T$\n",
        "- we use a batch of `<sos>` tokens as the first `input`, $y_1$\n",
        "- we then decode within a loop:\n",
        "  - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and the context vector, $z$, into the decoder\n",
        "  - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n",
        "  - we then decide if we are going to teacher force or not, setting the next input as appropriate (either the ground truth next token in the target sequence or the highest predicted next token)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11CnDhTkEzFd"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        text, text_lengths = src\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # packed_seq = nn.utils.rnn.pack_padded_sequence(embedded, \n",
        "        #                                                text_lengths.cpu(),\n",
        "        #                                                batch_first=False,\n",
        "        #                                                enforce_sorted=False)        \n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded) #no cell state! \n",
        "        ## outputs is a packed sequence but since it is not used we will not \n",
        "        ## unpack it         \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Hb0dXWHLsG"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRA8hkiLEzFh"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded) #no cell state!\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, context):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #context = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        #context = [1, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # print(embedded.shape, hidden.shape, context.shape)\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "            \n",
        "        #emb_con = [1, batch size, emb dim + hid dim]\n",
        "            \n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \n",
        "                           dim = 1)\n",
        "        \n",
        "        #output = [batch size, emb dim + hid dim * 2]\n",
        "        \n",
        "        prediction = self.fc_out(output)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDyNrQ8VEzFk"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is the context\n",
        "        context = self.encoder(src)\n",
        "        \n",
        "        #context also used as the initial hidden state of the decoder\n",
        "        hidden = context\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state and the context state\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxVMMPUyEzFk"
      },
      "source": [
        "# Training the Seq2Seq Model\n",
        "\n",
        "The rest of this session is very similar to the previous one. \n",
        "\n",
        "We initialise our encoder, decoder and seq2seq model (placing it on the GPU if we have one). As before, the embedding dimensions and the amount of dropout used can be different between the encoder and the decoder, but the hidden dimensions must remain the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDG6jOSuEzFk"
      },
      "source": [
        "# INPUT_DIM = len(SRC.vocab)\n",
        "# OUTPUT_DIM = len(TRG.vocab)\n",
        "\n",
        "INPUT_DIM = len(Sentence.vocab)\n",
        "OUTPUT_DIM = len(Label.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIc1l6CEzFk"
      },
      "source": [
        "Next, we initialize our parameters. The paper states the parameters are initialized from a normal distribution with a mean of 0 and a standard deviation of 0.01, i.e. $\\mathcal{N}(0, 0.01)$. \n",
        "\n",
        "It also states we should initialize the recurrent parameters to a special initialization, however to keep things simple we'll also initialize them to $\\mathcal{N}(0, 0.01)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgqMqq-oEzFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562dbe0b-29e2-4c09-d67d-5c4c4fcd3cb3"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(11736, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(3515, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=3515, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd1QoOsUEzFl"
      },
      "source": [
        "We print out the number of parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IggCwIBgEzFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d2b0ee-9868-44a8-a60c-300feabe3bc8"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 11,558,843 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiH54qzHEzFl"
      },
      "source": [
        "We initiaize our optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO1_eoG7EzFl"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwkV3FKlEzFl"
      },
      "source": [
        "We also initialize the loss function, making sure to ignore the loss on `<pad>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0DAbGbcEzFl"
      },
      "source": [
        "#TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "TRG_PAD_IDX = Label.vocab.stoi[Label.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTay7E9rEzFm"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OYuoFdEzFm"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.sentence\n",
        "        trg = batch.label\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        # print(\"output_dim:\", output.shape)\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        # print(\"output_dim before loss:\", output.shape, trg.shape)\n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rfUx5lhEzFm"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw022pw0EzFm"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            # src = batch.src\n",
        "            # trg = batch.trg\n",
        "            src = batch.sentence\n",
        "            trg = batch.label\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E43h8dnQEzFm"
      },
      "source": [
        "We'll also define the function that calculates how long an epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTAmu3-EEzFm"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbCGyO4ZEzFm"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjFyRUK9EzFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d452cbe9-0ed6-44ce-98cd-b6fa763517c8"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\n",
        "    \n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 7.747 | Train PPL: 2315.737\n",
            "\t Val. Loss: 6.972 |  Val. PPL: 1066.356\n",
            "Epoch: 02 | Time: 0m 2s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.608\n",
            "\t Val. Loss: 6.934 |  Val. PPL: 1026.484\n",
            "Epoch: 03 | Time: 0m 2s\n",
            "\tTrain Loss: 6.304 | Train PPL: 546.912\n",
            "\t Val. Loss: 6.981 |  Val. PPL: 1076.070\n",
            "Epoch: 04 | Time: 0m 2s\n",
            "\tTrain Loss: 6.271 | Train PPL: 528.765\n",
            "\t Val. Loss: 6.996 |  Val. PPL: 1091.783\n",
            "Epoch: 05 | Time: 0m 2s\n",
            "\tTrain Loss: 6.240 | Train PPL: 512.814\n",
            "\t Val. Loss: 6.996 |  Val. PPL: 1092.124\n",
            "Epoch: 06 | Time: 0m 2s\n",
            "\tTrain Loss: 6.206 | Train PPL: 495.889\n",
            "\t Val. Loss: 6.990 |  Val. PPL: 1085.880\n",
            "Epoch: 07 | Time: 0m 2s\n",
            "\tTrain Loss: 6.173 | Train PPL: 479.593\n",
            "\t Val. Loss: 6.982 |  Val. PPL: 1077.588\n",
            "Epoch: 08 | Time: 0m 2s\n",
            "\tTrain Loss: 6.142 | Train PPL: 465.057\n",
            "\t Val. Loss: 6.974 |  Val. PPL: 1068.542\n",
            "Epoch: 09 | Time: 0m 2s\n",
            "\tTrain Loss: 6.114 | Train PPL: 452.347\n",
            "\t Val. Loss: 6.967 |  Val. PPL: 1060.803\n",
            "Epoch: 10 | Time: 0m 2s\n",
            "\tTrain Loss: 6.087 | Train PPL: 440.129\n",
            "\t Val. Loss: 6.964 |  Val. PPL: 1058.242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXHg1YUjEnXz",
        "outputId": "c94f0788-557e-442d-8fd2-a2ae94eb503d"
      },
      "source": [
        "N_EPOCHS = 20\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 6.061 | Train PPL: 428.615\n",
            "\t Val. Loss: 6.968 |  Val. PPL: 1062.412\n",
            "Epoch: 02 | Time: 0m 2s\n",
            "\tTrain Loss: 6.031 | Train PPL: 416.110\n",
            "\t Val. Loss: 6.974 |  Val. PPL: 1068.579\n",
            "Epoch: 03 | Time: 0m 2s\n",
            "\tTrain Loss: 6.001 | Train PPL: 403.660\n",
            "\t Val. Loss: 6.971 |  Val. PPL: 1064.756\n",
            "Epoch: 04 | Time: 0m 2s\n",
            "\tTrain Loss: 5.964 | Train PPL: 389.241\n",
            "\t Val. Loss: 6.948 |  Val. PPL: 1041.324\n",
            "Epoch: 05 | Time: 0m 2s\n",
            "\tTrain Loss: 5.924 | Train PPL: 373.858\n",
            "\t Val. Loss: 6.908 |  Val. PPL: 999.754\n",
            "Epoch: 06 | Time: 0m 2s\n",
            "\tTrain Loss: 5.881 | Train PPL: 358.269\n",
            "\t Val. Loss: 6.853 |  Val. PPL: 946.454\n",
            "Epoch: 07 | Time: 0m 2s\n",
            "\tTrain Loss: 5.832 | Train PPL: 340.925\n",
            "\t Val. Loss: 6.802 |  Val. PPL: 899.902\n",
            "Epoch: 08 | Time: 0m 2s\n",
            "\tTrain Loss: 5.782 | Train PPL: 324.481\n",
            "\t Val. Loss: 6.760 |  Val. PPL: 862.300\n",
            "Epoch: 09 | Time: 0m 2s\n",
            "\tTrain Loss: 5.728 | Train PPL: 307.236\n",
            "\t Val. Loss: 6.720 |  Val. PPL: 828.767\n",
            "Epoch: 10 | Time: 0m 2s\n",
            "\tTrain Loss: 5.674 | Train PPL: 291.290\n",
            "\t Val. Loss: 6.685 |  Val. PPL: 799.948\n",
            "Epoch: 11 | Time: 0m 2s\n",
            "\tTrain Loss: 5.611 | Train PPL: 273.390\n",
            "\t Val. Loss: 6.647 |  Val. PPL: 770.535\n",
            "Epoch: 12 | Time: 0m 2s\n",
            "\tTrain Loss: 5.546 | Train PPL: 256.238\n",
            "\t Val. Loss: 6.606 |  Val. PPL: 739.750\n",
            "Epoch: 13 | Time: 0m 2s\n",
            "\tTrain Loss: 5.478 | Train PPL: 239.260\n",
            "\t Val. Loss: 6.564 |  Val. PPL: 709.155\n",
            "Epoch: 14 | Time: 0m 2s\n",
            "\tTrain Loss: 5.409 | Train PPL: 223.440\n",
            "\t Val. Loss: 6.523 |  Val. PPL: 680.616\n",
            "Epoch: 15 | Time: 0m 2s\n",
            "\tTrain Loss: 5.335 | Train PPL: 207.481\n",
            "\t Val. Loss: 6.481 |  Val. PPL: 652.783\n",
            "Epoch: 16 | Time: 0m 2s\n",
            "\tTrain Loss: 5.255 | Train PPL: 191.487\n",
            "\t Val. Loss: 6.442 |  Val. PPL: 627.596\n",
            "Epoch: 17 | Time: 0m 2s\n",
            "\tTrain Loss: 5.179 | Train PPL: 177.456\n",
            "\t Val. Loss: 6.403 |  Val. PPL: 603.485\n",
            "Epoch: 18 | Time: 0m 2s\n",
            "\tTrain Loss: 5.094 | Train PPL: 163.029\n",
            "\t Val. Loss: 6.360 |  Val. PPL: 578.442\n",
            "Epoch: 19 | Time: 0m 2s\n",
            "\tTrain Loss: 5.015 | Train PPL: 150.618\n",
            "\t Val. Loss: 6.321 |  Val. PPL: 556.098\n",
            "Epoch: 20 | Time: 0m 2s\n",
            "\tTrain Loss: 4.940 | Train PPL: 139.729\n",
            "\t Val. Loss: 6.280 |  Val. PPL: 533.784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uGl0qfzjKcs",
        "outputId": "4d34edf2-c7ce-40dc-b526-2c7641e156fe"
      },
      "source": [
        "N_EPOCHS = 20\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = 6.280 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 4.861 | Train PPL: 129.123\n",
            "\t Val. Loss: 6.240 |  Val. PPL: 512.902\n",
            "Epoch: 02 | Time: 0m 2s\n",
            "\tTrain Loss: 4.778 | Train PPL: 118.886\n",
            "\t Val. Loss: 6.199 |  Val. PPL: 492.367\n",
            "Epoch: 03 | Time: 0m 2s\n",
            "\tTrain Loss: 4.709 | Train PPL: 110.917\n",
            "\t Val. Loss: 6.166 |  Val. PPL: 476.265\n",
            "Epoch: 04 | Time: 0m 2s\n",
            "\tTrain Loss: 4.635 | Train PPL: 103.028\n",
            "\t Val. Loss: 6.127 |  Val. PPL: 458.265\n",
            "Epoch: 05 | Time: 0m 2s\n",
            "\tTrain Loss: 4.572 | Train PPL:  96.717\n",
            "\t Val. Loss: 6.076 |  Val. PPL: 435.379\n",
            "Epoch: 06 | Time: 0m 2s\n",
            "\tTrain Loss: 4.504 | Train PPL:  90.380\n",
            "\t Val. Loss: 6.041 |  Val. PPL: 420.411\n",
            "Epoch: 07 | Time: 0m 2s\n",
            "\tTrain Loss: 4.437 | Train PPL:  84.509\n",
            "\t Val. Loss: 6.005 |  Val. PPL: 405.556\n",
            "Epoch: 08 | Time: 0m 2s\n",
            "\tTrain Loss: 4.375 | Train PPL:  79.443\n",
            "\t Val. Loss: 5.974 |  Val. PPL: 392.916\n",
            "Epoch: 09 | Time: 0m 2s\n",
            "\tTrain Loss: 4.302 | Train PPL:  73.848\n",
            "\t Val. Loss: 5.942 |  Val. PPL: 380.770\n",
            "Epoch: 10 | Time: 0m 2s\n",
            "\tTrain Loss: 4.238 | Train PPL:  69.284\n",
            "\t Val. Loss: 5.912 |  Val. PPL: 369.373\n",
            "Epoch: 11 | Time: 0m 2s\n",
            "\tTrain Loss: 4.179 | Train PPL:  65.309\n",
            "\t Val. Loss: 5.887 |  Val. PPL: 360.337\n",
            "Epoch: 12 | Time: 0m 2s\n",
            "\tTrain Loss: 4.129 | Train PPL:  62.102\n",
            "\t Val. Loss: 5.861 |  Val. PPL: 350.909\n",
            "Epoch: 13 | Time: 0m 2s\n",
            "\tTrain Loss: 4.055 | Train PPL:  57.682\n",
            "\t Val. Loss: 5.838 |  Val. PPL: 342.944\n",
            "Epoch: 14 | Time: 0m 2s\n",
            "\tTrain Loss: 4.000 | Train PPL:  54.574\n",
            "\t Val. Loss: 5.823 |  Val. PPL: 337.986\n",
            "Epoch: 15 | Time: 0m 2s\n",
            "\tTrain Loss: 3.941 | Train PPL:  51.451\n",
            "\t Val. Loss: 5.804 |  Val. PPL: 331.611\n",
            "Epoch: 16 | Time: 0m 2s\n",
            "\tTrain Loss: 3.888 | Train PPL:  48.802\n",
            "\t Val. Loss: 5.780 |  Val. PPL: 323.688\n",
            "Epoch: 17 | Time: 0m 2s\n",
            "\tTrain Loss: 3.823 | Train PPL:  45.747\n",
            "\t Val. Loss: 5.760 |  Val. PPL: 317.330\n",
            "Epoch: 18 | Time: 0m 2s\n",
            "\tTrain Loss: 3.768 | Train PPL:  43.311\n",
            "\t Val. Loss: 5.739 |  Val. PPL: 310.815\n",
            "Epoch: 19 | Time: 0m 2s\n",
            "\tTrain Loss: 3.721 | Train PPL:  41.302\n",
            "\t Val. Loss: 5.724 |  Val. PPL: 306.219\n",
            "Epoch: 20 | Time: 0m 2s\n",
            "\tTrain Loss: 3.673 | Train PPL:  39.350\n",
            "\t Val. Loss: 5.709 |  Val. PPL: 301.658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azg8cfF6jbjo",
        "outputId": "5885b11a-9cca-466a-b877-5674b917f815"
      },
      "source": [
        "N_EPOCHS = 30\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = 5.709 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 3.601 | Train PPL:  36.625\n",
            "\t Val. Loss: 5.693 |  Val. PPL: 296.919\n",
            "Epoch: 02 | Time: 0m 2s\n",
            "\tTrain Loss: 3.560 | Train PPL:  35.177\n",
            "\t Val. Loss: 5.681 |  Val. PPL: 293.331\n",
            "Epoch: 03 | Time: 0m 2s\n",
            "\tTrain Loss: 3.518 | Train PPL:  33.715\n",
            "\t Val. Loss: 5.669 |  Val. PPL: 289.643\n",
            "Epoch: 04 | Time: 0m 2s\n",
            "\tTrain Loss: 3.460 | Train PPL:  31.827\n",
            "\t Val. Loss: 5.651 |  Val. PPL: 284.434\n",
            "Epoch: 05 | Time: 0m 2s\n",
            "\tTrain Loss: 3.416 | Train PPL:  30.433\n",
            "\t Val. Loss: 5.632 |  Val. PPL: 279.289\n",
            "Epoch: 06 | Time: 0m 2s\n",
            "\tTrain Loss: 3.371 | Train PPL:  29.111\n",
            "\t Val. Loss: 5.625 |  Val. PPL: 277.139\n",
            "Epoch: 07 | Time: 0m 2s\n",
            "\tTrain Loss: 3.317 | Train PPL:  27.581\n",
            "\t Val. Loss: 5.610 |  Val. PPL: 273.021\n",
            "Epoch: 08 | Time: 0m 2s\n",
            "\tTrain Loss: 3.268 | Train PPL:  26.251\n",
            "\t Val. Loss: 5.599 |  Val. PPL: 270.048\n",
            "Epoch: 09 | Time: 0m 2s\n",
            "\tTrain Loss: 3.227 | Train PPL:  25.203\n",
            "\t Val. Loss: 5.584 |  Val. PPL: 266.206\n",
            "Epoch: 10 | Time: 0m 2s\n",
            "\tTrain Loss: 3.182 | Train PPL:  24.087\n",
            "\t Val. Loss: 5.575 |  Val. PPL: 263.821\n",
            "Epoch: 11 | Time: 0m 2s\n",
            "\tTrain Loss: 3.136 | Train PPL:  23.009\n",
            "\t Val. Loss: 5.569 |  Val. PPL: 262.266\n",
            "Epoch: 12 | Time: 0m 2s\n",
            "\tTrain Loss: 3.091 | Train PPL:  22.000\n",
            "\t Val. Loss: 5.562 |  Val. PPL: 260.227\n",
            "Epoch: 13 | Time: 0m 2s\n",
            "\tTrain Loss: 3.051 | Train PPL:  21.134\n",
            "\t Val. Loss: 5.553 |  Val. PPL: 258.014\n",
            "Epoch: 14 | Time: 0m 2s\n",
            "\tTrain Loss: 3.009 | Train PPL:  20.269\n",
            "\t Val. Loss: 5.547 |  Val. PPL: 256.593\n",
            "Epoch: 15 | Time: 0m 2s\n",
            "\tTrain Loss: 2.982 | Train PPL:  19.732\n",
            "\t Val. Loss: 5.547 |  Val. PPL: 256.561\n",
            "Epoch: 16 | Time: 0m 2s\n",
            "\tTrain Loss: 2.936 | Train PPL:  18.843\n",
            "\t Val. Loss: 5.540 |  Val. PPL: 254.637\n",
            "Epoch: 17 | Time: 0m 2s\n",
            "\tTrain Loss: 2.893 | Train PPL:  18.041\n",
            "\t Val. Loss: 5.537 |  Val. PPL: 254.017\n",
            "Epoch: 18 | Time: 0m 2s\n",
            "\tTrain Loss: 2.864 | Train PPL:  17.530\n",
            "\t Val. Loss: 5.530 |  Val. PPL: 252.026\n",
            "Epoch: 19 | Time: 0m 2s\n",
            "\tTrain Loss: 2.824 | Train PPL:  16.852\n",
            "\t Val. Loss: 5.531 |  Val. PPL: 252.334\n",
            "Epoch: 20 | Time: 0m 2s\n",
            "\tTrain Loss: 2.792 | Train PPL:  16.312\n",
            "\t Val. Loss: 5.527 |  Val. PPL: 251.476\n",
            "Epoch: 21 | Time: 0m 2s\n",
            "\tTrain Loss: 2.754 | Train PPL:  15.705\n",
            "\t Val. Loss: 5.520 |  Val. PPL: 249.677\n",
            "Epoch: 22 | Time: 0m 2s\n",
            "\tTrain Loss: 2.724 | Train PPL:  15.238\n",
            "\t Val. Loss: 5.518 |  Val. PPL: 249.094\n",
            "Epoch: 23 | Time: 0m 2s\n",
            "\tTrain Loss: 2.689 | Train PPL:  14.724\n",
            "\t Val. Loss: 5.512 |  Val. PPL: 247.742\n",
            "Epoch: 24 | Time: 0m 2s\n",
            "\tTrain Loss: 2.658 | Train PPL:  14.272\n",
            "\t Val. Loss: 5.510 |  Val. PPL: 247.147\n",
            "Epoch: 25 | Time: 0m 2s\n",
            "\tTrain Loss: 2.623 | Train PPL:  13.779\n",
            "\t Val. Loss: 5.511 |  Val. PPL: 247.464\n",
            "Epoch: 26 | Time: 0m 2s\n",
            "\tTrain Loss: 2.597 | Train PPL:  13.419\n",
            "\t Val. Loss: 5.505 |  Val. PPL: 245.804\n",
            "Epoch: 27 | Time: 0m 2s\n",
            "\tTrain Loss: 2.573 | Train PPL:  13.108\n",
            "\t Val. Loss: 5.499 |  Val. PPL: 244.427\n",
            "Epoch: 28 | Time: 0m 2s\n",
            "\tTrain Loss: 2.536 | Train PPL:  12.628\n",
            "\t Val. Loss: 5.494 |  Val. PPL: 243.114\n",
            "Epoch: 29 | Time: 0m 2s\n",
            "\tTrain Loss: 2.505 | Train PPL:  12.239\n",
            "\t Val. Loss: 5.492 |  Val. PPL: 242.854\n",
            "Epoch: 30 | Time: 0m 2s\n",
            "\tTrain Loss: 2.478 | Train PPL:  11.919\n",
            "\t Val. Loss: 5.484 |  Val. PPL: 240.838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDT6kRLijzEZ",
        "outputId": "5dfcb35e-87e6-41fb-b2cd-d316e23eed63"
      },
      "source": [
        "N_EPOCHS = 50\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = 5.484 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 2.445 | Train PPL:  11.535\n",
            "\t Val. Loss: 5.488 |  Val. PPL: 241.844\n",
            "Epoch: 02 | Time: 0m 2s\n",
            "\tTrain Loss: 2.425 | Train PPL:  11.305\n",
            "\t Val. Loss: 5.480 |  Val. PPL: 239.949\n",
            "Epoch: 03 | Time: 0m 2s\n",
            "\tTrain Loss: 2.403 | Train PPL:  11.058\n",
            "\t Val. Loss: 5.477 |  Val. PPL: 239.027\n",
            "Epoch: 04 | Time: 0m 2s\n",
            "\tTrain Loss: 2.374 | Train PPL:  10.738\n",
            "\t Val. Loss: 5.471 |  Val. PPL: 237.608\n",
            "Epoch: 05 | Time: 0m 2s\n",
            "\tTrain Loss: 2.345 | Train PPL:  10.437\n",
            "\t Val. Loss: 5.465 |  Val. PPL: 236.272\n",
            "Epoch: 06 | Time: 0m 2s\n",
            "\tTrain Loss: 2.324 | Train PPL:  10.217\n",
            "\t Val. Loss: 5.460 |  Val. PPL: 235.171\n",
            "Epoch: 07 | Time: 0m 2s\n",
            "\tTrain Loss: 2.302 | Train PPL:   9.992\n",
            "\t Val. Loss: 5.450 |  Val. PPL: 232.854\n",
            "Epoch: 08 | Time: 0m 2s\n",
            "\tTrain Loss: 2.273 | Train PPL:   9.711\n",
            "\t Val. Loss: 5.454 |  Val. PPL: 233.587\n",
            "Epoch: 09 | Time: 0m 2s\n",
            "\tTrain Loss: 2.253 | Train PPL:   9.516\n",
            "\t Val. Loss: 5.447 |  Val. PPL: 231.988\n",
            "Epoch: 10 | Time: 0m 2s\n",
            "\tTrain Loss: 2.221 | Train PPL:   9.218\n",
            "\t Val. Loss: 5.449 |  Val. PPL: 232.481\n",
            "Epoch: 11 | Time: 0m 2s\n",
            "\tTrain Loss: 2.196 | Train PPL:   8.987\n",
            "\t Val. Loss: 5.445 |  Val. PPL: 231.562\n",
            "Epoch: 12 | Time: 0m 2s\n",
            "\tTrain Loss: 2.175 | Train PPL:   8.802\n",
            "\t Val. Loss: 5.441 |  Val. PPL: 230.705\n",
            "Epoch: 13 | Time: 0m 2s\n",
            "\tTrain Loss: 2.155 | Train PPL:   8.632\n",
            "\t Val. Loss: 5.441 |  Val. PPL: 230.752\n",
            "Epoch: 14 | Time: 0m 2s\n",
            "\tTrain Loss: 2.132 | Train PPL:   8.430\n",
            "\t Val. Loss: 5.440 |  Val. PPL: 230.392\n",
            "Epoch: 15 | Time: 0m 2s\n",
            "\tTrain Loss: 2.110 | Train PPL:   8.250\n",
            "\t Val. Loss: 5.438 |  Val. PPL: 230.004\n",
            "Epoch: 16 | Time: 0m 2s\n",
            "\tTrain Loss: 2.086 | Train PPL:   8.051\n",
            "\t Val. Loss: 5.439 |  Val. PPL: 230.203\n",
            "Epoch: 17 | Time: 0m 2s\n",
            "\tTrain Loss: 2.062 | Train PPL:   7.860\n",
            "\t Val. Loss: 5.439 |  Val. PPL: 230.242\n",
            "Epoch: 18 | Time: 0m 2s\n",
            "\tTrain Loss: 2.044 | Train PPL:   7.722\n",
            "\t Val. Loss: 5.428 |  Val. PPL: 227.606\n",
            "Epoch: 19 | Time: 0m 2s\n",
            "\tTrain Loss: 2.025 | Train PPL:   7.574\n",
            "\t Val. Loss: 5.433 |  Val. PPL: 228.945\n",
            "Epoch: 20 | Time: 0m 2s\n",
            "\tTrain Loss: 2.008 | Train PPL:   7.447\n",
            "\t Val. Loss: 5.428 |  Val. PPL: 227.593\n",
            "Epoch: 21 | Time: 0m 2s\n",
            "\tTrain Loss: 1.995 | Train PPL:   7.353\n",
            "\t Val. Loss: 5.425 |  Val. PPL: 226.936\n",
            "Epoch: 22 | Time: 0m 2s\n",
            "\tTrain Loss: 1.974 | Train PPL:   7.200\n",
            "\t Val. Loss: 5.431 |  Val. PPL: 228.364\n",
            "Epoch: 23 | Time: 0m 2s\n",
            "\tTrain Loss: 1.951 | Train PPL:   7.037\n",
            "\t Val. Loss: 5.425 |  Val. PPL: 226.909\n",
            "Epoch: 24 | Time: 0m 2s\n",
            "\tTrain Loss: 1.943 | Train PPL:   6.977\n",
            "\t Val. Loss: 5.433 |  Val. PPL: 228.762\n",
            "Epoch: 25 | Time: 0m 2s\n",
            "\tTrain Loss: 1.920 | Train PPL:   6.824\n",
            "\t Val. Loss: 5.427 |  Val. PPL: 227.432\n",
            "Epoch: 26 | Time: 0m 2s\n",
            "\tTrain Loss: 1.905 | Train PPL:   6.719\n",
            "\t Val. Loss: 5.427 |  Val. PPL: 227.423\n",
            "Epoch: 27 | Time: 0m 2s\n",
            "\tTrain Loss: 1.889 | Train PPL:   6.615\n",
            "\t Val. Loss: 5.429 |  Val. PPL: 227.880\n",
            "Epoch: 28 | Time: 0m 2s\n",
            "\tTrain Loss: 1.873 | Train PPL:   6.508\n",
            "\t Val. Loss: 5.434 |  Val. PPL: 229.069\n",
            "Epoch: 29 | Time: 0m 2s\n",
            "\tTrain Loss: 1.850 | Train PPL:   6.362\n",
            "\t Val. Loss: 5.430 |  Val. PPL: 228.038\n",
            "Epoch: 30 | Time: 0m 2s\n",
            "\tTrain Loss: 1.842 | Train PPL:   6.307\n",
            "\t Val. Loss: 5.427 |  Val. PPL: 227.426\n",
            "Epoch: 31 | Time: 0m 2s\n",
            "\tTrain Loss: 1.825 | Train PPL:   6.203\n",
            "\t Val. Loss: 5.432 |  Val. PPL: 228.666\n",
            "Epoch: 32 | Time: 0m 2s\n",
            "\tTrain Loss: 1.806 | Train PPL:   6.085\n",
            "\t Val. Loss: 5.435 |  Val. PPL: 229.192\n",
            "Epoch: 33 | Time: 0m 2s\n",
            "\tTrain Loss: 1.794 | Train PPL:   6.012\n",
            "\t Val. Loss: 5.428 |  Val. PPL: 227.698\n",
            "Epoch: 34 | Time: 0m 2s\n",
            "\tTrain Loss: 1.773 | Train PPL:   5.891\n",
            "\t Val. Loss: 5.423 |  Val. PPL: 226.517\n",
            "Epoch: 35 | Time: 0m 2s\n",
            "\tTrain Loss: 1.762 | Train PPL:   5.827\n",
            "\t Val. Loss: 5.429 |  Val. PPL: 227.812\n",
            "Epoch: 36 | Time: 0m 2s\n",
            "\tTrain Loss: 1.753 | Train PPL:   5.774\n",
            "\t Val. Loss: 5.418 |  Val. PPL: 225.335\n",
            "Epoch: 37 | Time: 0m 2s\n",
            "\tTrain Loss: 1.735 | Train PPL:   5.667\n",
            "\t Val. Loss: 5.421 |  Val. PPL: 226.186\n",
            "Epoch: 38 | Time: 0m 2s\n",
            "\tTrain Loss: 1.723 | Train PPL:   5.602\n",
            "\t Val. Loss: 5.424 |  Val. PPL: 226.781\n",
            "Epoch: 39 | Time: 0m 2s\n",
            "\tTrain Loss: 1.714 | Train PPL:   5.552\n",
            "\t Val. Loss: 5.423 |  Val. PPL: 226.561\n",
            "Epoch: 40 | Time: 0m 2s\n",
            "\tTrain Loss: 1.692 | Train PPL:   5.433\n",
            "\t Val. Loss: 5.425 |  Val. PPL: 226.960\n",
            "Epoch: 41 | Time: 0m 2s\n",
            "\tTrain Loss: 1.684 | Train PPL:   5.387\n",
            "\t Val. Loss: 5.423 |  Val. PPL: 226.638\n",
            "Epoch: 42 | Time: 0m 2s\n",
            "\tTrain Loss: 1.672 | Train PPL:   5.323\n",
            "\t Val. Loss: 5.429 |  Val. PPL: 227.955\n",
            "Epoch: 43 | Time: 0m 2s\n",
            "\tTrain Loss: 1.657 | Train PPL:   5.242\n",
            "\t Val. Loss: 5.423 |  Val. PPL: 226.658\n",
            "Epoch: 44 | Time: 0m 2s\n",
            "\tTrain Loss: 1.643 | Train PPL:   5.172\n",
            "\t Val. Loss: 5.428 |  Val. PPL: 227.703\n",
            "Epoch: 45 | Time: 0m 2s\n",
            "\tTrain Loss: 1.641 | Train PPL:   5.159\n",
            "\t Val. Loss: 5.421 |  Val. PPL: 226.170\n",
            "Epoch: 46 | Time: 0m 2s\n",
            "\tTrain Loss: 1.617 | Train PPL:   5.038\n",
            "\t Val. Loss: 5.429 |  Val. PPL: 227.848\n",
            "Epoch: 47 | Time: 0m 2s\n",
            "\tTrain Loss: 1.604 | Train PPL:   4.971\n",
            "\t Val. Loss: 5.426 |  Val. PPL: 227.173\n",
            "Epoch: 48 | Time: 0m 2s\n",
            "\tTrain Loss: 1.599 | Train PPL:   4.949\n",
            "\t Val. Loss: 5.433 |  Val. PPL: 228.753\n",
            "Epoch: 49 | Time: 0m 2s\n",
            "\tTrain Loss: 1.592 | Train PPL:   4.911\n",
            "\t Val. Loss: 5.433 |  Val. PPL: 228.923\n",
            "Epoch: 50 | Time: 0m 2s\n",
            "\tTrain Loss: 1.570 | Train PPL:   4.807\n",
            "\t Val. Loss: 5.435 |  Val. PPL: 229.236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxSXWHMakX9S"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=5e-6, weight_decay=0.0005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElnQtHVqlkSf",
        "outputId": "19685ae8-dc80-47af-d6b1-e48e37ad43b9"
      },
      "source": [
        "best_valid_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.96515295902888"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGCUoX7gkfQF",
        "outputId": "28947cb6-e493-47be-dc55-eb85a4311375"
      },
      "source": [
        "N_EPOCHS = 50\r\n",
        "CLIP = 1\r\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\r\n",
        "best_valid_loss = 4.9634 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 2.041 | Train PPL:   7.702\n",
            "\t Val. Loss: 4.956 |  Val. PPL: 141.985\n",
            "Epoch: 02 | Time: 0m 2s\n",
            "\tTrain Loss: 2.037 | Train PPL:   7.665\n",
            "\t Val. Loss: 4.958 |  Val. PPL: 142.298\n",
            "Epoch: 03 | Time: 0m 2s\n",
            "\tTrain Loss: 2.030 | Train PPL:   7.611\n",
            "\t Val. Loss: 4.958 |  Val. PPL: 142.280\n",
            "Epoch: 04 | Time: 0m 2s\n",
            "\tTrain Loss: 2.035 | Train PPL:   7.649\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.447\n",
            "Epoch: 05 | Time: 0m 2s\n",
            "\tTrain Loss: 2.033 | Train PPL:   7.634\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.554\n",
            "Epoch: 06 | Time: 0m 2s\n",
            "\tTrain Loss: 2.027 | Train PPL:   7.589\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.524\n",
            "Epoch: 07 | Time: 0m 2s\n",
            "\tTrain Loss: 2.032 | Train PPL:   7.628\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.545\n",
            "Epoch: 08 | Time: 0m 2s\n",
            "\tTrain Loss: 2.031 | Train PPL:   7.622\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.499\n",
            "Epoch: 09 | Time: 0m 2s\n",
            "\tTrain Loss: 2.027 | Train PPL:   7.590\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.523\n",
            "Epoch: 10 | Time: 0m 2s\n",
            "\tTrain Loss: 2.027 | Train PPL:   7.595\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.436\n",
            "Epoch: 11 | Time: 0m 2s\n",
            "\tTrain Loss: 2.026 | Train PPL:   7.580\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.548\n",
            "Epoch: 12 | Time: 0m 2s\n",
            "\tTrain Loss: 2.020 | Train PPL:   7.539\n",
            "\t Val. Loss: 4.958 |  Val. PPL: 142.334\n",
            "Epoch: 13 | Time: 0m 2s\n",
            "\tTrain Loss: 2.014 | Train PPL:   7.496\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.540\n",
            "Epoch: 14 | Time: 0m 2s\n",
            "\tTrain Loss: 2.018 | Train PPL:   7.520\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.471\n",
            "Epoch: 15 | Time: 0m 2s\n",
            "\tTrain Loss: 2.016 | Train PPL:   7.507\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.414\n",
            "Epoch: 16 | Time: 0m 2s\n",
            "\tTrain Loss: 2.016 | Train PPL:   7.512\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.499\n",
            "Epoch: 17 | Time: 0m 2s\n",
            "\tTrain Loss: 2.011 | Train PPL:   7.474\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.553\n",
            "Epoch: 18 | Time: 0m 2s\n",
            "\tTrain Loss: 2.015 | Train PPL:   7.500\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.520\n",
            "Epoch: 19 | Time: 0m 2s\n",
            "\tTrain Loss: 2.014 | Train PPL:   7.491\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.642\n",
            "Epoch: 20 | Time: 0m 2s\n",
            "\tTrain Loss: 2.011 | Train PPL:   7.471\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.690\n",
            "Epoch: 21 | Time: 0m 2s\n",
            "\tTrain Loss: 2.012 | Train PPL:   7.475\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.573\n",
            "Epoch: 22 | Time: 0m 2s\n",
            "\tTrain Loss: 2.014 | Train PPL:   7.491\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.519\n",
            "Epoch: 23 | Time: 0m 2s\n",
            "\tTrain Loss: 2.012 | Train PPL:   7.482\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.492\n",
            "Epoch: 24 | Time: 0m 2s\n",
            "\tTrain Loss: 2.002 | Train PPL:   7.405\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.566\n",
            "Epoch: 25 | Time: 0m 2s\n",
            "\tTrain Loss: 2.011 | Train PPL:   7.470\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.438\n",
            "Epoch: 26 | Time: 0m 2s\n",
            "\tTrain Loss: 2.005 | Train PPL:   7.430\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.510\n",
            "Epoch: 27 | Time: 0m 2s\n",
            "\tTrain Loss: 2.008 | Train PPL:   7.445\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.534\n",
            "Epoch: 28 | Time: 0m 2s\n",
            "\tTrain Loss: 1.999 | Train PPL:   7.380\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.556\n",
            "Epoch: 29 | Time: 0m 2s\n",
            "\tTrain Loss: 1.999 | Train PPL:   7.380\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.615\n",
            "Epoch: 30 | Time: 0m 2s\n",
            "\tTrain Loss: 2.002 | Train PPL:   7.400\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.585\n",
            "Epoch: 31 | Time: 0m 2s\n",
            "\tTrain Loss: 2.002 | Train PPL:   7.403\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.583\n",
            "Epoch: 32 | Time: 0m 2s\n",
            "\tTrain Loss: 2.003 | Train PPL:   7.413\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.580\n",
            "Epoch: 33 | Time: 0m 2s\n",
            "\tTrain Loss: 2.001 | Train PPL:   7.400\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.585\n",
            "Epoch: 34 | Time: 0m 2s\n",
            "\tTrain Loss: 1.997 | Train PPL:   7.367\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.534\n",
            "Epoch: 35 | Time: 0m 2s\n",
            "\tTrain Loss: 1.995 | Train PPL:   7.355\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.585\n",
            "Epoch: 36 | Time: 0m 2s\n",
            "\tTrain Loss: 1.996 | Train PPL:   7.361\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.666\n",
            "Epoch: 37 | Time: 0m 2s\n",
            "\tTrain Loss: 1.996 | Train PPL:   7.356\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.630\n",
            "Epoch: 38 | Time: 0m 2s\n",
            "\tTrain Loss: 1.996 | Train PPL:   7.362\n",
            "\t Val. Loss: 4.962 |  Val. PPL: 142.864\n",
            "Epoch: 39 | Time: 0m 2s\n",
            "\tTrain Loss: 1.992 | Train PPL:   7.327\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.756\n",
            "Epoch: 40 | Time: 0m 2s\n",
            "\tTrain Loss: 1.998 | Train PPL:   7.377\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.738\n",
            "Epoch: 41 | Time: 0m 2s\n",
            "\tTrain Loss: 1.992 | Train PPL:   7.333\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.794\n",
            "Epoch: 42 | Time: 0m 2s\n",
            "\tTrain Loss: 1.995 | Train PPL:   7.351\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.758\n",
            "Epoch: 43 | Time: 0m 2s\n",
            "\tTrain Loss: 1.995 | Train PPL:   7.352\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.701\n",
            "Epoch: 44 | Time: 0m 2s\n",
            "\tTrain Loss: 1.985 | Train PPL:   7.280\n",
            "\t Val. Loss: 4.962 |  Val. PPL: 142.863\n",
            "Epoch: 45 | Time: 0m 2s\n",
            "\tTrain Loss: 1.990 | Train PPL:   7.315\n",
            "\t Val. Loss: 4.962 |  Val. PPL: 142.815\n",
            "Epoch: 46 | Time: 0m 2s\n",
            "\tTrain Loss: 1.988 | Train PPL:   7.304\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.743\n",
            "Epoch: 47 | Time: 0m 2s\n",
            "\tTrain Loss: 1.990 | Train PPL:   7.318\n",
            "\t Val. Loss: 4.962 |  Val. PPL: 142.864\n",
            "Epoch: 48 | Time: 0m 2s\n",
            "\tTrain Loss: 1.982 | Train PPL:   7.254\n",
            "\t Val. Loss: 4.962 |  Val. PPL: 142.848\n",
            "Epoch: 49 | Time: 0m 2s\n",
            "\tTrain Loss: 1.983 | Train PPL:   7.262\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.738\n",
            "Epoch: 50 | Time: 0m 2s\n",
            "\tTrain Loss: 1.988 | Train PPL:   7.303\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.791\n",
            "4.955717941125234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6WKCHQNrw1i"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=5e-8, weight_decay=0.005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNivIH5ir1HI",
        "outputId": "c1a6fa86-985a-49de-87ce-47eb4f6f730b"
      },
      "source": [
        "N_EPOCHS = 20\r\n",
        "CLIP = 1\r\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\r\n",
        "best_valid_loss = 4.890922844409943 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 2.241 | Train PPL:   9.402\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.906\n",
            "Epoch: 02 | Time: 0m 2s\n",
            "\tTrain Loss: 2.248 | Train PPL:   9.473\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.938\n",
            "Epoch: 03 | Time: 0m 2s\n",
            "\tTrain Loss: 2.255 | Train PPL:   9.537\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.961\n",
            "Epoch: 04 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.477\n",
            "\t Val. Loss: 4.891 |  Val. PPL: 133.118\n",
            "Epoch: 05 | Time: 0m 2s\n",
            "\tTrain Loss: 2.259 | Train PPL:   9.572\n",
            "\t Val. Loss: 4.891 |  Val. PPL: 133.125\n",
            "Epoch: 06 | Time: 0m 2s\n",
            "\tTrain Loss: 2.255 | Train PPL:   9.532\n",
            "\t Val. Loss: 4.891 |  Val. PPL: 133.151\n",
            "Epoch: 07 | Time: 0m 2s\n",
            "\tTrain Loss: 2.262 | Train PPL:   9.598\n",
            "\t Val. Loss: 4.892 |  Val. PPL: 133.192\n",
            "Epoch: 08 | Time: 0m 2s\n",
            "\tTrain Loss: 2.263 | Train PPL:   9.611\n",
            "\t Val. Loss: 4.892 |  Val. PPL: 133.223\n",
            "Epoch: 09 | Time: 0m 2s\n",
            "\tTrain Loss: 2.267 | Train PPL:   9.647\n",
            "\t Val. Loss: 4.892 |  Val. PPL: 133.251\n",
            "Epoch: 10 | Time: 0m 2s\n",
            "\tTrain Loss: 2.265 | Train PPL:   9.628\n",
            "\t Val. Loss: 4.893 |  Val. PPL: 133.313\n",
            "Epoch: 11 | Time: 0m 2s\n",
            "\tTrain Loss: 2.269 | Train PPL:   9.669\n",
            "\t Val. Loss: 4.893 |  Val. PPL: 133.346\n",
            "Epoch: 12 | Time: 0m 2s\n",
            "\tTrain Loss: 2.273 | Train PPL:   9.708\n",
            "\t Val. Loss: 4.893 |  Val. PPL: 133.371\n",
            "Epoch: 13 | Time: 0m 2s\n",
            "\tTrain Loss: 2.276 | Train PPL:   9.737\n",
            "\t Val. Loss: 4.893 |  Val. PPL: 133.404\n",
            "Epoch: 14 | Time: 0m 2s\n",
            "\tTrain Loss: 2.279 | Train PPL:   9.770\n",
            "\t Val. Loss: 4.894 |  Val. PPL: 133.433\n",
            "Epoch: 15 | Time: 0m 2s\n",
            "\tTrain Loss: 2.284 | Train PPL:   9.814\n",
            "\t Val. Loss: 4.894 |  Val. PPL: 133.461\n",
            "Epoch: 16 | Time: 0m 2s\n",
            "\tTrain Loss: 2.285 | Train PPL:   9.825\n",
            "\t Val. Loss: 4.894 |  Val. PPL: 133.499\n",
            "Epoch: 17 | Time: 0m 2s\n",
            "\tTrain Loss: 2.290 | Train PPL:   9.875\n",
            "\t Val. Loss: 4.894 |  Val. PPL: 133.540\n",
            "Epoch: 18 | Time: 0m 2s\n",
            "\tTrain Loss: 2.292 | Train PPL:   9.892\n",
            "\t Val. Loss: 4.895 |  Val. PPL: 133.597\n",
            "Epoch: 19 | Time: 0m 2s\n",
            "\tTrain Loss: 2.295 | Train PPL:   9.920\n",
            "\t Val. Loss: 4.895 |  Val. PPL: 133.626\n",
            "Epoch: 20 | Time: 0m 2s\n",
            "\tTrain Loss: 2.301 | Train PPL:   9.981\n",
            "\t Val. Loss: 4.895 |  Val. PPL: 133.678\n",
            "4.88964198033015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcsr26pardUo",
        "outputId": "7dd7f8bc-b25e-4b0f-d69a-cd7e9276a6b8"
      },
      "source": [
        "N_EPOCHS = 20\r\n",
        "CLIP = 1\r\n",
        "model.load_state_dict(torch.load('/content/CommonQA.pt'))\r\n",
        "best_valid_loss = 4.890922844409943 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 2.241 | Train PPL:   9.402\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.904\n",
            "Epoch: 02 | Time: 0m 2s\n",
            "\tTrain Loss: 2.242 | Train PPL:   9.415\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.907\n",
            "Epoch: 03 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.474\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.911\n",
            "Epoch: 04 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.475\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.913\n",
            "Epoch: 05 | Time: 0m 2s\n",
            "\tTrain Loss: 2.248 | Train PPL:   9.470\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.915\n",
            "Epoch: 06 | Time: 0m 2s\n",
            "\tTrain Loss: 2.247 | Train PPL:   9.463\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.918\n",
            "Epoch: 07 | Time: 0m 2s\n",
            "\tTrain Loss: 2.245 | Train PPL:   9.439\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.920\n",
            "Epoch: 08 | Time: 0m 2s\n",
            "\tTrain Loss: 2.248 | Train PPL:   9.465\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.924\n",
            "Epoch: 09 | Time: 0m 2s\n",
            "\tTrain Loss: 2.242 | Train PPL:   9.415\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.927\n",
            "Epoch: 10 | Time: 0m 2s\n",
            "\tTrain Loss: 2.252 | Train PPL:   9.511\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.932\n",
            "Epoch: 11 | Time: 0m 2s\n",
            "\tTrain Loss: 2.247 | Train PPL:   9.455\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.936\n",
            "Epoch: 12 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.476\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.938\n",
            "Epoch: 13 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.479\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.942\n",
            "Epoch: 14 | Time: 0m 2s\n",
            "\tTrain Loss: 2.252 | Train PPL:   9.506\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.945\n",
            "Epoch: 15 | Time: 0m 2s\n",
            "\tTrain Loss: 2.250 | Train PPL:   9.490\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.948\n",
            "Epoch: 16 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.480\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.947\n",
            "Epoch: 17 | Time: 0m 2s\n",
            "\tTrain Loss: 2.250 | Train PPL:   9.485\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.949\n",
            "Epoch: 18 | Time: 0m 2s\n",
            "\tTrain Loss: 2.246 | Train PPL:   9.447\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.953\n",
            "Epoch: 19 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.479\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.955\n",
            "Epoch: 20 | Time: 0m 2s\n",
            "\tTrain Loss: 2.252 | Train PPL:   9.511\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.959\n",
            "4.889623761177063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztR5mNm8EzFn"
      },
      "source": [
        "Finally, we test the model on the test set using these \"best\" parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaJo3X9aEzFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e02fec-6f49-4a92-8619-4c7a3547c9a7"
      },
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 10.525 | Test PPL: 37253.098 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY7SsC8TEzFn"
      },
      "source": [
        "Just looking at the test loss, we get better performance. This is a pretty good sign that this model architecture is doing something right! Relieving the information compression seems like the way forard, and in the next tutorial we'll expand on this even further with *attention*."
      ]
    }
  ]
}
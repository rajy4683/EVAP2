{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "ENDS9_Attn_AmbigQa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EVAP2/blob/master/ENDS9_Attn_AmbigQa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzFdPi_8-3fY",
        "outputId": "22bd7fad-a48b-4c8a-f410-4d64d0a0dee3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jan  7 17:29:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KDJ9ysmCdyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9442b3bd-e100-4187-84a9-d873ef50b491"
      },
      "source": [
        "%%bash\n",
        "python -m spacy download en\n",
        "python -m spacy download de"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.1.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (51.1.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py): started\n",
            "  Building wheel for de-core-news-sm (setup.py): finished with status 'done'\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907057 sha256=1f6380b0c7b2679cbea0b13a29d5a06a28019a7bdd460494d8c25855b027b404\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5sojlhy5/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXPzJpICCdeo"
      },
      "source": [
        "**Restart Notebook**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rySJ5AJb9ejE"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD1fj0R39ejE"
      },
      "source": [
        "Set the random seeds for reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZJrN5ry9ejE"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4Cul6h19ejF"
      },
      "source": [
        "Load the German and English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xxVGIID6vL4",
        "outputId": "8a6798b6-2d15-4c6c-edbe-353efbe3b2de"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmVlgfQY9ejF"
      },
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3BulNoP5slo",
        "outputId": "59cb11b3-c1e0-4bd5-b0ba-a61ff1198931"
      },
      "source": [
        "!unzip /content/ambignq_light.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/ambignq_light.zip\n",
            "   creating: ambignq_light/\n",
            "  inflating: ambignq_light/dev_light.json  \n",
            "  inflating: ambignq_light/LICENSE   \n",
            "  inflating: ambignq_light/train_light.json  \n",
            "  inflating: dev_light.json          \n",
            "  inflating: LICENSE                 \n",
            "  inflating: train_light.json        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4ZDUMCT_Wyn"
      },
      "source": [
        "sentence = []\r\n",
        "label = []\r\n",
        "with open('/content/OpenBookQa/train.jsonl') as h:\r\n",
        "    for line in h:\r\n",
        "        example = json.loads(line)\r\n",
        "        print(example.keys())\r\n",
        "        # scores = []\r\n",
        "        merged_choices = ' A: '.join([choice['text'] for choice in example['question']['choices']])\r\n",
        "        input = 'Q: ' + example['question']['stem'] + ' A: ' + merged_choices\r\n",
        "        correct_answer = [ choice['text'] for choice in example['question']['choices'] if choice['label'] == example['answerKey'] ][0]\r\n",
        "        sentence.append(input)\r\n",
        "        label.append(correct_answer)\r\n",
        "        #print(input, correct_answer)\r\n",
        "dataset_df = pd.DataFrame({'sentence':sentence, 'label':label})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "MQxTkA_KkUNg",
        "outputId": "dc697465-6b25-4ec8-c5e3-e73af63146c8"
      },
      "source": [
        "dataset_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q: The sun is responsible for A: puppies learn...</td>\n",
              "      <td>plants sprouting, blooming and wilting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q: When standing miles away from Mount Rushmor...</td>\n",
              "      <td>the mountains seem smaller than in photographs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q: When food is reduced in the stomach A: the ...</td>\n",
              "      <td>nutrients are being deconstructed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q: Stars are A: warm lights that float A: made...</td>\n",
              "      <td>great balls of gas burning billions of miles away</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q: You can make a telescope with a A: straw A:...</td>\n",
              "      <td>mailing tube</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4952</th>\n",
              "      <td>Q: A bulldozer alters the area of A: skyscrape...</td>\n",
              "      <td>skyscrapers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4953</th>\n",
              "      <td>Q: An organism that can survive without the he...</td>\n",
              "      <td>Brewer's yeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4954</th>\n",
              "      <td>Q: The nimbleness of this animal is a key adap...</td>\n",
              "      <td>the antelope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4955</th>\n",
              "      <td>Q: Birds will have different kinds of beaks de...</td>\n",
              "      <td>organisms they hunt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4956</th>\n",
              "      <td>Q: Harriet wants to know the area of a rectang...</td>\n",
              "      <td>a ruler</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4957 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence                                              label\n",
              "0     Q: The sun is responsible for A: puppies learn...             plants sprouting, blooming and wilting\n",
              "1     Q: When standing miles away from Mount Rushmor...     the mountains seem smaller than in photographs\n",
              "2     Q: When food is reduced in the stomach A: the ...                  nutrients are being deconstructed\n",
              "3     Q: Stars are A: warm lights that float A: made...  great balls of gas burning billions of miles away\n",
              "4     Q: You can make a telescope with a A: straw A:...                                       mailing tube\n",
              "...                                                 ...                                                ...\n",
              "4952  Q: A bulldozer alters the area of A: skyscrape...                                        skyscrapers\n",
              "4953  Q: An organism that can survive without the he...                                     Brewer's yeast\n",
              "4954  Q: The nimbleness of this animal is a key adap...                                       the antelope\n",
              "4955  Q: Birds will have different kinds of beaks de...                                organisms they hunt\n",
              "4956  Q: Harriet wants to know the area of a rectang...                                            a ruler\n",
              "\n",
              "[4957 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO5tFcCf6Rjk"
      },
      "source": [
        "import itertools\r\n",
        "import json\r\n",
        "import os\r\n",
        "import csv\r\n",
        "import errno\r\n",
        "import random\r\n",
        "from random import shuffle\r\n",
        "from typing import List\r\n",
        "import spacy\r\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuxgHLzc6GeY"
      },
      "source": [
        "def ambigqa():\r\n",
        "    def read_file(file, dir, split):\r\n",
        "        outfile = open(f\"{dir}/{split}.tsv\", \"w\")\r\n",
        "        outfile_meta = open(f\"{dir}/{split}_meta.tsv\", \"w\")\r\n",
        "        size = 0\r\n",
        "        with open(file, \"r\") as f:\r\n",
        "            json_file = json.load(f)\r\n",
        "            for item in tqdm(json_file):\r\n",
        "                question = item['question'].replace(\"\\n\", \" \").replace(\"\\t\", \" \")\r\n",
        "                single_answers_already_included = []\r\n",
        "                for anno in item[\"annotations\"]:\r\n",
        "                    if anno['type'] == \"singleAnswer\":\r\n",
        "                        for ans in anno['answer']:\r\n",
        "                            if ans not in single_answers_already_included:\r\n",
        "                                ans = ans.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\r\n",
        "                                outfile.write(f\"{question}\\t{ans}\\n\")\r\n",
        "                                outfile_meta.write(item['id'] + \"\\n\")\r\n",
        "                                single_answers_already_included.append(ans)\r\n",
        "                                size += 1\r\n",
        "                    else:\r\n",
        "                        answers = []\r\n",
        "                        for x in anno['qaPairs']:\r\n",
        "                            answers.append(x['answer'][0])\r\n",
        "\r\n",
        "                        answers = [x.strip() for x in answers]\r\n",
        "                        answers = list(set(answers))  # to drop duplicate answers\r\n",
        "                        for i, ordering in enumerate(itertools.permutations(answers)):\r\n",
        "                            if i >= 3:\r\n",
        "                                break\r\n",
        "                            ans_str = \" [SEP] \".join(ordering).replace(\"\\n\", \" \").replace(\"\\t\", \" \")\r\n",
        "                            outfile.write(f\"{question}\\t{ans_str}\\n\")\r\n",
        "                            outfile_meta.write(item['id'] + \"\\n\")\r\n",
        "                            size += 1\r\n",
        "        return size\r\n",
        "\r\n",
        "    count_dev = read_file(\"/content/ambignq_light/dev_light.json\", \"/content/ambigqa\", \"dev\")\r\n",
        "    count_train = read_file(\"/content/ambignq_light/train_light.json\", \"/content/ambigqa\", \"train\")\r\n",
        "    count_test = 0\r\n",
        "\r\n",
        "    # Create TSVs and get counts.\r\n",
        "    with open(\"ambigqa/counts.json\", \"w\") as outfile:\r\n",
        "        json.dump({\"train\": count_train, \"dev\": count_dev, \"test\": count_test}, outfile)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8M_SRwm6PZ7",
        "outputId": "e7238639-4a14-4c01-af59-8a54c92513d0"
      },
      "source": [
        "ambigqa()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2002/2002 [00:00<00:00, 130643.75it/s]\n",
            "100%|██████████| 10036/10036 [00:00<00:00, 243129.80it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw0jF9DN7A0g"
      },
      "source": [
        "#dataset = pd.read_csv(\"/content/dev.tsv\", sep='\\t', header=None)\r\n",
        "dataset = pd.read_csv(\"/content/ambigqa/train.tsv\", sep='\\t', header=None)\r\n",
        "dataset.columns = [\"sentence\", \"label\"]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSzI_oby9ejF"
      },
      "source": [
        "We create the tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8iY7pVS9ejF"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB3nEW--9ejF"
      },
      "source": [
        "The fields remain the same as before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1DXtJYx9ejF"
      },
      "source": [
        "Load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ztszveY7W61"
      },
      "source": [
        "from torchtext import data \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "Sentence = data.Field(sequential = True, tokenize = 'spacy', batch_first =False, include_lengths=False, lower=False)\r\n",
        "Label = data.Field(sequential =True, tokenize ='spacy', is_target=False, batch_first =False, include_lengths=False, lower=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N-aDvBXPI_u"
      },
      "source": [
        "## Pruned dataset\r\n",
        "Dataset was pruned to 5000 entries given the size of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHubd2OF7bA6"
      },
      "source": [
        "#dataset = dataset_df\r\n",
        "fields = [('sentence', Sentence),('label',Label)]\r\n",
        "#example = [data.Example.fromlist([dataset.sentence[i],dataset.label[i]], fields) for i in range(dataset.shape[0])] \r\n",
        "example = [data.Example.fromlist([dataset.sentence[i],dataset.label[i]], fields) for i in range(5000)] \r\n",
        "commonqa_ds = data.Dataset(example, fields)\r\n",
        "(train, valid) = commonqa_ds.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spbkkebl7hed"
      },
      "source": [
        "Sentence.build_vocab(train)\r\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHjXASdO7lpN",
        "outputId": "5b0f2bc7-be5b-4cf3-af40-ba6f14608c81"
      },
      "source": [
        "print('Size of input vocab : ', len(Sentence.vocab))\r\n",
        "print('Size of label vocab : ', len(Label.vocab))\r\n",
        "print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\r\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  4450\n",
            "Size of label vocab :  5708\n",
            "Top 10 words appreared repeatedly : [('?', 4250), ('the', 3719), ('Who', 1797), ('in', 1581), ('of', 1437), ('is', 899), ('When', 842), ('did', 578), ('What', 540), ('was', 521)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7f7460c55a60>, {'<unk>': 0, '<pad>': 1, 'SEP': 2, '[': 3, ']': 4, ',': 5, 'and': 6, 'the': 7, '2017': 8, 'of': 9, '-': 10, '2018': 11, 'September': 12, 'July': 13, 'November': 14, 'October': 15, 'May': 16, 'The': 17, 'August': 18, 'March': 19, 'April': 20, 'June': 21, '\"': 22, '1': 23, '2016': 24, 'December': 25, 'January': 26, '2015': 27, '12': 28, '15': 29, 'John': 30, '2': 31, '6': 32, '25': 33, '13': 34, '16': 35, 'Michael': 36, 'in': 37, '17': 38, '23': 39, '20': 40, 'New': 41, ')': 42, '21': 43, '10': 44, '9': 45, 'February': 46, '(': 47, 'United': 48, '3': 49, '7': 50, '8': 51, 'James': 52, '26': 53, '18': 54, '5': 55, 'to': 56, \"'s\": 57, '11': 58, '14': 59, '30': 60, 'a': 61, '2012': 62, '4': 63, '2014': 64, '19': 65, '27': 66, '24': 67, 'States': 68, 'de': 69, '22': 70, 'Charles': 71, '2013': 72, '2019': 73, 'Jr.': 74, '1998': 75, '28': 76, 'William': 77, 'years': 78, '2006': 79, '31': 80, 'King': 81, 'David': 82, 'II': 83, 'Richard': 84, 'Thomas': 85, '2005': 86, '2009': 87, 'Paul': 88, 'South': 89, '29': 90, 'Jennifer': 91, 'Martin': 92, '.': 93, 'with': 94, '1991': 95, '1997': 96, '2008': 97, 'or': 98, 'France': 99, 'Taylor': 100, '2001': 101, '2007': 102, 'Jones': 103, '1974': 104, '2000': 105, 'President': 106, '–': 107, 'Chris': 108, 'Frank': 109, 'Peter': 110, 'V.': 111, '1973': 112, 'Brazil': 113, 'George': 114, 'III': 115, 'Robert': 116, 'State': 117, '1971': 118, '1988': 119, '2010': 120, 'Airport': 121, 'Alexander': 122, 'England': 123, 'International': 124, 'Joseph': 125, 'Louis': 126, 'ago': 127, '1945': 128, '1980': 129, '2003': 130, '2011': 131, 'Anthony': 132, 'Daniel': 133, 'Mike': 134, 'Patrick': 135, 'Republic': 136, 'Season': 137, 'million': 138, '1994': 139, 'California': 140, 'Mary': 141, 'Ryan': 142, 'Sir': 143, 'Wilson': 144, 'billion': 145, '1977': 146, '1981': 147, '1985': 148, '1987': 149, '1990': 150, 'Angeles': 151, 'Ashley': 152, 'British': 153, 'India': 154, 'Jason': 155, 'Los': 156, 'North': 157, 'Sam': 158, 'Tom': 159, 'York': 160, 'Zealand': 161, 'v.': 162, '/': 163, '1999': 164, 'American': 165, 'Harris': 166, 'Henry': 167, 'Smith': 168, 'War': 169, 'Williams': 170, 'World': 171, 'between': 172, 'one': 173, '2004': 174, 'Davis': 175, 'Germany': 176, 'Jeff': 177, 'Lewis': 178, 'River': 179, 'Stephen': 180, '1620': 181, '1965': 182, '2002': 183, 'Africa': 184, 'Brown': 185, 'Elizabeth': 186, 'Francis': 187, 'Hall': 188, 'Harry': 189, 'I': 190, 'Jordan': 191, 'Lee': 192, 'Moore': 193, 'Plate': 194, 'Roger': 195, 'Washington': 196, 'by': 197, 'from': 198, '&': 199, '1968': 200, 'Adams': 201, 'Arthur': 202, 'Canada': 203, 'Charlie': 204, 'Christopher': 205, 'Day': 206, 'Dylan': 207, 'J.': 208, 'Lake': 209, 'Saint': 210, 'Scott': 211, 'Sunday': 212, 'Thompson': 213, 'University': 214, 'White': 215, 'as': 216, '1954': 217, '1961': 218, 'Ben': 219, 'City': 220, 'Cristiano': 221, 'Edward': 222, 'Eric': 223, 'Franklin': 224, 'General': 225, 'Green': 226, 'Howard': 227, 'Isaac': 228, 'Islands': 229, 'Jack': 230, 'Jean': 231, 'Pacific': 232, 'Perry': 233, 'Philip': 234, 'Ronaldo': 235, 'St.': 236, 'Tony': 237, 'was': 238, '1901': 239, '1992': 240, '2020': 241, 'Anna': 242, 'Apostle': 243, 'China': 244, 'Columbia': 245, 'Drew': 246, 'Johnny': 247, 'Karl': 248, 'Keith': 249, 'Kevin': 250, 'Kim': 251, 'Ltd': 252, 'Mexico': 253, 'Russell': 254, 'Street': 255, 'Texas': 256, 'for': 257, 'on': 258, '×': 259, '1925': 260, '1938': 261, '1995': 262, '34': 263, '?': 264, 'Alan': 265, 'Alison': 266, 'Bill': 267, 'Carolina': 268, 'Cook': 269, 'Dean': 270, 'Field': 271, 'Jersey': 272, 'Kelly': 273, 'Kingdom': 274, 'Miller': 275, 'Prince': 276, 'Rose': 277, 'Spain': 278, 'Steve': 279, 'Stewart': 280, 'T.': 281, 'Urban': 282, 'at': 283, 'hours': 284, 'm': 285, '%': 286, '1967': 287, '1969': 288, '1976': 289, 'A': 290, 'Aaron': 291, 'Albert': 292, 'Austin': 293, 'Black': 294, 'Carter': 295, 'Christian': 296, 'Colin': 297, 'Ed': 298, 'Emma': 299, 'Episode': 300, 'Eva': 301, 'Francisco': 302, 'G.': 303, 'Hans': 304, 'House': 305, 'Jackson': 306, 'Jennings': 307, 'Jerry': 308, 'Jimmy': 309, 'Kennedy': 310, 'London': 311, 'Lythgoe': 312, 'Mark': 313, 'Matthew': 314, 'Men': 315, 'Pennsylvania': 316, 'Ray': 317, 'Roberts': 318, 'Trump': 319, 'Tyler': 320, 'Vancouver': 321, 'Western': 322, 'When': 323, 'Will': 324, 'Winter': 325, 'northern': 326, 'team': 327, 'three': 328, '°': 329, '0': 330, '1931': 331, '1946': 332, '1957': 333, '1982': 334, '1984': 335, '1986': 336, 'Air': 337, 'Alex': 338, 'Anderson': 339, 'Bay': 340, 'Bobby': 341, 'Brady': 342, 'Brees': 343, 'Commission': 344, 'Congress': 345, 'Connick': 346, 'County': 347, 'Dan': 348, 'Donald': 349, 'Florida': 350, 'Fred': 351, 'Gary': 352, 'Gordon': 353, 'Halloween': 354, 'Hamilton': 355, 'Houston': 356, 'José': 357, 'L.': 358, 'Lopez': 359, 'Maccabees': 360, 'Manchester': 361, 'Mann': 362, 'Max': 363, 'Nancy': 364, 'Nicholas': 365, 'Robinson': 366, 'Solstice': 367, 'Soviet': 368, 'Stadium': 369, 'Terry': 370, 'Tim': 371, 'Van': 372, 'after': 373, 'episode': 374, 'five': 375, 'joint': 376, 'mm': 377, 'over': 378, 'percent': 379, 'salamander': 380, 'season': 381, 'six': 382, 'south': 383, '€': 384, '!': 385, '1950': 386, '1958': 387, '1975': 388, '1989': 389, '40': 390, 'A.': 391, 'Adam': 392, 'Anne': 393, 'Asia': 394, 'Atlanta': 395, 'Carpenter': 396, 'Cleveland': 397, 'Club': 398, 'Council': 399, 'Daniels': 400, 'Dawn': 401, 'Delta': 402, 'East': 403, 'Erik': 404, 'Force': 405, 'Geoff': 406, 'Italy': 407, 'Jake': 408, 'Jonathan': 409, 'L4': 410, 'Las': 411, 'Leviticus': 412, 'Luke': 413, 'M.': 414, 'Marquis': 415, 'Nathan': 416, 'Nigel': 417, 'Ohio': 418, 'Park': 419, 'Pete': 420, 'Ram': 421, 'Randy': 422, 'Rob': 423, 'Sarah': 424, 'Second': 425, 'Speaker': 426, 'Super': 427, 'Susan': 428, 'Thunder': 429, 'Timothy': 430, 'Vegas': 431, 'Wales': 432, 'Yorkshire': 433, 'being': 434, 'during': 435, 'ft': 436, 'is': 437, 'it': 438, 'late': 439, 'not': 440, 'p.m.': 441, 'seven': 442, 'two': 443, '1776': 444, '1783': 445, '1909': 446, '1910': 447, '1921': 448, '1927': 449, '1933': 450, '1943': 451, '1951': 452, '1978': 453, '1993': 454, '48': 455, ':': 456, 'Amy': 457, 'Australia': 458, 'Bailey': 459, 'Baldwin': 460, 'Bank': 461, 'Barry': 462, 'Bob': 463, 'Bonnie': 464, 'Bowl': 465, 'C.': 466, 'Cassidy': 467, 'Catherine': 468, 'Cornwallis': 469, 'Cree': 470, 'Dana': 471, 'Democratic': 472, 'Depp': 473, 'E.': 474, 'Edelman': 475, 'Evans': 476, 'Exodus': 477, 'F.': 478, 'FIFA': 479, 'Federer': 480, 'Frances': 481, 'Friedrich': 482, 'Gabrielle': 483, 'Grant': 484, 'Jefferson': 485, 'Jesus': 486, 'Jill': 487, 'Jo': 488, 'Kentucky': 489, 'Kings': 490, 'Kovind': 491, 'Lennon': 492, 'Lloyd': 493, 'Lord': 494, 'Lot': 495, 'Maryland': 496, 'Michelle': 497, 'Milioti': 498, 'Mitchell': 499, 'Moody': 500, 'Morris': 501, 'Nath': 502, 'Nevada': 503, 'No': 504, 'Paige': 505, 'Quincy': 506, 'Rachel': 507, 'Red': 508, 'Ronald': 509, 'Roosevelt': 510, 'Samuel': 511, 'Sean': 512, 'Seth': 513, 'Show': 514, 'Sinatra': 515, 'Sisters': 516, 'Stuart': 517, 'Sun': 518, 'Tracy': 519, 'Trevor': 520, 'Union': 521, 'V': 522, 'Vincent': 523, 'Waylon': 524, 'West': 525, 'blood': 526, 'century': 527, 'near': 528, 'president': 529, 'second': 530, \"'\": 531, '130': 532, '136': 533, '1914': 534, '1917': 535, '1942': 536, '1948': 537, '1955': 538, '1970': 539, '300,000': 540, '32': 541, '44': 542, '47': 543, '53.5': 544, 'Abdul': 545, 'Additions': 546, 'Adolph': 547, 'Alaska': 548, 'Alyson': 549, 'Amanda': 550, 'Anaheim': 551, 'Andy': 552, 'Annette': 553, 'Arizona': 554, 'Bacharach': 555, 'Beijing': 556, 'Billy': 557, 'Bone': 558, 'Boyz': 559, 'Brad': 560, 'Brett': 561, 'Brightman': 562, 'Burt': 563, 'Byron': 564, 'CBS': 565, 'Captain': 566, 'Chhote': 567, 'Claire': 568, 'Clement': 569, 'Cravalho': 570, 'Cup': 571, 'Dallas': 572, 'Dame': 573, 'Danny': 574, 'Darling': 575, 'Dead': 576, 'Denver': 577, 'Dick': 578, 'Diggy': 579, 'Disneyland': 580, 'Excel': 581, 'F.C.': 582, 'Falls': 583, 'Franco': 584, 'Galilei': 585, 'Galileo': 586, 'Girl': 587, 'God': 588, 'Great': 589, 'Hal': 590, 'Hannigan': 591, 'Harrison': 592, 'Hart': 593, 'High': 594, 'Hudson': 595, 'Hughes': 596, 'Hussein': 597, 'Island': 598, 'Japan': 599, 'Jim': 600, 'Joe': 601, 'Johnson': 602, 'Jon': 603, 'Judith': 604, 'Justin': 605, 'Keating': 606, 'Kenya': 607, 'Kossoy': 608, 'Krauss': 609, 'L5': 610, 'Late': 611, 'Laura': 612, 'Lieutenant': 613, 'Lindsay': 614, 'Little': 615, 'Louisiana': 616, 'Lucas': 617, 'Mallorca': 618, 'Manning': 619, 'Maria': 620, 'Marie': 621, 'Massachusetts': 622, 'McFly': 623, 'Missouri': 624, 'Mosby': 625, 'Nadal': 626, 'Nelson': 627, 'Nick': 628, 'Old': 629, 'Olivia': 630, 'On': 631, 'Paula': 632, 'Payne': 633, 'Peck': 634, 'People': 635, 'Preston': 636, 'Price': 637, 'Quinn': 638, 'Rafael': 639, 'Rams': 640, 'Rebecca': 641, 'Rexha': 642, 'Richardson': 643, 'Ron': 644, 'Russia': 645, 'Saddam': 646, 'Senate': 647, 'Shada': 648, 'Shaw': 649, 'Simon': 650, 'Singh': 651, 'Sirach': 652, 'Slowly': 653, 'Spanish': 654, 'Starlighters': 655, 'Subah': 656, 'Team': 657, 'Tell': 658, 'Time': 659, 'U.S.': 660, 'Versailles': 661, 'Villanova': 662, 'Walter': 663, 'Welch': 664, 'Wright': 665, 'You': 666, 'Young': 667, 'Zimmer': 668, 'bones': 669, 'crossing': 670, 'discovered': 671, 'end': 672, 'ended': 673, 'featuring': 674, 'fifty': 675, 'group': 676, 'her': 677, 'his': 678, 'mid': 679, 'plates': 680, 'red': 681, 'salt': 682, 'subducted': 683, 'system': 684, 'their': 685, 'which': 686, 'y': 687, '145': 688, '1834': 689, '1864': 690, '1892': 691, '1908': 692, '1935': 693, '1939': 694, '1947': 695, '1949': 696, '1962': 697, '1966': 698, '1979': 699, '1983': 700, '1996': 701, '1st': 702, '22nd': 703, '330': 704, '41': 705, '53': 706, '78': 707, '8:00': 708, 'Al': 709, 'Alastair': 710, 'Alicia': 711, 'Alpha': 712, 'Andrew': 713, 'Antoine': 714, 'Assembly': 715, 'BBC': 716, 'BCE': 717, 'Banerjee': 718, 'Benson': 719, 'Big': 720, 'Blue': 721, 'Boston': 722, 'Brian': 723, 'Bronson': 724, 'Bush': 725, 'CE': 726, 'CK': 727, 'Cameron': 728, 'Carey': 729, 'Carl': 730, 'Carole': 731, 'Chelsea': 732, 'Cheryl': 733, 'Cobie': 734, 'Coleman': 735, 'College': 736, 'Constitution': 737, 'Cooper': 738, 'Cristin': 739, 'Crosby': 740, 'Curry': 741, 'Dale': 742, 'Damon': 743, 'De': 744, 'Dennis': 745, 'Deputy': 746, 'Dwight': 747, 'E': 748, 'Eaton': 749, 'Elena': 750, 'End': 751, 'English': 752, 'Entertainment': 753, 'Ethiopia': 754, 'Europe': 755, 'Federal': 756, 'Felix': 757, 'Finch': 758, 'First': 759, 'Fisher': 760, 'Ford': 761, 'Fourth': 762, 'Fox': 763, 'French': 764, 'G': 765, 'Gage': 766, 'German': 767, 'Graham': 768, 'Gray': 769, 'Greg': 770, 'Hathaway': 771, 'Herbert': 772, 'Holliday': 773, 'Howe': 774, 'Hugo': 775, 'Hurst': 776, 'IV': 777, 'Indian': 778, 'Instagram': 779, 'Internet': 780, 'Israel': 781, 'J': 782, 'Jane': 783, 'Jay': 784, 'Jeremiah': 785, 'Jeremy': 786, 'Jessica': 787, 'Jodi': 788, 'Josef': 789, 'Josh': 790, 'Just': 791, 'Kaitlyn': 792, 'Kane': 793, 'Khan': 794, 'Kirsten': 795, 'Kumar': 796, 'La': 797, 'Lafayette': 798, 'Lawes': 799, 'Lynne': 800, 'Léon': 801, 'Main': 802, 'Manuel': 803, 'Marshall': 804, 'Matt': 805, 'McCartney': 806, 'McEwen': 807, 'Mediterranean': 808, 'Meredith': 809, 'Minister': 810, 'Minnesota': 811, 'Muhammad': 812, 'Mukherjee': 813, 'National': 814, 'Netherlands': 815, 'Newman': 816, 'Newton': 817, 'O': 818, 'Officer': 819, 'Orlando': 820, 'Orthodox': 821, 'Palace': 822, 'Paris': 823, 'Patil': 824, 'Patti': 825, 'Phil': 826, 'Philadelphia': 827, 'Philippines': 828, 'Pittsburgh': 829, 'Pranab': 830, 'Pratibha': 831, 'R.': 832, 'Rahul': 833, 'Representatives': 834, 'Resort': 835, 'Ricky': 836, 'Romano': 837, 'Rouge': 838, 'Royal': 839, 'Sabha': 840, 'Sally': 841, 'Sanjay': 842, 'Shannon': 843, 'Smulders': 844, 'Spieth': 845, 'Sr': 846, 'Stevie': 847, 'Sudan': 848, 'Swift': 849, \"T'Chaka\": 850, 'Theodore': 851, 'This': 852, 'Turner': 853, 'USA': 854, 'Ulf': 855, 'Valley': 856, 'Victory': 857, 'Vikander': 858, 'Virginia': 859, 'W': 860, 'W.': 861, 'Wall': 862, 'Wallace': 863, 'Warren': 864, 'Wellington': 865, 'Wonder': 866, 'about': 867, 'area': 868, 'be': 869, 'clear': 870, 'comedy': 871, 'do': 872, 'energy': 873, 'football': 874, 'four': 875, 'groups': 876, 'has': 877, 'human': 878, 'metal': 879, 'minutes': 880, 'national': 881, 'other': 882, 'rock': 883, 'school': 884, 'smaller': 885, 'territory': 886, 'that': 887, 'unknown': 888, 'usually': 889, 'year': 890, ' ': 891, '$': 892, '1,000': 893, '10,000': 894, '12,714': 895, '12,756.3': 896, '12th': 897, '1765': 898, '1775': 899, '18th': 900, '1911': 901, '1916': 902, '1920': 903, '1930': 904, '1934': 905, '1941': 906, '1980s': 907, '24th': 908, '311': 909, '35': 910, '36': 911, '4.5': 912, '50': 913, '80': 914, '82': 915, 'Aces': 916, 'Act': 917, 'Adcock': 918, 'Administration': 919, 'Alexandra': 920, 'Algeria': 921, 'Ali': 922, 'Amelia': 923, 'Amendment': 924, 'America': 925, 'Andersen': 926, 'Ann': 927, 'Annapolis': 928, 'Antonio': 929, 'Apollos': 930, 'Arabia': 931, 'Arena': 932, 'Arnold': 933, 'Asteroid': 934, 'Atticus': 935, 'Attlee': 936, 'Audrey': 937, 'Auerbach': 938, 'B': 939, 'B.I.G.': 940, 'Barack': 941, 'Barnabas': 942, 'Baxter': 943, 'Began': 944, 'Bennett': 945, 'Berlin': 946, 'Blake': 947, 'Blood': 948, 'Bradley': 949, 'Bridge': 950, 'Brijwasi': 951, 'Britain': 952, 'Brock': 953, 'Bronco': 954, 'Bryan': 955, 'Buster': 956, 'CA': 957, 'CC': 958, 'Cannavale': 959, 'Cara': 960, 'Cardinals': 961, 'Carlos': 962, 'Carnes': 963, 'Carolyn': 964, 'Cayman': 965, 'Central': 966, 'Chancourtois': 967, 'Christie': 968, 'Civil': 969, 'Congo': 970, 'Crawford': 971, 'Crystal': 972, 'D.': 973, 'Dark': 974, 'Dave': 975, 'Davidson': 976, 'Delhi': 977, 'Derulo': 978, 'Desmond': 979, 'Deuteronomy': 980, 'Diddle': 981, 'Diego': 982, 'Disney': 983, 'District': 984, 'Dmitri': 985, 'Donna': 986, 'Donny': 987, 'Dorothy': 988, 'Dr.': 989, 'Dravid': 990, 'Duke': 991, 'Dutch': 992, 'Eastern': 993, 'Elliott': 994, 'Emily': 995, 'Ernest': 996, 'Ernie': 997, 'Falcons': 998, 'Faye': 999, 'Fire': 1000, 'Flack': 1001, 'Football': 1002, 'Forke': 1003, 'Four': 1004, 'Freddie': 1005, 'Fuller': 1006, 'Gardner': 1007, 'Gates': 1008, 'Genesis': 1009, 'Georgia': 1010, 'Girls': 1011, 'Gold': 1012, 'Governor': 1013, 'Grand': 1014, 'Greek': 1015, 'Gregory': 1016, 'H': 1017, 'H.': 1018, 'Haley': 1019, 'Hannibal': 1020, 'Hardy': 1021, 'Hemant': 1022, 'Home': 1023, 'Hope': 1024, 'Iran': 1025, 'Iraq': 1026, 'Jacob': 1027, 'Jacques': 1028, 'Jain': 1029, 'Jeffrey': 1030, 'Jewel': 1031, 'Joel': 1032, 'Joshi': 1033, 'Judy': 1034, 'Karen': 1035, 'Kathleen': 1036, 'Kavanaugh': 1037, 'Kenny': 1038, 'Kohli': 1039, 'Kris': 1040, 'Kristofferson': 1041, 'Kroll': 1042, 'Kuwait': 1043, 'Kylo': 1044, 'L3': 1045, 'Le': 1046, 'Lou': 1047, 'López': 1048, 'Madison': 1049, 'Madrid': 1050, 'Magic': 1051, 'Mali': 1052, 'Man': 1053, 'Manoj': 1054, 'Mara': 1055, 'Marc': 1056, 'Martha': 1057, 'Mauna': 1058, 'McConnell': 1059, 'McLean': 1060, 'Mendeleev': 1061, 'Messi': 1062, 'Michigan': 1063, 'Mohawk': 1064, 'Montana': 1065, 'Montgomery': 1066, 'Morgan': 1067, 'Morocco': 1068, 'Mount': 1069, 'Mrs.': 1070, 'Murphy': 1071, 'Murray': 1072, 'Nathaniel': 1073, 'Neal': 1074, 'Neil': 1075, 'Network': 1076, 'Niger': 1077, 'Nigeria': 1078, 'Norman': 1079, 'Northern': 1080, 'Notorious': 1081, \"O'Donoghue\": 1082, 'Obama': 1083, 'Organization': 1084, 'Otis': 1085, 'Ottoman': 1086, 'PM': 1087, 'Pakistan': 1088, 'Pearl': 1089, 'Peninsula': 1090, 'Pentateuch': 1091, 'Potsdam': 1092, 'Potts': 1093, 'Prime': 1094, 'Priscilla': 1095, 'Ptolemy': 1096, 'Pujols': 1097, 'Quebec': 1098, 'Ralph': 1099, 'Raymond': 1100, 'Real': 1101, 'Regina': 1102, 'Rhodes': 1103, 'Roberta': 1104, 'Robin': 1105, 'Rockapella': 1106, 'Roy': 1107, 'Sabella': 1108, 'Salt': 1109, 'Sandra': 1110, 'Sankt': 1111, 'Saudi': 1112, 'School': 1113, 'Shadows': 1114, 'Sher': 1115, 'Shortbread': 1116, 'Sidney': 1117, 'Sloan': 1118, 'Son': 1119, 'Stanton': 1120, 'Steph': 1121, 'Steven': 1122, 'Stranger': 1123, 'Strauss': 1124, 'Summer': 1125, 'Suzanne': 1126, 'Syria': 1127, 'Tales': 1128, 'Tanzania': 1129, 'Tennessee': 1130, 'Thailand': 1131, 'Thanks': 1132, 'Thursday': 1133, 'Tides': 1134, 'Tigers': 1135, 'Tommy': 1136, 'Turkey': 1137, 'Utah': 1138, 'Vanessa': 1139, 'Victoria': 1140, 'Virat': 1141, 'Voyager': 1142, 'Wardell': 1143, 'Water': 1144, 'Wayne': 1145, 'Weather': 1146, 'Wendy': 1147, 'Westbrook': 1148, 'Wind': 1149, 'Wisconsin': 1150, 'Wong': 1151, 'Yuengling': 1152, 'Zeta': 1153, 'advice': 1154, 'all': 1155, 'are': 1156, 'ball': 1157, 'bank': 1158, 'below': 1159, 'blue': 1160, 'c.': 1161, 'coast': 1162, 'consent': 1163, 'court': 1164, 'data': 1165, 'di': 1166, 'different': 1167, 'dollar': 1168, 'early': 1169, 'eight': 1170, 'explosion': 1171, 'force': 1172, 'form': 1173, 'governor': 1174, 'inches': 1175, 'its': 1176, 'kilometres': 1177, 'km': 1178, 'layer': 1179, 'league': 1180, 'masking': 1181, 'model': 1182, 'mosque': 1183, 'off': 1184, 'optic': 1185, 'original': 1186, 'overs': 1187, 'parts': 1188, 'per': 1189, 'plate': 1190, 'seconds': 1191, 'states': 1192, 'tail': 1193, 'tea': 1194, 'through': 1195, 'town': 1196, 'twice': 1197, 'west': 1198, 'when': 1199, 'wild': 1200, 'young': 1201, \"'re\": 1202, '+': 1203, '1,400': 1204, '10.7': 1205, '105': 1206, '114': 1207, '117': 1208, '117.92': 1209, '124': 1210, '1260': 1211, '1309': 1212, '1313': 1213, '1378': 1214, '1393': 1215, '1394': 1216, '1395': 1217, '151': 1218, '1526': 1219, '158.2': 1220, '15–18': 1221, '166': 1222, '167': 1223, '16–19': 1224, '17.0': 1225, '1746': 1226, '1781': 1227, '17th': 1228, '18.0': 1229, '180': 1230, '1848': 1231, '1857': 1232, '188': 1233, '189': 1234, '1895': 1235, '1896': 1236, '18–21': 1237, '1900': 1238, '1915': 1239, '1928': 1240, '193': 1241, '1936': 1242, '1937–40': 1243, '1956': 1244, '1960': 1245, '199': 1246, '19:18': 1247, '19:34': 1248, '19th': 1249, '1⁄2': 1250, '2,000': 1251, '2.5': 1252, '200,000': 1253, '2017-October': 1254, '202': 1255, '2021': 1256, '2024': 1257, '2028': 1258, '2030': 1259, '21st': 1260, '23,093': 1261, '25,233': 1262, '250': 1263, '266': 1264, '27,282': 1265, '27,310': 1266, '27,387': 1267, '28,162': 1268, '28.1': 1269, '29,330': 1270, '306': 1271, '31:15': 1272, '33.81': 1273, '340': 1274, '350,000': 1275, '37': 1276, '3708': 1277, '38': 1278, '40-hour': 1279, '400,000': 1280, '445,000': 1281, '445.6': 1282, '450,000': 1283, '49': 1284, '52': 1285, '555': 1286, '587': 1287, '597': 1288, '5:14': 1289, '6,200': 1290, '60': 1291, '609–598': 1292, '6:31': 1293, '7.3': 1294, '7.40': 1295, '7.50': 1296, '7.60': 1297, '74.5': 1298, '77.9': 1299, '79.5': 1300, '7:12': 1301, '800,000': 1302, '85': 1303, '88': 1304, '9,600': 1305, '9.5': 1306, '9:45': 1307, '9:55': 1308, 'AST': 1309, 'AV': 1310, 'AVRI': 1311, 'AZ': 1312, 'Abbie': 1313, 'Adl': 1314, 'Ado': 1315, 'Affleck': 1316, 'African': 1317, 'Agnes': 1318, 'Agnihotri': 1319, 'Agrawal': 1320, 'Aguinaldo': 1321, 'Aiden': 1322, 'Ainsworth': 1323, 'Akinwande': 1324, 'Alabama': 1325, 'Aldrich': 1326, 'Allen': 1327, 'Almsgiving': 1328, 'Alpert': 1329, 'Amok': 1330, 'Amsterdam': 1331, 'Andersson': 1332, 'Andmesh': 1333, 'Angela': 1334, 'Anglo': 1335, 'Apple': 1336, 'Aretha': 1337, 'Ariana': 1338, 'Ariel': 1339, 'Aristotle': 1340, 'Arkansas': 1341, 'Aroma': 1342, 'Arterton': 1343, 'Ashnoor': 1344, 'Aspartate': 1345, 'Assets': 1346, 'Auckland': 1347, 'Augustana': 1348, 'Augustine': 1349, 'Auliʻi': 1350, 'Aviazione': 1351, 'B.': 1352, 'BB': 1353, 'Babatunde': 1354, 'Bacon': 1355, 'Badale': 1356, 'Badelt': 1357, 'Bakay': 1358, 'Band': 1359, 'Barbier': 1360, 'Barbossa': 1361, 'Baron': 1362, 'Barroso': 1363, 'Bartholomew': 1364, 'Baruch': 1365, 'Basil': 1366, 'Bassin': 1367, 'Baton': 1368, 'Beauregard': 1369, 'Bebe': 1370, 'Beck': 1371, 'Beitia': 1372, 'Belinda': 1373, 'Bell': 1374, 'Bellagio': 1375, 'Bellard': 1376, 'Benjamin': 1377, 'Bentinck': 1378, 'Bernard': 1379, 'Berruti': 1380, 'Berry': 1381, 'Bertrup': 1382, 'Beta': 1383, 'Beth': 1384, 'Bett': 1385, 'Better': 1386, 'Bettye': 1387, 'Beulah': 1388, 'Bican': 1389, 'Bing': 1390, 'Binghamton': 1391, 'Birinus': 1392, 'Birtwhistle': 1393, 'Blackbeard': 1394, 'Blando': 1395, 'Bloom': 1396, 'Blount': 1397, 'Bo': 1398, 'Boente': 1399, 'Boggess': 1400, 'Bom': 1401, 'Bon': 1402, 'Bondi': 1403, 'Bordeaux': 1404, 'Borden': 1405, 'Bosch': 1406, 'Bose': 1407, 'Bougainville': 1408, 'Boulevard': 1409, 'Bowman': 1410, 'Boyle': 1411, 'Bracciano': 1412, 'Brand': 1413, 'Bratton': 1414, 'Brest': 1415, 'Britten': 1416, 'Brogdon': 1417, 'Browns': 1418, 'Bruce': 1419, 'Bryant': 1420, 'Bryn': 1421, 'Buckingham': 1422, 'Bukhari': 1423, 'Bunche': 1424, 'Burgess': 1425, 'Burgundy': 1426, 'Burnley': 1427, 'Burrafato': 1428, 'C': 1429, 'Calderón': 1430, 'Caldiero': 1431, 'Calene': 1432, 'Callista': 1433, 'Camp': 1434, 'Cana': 1435, 'Candice': 1436, 'Canterbury': 1437, 'Cardinal': 1438, 'Caroline': 1439, 'Carroll': 1440, 'Cary': 1441, 'Castree': 1442, 'Catholic': 1443, 'Celtic': 1444, 'Cena': 1445, 'Chamberlin': 1446, 'Chapman': 1447, 'Chase': 1448, 'Cheese': 1449, 'Cheng': 1450, 'Cher': 1451, 'Chesapeake': 1452, 'Chicago': 1453, 'Chile': 1454, 'Chimuanya': 1455, 'Chip': 1456, 'Chori': 1457, 'Christina': 1458, 'Christoph': 1459, 'Christy': 1460, 'Chrowder': 1461, 'Chuck': 1462, 'Château': 1463, 'Ciara': 1464, 'Clack': 1465, 'Claude': 1466, 'Clergy': 1467, 'Coach': 1468, 'Coast': 1469, 'Cocos': 1470, 'Coghlan': 1471, 'Cohenour': 1472, 'Collide': 1473, 'Collins': 1474, 'Colonies': 1475, 'Colorado': 1476, 'Columbus': 1477, 'Commercial': 1478, 'Company': 1479, 'Connecticut': 1480, 'Connors': 1481, 'Conrad': 1482, 'Conroy': 1483, 'Constituent': 1484, 'Cooke': 1485, 'Coppee': 1486, 'Corlett': 1487, 'Cornelia': 1488, 'Cosby': 1489, 'Coulson': 1490, 'Court': 1491, 'Coyne': 1492, 'Cozumel': 1493, 'Craddick': 1494, 'Creatine': 1495, 'Crennel': 1496, 'Crosbie': 1497, 'Crosbys': 1498, 'Crouch': 1499, 'Cy': 1500, 'Cynthia': 1501, 'Czech': 1502, 'Czechia': 1503, \"D'Arcy\": 1504, 'DC': 1505, 'Dad': 1506, 'Daeg': 1507, 'Daisy': 1508, 'Dales': 1509, 'Daltrey': 1510, 'Danielle': 1511, 'Darren': 1512, 'Darrowby': 1513, 'Dartmouth': 1514, 'Davide': 1515, 'Dawnn': 1516, 'DePrima': 1517, 'Deborah': 1518, 'Dee': 1519, 'Democritus': 1520, 'Dewey': 1521, 'Dibaba': 1522, 'Didier': 1523, 'Diesterweg': 1524, 'Dikembe': 1525, 'Dil': 1526, 'Dillane': 1527, 'Dion': 1528, 'Diệm': 1529, 'Dobbyn': 1530, 'Dodger': 1531, 'Doina': 1532, 'Dolphy': 1533, 'Donahue': 1534, 'Doris': 1535, 'Dorothea': 1536, 'Dorsey': 1537, 'Double': 1538, 'Douglas': 1539, 'Downing': 1540, 'Dr': 1541, 'Dragon': 1542, 'Drogba': 1543, 'Duczmal': 1544, 'Eamonn': 1545, 'Earth': 1546, 'Eazy': 1547, 'Ebb': 1548, 'Ecma': 1549, 'Eddie': 1550, 'Eddy': 1551, 'Eden': 1552, 'Edinburgh': 1553, 'Edith': 1554, 'Eduardo': 1555, 'Edwardian': 1556, 'Effie': 1557, 'Egan': 1558, 'Egypt': 1559, 'Eiko': 1560, 'El': 1561, 'Elaine': 1562, 'Ellis': 1563, 'Elrod': 1564, 'Emanuel': 1565, 'Emerging': 1566, 'Emilio': 1567, 'Ends': 1568, 'Engineering': 1569, 'Englad': 1570, 'Enrique': 1571, 'Esdras': 1572, 'Estate': 1573, 'Esther': 1574, 'Estuary': 1575, 'Eve': 1576, 'Everton': 1577, 'Exosphere': 1578, 'F': 1579, 'Fabrice': 1580, 'Faerch': 1581, 'Family': 1582, 'Famy': 1583, 'Farooq': 1584, 'Farrah': 1585, 'Fasting': 1586, 'Fata': 1587, 'Fatt': 1588, 'Felipe': 1589, 'Fiennes': 1590, 'Fiji': 1591, 'Finley': 1592, 'Fish': 1593, 'Fleet': 1594, 'Flores': 1595, 'Flynn': 1596, 'Follows': 1597, 'Forest': 1598, 'Fort': 1599, 'Frakes': 1600, 'François': 1601, 'Freiherr': 1602, 'Freitas': 1603, 'Friday': 1604, 'Friend': 1605, 'Fulton': 1606, 'Gabriel': 1607, 'Gabriela': 1608, 'Galatians': 1609, 'Galway': 1610, 'Gandhi': 1611, 'Garland': 1612, 'Garonne': 1613, 'Genzebe': 1614, 'Geoffrey': 1615, 'Georg': 1616, 'Gerard': 1617, 'Germain': 1618, 'Giblin': 1619, 'Gila': 1620, 'Giles': 1621, 'Gillian': 1622, 'Gingrich': 1623, 'Giulio': 1624, 'Glasgow': 1625, 'Glen': 1626, 'Gloucestershire': 1627, 'Glycogen': 1628, 'Gonzalez': 1629, 'Goulding': 1630, 'Government': 1631, 'Grande': 1632, 'Grayling': 1633, 'Greaves': 1634, 'Greening': 1635, 'Greta': 1636, 'Guedj': 1637, 'Guerrouj': 1638, 'Guy': 1639, 'Guyton': 1640, 'Gómez': 1641, 'Hackett': 1642, 'Halle': 1643, 'Hamm': 1644, 'Hangzhou': 1645, 'Hanks': 1646, 'Hannah': 1647, 'Hanseatic': 1648, 'Hanshaw': 1649, 'Harbour': 1650, 'Harvey': 1651, 'Havana': 1652, 'Hayley': 1653, 'Haysbert': 1654, 'Healy': 1655, 'Heathrow': 1656, 'Heavies': 1657, 'Hefford': 1658, 'Herb': 1659, 'Herbie': 1660, 'Hermann': 1661, 'Hernandez': 1662, 'Hero': 1663, 'Hicham': 1664, 'Hiddleston': 1665, 'Highmore': 1666, 'Hilliard': 1667, 'Hilsdon': 1668, 'Hilton': 1669, 'Hinojosa': 1670, 'Hit': 1671, 'Hobbes': 1672, 'Holloway': 1673, 'Holly': 1674, 'Holt': 1675, 'Hoon': 1676, 'Hopkins': 1677, 'Horacio': 1678, 'Howie': 1679, 'Huey': 1680, 'Huluvadi': 1681, 'Humeroradial': 1682, 'Humeroulnar': 1683, 'IMA': 1684, 'IPL': 1685, 'Ian': 1686, 'Ibrahimović': 1687, 'Ice': 1688, 'Iden': 1689, 'Ifans': 1690, 'Ike': 1691, 'Imamah': 1692, 'Imperial': 1693, 'Inc.': 1694, 'Inconclusive': 1695, 'Indies': 1696, 'Indira': 1697, 'Industrial': 1698, 'Investments': 1699, 'Isaak': 1700, 'Ischemia': 1701, 'Isley': 1702, 'Italian': 1703, 'Ivan': 1704, 'Jackie': 1705, 'Jai': 1706, 'Jamie': 1707, 'Jandamarra': 1708, 'Janeiro': 1709, 'Javine': 1710, 'Jayna': 1711, 'Jenkins': 1712, 'Jeri': 1713, 'Jesús': 1714, 'Jet': 1715, 'Jiles': 1716, 'Joaquin': 1717, 'Johansson': 1718, 'Johnathan': 1719, 'Jongh': 1720, 'Journey': 1721, 'Joyce': 1722, 'Judge': 1723, 'Jules': 1724, 'Juncker': 1725, 'Justine': 1726, 'K': 1727, \"Kamakawiwo'ole\": 1728, 'Kamaleng': 1729, 'Kamchatka': 1730, 'Kasznar': 1731, 'Kate': 1732, 'Katherine': 1733, 'Kaul': 1734, 'Kaun': 1735, 'Kaur': 1736, 'Keenan': 1737, 'Keppel': 1738, 'Kerry': 1739, 'Key': 1740, 'Kiiza': 1741, 'Kills': 1742, 'Kinase': 1743, 'Kinnear': 1744, 'Kishan': 1745, 'Klaus': 1746, 'Klaw': 1747, 'Klugman': 1748, 'Knochenhauer': 1749, 'Knowles': 1750, 'Kocheril': 1751, 'Kondo': 1752, 'Kristien': 1753, 'Krzyzewski': 1754, 'Kuki': 1755, 'Kurt': 1756, 'LDH': 1757, 'LaBelle': 1758, 'LaMarche': 1759, 'LaVette': 1760, 'Lactate': 1761, 'Laird': 1762, 'Lakk': 1763, 'Lambert': 1764, 'Lampard': 1765, 'Lana': 1766, 'Landsteiner': 1767, 'Laney': 1768, 'Lapa': 1769, 'Larry': 1770, 'Last': 1771, 'Latia': 1772, 'Lauren': 1773, 'Lausanne': 1774, 'Lavoie': 1775, 'Laye': 1776, 'Layla': 1777, 'LeBron': 1778, 'LeGros': 1779, 'Lea': 1780, 'Legionaria': 1781, 'Leiber': 1782, 'Leigh': 1783, 'Lent': 1784, 'Leo': 1785, 'Lepiato': 1786, 'Letter': 1787, 'Levine': 1788, 'Like': 1789, 'Lin': 1790, 'Linden': 1791, 'Lindley': 1792, 'Lindsey': 1793, 'Lineker': 1794, 'Lisa': 1795, 'Litovsk': 1796, 'Liu': 1797, 'Lively': 1798, 'Locke': 1799, 'Lockwood': 1800, 'Lok': 1801, 'Lorraine': 1802, 'Love': 1803, 'Ludi': 1804, 'Ludwig': 1805, 'Lukis': 1806, 'Luther': 1807, 'Lynda': 1808, 'Lynn': 1809, 'Léa': 1810, 'MB': 1811, 'Maciej': 1812, 'Mafabi': 1813, 'Mager': 1814, 'Mahony': 1815, 'Maisy': 1816, 'Malcolm': 1817, 'Mallard': 1818, 'Manasseh': 1819, 'Mandarin': 1820, 'Mane': 1821, 'Maple': 1822, 'Margaretha': 1823, 'Mariana': 1824, 'Marietta': 1825, 'Mariyappan': 1826, 'Martins': 1827, 'Marty': 1828, 'Marvin': 1829, 'Mascolo': 1830, 'Mattel': 1831, 'Maurice': 1832, 'Maxwell': 1833, 'Maynila': 1834, 'Mb': 1835, 'McAuley': 1836, 'McElhone': 1837, 'McGrath': 1838, 'McGuire': 1839, 'McKennon': 1840, 'McLoughlin': 1841, 'McMaster': 1842, 'McWherter': 1843, 'Media': 1844, 'Megan': 1845, 'Melba': 1846, 'Melinte': 1847, 'Mendoza': 1848, 'Mera': 1849, 'Merckx': 1850, 'Mercury': 1851, 'Merkel': 1852, 'Mesosphere': 1853, \"Mi'ad\": 1854, 'Miko': 1855, 'Mildred': 1856, 'Miletus': 1857, 'Milner': 1858, 'Minutes': 1859, 'Mirrors': 1860, 'Mishka': 1861, 'Mississippi': 1862, 'Moana': 1863, 'Monash': 1864, 'Monica': 1865, 'Montegut': 1866, 'Moritz': 1867, 'Morrison': 1868, 'Morton': 1869, 'Moss': 1870, 'Most': 1871, 'Motor': 1872, 'Moulton': 1873, 'Movement': 1874, 'Much': 1875, 'Mudie': 1876, 'Muirhead': 1877, 'Muldoon': 1878, 'Mulvaney': 1879, 'Muslim': 1880, 'Mutombo': 1881, 'Myoglobin': 1882, 'N': 1883, 'Nachdi': 1884, 'Nadav': 1885, 'Naek': 1886, 'Naked': 1887, 'Nandala': 1888, 'Narayanan': 1889, 'Nassau': 1890, 'Natalie': 1891, 'Natascha': 1892, 'Nationalist': 1893, 'Natorp': 1894, 'Natwick': 1895, 'Nayyar': 1896, 'Nazca': 1897, 'Ned': 1898, 'Nellie': 1899, 'Neuilly': 1900, 'Newcastle': 1901, 'Newfoundland': 1902, 'News': 1903, 'Neymar': 1904, 'Ngô': 1905, 'Nichols': 1906, 'Nieto': 1907, 'Nights': 1908, 'Noam': 1909, 'Northeastern': 1910, 'Norwich': 1911, 'Notre': 1912, 'Nubuwwah': 1913, \"O'Flanagan\": 1914, \"O'Mara\": 1915, 'OH': 1916, 'Ocean': 1917, 'Oduber': 1918, 'Oguttu': 1919, 'Oluwole': 1920, 'Ono': 1921, 'Oregon': 1922, 'Oriakhi': 1923, 'Osmon': 1924, 'Ouellette': 1925, 'Owen': 1926, 'P.': 1927, 'PR': 1928, 'Pagans': 1929, 'Paing': 1930, 'Palmeiro': 1931, 'Parissi': 1932, 'Partida': 1933, 'Partridge': 1934, 'Pasadena': 1935, 'Patmos': 1936, 'Paulo': 1937, 'Peg': 1938, 'Pekinese': 1939, 'Pelosi': 1940, 'Pemulwuy': 1941, 'Penguins': 1942, 'Pentatonix': 1943, 'Peters': 1944, 'Petersburg': 1945, 'Peyton': 1946, 'Peña': 1947, 'Phi': 1948, 'Phoenix': 1949, 'Photina': 1950, 'Pilgrimage': 1951, 'Pinxtones': 1952, 'Point': 1953, 'Pontiac': 1954, 'Porco': 1955, 'Postl': 1956, 'Pou': 1957, 'Povenmire': 1958, 'Powell': 1959, 'Prayer': 1960, 'Pressly': 1961, 'Prima': 1962, 'Pro': 1963, 'Prodi': 1964, 'Progressive': 1965, 'Protocol': 1966, 'Proximal': 1967, 'Prytz': 1968, 'Psalm': 1969, 'Puerto': 1970, 'Punta': 1971, 'Putin': 1972, 'Pyramid': 1973, 'Q4': 1974, 'Queen': 1975, 'Quirós': 1976, 'Qusay': 1977, 'Rabbani': 1978, 'Rabbit': 1979, 'Rachael': 1980, 'Radnor': 1981, 'Radson': 1982, 'Raf': 1983, 'Rafe': 1984, 'Rajesh': 1985, 'Raman': 1986, 'Ramesh': 1987, 'Ramonda': 1988, 'Ranchero': 1989, 'Range': 1990, 'Raposo': 1991, 'Rashida': 1992, 'Raza': 1993, 'Reagan': 1994, 'Realistic': 1995, 'Redford': 1996, 'Reese': 1997, 'Reeves': 1998, 'Reid': 1999, 'Reimers': 2000, 'Ren': 2001, 'Rene': 2002, 'Renshaw': 2003, 'Rewind': 2004, 'Reyes': 2005, 'Rhesus': 2006, 'Rhonda': 2007, 'Rhys': 2008, 'Rice': 2009, 'Richthofen': 2010, 'Rico': 2011, 'Ridge': 2012, 'Ring': 2013, 'Rio': 2014, 'Robyn': 2015, 'Roderick': 2016, 'Rodrigo': 2017, 'Rollins': 2018, 'Rome': 2019, 'Romeo': 2020, 'Ronan': 2021, 'Rooker': 2022, 'Rooney': 2023, 'Rosalind': 2024, 'Rouse': 2025, 'Rousseau': 2026, 'Roxanne': 2027, 'Rupert': 2028, 'Rusev': 2029, 'Russa': 2030, 'Rusty': 2031, 'Ruth': 2032, 'Safronov': 2033, 'Salat': 2034, 'Salaza': 2035, 'Sangster': 2036, 'Satin': 2037, 'Satish': 2038, 'Saw': 2039, 'Scarlett': 2040, 'Scooter': 2041, 'Seattle': 2042, 'Seduction': 2043, 'Seeger': 2044, 'Segel': 2045, 'Seine': 2046, 'Selfridge': 2047, 'Sessions': 2048, 'Set': 2049, 'Severn': 2050, 'Sevier': 2051, 'Sevilla': 2052, 'Seydoux': 2053, 'Seymour': 2054, 'Shai': 2055, 'Sharad': 2056, 'Shattered': 2057, 'Shedd': 2058, 'Sheila': 2059, 'Shellback': 2060, 'Shepard': 2061, 'Shiavone': 2062, 'Shigeru': 2063, 'Shivangi': 2064, 'Shiwen': 2065, 'Shooter': 2066, 'Shuri': 2067, 'Sierra': 2068, 'Sigfridsson': 2069, 'Simeon': 2070, 'Singer': 2071, 'Sisto': 2072, 'Skarsgård': 2073, 'Skerritt': 2074, 'Slate': 2075, 'So': 2076, 'Solom': 2077, 'Solti': 2078, 'Sophie': 2079, 'Southport': 2080, 'Soyinka': 2081, 'Spinners': 2082, 'Split': 2083, 'Spruce': 2084, 'Squamish': 2085, 'Square': 2086, 'Stacy': 2087, 'Stan': 2088, 'Standardization': 2089, 'Stanford': 2090, 'Stardust': 2091, 'Station': 2092, 'Sterling': 2093, 'Stevens': 2094, 'Stock': 2095, 'Stoller': 2096, 'Stormtrooper': 2097, 'Stratosphere': 2098, 'Straus': 2099, 'Studios': 2100, 'Succession': 2101, 'Sunderland': 2102, 'Suns': 2103, 'Supervision': 2104, 'Suppan': 2105, 'Sweatman': 2106, 'Sweden': 2107, 'Sweety': 2108, 'Swinton': 2109, 'Switzerland': 2110, 'Syracuse': 2111, 'São': 2112, 'Sèvres': 2113, \"T'Challa\": 2114, 'TBS': 2115, 'TCU': 2116, 'TMS': 2117, 'Tanya': 2118, 'Tarzana': 2119, 'Task': 2120, 'Tau': 2121, 'Tavárez': 2122, 'Tawhid': 2123, 'Tera': 2124, 'Thales': 2125, 'Thangavelu': 2126, 'Thau': 2127, 'Thermosphere': 2128, 'Thirteen': 2129, 'Tiffin': 2130, 'Tiggy': 2131, 'Tisdale': 2132, 'Tobit': 2133, 'Today': 2134, 'Todd': 2135, 'Tommies': 2136, 'Tonga': 2137, 'Torrens': 2138, 'Touchwood': 2139, 'Toulouse': 2140, 'Tower': 2141, 'Towns': 2142, 'Townsend': 2143, 'Transamerica': 2144, 'Travis': 2145, 'Tresco': 2146, 'Tretiak': 2147, 'Troponin': 2148, 'Troposhere': 2149, 'Trouble': 2150, 'Troy': 2151, 'TruTV': 2152, 'Trueb': 2153, 'Truman': 2154, 'Trumbo': 2155, 'Tuzenbach': 2156, 'Twelve': 2157, 'Two': 2158, 'U2': 2159, 'US$': 2160, 'USS': 2161, 'Uday': 2162, 'Ufodike': 2163, 'Ukrainian': 2164, 'Ukulele': 2165, 'Up': 2166, 'Urinary': 2167, 'Urleen': 2168, 'Usos': 2169, 'Vanek': 2170, 'Vanilla': 2171, 'Vannary': 2172, 'Venetian': 2173, 'Vermont': 2174, 'Vernon': 2175, 'Versio': 2176, 'Vicente': 2177, 'Vicki': 2178, 'Victor': 2179, 'Vikings': 2180, 'Vladimir': 2181, 'Vladislav': 2182, 'W3C': 2183, 'WW': 2184, 'Wafula': 2185, 'Walmart': 2186, 'Wanamaker': 2187, 'Warriors': 2188, 'Warwick': 2189, 'Wash': 2190, 'Watson': 2191, 'Weiss': 2192, 'Wennerström': 2193, 'Whitley': 2194, 'Whitney': 2195, 'Wickenheiser': 2196, 'Wiggins': 2197, 'Wilbur': 2198, 'Wildcats': 2199, 'Willie': 2200, 'Willow': 2201, 'Wilmington': 2202, 'Winnie': 2203, 'Winters': 2204, 'Wisdom': 2205, 'Witherspoon': 2206, 'Wolfram': 2207, 'Wood': 2208, 'Wynn': 2209, 'Yaar': 2210, 'Yamada': 2211, 'Yates': 2212, 'Yee': 2213, 'Yong': 2214, 'Yuling': 2215, 'Zanelli': 2216, 'Zaw': 2217, 'Zhu': 2218, 'Zlatan': 2219, 'Zoanne': 2220, 'Zuri': 2221, 'act': 2222, 'adult': 2223, 'age': 2224, 'agency': 2225, 'airdate': 2226, 'albumin': 2227, 'along': 2228, 'among': 2229, 'an': 2230, 'aqueducts': 2231, 'arcs': 2232, 'army': 2233, 'astronomer': 2234, 'basin': 2235, 'bassist': 2236, 'behind': 2237, 'beneath': 2238, 'booking': 2239, 'books': 2240, 'both': 2241, 'brain': 2242, 'c.1944': 2243, 'called': 2244, 'cancer': 2245, 'cause': 2246, 'certain': 2247, 'ching': 2248, 'chloride': 2249, 'clitoris': 2250, 'cm': 2251, 'college': 2252, 'collision': 2253, 'collisions': 2254, 'continue': 2255, 'criminology': 2256, 'crucifixes': 2257, 'cytoplasm': 2258, 'daimyō': 2259, 'dehydrogenase': 2260, 'democracy': 2261, 'deuterocanonical': 2262, 'dialect': 2263, 'dock': 2264, 'du': 2265, 'east': 2266, 'eleven': 2267, 'en': 2268, 'episodes': 2269, 'era': 2270, 'exile': 2271, 'extracellular': 2272, 'faction': 2273, 'faith': 2274, 'family': 2275, 'fat': 2276, 'feet': 2277, 'fiction': 2278, 'fluoride': 2279, 'folklore': 2280, 'found': 2281, 'front': 2282, 'g': 2283, 'gets': 2284, 'habaneros': 2285, 'halite': 2286, 'hard': 2287, 'immediately': 2288, 'including': 2289, 'interval': 2290, 'isoenzyme': 2291, 'job': 2292, 'junta': 2293, 'lagoon': 2294, 'legislative': 2295, 'lit': 2296, 'lithospheric': 2297, 'loan': 2298, 'luminous': 2299, 'majority': 2300, 'may': 2301, 'meatus': 2302, 'miles': 2303, 'minimum': 2304, 'mitochondria': 2305, 'modified': 2306, 'monarchy': 2307, 'monster': 2308, 'movement': 2309, 'moving': 2310, 'mud': 2311, 'nasone': 2312, 'natriuretic': 2313, 'nobles': 2314, 'node': 2315, 'non': 2316, 'none': 2317, 'north': 2318, 'ocean': 2319, 'others': 2320, 'owned': 2321, 'patient': 2322, 'peptide': 2323, 'phosphorylase': 2324, 'player': 2325, 'playing': 2326, 'present': 2327, 'presidential': 2328, 'profession': 2329, 'provinces': 2330, 'radio': 2331, 'radioulnar': 2332, 'rate': 2333, 'reactions': 2334, 'rebel': 2335, 'region': 2336, 'ren': 2337, 'republika': 2338, 'required': 2339, 'secretary': 2340, 'sent': 2341, 'she': 2342, 'sheriff': 2343, 'shōgun': 2344, 'sick': 2345, 'singing': 2346, 'single': 2347, 'sixty': 2348, 'spotted': 2349, 'state': 2350, 'stealth': 2351, 'support': 2352, 'sur': 2353, 'tag': 2354, 'teaching': 2355, 'tectonic': 2356, 'ten': 2357, 'test': 2358, 'times': 2359, 'transaminase': 2360, 'transfusion': 2361, 'unified': 2362, 'vagina': 2363, 'ventricles': 2364, 'ventriloquism': 2365, 'vestibule': 2366, 'veteran': 2367, 'video': 2368, 'von': 2369, 'vulval': 2370, 'week': 2371, 'westward': 2372, 'white': 2373, 'who': 2374, 'wife': 2375, 'wooden': 2376, 'Île': 2377, 'Česko': 2378, 'Česká': 2379, 'Đình': 2380, '0.64': 2381, '05:50': 2382, '06:00': 2383, '08:15': 2384, '1,202': 2385, '1,602': 2386, '1.5': 2387, '100': 2388, '100,000': 2389, '106': 2390, '11,000': 2391, '11,745': 2392, '11th': 2393, '12:33:51': 2394, '1348': 2395, '1362': 2396, '14,2019': 2397, '1447': 2398, '1492': 2399, '15,2018': 2400, '1500': 2401, '1517': 2402, '1535': 2403, '1611': 2404, '1665': 2405, '1721': 2406, '1740': 2407, '1756': 2408, '1784': 2409, '1787': 2410, '1792': 2411, '1798': 2412, '18.5': 2413, '1800': 2414, '1802': 2415, '1804': 2416, '1835': 2417, '1845': 2418, '1853': 2419, '1861': 2420, '1862': 2421, '1863': 2422, '1865': 2423, '1870': 2424, '1871': 2425, '1876': 2426, '1877': 2427, '1887': 2428, '1890': 2429, '1893': 2430, '1897': 2431, '18:57': 2432, '19-March': 2433, '1907': 2434, '191': 2435, '1920s': 2436, '1922': 2437, '1937': 2438, '1940': 2439, '1944': 2440, '1959': 2441, '1963': 2442, '1964': 2443, '1970s': 2444, '1972': 2445, '1990s': 2446, '1996–1997': 2447, '1⁄4': 2448, '2,2019': 2449, '2.31': 2450, '20,000': 2451, '2017–18': 2452, '2018-December': 2453, '2020-March': 2454, '207,778': 2455, '210,565': 2456, '219,773': 2457, '223': 2458, '225': 2459, '231,636': 2460, '243': 2461, '260': 2462, '270': 2463, '29.78': 2464, '290': 2465, '32,292': 2466, '328': 2467, '362': 2468, '393': 2469, '395': 2470, '4,540': 2471, '4.26': 2472, '45': 2473, '4568.2': 2474, '5,000': 2475, '5,987': 2476, '500': 2477, '530': 2478, '55': 2479, '5–4': 2480, '6.18': 2481, '6.5': 2482, '72': 2483, '7th': 2484, '83.5': 2485, '96': 2486, '97': 2487, '973': 2488, '986': 2489, \"A'ja\": 2490, 'AFC': 2491, 'AJK': 2492, 'ANZ': 2493, 'Abaire': 2494, 'Abigail': 2495, 'Absolem': 2496, 'Absolute': 2497, 'Abstergo': 2498, 'Acker': 2499, 'Acts': 2500, 'Adele': 2501, 'Ademir': 2502, 'Adrian': 2503, 'Advertiser': 2504, 'Afghanistan': 2505, 'After': 2506, 'Aga': 2507, 'Airlines': 2508, 'Airplanes': 2509, 'Ajax': 2510, 'Akashi': 2511, 'Akhenaten': 2512, 'Alanis': 2513, 'Alaric': 2514, 'Albany': 2515, 'Alec': 2516, 'Alecia': 2517, 'Alexandre': 2518, 'Alexandria': 2519, 'Alien': 2520, 'Alliance': 2521, 'Allies': 2522, 'Aloe': 2523, 'Aloha': 2524, 'Alone': 2525, 'Amleto': 2526, 'Andrews': 2527, 'Angelina': 2528, 'Angola': 2529, 'Anguillara': 2530, 'Anika': 2531, 'Anshuman': 2532, 'Antarctica': 2533, 'Antilles': 2534, 'Apophenia': 2535, 'Apparent': 2536, 'Apr': 2537, 'Aram': 2538, 'Area': 2539, 'Armisen': 2540, 'Armstrong': 2541, 'Arnel': 2542, 'Arsenal': 2543, 'Articles': 2544, 'Arvid': 2545, 'Aryan': 2546, 'Ascension': 2547, 'Ashe': 2548, 'Ashok': 2549, 'At': 2550, 'Atkin': 2551, 'Atwood': 2552, 'Auburn': 2553, 'Audra': 2554, 'Augsburg': 2555, 'Authentication': 2556, 'Aves': 2557, 'Avicii': 2558, 'Axis': 2559, 'Azikiwe': 2560, 'BOS': 2561, 'Babyface': 2562, 'Bagley': 2563, 'Bahama': 2564, 'Balladeer': 2565, 'Baltimore': 2566, 'Bananas': 2567, 'Bani': 2568, 'Banking': 2569, 'Barbeau': 2570, 'Barbra': 2571, 'Barker': 2572, 'Bart': 2573, 'Bartel': 2574, 'Bates': 2575, 'Batt': 2576, 'Battle': 2577, 'Baume': 2578, 'Bautista': 2579, 'Bavier': 2580, 'Bayelsa': 2581, 'Bayern': 2582, 'Beachcombers': 2583, 'Beast': 2584, 'Beaumont': 2585, 'Beauty': 2586, 'Bello': 2587, 'Bengal': 2588, 'Benn': 2589, 'Bessel': 2590, 'Bessemer': 2591, 'Betty': 2592, 'Bhutan': 2593, 'Bieber': 2594, 'Biermann': 2595, 'Blacc': 2596, 'Blackwood': 2597, 'Bland': 2598, 'Blight': 2599, 'Blondes': 2600, 'Bloomington': 2601, 'Blumer': 2602, 'BodyRockers': 2603, 'Boeser': 2604, 'Boi': 2605, 'Bolt': 2606, 'Bolton': 2607, 'Bonds': 2608, 'Boogie': 2609, 'Book': 2610, 'Bordeauxdog': 2611, 'Borgo': 2612, 'Botswana': 2613, 'Botticelli': 2614, 'Botto': 2615, 'Bourne': 2616, 'Boxing': 2617, 'Bradford': 2618, 'Brahma': 2619, 'Braid': 2620, 'Brave': 2621, 'Bread': 2622, 'Bregman': 2623, 'Bridal': 2624, 'Briggs': 2625, 'Brockville': 2626, 'Brody': 2627, 'Brolin': 2628, 'Broncos': 2629, 'Brookman': 2630, 'Brooks': 2631, 'Browne': 2632, 'Brunson': 2633, 'Brynner': 2634, 'Buccaneers': 2635, 'Buchholz': 2636, 'Bucky': 2637, 'Buffer': 2638, 'Bulgaria': 2639, 'Bulldogs': 2640, 'Bullock': 2641, 'Burbank': 2642, 'Burch': 2643, 'Burruss': 2644, 'Butler': 2645, 'Buttercup': 2646, 'Buzz': 2647, 'Béguyer': 2648, 'CAF': 2649, 'CHAP': 2650, 'CONCACAF': 2651, 'CONMEBOL': 2652, 'Caesar': 2653, 'Cafe': 2654, 'Caitlin': 2655, 'Calcutta': 2656, 'Caleb': 2657, 'Calhern': 2658, 'Camilla': 2659, 'Camille': 2660, 'Cantor': 2661, 'Capaldi': 2662, 'Cape': 2663, 'Carli': 2664, 'Carlin': 2665, 'Carson': 2666, 'Cash': 2667, 'Caswell': 2668, 'Cat': 2669, 'Catalonia': 2670, 'Cathy': 2671, 'Cecilienhof': 2672, 'Cencio': 2673, 'Center': 2674, 'Chabert': 2675, 'Chad': 2676, 'Chairman': 2677, 'Chalet': 2678, 'Chamling': 2679, 'Chandan': 2680, 'Charity': 2681, 'Chas': 2682, 'Checkmate': 2683, 'Chest': 2684, 'Chickens': 2685, 'Chimanimani': 2686, 'Chloe': 2687, 'Chocolate': 2688, 'Choir': 2689, 'Chong': 2690, 'Chopra': 2691, 'Chow': 2692, 'Christ': 2693, 'Christal': 2694, 'Christensen': 2695, 'Christianity': 2696, 'Christmas': 2697, 'Christoffel': 2698, 'Châteauneuf': 2699, 'Circle': 2700, 'Cities': 2701, 'Claramae': 2702, 'Clark': 2703, 'Claudius': 2704, 'Clay': 2705, 'Cliffe': 2706, 'Clive': 2707, 'Cloris': 2708, 'Clyde': 2709, 'Coburn': 2710, 'Coliseum': 2711, 'Collection': 2712, 'Colossus': 2713, 'Come': 2714, 'Comics': 2715, 'Commerce': 2716, 'Commissioners': 2717, 'Common': 2718, 'Condon': 2719, 'Confederate': 2720, 'Confederation': 2721, 'Conference': 2722, 'Connor': 2723, 'Conqueror': 2724, 'Convention': 2725, 'Conway': 2726, 'Cookies': 2727, 'Cooley': 2728, 'Coots': 2729, 'Corbin': 2730, 'Cordereau': 2731, 'Corey': 2732, 'Cori': 2733, 'Cornelissen': 2734, 'Cornwall': 2735, 'Corporation': 2736, 'Costa': 2737, 'Costello': 2738, 'Cott': 2739, 'Courtney': 2740, 'Cox': 2741, 'Craig': 2742, 'Cream': 2743, 'Creek': 2744, 'Creme': 2745, 'Crescent': 2746, 'Croce': 2747, 'Cropper': 2748, 'Cruise': 2749, 'Cruz': 2750, 'Cuba': 2751, 'Cuizza': 2752, 'Curse': 2753, 'Curtiss': 2754, 'D': 2755, 'D.C.': 2756, 'D.G.': 2757, 'DSM': 2758, 'Dampier': 2759, 'Dance': 2760, 'Dane': 2761, 'Danyang': 2762, 'Dapo': 2763, 'Darbyshire': 2764, 'Darci': 2765, 'Dardanelle': 2766, 'Daredevil': 2767, 'Dari': 2768, 'Dashwood': 2769, 'Davidtz': 2770, 'Dawkins': 2771, 'Daylights': 2772, 'DeHaan': 2773, 'Deele': 2774, 'Deep': 2775, 'Delivery': 2776, 'Delk': 2777, 'Denis': 2778, 'Derby': 2779, 'Desai': 2780, 'Destiny': 2781, 'Devall': 2782, 'Devine': 2783, 'Devisingh': 2784, 'Devon': 2785, 'Dewan': 2786, 'Dexter': 2787, 'Diadophis': 2788, 'Diana': 2789, 'Diggins': 2790, 'Digman': 2791, 'Dimas': 2792, 'Dingle': 2793, 'Dinsmore': 2794, 'Djenné': 2795, 'Djokovic': 2796, 'Doctrine': 2797, 'Dodig': 2798, 'Dogg': 2799, 'Dogue': 2800, 'Dormer': 2801, 'Dottie': 2802, 'Downes': 2803, 'Downey': 2804, 'Downtown': 2805, 'Draper': 2806, 'Drayton': 2807, 'Dubois': 2808, 'Dunant': 2809, 'Dunaway': 2810, 'Dunes': 2811, 'Dunk': 2812, 'Dunlap': 2813, 'Dunne': 2814, 'Dutt': 2815, 'Dwayne': 2816, 'Dworsky': 2817, 'Dye': 2818, 'Dyke': 2819, 'Dynamic': 2820, 'E21': 2821, 'EAP': 2822, 'Earl': 2823, 'Ebina': 2824, 'Edwards': 2825, 'Egbert': 2826, 'Ehrenfried': 2827, 'Eileen': 2828, 'Einziger': 2829, 'Eion': 2830, 'Elections': 2831, 'Eli': 2832, 'Elles': 2833, 'Ellison': 2834, 'Elsa': 2835, 'Elton': 2836, 'Elveden': 2837, 'Ely': 2838, 'Embeth': 2839, 'Emmett': 2840, 'Empire': 2841, 'Engelbrecht': 2842, 'Entner': 2843, 'Enzo': 2844, 'Estates': 2845, 'Eswatini': 2846, 'Evangelist': 2847, 'Executive': 2848, 'FDR': 2849, 'FL': 2850, 'FTC': 2851, 'Fagerbakke': 2852, 'Faraday': 2853, 'Fargo': 2854, 'Farley': 2855, 'Farrar': 2856, 'Farrell': 2857, 'Fat': 2858, 'Fauves': 2859, 'Fe': 2860, 'Feliciano': 2861, 'Ferguson': 2862, 'Fernand': 2863, 'Fernandez': 2864, 'Ferrell': 2865, 'Fertile': 2866, 'Films': 2867, 'Finland': 2868, 'Firkus': 2869, 'Firth': 2870, 'Fishburne': 2871, 'Flemyng': 2872, 'Florence': 2873, 'Fontaine': 2874, 'Foray': 2875, 'Foreman': 2876, 'Forsyth': 2877, 'Forty': 2878, 'Foster': 2879, 'Francesco': 2880, 'Francks': 2881, 'Fray': 2882, 'Freuder': 2883, 'Friedle': 2884, 'Frisco': 2885, 'Frontier': 2886, 'Frye': 2887, 'Frédéric': 2888, 'Furlong': 2889, 'Gadot': 2890, 'Gaekwad': 2891, 'Gaerlan': 2892, 'Gail': 2893, 'Gal': 2894, 'Gananoque': 2895, 'Ganges': 2896, 'Gap': 2897, 'Gaulos': 2898, 'Gauss': 2899, 'Gehlot': 2900, 'Gemberling': 2901, 'Gene': 2902, 'Gentlemen': 2903, 'Geocentric': 2904, 'Germans': 2905, 'Gerry': 2906, 'Ghost': 2907, 'Giants': 2908, 'Gibsons': 2909, 'Gielgud': 2910, 'Gilkison': 2911, 'Gillan': 2912, 'Gillespie': 2913, 'Gilmore': 2914, 'Gilmour': 2915, 'Glacier': 2916, 'Glenn': 2917, 'Goatherd': 2918, 'Goffin': 2919, 'Gondwana': 2920, 'Goodman': 2921, 'Gottwald': 2922, 'Grace': 2923, 'Grass': 2924, 'Grattan': 2925, 'Gravitron': 2926, 'Grayson': 2927, 'Greater': 2928, 'Griffiths': 2929, 'Grill': 2930, 'GrimSkunk': 2931, 'Grimes': 2932, 'Grosz': 2933, 'Group': 2934, 'Gulf': 2935, 'Généreux': 2936, 'Göckel': 2937, 'Habsburg': 2938, 'Hale': 2939, 'Half': 2940, 'Hamblen': 2941, 'Hamlin': 2942, 'Hampton': 2943, 'Handy': 2944, 'Hank': 2945, 'Hannaford': 2946, 'Hanover': 2947, 'Hanuman': 2948, 'Hanyu': 2949, 'Hardin': 2950, 'Harline': 2951, 'Harold': 2952, 'Harroun': 2953, 'Hastings': 2954, 'Haugh': 2955, 'Haven': 2956, 'Hawaii': 2957, 'Hawkesbury': 2958, 'Hawkins': 2959, 'Haydn': 2960, 'Hayes': 2961, 'Heavy': 2962, 'Heinrich': 2963, 'Heir': 2964, 'Hemisphere': 2965, 'Henri': 2966, 'Hercules': 2967, 'Herod': 2968, 'Hertz': 2969, 'Hillis': 2970, 'Himadri': 2971, 'Hindu': 2972, 'Hiroshima': 2973, 'Hispaniola': 2974, 'Hoboken': 2975, 'Hobson': 2976, 'Hofford': 2977, 'Hoi': 2978, 'Holtzapffel': 2979, 'Homan': 2980, 'Hoo': 2981, 'Hook': 2982, 'Hopper': 2983, 'Horst': 2984, 'Horton': 2985, 'Hotspur': 2986, 'Houghton': 2987, 'How': 2988, 'Hu': 2989, 'Hudgens': 2990, 'Hughie': 2991, 'Huns': 2992, 'Hérault': 2993, 'ICC': 2994, 'IX': 2995, 'Iain': 2996, 'IndARC': 2997, 'Infared': 2998, 'Ingredient': 2999, 'Interstate': 3000, 'Io': 3001, 'Irish': 3002, 'Irvine': 3003, 'Ivers': 3004, 'J.E.B.': 3005, 'J.Hud': 3006, 'J.M.': 3007, 'JZ': 3008, \"Ja'net\": 3009, 'Jace': 3010, 'Jagodowski': 3011, 'Jaime': 3012, 'Jalen': 3013, 'Jamestown': 3014, 'Jammu': 3015, 'Janel': 3016, 'Javier': 3017, 'Jenny': 3018, 'Jessi': 3019, 'Jets': 3020, 'Jewish': 3021, 'Jianyu': 3022, 'Joanne': 3023, 'Johan': 3024, 'Johann': 3025, 'Jolie': 3026, 'Jong': 3027, 'Jovi': 3028, 'Juan': 3029, 'Judiasm': 3030, 'Julia': 3031, 'Julie': 3032, 'Julius': 3033, 'Jupiter': 3034, 'Kaew': 3035, 'Kaikyō': 3036, 'Kamboh': 3037, 'Kan': 3038, 'Kandi': 3039, 'Karimi': 3040, 'Karla': 3041, 'Karnataka': 3042, 'Kashmir': 3043, 'Kasirer': 3044, 'Kater': 3045, 'Kathy': 3046, 'Katie': 3047, 'Katy': 3048, 'Ke': 3049, 'Kea': 3050, 'Keefe': 3051, 'Kei': 3052, 'Kenickie': 3053, 'Kenneth': 3054, 'Kent': 3055, 'Kepler': 3056, 'Kept': 3057, 'Ketch': 3058, 'Kgalema': 3059, 'Khachaturian': 3060, 'Khesar': 3061, 'Kimple': 3062, 'Kinnaman': 3063, 'Kiplingcotes': 3064, 'Kiran': 3065, 'Kittles': 3066, 'Kjellberg': 3067, 'Klay': 3068, 'Kliment': 3069, 'Kloss': 3070, 'Knopfler': 3071, 'Kon': 3072, 'Konstantine': 3073, 'Korea': 3074, 'Korman': 3075, 'Krause': 3076, 'Kreviazuk': 3077, 'Krishnamachari': 3078, 'Kristie': 3079, 'Kristle': 3080, 'Kristyn': 3081, 'Krosney': 3082, 'Krul': 3083, 'Kuan': 3084, 'Kunia': 3085, 'Kunshan': 3086, 'Kuti': 3087, 'Kyla': 3088, 'LBB': 3089, 'LaVoie': 3090, 'Lacey': 3091, 'Ladakh': 3092, 'Lajpat': 3093, 'Lakehurst': 3094, 'Laker': 3095, 'Lakers': 3096, 'Lala': 3097, 'Lambs': 3098, 'Lancashire': 3099, 'Landry': 3100, 'Lane': 3101, 'Lange': 3102, 'Langle': 3103, 'Langridge': 3104, 'Lao': 3105, 'Lara': 3106, 'Laurence': 3107, 'Law': 3108, 'Lawrance': 3109, 'Lazzeri': 3110, 'Leachman': 3111, 'Leaders': 3112, 'Leah': 3113, 'Leakes': 3114, 'Leary': 3115, 'Legendary': 3116, 'Lemonades': 3117, 'Leningrad': 3118, 'Lennie': 3119, 'Leoni': 3120, 'Lesley': 3121, 'Lesnar': 3122, 'Less': 3123, 'Let': 3124, 'Levite': 3125, 'Libya': 3126, 'Life': 3127, 'Lighthouse': 3128, 'Lighting': 3129, 'Lightyear': 3130, 'Limited': 3131, 'Lincoln': 3132, 'Linda': 3133, 'Lionel': 3134, 'Lionsgate': 3135, 'Lisbon': 3136, 'Living': 3137, 'Loa': 3138, 'Logan': 3139, 'Lonely': 3140, 'Lonnie': 3141, 'Louise': 3142, 'Louisville': 3143, 'Lovato': 3144, 'Ltd.': 3145, 'Luca': 3146, 'Luddington': 3147, 'Lukasz': 3148, 'Lyfe': 3149, 'Lyrae': 3150, 'Lyte': 3151, 'MBE': 3152, 'MEO': 3153, 'Ma': 3154, 'Mabe': 3155, 'MacFarlane': 3156, 'MacGregor': 3157, 'Macklin': 3158, 'Maddie': 3159, 'Mae': 3160, 'Maitland': 3161, 'Major': 3162, 'Malory': 3163, 'Manfred': 3164, 'Manhunter': 3165, 'Maracanã': 3166, 'Marcelo': 3167, 'Marcille': 3168, 'Marco': 3169, 'Marcus': 3170, 'Marilyn': 3171, 'Marinus': 3172, 'Marla': 3173, 'Marley': 3174, 'Marlo': 3175, 'Marquesas': 3176, 'Marquess': 3177, 'Martina': 3178, 'Martínez': 3179, 'Marulo': 3180, 'Mason': 3181, 'Massena': 3182, 'Massey': 3183, 'Mastiff': 3184, 'Mathai': 3185, 'Maury': 3186, 'Mausolus': 3187, 'Mawhood': 3188, 'Maximus': 3189, 'Maya': 3190, 'Mbakwe': 3191, 'Mbeki': 3192, 'McArthur': 3193, 'McCown': 3194, 'McDavid': 3195, 'McDermott': 3196, 'McDonagh': 3197, 'McDonald': 3198, 'Mead': 3199, 'Medallions': 3200, 'Mehmed': 3201, 'Mehran': 3202, 'Mei': 3203, 'Meiosis': 3204, 'Melinda': 3205, 'Melissa': 3206, 'Melo': 3207, 'Melody': 3208, 'Melvin': 3209, 'Memorial': 3210, 'Methuselah': 3211, 'Mic': 3212, 'Micheal': 3213, 'Midway': 3214, 'Mighty': 3215, 'Mila': 3216, 'Miles': 3217, 'Millennium': 3218, 'Milne': 3219, 'Milton': 3220, 'Miskew': 3221, 'Mitch': 3222, 'Mitsubishi': 3223, 'Mix': 3224, 'Mohammed': 3225, 'Molly': 3226, 'Momsen': 3227, 'Mona': 3228, 'Monroe': 3229, 'Montreal': 3230, 'Moose': 3231, 'Mopti': 3232, 'Morissette': 3233, 'Morristown': 3234, 'Moseley': 3235, 'Motlanthe': 3236, 'Moulin': 3237, 'Mulk': 3238, 'Mundaka': 3239, 'Munich': 3240, 'Muniz': 3241, 'Murtala': 3242, 'Museum': 3243, 'Music': 3244, 'Mutarazi': 3245, 'Mvuyelwa': 3246, 'Myers': 3247, 'Myron': 3248, 'Mysore': 3249, 'MØ': 3250, 'NADP': 3251, 'NADPH': 3252, 'NFL': 3253, 'Nadine': 3254, 'Nasseri': 3255, 'Natale': 3256, 'Nationals': 3257, 'Naval': 3258, 'Navarro': 3259, 'Naveen': 3260, 'Navratilova': 3261, 'Nawab': 3262, 'Naykia': 3263, 'Nayudu': 3264, 'Nazareth': 3265, 'NeNe': 3266, 'Nehru': 3267, 'Nehwal': 3268, 'Neill': 3269, 'Nepean': 3270, 'Neues': 3271, 'Next': 3272, 'Nielsen': 3273, 'Nighy': 3274, 'Nissan': 3275, 'Nitty': 3276, 'Nobles': 3277, 'Noel': 3278, 'Noise': 3279, 'Nondisjunction': 3280, 'None': 3281, 'Noni': 3282, 'Norma': 3283, 'Norsemen': 3284, 'Not': 3285, 'Novak': 3286, 'Novelli': 3287, 'Numbers': 3288, \"O'Brian\": 3289, \"O'Brien\": 3290, \"O'Connor\": 3291, \"O'Donnell\": 3292, \"O'Hara\": 3293, \"O'Keefe\": 3294, \"O'Shaughnessy\": 3295, 'OBE': 3296, 'OFC': 3297, 'OU': 3298, 'Ogdensburg': 3299, 'Olympiastadion': 3300, 'Ontario': 3301, 'Opposition': 3302, 'Orange': 3303, 'Orioles': 3304, 'Orleans': 3305, 'Ormond': 3306, 'Ornette': 3307, 'Oscar': 3308, 'Osment': 3309, 'Out': 3310, 'Oyama': 3311, 'Ozone': 3312, 'PAP': 3313, 'Packers': 3314, 'Page': 3315, 'Paine': 3316, 'Pak': 3317, 'Palestinian': 3318, 'Pam': 3319, 'Pantera': 3320, 'Panthers': 3321, 'Pape': 3322, 'Pappas': 3323, 'Paramount': 3324, 'Pareidolia': 3325, 'Parish': 3326, 'Parker': 3327, 'Parliament': 3328, 'Parrish': 3329, 'Parvati': 3330, 'Pascoe': 3331, 'Passy': 3332, 'Past': 3333, 'Pat': 3334, 'Patriots': 3335, 'Pattie': 3336, 'Peasants': 3337, 'Peifer': 3338, 'Pele': 3339, 'Pence': 3340, 'Pennines': 3341, 'Pentafluoridophosphorus': 3342, 'Pentafluorophosphorane': 3343, 'Pepin': 3344, 'Pepperdine': 3345, 'Perkins': 3346, 'Perlman': 3347, 'Perpetual': 3348, 'Persuasions': 3349, 'Perth': 3350, 'Peterburg': 3351, 'Petrus': 3352, 'PewDiePie': 3353, 'Philbin': 3354, 'Phillips': 3355, 'Phoebe': 3356, 'Phosphorus': 3357, 'Phosphorus(V': 3358, 'Phra': 3359, 'Picon': 3360, 'Pictures': 3361, 'Pieter': 3362, 'Pietersen': 3363, 'Pimps': 3364, 'Pineda': 3365, 'Placid': 3366, 'Poledouris': 3367, 'Polly': 3368, 'Polynesia': 3369, 'Pope': 3370, 'Popplewell': 3371, 'Porsha': 3372, 'Portugal': 3373, 'Potter': 3374, 'Powers': 3375, 'Pozzo': 3376, 'Pratt': 3377, 'Prefer': 3378, 'Priest': 3379, 'Princess': 3380, 'Prine': 3381, 'Priyanka': 3382, 'Productions': 3383, 'Profanities': 3384, 'Promises': 3385, 'Protection': 3386, 'Puente': 3387, 'Pusher': 3388, 'Putsoa': 3389, 'Pytheas': 3390, 'Qatar': 3391, 'Quan': 3392, 'Ragland': 3393, 'Rai': 3394, 'Raitt': 3395, 'Raje': 3396, 'Rajya': 3397, 'Randall': 3398, 'Rangaswamy': 3399, 'Realm': 3400, 'Redden': 3401, 'Redding': 3402, 'Redvers': 3403, 'Reginald': 3404, 'Region': 3405, 'Regional': 3406, 'Reich': 3407, 'Reilly': 3408, 'Remy': 3409, 'Renault': 3410, 'Renteria': 3411, 'Reports': 3412, 'Resorts': 3413, 'Reuter': 3414, 'Rhythm': 3415, 'Rica': 3416, 'Richter': 3417, 'Rights': 3418, 'Rihanna': 3419, 'Riot': 3420, 'Rising': 3421, 'Ritchie': 3422, 'Rivières': 3423, 'Robertson': 3424, 'Rock': 3425, 'Rockville': 3426, 'Rod': 3427, 'Rodríguez': 3428, 'Rogen': 3429, 'Rogers': 3430, 'Rokes': 3431, 'Roman': 3432, 'Root': 3433, 'Roots': 3434, 'Rotor': 3435, 'RuPaul': 3436, 'Rudolph': 3437, 'Ruhr': 3438, 'Rupp': 3439, 'S1': 3440, 'S11': 3441, 'Sabazia': 3442, 'Sachin': 3443, 'Saina': 3444, 'Saltoro': 3445, 'Sammy': 3446, 'Samson': 3447, 'San': 3448, 'Santa': 3449, 'Saskatchewan': 3450, 'Satine': 3451, 'Save': 3452, 'Scanlan': 3453, 'Schon': 3454, 'Schumer': 3455, 'Schuster': 3456, 'Scindia': 3457, 'Scot': 3458, 'Scottish': 3459, 'Scout': 3460, 'Searcy': 3461, 'Seaview': 3462, 'Secondary': 3463, 'Secor': 3464, 'SeegerPeter': 3465, 'Seely': 3466, 'Sense': 3467, 'Serafinowicz': 3468, 'Sergeant': 3469, 'Server': 3470, 'Seycove': 3471, 'Shah': 3472, 'Shamea': 3473, 'Shantha': 3474, 'Sharon': 3475, 'Sheen': 3476, 'Shelby': 3477, 'Shep': 3478, 'Shepherd': 3479, 'Shereé': 3480, 'Sherman': 3481, 'Sherwood': 3482, 'Shetty': 3483, 'Shikai': 3484, 'Shinichiro': 3485, 'Shiva': 3486, 'Shoma': 3487, 'Siachen': 3488, 'Silence': 3489, 'Silentó': 3490, 'Silesia': 3491, 'Silvestri': 3492, 'Sinderby': 3493, 'Singleton': 3494, 'Sixth': 3495, 'Slim': 3496, 'Sloane': 3497, 'Snoop': 3498, 'Sobolov': 3499, 'Sobule': 3500, 'Sokol': 3501, 'Solomon': 3502, 'Song': 3503, 'Songs': 3504, 'Sophia': 3505, 'Soucie': 3506, 'Sound': 3507, 'Springs': 3508, 'Spud': 3509, 'Srikkanth': 3510, 'Stanley': 3511, 'Stars': 3512, 'Step': 3513, 'Stephens': 3514, 'Stilgoe': 3515, 'Stone': 3516, 'Stonewall': 3517, 'Stop': 3518, 'Streisand': 3519, 'Strong': 3520, 'Stryker': 3521, 'Suffolk': 3522, 'Sullivan': 3523, 'Sully': 3524, 'Sultan': 3525, 'Summers': 3526, 'Suárez': 3527, 'Sweeney': 3528, 'Swiss': 3529, 'Séchelles': 3530, 'Taiwan': 3531, 'Tampa': 3532, 'Tanakh': 3533, 'Tara': 3534, 'Tasker': 3535, 'Tatasciore': 3536, 'Teas': 3537, 'Tendulkar': 3538, 'Tenochtitlan': 3539, 'Terence': 3540, 'Terminal': 3541, 'Terrebonne': 3542, 'Territory': 3543, 'Tesori': 3544, 'Testament': 3545, 'Texans': 3546, 'Thaba': 3547, 'Thabo': 3548, 'Thewlis': 3549, 'Thirty': 3550, 'Thom': 3551, 'Thursdays': 3552, 'Tibbets': 3553, 'Tiger': 3554, 'Timi': 3555, 'Tino': 3556, 'Titchener': 3557, 'Tito': 3558, 'Tlacopan': 3559, 'Tomb': 3560, 'Torre': 3561, 'Torsen': 3562, 'Tortuguero': 3563, 'Totalitarianism': 3564, 'Tottenham': 3565, 'Toyota': 3566, 'Trade': 3567, 'Transvaal': 3568, 'Trey': 3569, 'Tristano': 3570, 'Triumphs': 3571, 'Trixie': 3572, 'Trois': 3573, 'Trooper': 3574, 'Trust': 3575, 'Truth': 3576, 'Tré': 3577, 'Turman': 3578, 'Turtle': 3579, 'Twenty': 3580, 'Twin': 3581, 'Téa': 3582, 'UEFA': 3583, 'US': 3584, 'UTC': 3585, 'Uno': 3586, 'Upanishad': 3587, 'Upper': 3588, 'Uruguay': 3589, 'Utechin': 3590, 'VIII': 3591, 'VanderWaal': 3592, 'Vanderwaal': 3593, 'Vanity': 3594, 'Vasundhara': 3595, 'Vedas': 3596, 'Vega': 3597, 'Vegard': 3598, 'Veil': 3599, 'Venus': 3600, 'Veronica': 3601, 'Vidisha': 3602, 'Villeneuve': 3603, 'Virgin': 3604, 'Virginian': 3605, 'Visa': 3606, 'Vishnu': 3607, 'Vittorio': 3608, 'Vivian': 3609, 'Voroshilov': 3610, 'Walker': 3611, 'Wallach': 3612, 'Walsh': 3613, 'Walt': 3614, 'Waqar': 3615, 'Warburton': 3616, 'Wat': 3617, 'Watanabe': 3618, 'Waterford': 3619, 'Wawrinka': 3620, 'Weagle': 3621, 'Weaving': 3622, 'Wei': 3623, 'Wen': 3624, 'Wengert': 3625, 'Wessels': 3626, 'Westen': 3627, 'Wheen': 3628, 'Whispers': 3629, 'Whistle': 3630, 'Whitfield': 3631, 'Whitmore': 3632, 'Wieden': 3633, 'Wieden+Kennedy': 3634, 'Wieners': 3635, 'Wilhelm': 3636, 'Wilkinson': 3637, 'Williamson': 3638, 'Willis': 3639, 'Windows': 3640, 'Winwood': 3641, 'Wolverines': 3642, 'Women': 3643, 'Woodsmen': 3644, 'Works': 3645, 'Worley': 3646, 'Wormer': 3647, 'Worthington': 3648, 'Wow': 3649, 'Wray': 3650, 'Wroblewitz': 3651, 'Wundt': 3652, 'Wupper': 3653, 'XI': 3654, 'XXVI': 3655, 'XXXIX': 3656, 'Yankees': 3657, 'Yann': 3658, 'Yell': 3659, 'Yente': 3660, 'Ylvis': 3661, 'Ylvisåker': 3662, 'Yoakam': 3663, 'Yoko': 3664, 'Yondu': 3665, 'YouTube': 3666, 'Yuan': 3667, 'Yul': 3668, 'Yuma': 3669, 'Yuriko': 3670, 'Yuro': 3671, 'Yuzuru': 3672, 'Zerdin': 3673, 'Zerubbabel': 3674, 'Ziegler': 3675, 'Zolciak': 3676, 'Zoo': 3677, 'abietorum': 3678, 'acid': 3679, 'acting': 3680, 'action': 3681, 'administrator': 3682, 'advertising': 3683, 'agent': 3684, 'airplane': 3685, 'alascensis': 3686, 'apple': 3687, 'approximately': 3688, 'around': 3689, 'arrondissement': 3690, 'art': 3691, 'assassins': 3692, 'ayllu': 3693, 'back': 3694, 'ban': 3695, 'became': 3696, 'because': 3697, 'become': 3698, 'before': 3699, 'belt': 3700, 'birth': 3701, 'blackness': 3702, 'bone': 3703, 'bottom': 3704, 'boundary': 3705, 'bourgeoisie': 3706, 'brass': 3707, 'broken': 3708, 'burning': 3709, 'canal': 3710, 'canisters': 3711, 'car': 3712, 'card': 3713, 'cards': 3714, 'caregiver': 3715, 'cascadensis': 3716, 'cathedral': 3717, 'central': 3718, 'centred': 3719, 'channels': 3720, 'check': 3721, 'child': 3722, 'chitin': 3723, 'choir': 3724, 'church': 3725, 'citizens': 3726, 'class': 3727, 'coal': 3728, 'commission': 3729, 'common': 3730, 'concluding': 3731, 'confluence': 3732, 'consuls': 3733, 'consumption': 3734, 'containment': 3735, 'continent': 3736, 'continents': 3737, 'controller': 3738, 'conus': 3739, 'cooler': 3740, 'county': 3741, 'cricket': 3742, 'critical': 3743, 'crucigera': 3744, 'dark': 3745, 'delusions': 3746, 'desert': 3747, 'deterrence': 3748, 'directly': 3749, 'disappeared': 3750, 'disc': 3751, 'dogs': 3752, 'domain': 3753, 'down': 3754, 'driving': 3755, 'drought': 3756, 'earthquakes': 3757, 'eastern': 3758, 'electors': 3759, 'endometrium': 3760, 'ensi': 3761, 'entire': 3762, 'ethmoid': 3763, 'exceeds': 3764, 'executive': 3765, 'expressionism': 3766, 'external': 3767, 'extinct': 3768, 'extreme': 3769, 'fall': 3770, 'fallopian': 3771, 'families': 3772, 'faster': 3773, 'film': 3774, 'flag': 3775, 'flood': 3776, 'florida': 3777, 'fly': 3778, 'folk': 3779, 'forward': 3780, 'fraction': 3781, 'further': 3782, 'game': 3783, 'gas': 3784, 'gestation': 3785, 'glass': 3786, 'gospel': 3787, 'governing': 3788, 'grams': 3789, 'grandmother': 3790, 'gravitational': 3791, 'gravity': 3792, 'ground': 3793, 'growth': 3794, 'gypsum': 3795, 'had': 3796, 'hallucinations': 3797, 'hardware': 3798, 'heirs': 3799, 'hematite': 3800, 'high': 3801, 'himself': 3802, 'huge': 3803, 'humanity': 3804, 'imperial': 3805, 'included': 3806, 'information': 3807, 'installment': 3808, 'interspinous': 3809, 'intestine': 3810, 'into': 3811, 'invasion': 3812, 'joining': 3813, 'kick': 3814, 'kicked': 3815, 'king': 3816, 'labyrinth': 3817, 'landlocked': 3818, 'lane': 3819, 'largely': 3820, 'last': 3821, 'late-50s': 3822, 'lb': 3823, 'leave': 3824, 'levels': 3825, 'lugal': 3826, 'lumbar': 3827, 'macroura': 3828, 'magazines': 3829, 'magnetite': 3830, 'making': 3831, 'manœuvrable': 3832, 'marmots': 3833, 'mass': 3834, 'maxilla': 3835, 'meaning': 3836, 'medial': 3837, 'medullaris': 3838, 'membrane': 3839, 'method': 3840, 'mi': 3841, 'mick': 3842, 'midnight': 3843, 'midway': 3844, 'mine': 3845, 'minister': 3846, 'minor': 3847, 'minstrelsy': 3848, 'mix': 3849, 'molecules': 3850, 'more': 3851, 'most': 3852, 'move': 3853, 'movements': 3854, 'mya': 3855, 'names': 3856, 'nasal': 3857, 'natural': 3858, 'nature': 3859, 'naïve': 3860, 'necator': 3861, 'negative': 3862, 'next': 3863, 'nichts': 3864, 'nine': 3865, 'nondisjunction': 3866, 'northwestern': 3867, 'novelty': 3868, 'née': 3869, 'office': 3870, 'only': 3871, 'organised': 3872, 'oversee': 3873, 'ovule': 3874, 'owners': 3875, 'palate': 3876, 'palatine': 3877, 'pancreas': 3878, 'part': 3879, 'patriarchal': 3880, 'patwin': 3881, 'pedosphere': 3882, 'penalty': 3883, 'pentafluoride': 3884, 'people': 3885, 'person': 3886, 'persons': 3887, 'plain': 3888, 'planetary': 3889, 'play': 3890, 'points': 3891, 'popular': 3892, 'preparing': 3893, 'priest': 3894, 'priestly': 3895, 'prisoners': 3896, 'process': 3897, 'protected': 3898, 'pterygoid': 3899, 'punctatus': 3900, 'quote': 3901, 'races': 3902, 'rat': 3903, 'ratified': 3904, 'recombination': 3905, 'regalis': 3906, 'regions': 3907, 'registered': 3908, 'removed': 3909, 'renewable': 3910, 'republic': 3911, 'requirement': 3912, 'resigns': 3913, 'resisting': 3914, 'restaurant': 3915, 'retaken': 3916, 'ringneck': 3917, 'rodents': 3918, 'rubricosa': 3919, 'sacred': 3920, 'salts': 3921, 'same': 3922, 'scattered': 3923, 'sedimentary': 3924, 'seed': 3925, 'self': 3926, 'series': 3927, 'settlers': 3928, 'shared': 3929, 'shortly': 3930, 'show': 3931, 'sides': 3932, 'skill': 3933, 'sliced': 3934, 'slope': 3935, 'slopes': 3936, 'slow': 3937, 'small': 3938, 'snake': 3939, 'socialist': 3940, 'sodium': 3941, 'solstice': 3942, 'son': 3943, 'song': 3944, 'soon': 3945, 'southeast': 3946, 'southern': 3947, 'specified': 3948, 'spectral': 3949, 'speech': 3950, 'stars': 3951, 'storm': 3952, 'style': 3953, 'sub': 3954, 'symbolizes': 3955, 'symptoms': 3956, 'teacher': 3957, 'terrestrial': 3958, 'testing': 3959, 'therefore': 3960, 'thickness': 3961, 'thinly': 3962, 'throughout': 3963, 'time': 3964, 'tiny': 3965, 'tobacco': 3966, 'together': 3967, 'tops': 3968, 'towns': 3969, 'trade': 3970, 'transformed': 3971, 'troposphere': 3972, 'tube': 3973, 'ul': 3974, 'un': 3975, 'under': 3976, 'until': 3977, 'urine': 3978, 'usherette': 3979, 'uterus': 3980, 'vehicle': 3981, 'version': 3982, 'vertebrae': 3983, 'vessel': 3984, 'veterans': 3985, 'vikings': 3986, 'vote': 3987, 'vulpes': 3988, 'wall': 3989, 'war': 3990, 'waters': 3991, 'way': 3992, 'weeks': 3993, 'western': 3994, 'widow': 3995, 'winter': 3996, 'within': 3997, 'woolery': 3998, 'words': 3999, 'zebra': 4000, '·': 4001, 'Émile': 4002, 'Škrtel': 4003, '#': 4004, '+22': 4005, '.22': 4006, '0.240846': 4007, '00′': 4008, '05h': 4009, '1,600': 4010, '1/2': 4011, '1000': 4012, '102': 4013, '103,875–124,232': 4014, '104': 4015, '108': 4016, '110': 4017, '110,000': 4018, '119': 4019, '12-October-2005': 4020, '1215': 4021, '122': 4022, '123': 4023, '125': 4024, '12:00': 4025, '13,017': 4026, '1300': 4027, '14.23': 4028, '150': 4029, '1500s': 4030, '15th': 4031, '1600': 4032, '1608': 4033, '164': 4034, '1689': 4035, '169': 4036, '1699': 4037, '17,000': 4038, '1701': 4039, '1707': 4040, '172.2': 4041, '1754': 4042, '1763': 4043, '1769': 4044, '1774': 4045, '17:8–16': 4046, '1805': 4047, '1812': 4048, '1836': 4049, '1846': 4050, '1849': 4051, '1850': 4052, '1874': 4053, '1889': 4054, '1898': 4055, '1904': 4056, '1912': 4057, '1919': 4058, '1929': 4059, '1932': 4060, '1952': 4061, '1961–62': 4062, '1967,1970': 4063, '1995–1998': 4064, '2,562–2,900': 4065, '2-D': 4066, '20.00': 4067, '2005–06': 4068, '2007-December': 4069, '22–26': 4070, '231': 4071, '233': 4072, '260,897': 4073, '2:3': 4074, '2nd': 4075, '3.465-billion': 4076, '3.77': 4077, '3000': 4078, '31,419': 4079, '31.94s': 4080, '34.98': 4081, '350': 4082, '353': 4083, '4,294,967,296': 4084, '4.3': 4085, '4.54': 4086, '417': 4087, '42': 4088, '428': 4089, '43': 4090, '4898': 4091, '51': 4092, '512': 4093, '52.2″': 4094, '54': 4095, '57.9': 4096, '5:30': 4097, '5th': 4098, '63': 4099, '67': 4100, '6th': 4101, '70': 4102, '71': 4103, '76.3': 4104, '8:05': 4105, '9,974': 4106, '95': 4107, '99': 4108, '9:00': 4109, '9:43': 4110, '9th': 4111, ';': 4112, 'AC': 4113, 'AD': 4114, 'AOC': 4115, 'ATP': 4116, 'AUH': 4117, 'Abagnale': 4118, 'Abraham': 4119, 'Abreu': 4120, 'Achilles': 4121, 'Ada': 4122, 'Adepo': 4123, 'Adolphe': 4124, 'Adrienne': 4125, 'Advani': 4126, 'Afanasieff': 4127, 'Agency': 4128, 'Ajit': 4129, 'Akbar': 4130, 'Alam': 4131, 'Alamo': 4132, 'Albarn': 4133, 'Aldridge': 4134, 'Alessandro': 4135, 'Alfred': 4136, 'All': 4137, 'Aloysius': 4138, 'Alpo': 4139, 'Alps': 4140, 'Aluminum': 4141, 'Amazon': 4142, 'Americas': 4143, 'An': 4144, 'Andre': 4145, 'Andrés': 4146, 'Angel': 4147, 'Angelica': 4148, 'Anikulapo': 4149, 'Ant': 4150, 'Appeal': 4151, 'Approximately': 4152, 'Apurímac': 4153, 'Ara': 4154, 'Arab': 4155, 'Arabian': 4156, 'Aravali': 4157, 'Arenado': 4158, 'Arenberg': 4159, 'Arete': 4160, 'Aristotelian': 4161, 'Aristotelianism': 4162, 'Armies': 4163, 'Army': 4164, 'Arroyo': 4165, 'Artie': 4166, 'Asa': 4167, 'Asher': 4168, 'Ashi': 4169, 'Assassin': 4170, 'Athenian': 4171, 'Atlantic': 4172, 'Atlas': 4173, 'Attainder': 4174, 'Australian': 4175, 'Austrian': 4176, 'Aveiro': 4177, 'Avenue': 4178, 'Award': 4179, 'Azie': 4180, 'BC': 4181, 'BMW': 4182, 'Babcock': 4183, 'Babel': 4184, 'Babylon': 4185, 'Bahadur': 4186, 'Bailee': 4187, 'Baker': 4188, 'Balfe': 4189, 'Ballard': 4190, 'Balraj': 4191, 'Bamford': 4192, 'Bangladesh': 4193, 'Barbara': 4194, 'Barden': 4195, 'Bartolomeu': 4196, 'Basile': 4197, 'Bassi': 4198, 'Bastille': 4199, 'Batman': 4200, 'Bavarian': 4201, 'Bayes': 4202, 'Beals': 4203, 'Bears': 4204, 'Beatty': 4205, 'Beau': 4206, 'Becky': 4207, 'Behan': 4208, 'Bellas': 4209, 'Bellinger': 4210, 'Bennington': 4211, 'Benoist': 4212, 'Bergeron': 4213, 'Berkshire': 4214, 'Besson': 4215, 'Bev': 4216, 'Beverley': 4217, 'Beyoncé': 4218, 'Bhabha': 4219, 'Bhagiratha': 4220, 'Bharata': 4221, 'Bhaskar': 4222, 'Bhim': 4223, 'Bhonsle': 4224, 'Bidet': 4225, 'Billie': 4226, 'Bizzy': 4227, 'Blakeley': 4228, 'Blanc': 4229, 'Block': 4230, 'Blues': 4231, 'Bocephus': 4232, 'Boeck': 4233, 'Bogataj': 4234, 'Bolsheviks': 4235, 'Bonham': 4236, 'Bonney': 4237, 'Borinquen': 4238, 'Borlaug': 4239, 'Bow': 4240, 'Bowling': 4241, 'Boy': 4242, 'Boyce': 4243, 'Boys': 4244, 'Bracco': 4245, 'Bracknell': 4246, 'Bradshaw': 4247, 'Bragg': 4248, 'Brain': 4249, 'Branch': 4250, 'Brass': 4251, 'Braxton': 4252, 'Breck': 4253, 'Brel': 4254, 'Brenda': 4255, 'Brewery': 4256, 'Brewing': 4257, 'Bridges': 4258, 'Bros.': 4259, 'Brothers': 4260, 'Bruns': 4261, 'Brussels': 4262, 'Buddh': 4263, 'Buddha': 4264, 'Buddhism': 4265, 'Buddy': 4266, 'Butea': 4267, 'C$': 4268, 'CBE': 4269, 'CHFRS': 4270, 'Caddo': 4271, 'Cai': 4272, 'Caligula': 4273, 'Calixa': 4274, 'Calment': 4275, 'Calvin': 4276, 'Camberley': 4277, 'Cammack': 4278, 'Cammie': 4279, 'Campbell': 4280, 'Cannes': 4281, 'Canonical': 4282, 'Caoife': 4283, 'Cargo': 4284, 'Carhart': 4285, 'Cariappa': 4286, 'Carla': 4287, 'Carly': 4288, 'Carmen': 4289, 'Cars': 4290, 'Carta': 4291, 'Cartersville': 4292, 'Category': 4293, 'Cathleen': 4294, 'Cattle': 4295, 'Cavendish': 4296, 'Cavitt': 4297, 'Cedric': 4298, 'Challenge': 4299, 'Chamberlain': 4300, 'Chambers': 4301, 'Champaign': 4302, 'Chandra': 4303, 'Channing': 4304, 'Chapel': 4305, 'Charlemagne': 4306, 'Charlotte': 4307, 'Cheating': 4308, 'Cheez': 4309, 'Chemjong': 4310, 'Chester': 4311, 'Cheswick': 4312, 'Child': 4313, 'Christa': 4314, 'Christine': 4315, 'Chukwu': 4316, 'Church': 4317, 'Circuit': 4318, 'Civilization': 4319, 'Clarke': 4320, 'Clause': 4321, 'Cleary': 4322, 'Clifford': 4323, 'Clint': 4324, 'Clinton': 4325, 'Clooney': 4326, 'Clufetos': 4327, 'Clydie': 4328, 'Code': 4329, 'Cody': 4330, 'Coercive': 4331, 'Cole': 4332, 'Colombia': 4333, 'Colonel': 4334, 'Complex': 4335, 'Compston': 4336, 'Computer': 4337, 'Condit': 4338, 'Conformity': 4339, 'Continuous': 4340, 'Controls': 4341, 'Conwell': 4342, 'Copland': 4343, 'Corbett': 4344, 'Corden': 4345, 'Corporate': 4346, 'Cosmic': 4347, 'Country': 4348, 'Cow': 4349, 'Creed': 4350, 'Creole': 4351, 'Crews': 4352, 'Criswell': 4353, 'Crumpit': 4354, 'Cuccittini': 4355, 'Culross': 4356, 'Cunégonde': 4357, 'Curtis': 4358, 'Cyrene': 4359, 'Cyril': 4360, 'Cytoplasm': 4361, 'DNA': 4362, 'DVC': 4363, 'DVD': 4364, 'Dada': 4365, 'Dadasaheb': 4366, 'Daei': 4367, 'Dahlia': 4368, 'Dainik': 4369, 'Daiya': 4370, 'Dakota': 4371, 'Danae': 4372, 'Dania': 4373, 'Dannii': 4374, 'Darleen': 4375, 'Dawson': 4376, 'DeLorenzo': 4377, 'DeWitt': 4378, 'Debby': 4379, 'Debicki': 4380, 'Dec': 4381, 'Declaration': 4382, 'Decoration': 4383, 'Deeley': 4384, 'Delano': 4385, 'Della': 4386, 'Demetria': 4387, 'Demi': 4388, 'Dent': 4389, 'Deon': 4390, 'Deoxyribonucleic': 4391, 'Derek': 4392, 'Desolation': 4393, 'Deutsches': 4394, 'Devonian': 4395, 'Devonne': 4396, 'Dhami': 4397, 'Dharma': 4398, 'Dian': 4399, 'Diane': 4400, 'Dias': 4401, 'Dichen': 4402, 'Diehl': 4403, 'Dinaric': 4404, 'Dinklage': 4405, 'Dire': 4406, 'Divas': 4407, 'Djeser': 4408, 'Djibouti': 4409, 'Dollar': 4410, 'Dolly': 4411, 'Dominic': 4412, 'Dominican': 4413, 'Don': 4414, 'Donnie': 4415, 'Dophalene': 4416, 'Drashti': 4417, 'Driver': 4418, 'Dub': 4419, 'Dudley': 4420, 'Duncan': 4421, 'Dunsinane': 4422, 'Duryodhana': 4423, 'Duvernay': 4424, 'EPA': 4425, 'ET': 4426, 'Eagle': 4427, 'Early': 4428, 'Eatenton': 4429, 'Edmonds': 4430, 'Efron': 4431, 'Eleanore': 4432, 'Electric': 4433, 'Elfman': 4434, 'Elias': 4435, 'Ella': 4436, 'Ellen': 4437, 'Elvis': 4438, 'Elysian': 4439, 'Emil': 4440, 'Emirates': 4441, 'Engels': 4442, 'Enlightened': 4443, 'Enola': 4444, 'Enterprise': 4445, 'Environmental': 4446, 'Equal': 4447, 'Eratosthenes': 4448, 'Erbe': 4449, 'Eritrea': 4450, 'Essendon': 4451, 'Estelle': 4452, 'Estes': 4453, 'Euclid': 4454, 'Eurowings': 4455, 'Evan': 4456, 'Everett': 4457, 'Everyone': 4458, 'Evie': 4459, 'Ewan': 4460, 'Excavators': 4461, 'FA': 4462, 'FM': 4463, 'Factors': 4464, 'Faggin': 4465, 'Fairley': 4466, 'Faison': 4467, 'Fakhr': 4468, 'Falkor': 4469, 'Faranan': 4470, 'Faris': 4471, 'Farr': 4472, 'Fator': 4473, 'Fauntleroy': 4474, 'Fawcett': 4475, 'Fedele': 4476, 'Federico': 4477, 'Fellowhip': 4478, 'Fiddle': 4479, 'Fields': 4480, 'Filipepi': 4481, 'Fillion': 4482, 'Final': 4483, 'Financial': 4484, 'Fitzpatrick': 4485, 'Five': 4486, 'Flav': 4487, 'Flavor': 4488, 'Flesh': 4489, 'Flower': 4490, 'Fold': 4491, 'Fools': 4492, 'Foote': 4493, 'For': 4494, 'Fork': 4495, 'Fossard': 4496, 'Fossey': 4497, 'Francois': 4498, 'Frankie': 4499, 'Frederick': 4500, 'Free': 4501, 'Freedom': 4502, 'Freeman': 4503, 'Frey': 4504, 'From': 4505, 'Fuchur': 4506, 'Funmilayo': 4507, 'Fury': 4508, 'Gabel': 4509, 'Galleria': 4510, 'Galloway': 4511, 'Galveston': 4512, 'Gambon': 4513, 'Gammage': 4514, 'Gandalf': 4515, 'Ganymede': 4516, 'Garudeshwar': 4517, 'Gastrointestinal': 4518, 'Gautama': 4519, 'Gay': 4520, 'Gayla': 4521, 'Gazette': 4522, 'Geldof': 4523, 'Gems': 4524, 'Gen': 4525, 'Gen.': 4526, 'Genevieve': 4527, 'Gere': 4528, 'Germanwings': 4529, 'Gibbons': 4530, 'Gilley': 4531, 'Ginnifer': 4532, 'Glycolysis': 4533, 'Gobind': 4534, 'Goldstein': 4535, 'Gomez': 4536, 'Goodwin': 4537, 'Gore': 4538, 'Gould': 4539, 'Governance': 4540, 'Governorate': 4541, 'Granville': 4542, 'Grateau': 4543, 'Greenbriar': 4544, 'Greenbush': 4545, 'Gregg': 4546, 'Grimm': 4547, 'Grocery': 4548, 'GroupBlock': 4549, 'Guffey': 4550, 'Guildford': 4551, 'Guilherme': 4552, 'Gunn': 4553, 'Guru': 4554, 'Gwendoline': 4555, 'HEB': 4556, 'Hackman': 4557, 'Hagara': 4558, 'Haitian': 4559, 'Halfday': 4560, 'Hammer': 4561, 'Hammond': 4562, 'Han': 4563, 'Handshake': 4564, 'Hanging': 4565, 'Hanoi': 4566, 'Harden': 4567, 'Harishchandra': 4568, 'Harmon': 4569, 'Harrop': 4570, 'Harshman': 4571, 'Hartwell': 4572, 'Havilland': 4573, 'Hawk': 4574, 'Hayden': 4575, 'Heart': 4576, 'Hecht': 4577, 'Heidi': 4578, 'Helena': 4579, 'Hellard': 4580, 'Helms': 4581, 'Henderson': 4582, 'Henley': 4583, 'Henson': 4584, 'Hewlett': 4585, 'Hey': 4586, 'Hi': 4587, 'Hicky': 4588, 'Hidden': 4589, 'Hinson': 4590, 'Hira': 4591, 'Hobbs': 4592, 'Hoffmann': 4593, 'Holes': 4594, 'Holley': 4595, 'Homi': 4596, 'Hong': 4597, 'Honourable': 4598, 'Hugh': 4599, 'Hughley': 4600, 'Humphrey': 4601, 'Hutton': 4602, 'Huy': 4603, 'Hydrogen': 4604, 'IIS': 4605, 'IMS': 4606, 'INFJ': 4607, 'IST': 4608, 'Illinois': 4609, 'Index': 4610, 'Indiana': 4611, 'Inkjet': 4612, 'Institute': 4613, 'Insular': 4614, 'Intolerable': 4615, 'Inverness': 4616, 'Iqbal': 4617, 'Irene': 4618, 'Iron': 4619, 'Irving': 4620, 'Iver': 4621, 'Ivey': 4622, 'Iwo': 4623, 'Iyengar': 4624, 'J.W.': 4625, 'Jacks': 4626, 'Jaguars': 4627, 'Jainism': 4628, 'Jauna': 4629, 'Jawaharlal': 4630, 'Jeanne': 4631, 'Jehangir': 4632, 'Jenna': 4633, 'Jericho': 4634, 'Jerome': 4635, 'Jews': 4636, 'Jigme': 4637, 'Jima': 4638, 'Jingzhong': 4639, 'Jobs': 4640, 'Joemat': 4641, 'Johri': 4642, 'Jonas': 4643, 'Jose': 4644, 'Judaism': 4645, 'Judgment': 4646, 'Judicial': 4647, 'Jumped': 4648, 'Juscelino': 4649, 'Justice': 4650, 'K.': 4651, 'Kabaka': 4652, 'Kaepernick': 4653, 'Kajagoogoo': 4654, 'Kalia': 4655, 'Kamahl': 4656, 'Kamal': 4657, 'Kamalesvaran': 4658, 'Kander': 4659, 'Kandiah': 4660, 'Kanto': 4661, 'Karamchand': 4662, 'Karin': 4663, 'Karolyn': 4664, 'Kathryn': 4665, 'Ken': 4666, 'Kenichi': 4667, 'Kerr': 4668, 'Khabib': 4669, 'Khalsa': 4670, 'Khoisan': 4671, 'Kimmel': 4672, 'Kingsley': 4673, 'Kirkpatrick': 4674, 'Klerk': 4675, 'Knight': 4676, 'Koch': 4677, 'Kong': 4678, 'Kotkin': 4679, 'Kramer': 4680, 'Krayzie': 4681, 'Kubitschek': 4682, 'Kyles': 4683, 'Kīlauea': 4684, 'L.I.V.E.': 4685, 'L.T.': 4686, 'LP': 4687, 'LR': 4688, 'Labrador': 4689, 'Lachman': 4690, 'Laina': 4691, 'Lakshadweep': 4692, 'Lal': 4693, 'Lamar': 4694, 'Lamont': 4695, 'Lancie': 4696, 'Land': 4697, 'Landon': 4698, 'Lanka': 4699, 'Lannister': 4700, 'Laozi': 4701, 'Latcherie': 4702, 'Laurent': 4703, 'Lavallee': 4704, 'Layzie': 4705, 'Lazarus': 4706, 'Leagh': 4707, 'Learning': 4708, 'Lebanon': 4709, 'Leeza': 4710, 'Legislative': 4711, 'Leilah': 4712, 'Leonardo': 4713, 'Lester': 4714, 'Letters': 4715, 'Liboiron': 4716, 'Lines': 4717, 'Linkin': 4718, 'Linus': 4719, 'Lions': 4720, 'Lithgow': 4721, 'Lobo': 4722, 'Lochkovian': 4723, 'Lokmanya': 4724, 'Lombard': 4725, 'Longcross': 4726, 'Longoria': 4727, 'Loren': 4728, 'Loretta': 4729, 'Lorne': 4730, 'Loughman': 4731, 'Lt': 4732, 'Lubowitz': 4733, 'Luc': 4734, 'Lucy': 4735, 'Lue': 4736, 'Lulama': 4737, 'Lulu': 4738, 'Lun': 4739, 'Luna': 4740, 'Lunik': 4741, 'Lusha': 4742, 'Lvovich': 4743, 'Lyon': 4744, 'MC': 4745, 'MV': 4746, 'Mabel': 4747, 'Madeline': 4748, 'Maersk': 4749, 'Magna': 4750, 'Maharaja': 4751, 'Mahatma': 4752, 'Mairi': 4753, 'Mako': 4754, 'Malala': 4755, 'Maldives': 4756, 'Malik': 4757, 'Malloy': 4758, 'Mamata': 4759, 'Mamta': 4760, 'Mandela': 4761, 'Manekshaw': 4762, 'Manny': 4763, 'Mantaro': 4764, 'Manyu': 4765, 'Marbella': 4766, 'Marcel': 4767, 'Mare': 4768, 'Margaret': 4769, 'Margo': 4770, 'Mariah': 4771, 'Mariano': 4772, 'Marini': 4773, 'Marié': 4774, 'Maroon': 4775, 'Marseille': 4776, 'Marsh': 4777, 'Marshal': 4778, 'Martinez': 4779, 'Martyn': 4780, 'Marvel': 4781, 'Marx': 4782, 'Marytheresa': 4783, 'Masala': 4784, 'Maseo': 4785, 'Masiela': 4786, 'Massalia': 4787, 'Masterson': 4788, 'Matilda': 4789, 'Matter': 4790, 'Matthai': 4791, 'Maureen': 4792, 'McCann': 4793, 'McClurkin': 4794, 'McEntire': 4795, 'McEwan': 4796, 'McGregor': 4797, 'McKinley': 4798, 'McMahon': 4799, 'McMillan': 4800, 'McQueen': 4801, 'Meghalaya': 4802, 'Mehta': 4803, 'Mellark': 4804, 'Members': 4805, 'Mervyn': 4806, 'Metro': 4807, 'Meza': 4808, 'Mickey': 4809, 'Mid': 4810, 'Midge': 4811, 'Mind': 4812, 'Minogue': 4813, 'Minty': 4814, 'Miranda': 4815, 'Mishay': 4816, 'Mismi': 4817, 'Miss': 4818, 'Mittal': 4819, 'Mizoram': 4820, 'Mohandas': 4821, 'Monday': 4822, 'Mongo': 4823, 'Monsignor': 4824, 'Montalbán': 4825, 'Montepulciano': 4826, 'Montford': 4827, 'Monty': 4828, 'Moon': 4829, 'Moran': 4830, 'Morarji': 4831, 'Morganfield': 4832, 'Morse': 4833, 'Mosley': 4834, 'Mother': 4835, 'Mountains': 4836, 'Mr.': 4837, 'Muni': 4838, 'Munro': 4839, 'Murdoc': 4840, 'Murdoch': 4841, 'Mutiny': 4842, 'Nagaland': 4843, 'Nagaraj': 4844, 'Naismith': 4845, 'Namgyel': 4846, 'Nandalal': 4847, 'Napes': 4848, 'Narmada': 4849, 'Narrenbeschwörung': 4850, 'Nasim': 4851, 'Nat': 4852, 'Naughty': 4853, 'Navy': 4854, 'Nazi': 4855, 'Nepal': 4856, 'Nevado': 4857, 'Newbern': 4858, 'Niccals': 4859, 'Nice': 4860, 'Nieuport': 4861, 'Nikolaj': 4862, 'Nile': 4863, 'Nilsson': 4864, 'Nina': 4865, 'Ninety': 4866, 'Nkem': 4867, 'Nnamdi': 4868, 'Nolan': 4869, 'Noodle': 4870, 'Norfolk': 4871, 'Nouveau': 4872, 'Nucleotide': 4873, 'Nurmagomedov': 4874, \"O'Jays\": 4875, 'OMAA': 4876, 'Obedience': 4877, 'Oct': 4878, 'Oct.': 4879, 'Octomom': 4880, 'Oden': 4881, 'Odyssey': 4882, 'Ogg': 4883, 'Oliver': 4884, 'Olmec': 4885, 'Olmecs': 4886, 'Olympics': 4887, 'Once': 4888, 'Opt': 4889, 'Orcein': 4890, 'Original': 4891, 'Output': 4892, 'Over': 4893, 'P.A.': 4894, 'PCP': 4895, 'PST': 4896, 'Pacers': 4897, 'Pacquiao': 4898, 'Palash': 4899, 'Paleozoic': 4900, 'Panini': 4901, 'Pankaj': 4902, 'Pantages': 4903, 'Pantaleoni': 4904, 'Paramesh': 4905, 'Parkwa': 4906, 'Parsons': 4907, 'Parti': 4908, 'Parton': 4909, 'Party': 4910, 'Password': 4911, 'Patel': 4912, 'Paulette': 4913, 'Pawan': 4914, 'Peace': 4915, 'Peebles': 4916, 'Peevey': 4917, 'Pelling': 4918, 'Penetrating': 4919, 'Pepper': 4920, 'Period': 4921, 'Peterson': 4922, 'Petronelli': 4923, 'Phalke': 4924, 'Phillip': 4925, 'Phosphorous': 4926, 'Picaresque': 4927, 'Pier': 4928, 'Pine': 4929, 'Pisa': 4930, 'Pitt': 4931, 'Planas': 4932, 'Planck': 4933, 'Play': 4934, 'Plug': 4935, 'Plymouth': 4936, 'Pogba': 4937, 'Pointed': 4938, 'Poiret': 4939, 'Pojas': 4940, 'Poland': 4941, 'Pollard': 4942, 'Pond': 4943, 'Port': 4944, 'Porter': 4945, 'Power': 4946, 'Pradeep': 4947, 'Prasad': 4948, 'Pripyat': 4949, 'Procter': 4950, 'Procurement': 4951, 'Profane': 4952, 'Professor': 4953, 'Prometheus': 4954, 'Public': 4955, 'Pyle': 4956, 'Quadri': 4957, 'Qualtrough': 4958, 'Quarter': 4959, 'Quartering': 4960, 'Queensland': 4961, 'Quigley': 4962, 'Raditz': 4963, 'Raj': 4964, 'Raja': 4965, 'Rajasthan': 4966, 'Rajendra': 4967, 'Raleigh': 4968, 'Ramirez': 4969, 'Rand': 4970, 'Randolph': 4971, 'Ransome': 4972, 'Raphaël': 4973, 'Rauch': 4974, 'Reality': 4975, 'Realty': 4976, 'Reba': 4977, 'Rebellion': 4978, 'Redeemer': 4979, 'Remi': 4980, 'Restless': 4981, 'Retting': 4982, 'Return': 4983, 'Revenue': 4984, 'Revolution': 4985, 'Reye': 4986, 'Rhett': 4987, 'Ricardo': 4988, 'Rich': 4989, 'Richie': 4990, 'Richmond': 4991, 'Right': 4992, 'Rises': 4993, 'Riviera': 4994, 'Roache': 4995, 'Roanoke': 4996, 'Roc': 4997, 'Rocket': 4998, 'Roiland': 4999, 'Roker': 5000, 'Roland': 5001, 'Ronnie': 5002, 'Rory': 5003, 'Ross': 5004, 'Rota': 5005, 'Route': 5006, 'Routhier': 5007, 'Rowell': 5008, 'Rudolf': 5009, 'Ruffin': 5010, 'Russel': 5011, 'Río': 5012, 'S.': 5013, 'S.S.R.N.': 5014, 'Saba': 5015, 'Sacagawea': 5016, 'Sacajawea': 5017, 'Sain': 5018, 'Sakakawea': 5019, 'Salivary': 5020, 'Salter': 5021, 'Salvatore': 5022, 'Samantha': 5023, 'Sambora': 5024, 'Sampson': 5025, 'Sandi': 5026, 'Sandro': 5027, 'Sands': 5028, 'Santos': 5029, 'Sara': 5030, 'Sardar': 5031, 'Satpura': 5032, 'Saturday': 5033, 'Saul': 5034, 'Sawai': 5035, 'Sayer': 5036, 'Scanavino': 5037, 'Scarlet': 5038, 'Schmit': 5039, 'Schnatter': 5040, 'Schuyler': 5041, 'Scotland': 5042, 'Scramble': 5043, 'Scrappy': 5044, 'Scully': 5045, 'Sea': 5046, 'Seals': 5047, 'Seaport': 5048, 'Seco': 5049, 'Sector': 5050, 'Sedgwick': 5051, 'Senegal': 5052, 'Senzeni': 5053, 'Sepoy': 5054, 'Sepse': 5055, 'Serbia': 5056, 'Serena': 5057, 'Serkis': 5058, 'Services': 5059, 'Settlement': 5060, 'Shaffer': 5061, 'Shanley': 5062, 'Shanna': 5063, 'Shashi': 5064, 'Shastri': 5065, 'Shaun': 5066, 'Shawn': 5067, 'Shazwan': 5068, 'Sherri': 5069, 'Shinpo': 5070, 'Shogi': 5071, 'Shook': 5072, 'Shukla': 5073, 'Sikhs': 5074, 'Silurian': 5075, 'Simmons': 5076, 'Simone': 5077, 'Simpsons': 5078, 'Sinclair': 5079, 'Singapore': 5080, 'Sinise': 5081, 'Siobhán': 5082, 'Sirius': 5083, 'Sistine': 5084, 'Sitka': 5085, 'Skelton': 5086, 'Skunk': 5087, 'Sky': 5088, 'Slater': 5089, 'Slocum': 5090, 'Smallwood': 5091, 'Smaug': 5092, 'Smita': 5093, 'Smithland': 5094, 'Socrates': 5095, 'Sokoloff': 5096, 'Something': 5097, 'Soni': 5098, 'Sonia': 5099, 'Sonny': 5100, 'Sony': 5101, 'Soul': 5102, 'Soup': 5103, 'Southeastern': 5104, 'Southern': 5105, 'Southside': 5106, 'Soyuz': 5107, 'Space': 5108, 'Spargur': 5109, 'Spears': 5110, 'Spektor': 5111, 'Sri': 5112, 'Srivastava': 5113, 'Stafford': 5114, 'Statler': 5115, 'Steamboat': 5116, 'Stockard': 5117, 'Stonetown': 5118, 'Storm': 5119, 'Strae': 5120, 'Straits': 5121, 'Sue': 5122, 'Sulzberger': 5123, 'Sundarbans': 5124, 'Sundram': 5125, 'Sunshine': 5126, 'Supreme': 5127, 'Surrey': 5128, 'Sursok': 5129, 'Sutton': 5130, 'Swampy': 5131, 'Sylvester': 5132, 'T': 5133, 'TNZ': 5134, 'Tabla': 5135, 'Tagore': 5136, 'Talkeetna': 5137, 'Tamatoa': 5138, 'Tammin': 5139, 'Tammy': 5140, 'Tanzanite': 5141, 'Tardif': 5142, 'Tarsus': 5143, 'Taxidermy': 5144, 'Tecumseh': 5145, 'Temuera': 5146, 'Terrence': 5147, 'Thakuri': 5148, 'Thea': 5149, 'Theatre': 5150, 'Themyscira': 5151, 'Third': 5152, 'Three': 5153, 'Thrissur': 5154, 'Ti': 5155, 'Tigre': 5156, 'Tilak': 5157, 'Tilda': 5158, 'Timberlake': 5159, 'Times': 5160, 'Tin': 5161, 'Tina': 5162, 'Tippin': 5163, 'Titanium': 5164, 'Tobacco': 5165, 'Toland': 5166, 'Toni': 5167, 'Toné': 5168, 'Towers': 5169, 'Trea': 5170, 'Tresvant': 5171, 'Trowbridge': 5172, 'Truist': 5173, 'Tuesday': 5174, 'Tughluq': 5175, 'Tunisia': 5176, 'Type': 5177, 'Tyrian': 5178, 'Tze': 5179, 'Tzu': 5180, 'USC': 5181, 'USOS': 5182, 'Uganda': 5183, 'Ulugh': 5184, 'Unexpected': 5185, 'Unofficial': 5186, 'Urbana': 5187, 'Ure': 5188, 'Usain': 5189, 'Valderrama': 5190, 'Valentine': 5191, 'VanZant': 5192, 'Vanderford': 5193, 'Vanni': 5194, 'Vanshidhar': 5195, 'Varane': 5196, 'Varble': 5197, 'Varsano': 5198, 'Vaughan': 5199, 'Vaughn': 5200, 'Vengaram': 5201, 'Very': 5202, 'Vice': 5203, 'Vieira': 5204, 'Vignette': 5205, 'Vin': 5206, 'Vinci': 5207, 'Vindhyan': 5208, 'Vinko': 5209, 'Volcano': 5210, 'Vornado': 5211, 'Wagner': 5212, 'Wagyu': 5213, 'Wang': 5214, 'Wangchuck': 5215, 'Warfield': 5216, 'Warner': 5217, 'Wednesday': 5218, 'Wehle': 5219, 'Weil': 5220, 'Weimar': 5221, 'Wes': 5222, 'Wesley': 5223, 'Westinghouse': 5224, 'Whaite': 5225, 'Whitey': 5226, 'Whitman': 5227, 'Whiz': 5228, 'Whoville': 5229, 'Wicks': 5230, 'Wiggles': 5231, 'Wil': 5232, 'Wilcher': 5233, 'Wild': 5234, 'Wilder': 5235, 'Willy': 5236, 'Wilmer': 5237, 'Wilt': 5238, 'Winningham': 5239, 'Wish': 5240, 'Woking': 5241, 'Womesh': 5242, 'Wondolowski': 5243, 'Woodland': 5244, 'Woodlands': 5245, 'Woodman': 5246, 'Woods': 5247, 'Work': 5248, 'Wyoming': 5249, 'X': 5250, 'XXVII': 5251, 'XXX': 5252, 'XXXVII': 5253, 'Xingwana': 5254, 'Yang': 5255, 'Yes': 5256, 'Zach': 5257, 'Zachary': 5258, 'Zaidi': 5259, 'Zeishan': 5260, 'Zhenwei': 5261, 'Zoe': 5262, 'Zokwana': 5263, '^': 5264, 'a.m.': 5265, 'ability': 5266, 'absolute': 5267, 'acceptance': 5268, 'acetate': 5269, 'actor': 5270, 'aesthetic': 5271, 'agriculture': 5272, 'alternative': 5273, 'aluminum': 5274, 'am': 5275, 'anhydrite': 5276, 'anthem': 5277, 'antibodies': 5278, 'antibody': 5279, 'appear': 5280, 'archil': 5281, 'armature': 5282, 'artists': 5283, 'bag': 5284, 'battle': 5285, 'beef': 5286, 'begins': 5287, 'belts': 5288, 'benefit': 5289, 'bin': 5290, 'bluestone': 5291, 'blurred': 5292, 'bonds': 5293, 'boost': 5294, 'boundaries': 5295, 'brainstem': 5296, 'branch': 5297, 'breeches': 5298, 'brother': 5299, 'bum': 5300, 'bus': 5301, 'calcium': 5302, 'cap': 5303, 'capsule': 5304, 'captured': 5305, 'carbonates': 5306, 'care': 5307, 'cat': 5308, 'ceiling': 5309, 'cell': 5310, 'cells': 5311, 'centre': 5312, 'changes': 5313, 'cheese': 5314, 'chicken': 5315, 'chocolate': 5316, 'cholesterol': 5317, 'city': 5318, 'civil': 5319, 'clearance': 5320, 'clearing': 5321, 'club': 5322, 'cold': 5323, 'colon': 5324, 'colonial': 5325, 'combined': 5326, 'commanded': 5327, 'communication': 5328, 'communism': 5329, 'communists': 5330, 'conduct': 5331, 'constitutional': 5332, 'continuously': 5333, 'controlled': 5334, 'copper': 5335, 'copyright': 5336, 'countryside': 5337, 'courts': 5338, 'cows': 5339, 'crab': 5340, 'creator': 5341, 'crew': 5342, 'criminal': 5343, 'da': 5344, 'dancers': 5345, 'days': 5346, 'deadline': 5347, \"dell'Accademia\": 5348, 'democratic': 5349, 'deputy': 5350, 'derivative': 5351, 'derivatives': 5352, 'descendant': 5353, 'destroyer': 5354, 'differential': 5355, 'digital': 5356, 'dimensional': 5357, 'dinosaur': 5358, 'disbanded': 5359, 'dish': 5360, 'dishwater': 5361, 'does': 5362, 'doo': 5363, 'dos': 5364, 'drive': 5365, 'duck': 5366, 'ear': 5367, 'easterly': 5368, 'electrons': 5369, 'elephant': 5370, 'ends': 5371, 'england': 5372, 'enzymes': 5373, 'epidermis': 5374, 'epithelium': 5375, 'ever': 5376, 'everest': 5377, 'extremes': 5378, 'fashion': 5379, 'fashionable': 5380, 'federal': 5381, 'female': 5382, 'fertilization': 5383, 'fewer': 5384, 'fighting': 5385, 'financial': 5386, 'first': 5387, 'fitting': 5388, 'flat': 5389, 'fleur': 5390, 'flopping': 5391, 'flower': 5392, 'flowers': 5393, 'folktale': 5394, 'following': 5395, 'formations': 5396, 'free': 5397, 'frequency': 5398, 'frescos': 5399, 'frost': 5400, 'full': 5401, 'games': 5402, 'ganjira': 5403, 'gentlemanly': 5404, 'glands': 5405, 'glomerulus': 5406, 'glucans': 5407, 'go': 5408, 'goaltending': 5409, 'government': 5410, 'granodiorite': 5411, 'great': 5412, 'guided': 5413, 'halides': 5414, 'hall': 5415, 'haploid': 5416, 'harem': 5417, 'have': 5418, 'headwaters': 5419, 'health': 5420, 'helium': 5421, 'hermaphrodite': 5422, 'highest': 5423, 'hip': 5424, 'hop': 5425, 'hopping': 5426, 'hosen': 5427, 'hotspot': 5428, 'house': 5429, 'image': 5430, 'impression': 5431, 'informant': 5432, 'informants': 5433, 'informer': 5434, 'infrastructure': 5435, 'inkjet': 5436, 'inspirational': 5437, 'inspired': 5438, 'instrument': 5439, 'insurance': 5440, 'intermembrane': 5441, 'international': 5442, 'internet': 5443, 'intersex': 5444, 'introduced': 5445, 'irregular': 5446, 'items': 5447, 'jargon': 5448, 'judicial': 5449, 'judiciary': 5450, 'just': 5451, 'katers': 5452, 'kg': 5453, 'kidneys': 5454, 'killed': 5455, 'kin': 5456, 'knelt': 5457, 'known': 5458, 'laboratory': 5459, 'lacmus': 5460, 'lake': 5461, 'lamb': 5462, 'landfill': 5463, 'language': 5464, 'later': 5465, 'leaders': 5466, 'leaves': 5467, 'ler': 5468, 'less': 5469, 'liberal': 5470, 'libéral': 5471, 'limited': 5472, 'lipids': 5473, 'lis': 5474, 'lithosphere': 5475, 'liver': 5476, 'living': 5477, 'lobster': 5478, 'loin': 5479, 'loose': 5480, 'lotus': 5481, 'lower': 5482, 'lucidum': 5483, 'lymphatic': 5484, 'lymphoma': 5485, 'maggots': 5486, 'manoeuvre': 5487, 'maximum': 5488, 'meat': 5489, 'medical': 5490, 'megalodon': 5491, 'member': 5492, 'mental': 5493, 'mentioned': 5494, 'meristem': 5495, 'meristems': 5496, 'mid-1997': 5497, 'middle': 5498, 'millennium': 5499, 'modulation': 5500, 'molecular': 5501, 'monosperma': 5502, 'months': 5503, 'morale': 5504, 'mortgage': 5505, 'mount': 5506, 'muscat': 5507, 'mushroom': 5508, 'n': 5509, 'named': 5510, 'nation': 5511, 'nervous': 5512, 'network': 5513, 'neurology': 5514, 'never': 5515, 'northwest': 5516, 'novel': 5517, 'nu': 5518, 'nucleotides': 5519, 'number': 5520, 'nut': 5521, 'nutrients': 5522, 'occasionally': 5523, 'oil': 5524, 'oils': 5525, 'old': 5526, 'older': 5527, 'olmec': 5528, 'orange': 5529, 'orchil': 5530, 'order': 5531, 'organic': 5532, 'organization': 5533, 'origin': 5534, 'originated': 5535, 'outside': 5536, 'overall': 5537, 'overlying': 5538, 'overseas': 5539, 'ownership': 5540, 'oxidize': 5541, 'oxygen': 5542, 'page': 5543, 'pants': 5544, 'parasitism': 5545, 'parents': 5546, 'party': 5547, 'passengers': 5548, 'pathologist': 5549, 'patrol': 5550, 'pattern': 5551, 'permeability': 5552, 'photosynthesis': 5553, 'physical': 5554, 'physician': 5555, 'physicians': 5556, 'pie': 5557, 'plasma': 5558, 'polysaccharides': 5559, 'popularized': 5560, 'porcupines': 5561, 'pork': 5562, 'pounds': 5563, 'prepare': 5564, 'prequel': 5565, 'preseason': 5566, 'preserver': 5567, 'prevent': 5568, 'primary': 5569, 'printers': 5570, 'prison': 5571, 'privileged': 5572, 'produced': 5573, 'prohibited': 5574, 'property': 5575, 'protecting': 5576, 'proteins': 5577, 'proverb': 5578, 'provides': 5579, 'provolone': 5580, 'psychiatrist': 5581, 'pump': 5582, 'rain': 5583, 'ranking': 5584, 'rapid': 5585, 'rapper': 5586, 'rear': 5587, 'reasons': 5588, 'regular': 5589, 'released': 5590, 'releasing': 5591, 'require': 5592, 'respective': 5593, 'restricted': 5594, 'retting': 5595, 'returning': 5596, 'ribeye': 5597, 'ridges': 5598, 'right': 5599, 'river': 5600, 'rivers': 5601, 'rondo': 5602, 'rookie': 5603, 'roughly': 5604, 'russia': 5605, 'sacrificed': 5606, 'safeguarding': 5607, 'saline': 5608, 'sandstone': 5609, 'sarsen': 5610, 'sat': 5611, 'savanna': 5612, 'scene': 5613, 'scientists': 5614, 'sea': 5615, 'secondary': 5616, 'sequel': 5617, 'shadow': 5618, 'shepherd': 5619, 'shopping': 5620, 'short': 5621, 'sidekick': 5622, 'sixteen': 5623, 'sixth': 5624, 'size': 5625, 'skills': 5626, 'slide': 5627, 'slip': 5628, 'snow': 5629, 'social': 5630, 'some': 5631, 'sometimes': 5632, 'southwestern': 5633, 'space': 5634, 'spears': 5635, 'special': 5636, 'specialties': 5637, 'species': 5638, 'spleen': 5639, 'spread': 5640, 'sq': 5641, 'star': 5642, 'station': 5643, 'steak': 5644, 'still': 5645, 'stocks': 5646, 'stones': 5647, 'stratified': 5648, 'stratum': 5649, 'student': 5650, 'students': 5651, 'studies': 5652, 'studio': 5653, 'subsidizes': 5654, 'suburb': 5655, 'sugar': 5656, 'sulfate': 5657, 'summer': 5658, 'surrounding': 5659, 'sweat': 5660, 'syndrome': 5661, 'synthase': 5662, 'table': 5663, 'taunting': 5664, 'telomere': 5665, 'temperature': 5666, 'tetrahedron': 5667, 'than': 5668, 'thereby': 5669, 'thermal': 5670, 'they': 5671, 'throw': 5672, 'tie': 5673, 'tied': 5674, 'tissue': 5675, 'tonsil': 5676, 'tonsilloliths': 5677, 'torpedo': 5678, 'tortured': 5679, 'tract': 5680, 'traders': 5681, 'tradition': 5682, 'translucent': 5683, 'transverse': 5684, 'twins': 5685, 'type': 5686, 'typically': 5687, 'under-19': 5688, 'underwriting': 5689, 'unit': 5690, 'unsportsmanlike': 5691, 'variants': 5692, 'various': 5693, 'very': 5694, 'vessels': 5695, 'visa': 5696, 'wartime': 5697, 'wear': 5698, 'weir': 5699, 'well': 5700, 'were': 5701, 'wheel': 5702, 'where': 5703, 'woman': 5704, 'worker': 5705, 'wreck': 5706, 'Édouard': 5707})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8v8gQgP70gY"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpBfZQk-7yTL"
      },
      "source": [
        "BATCH_SIZE=16\r\n",
        "train_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train, valid), \r\n",
        "    batch_size = BATCH_SIZE, \r\n",
        "    device = device, sort=False, shuffle=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot8vlVo09ejF"
      },
      "source": [
        "Build the vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtqKGxNO9ejF"
      },
      "source": [
        "Define the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey8yA-Yh9ejF"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqnAGiVU9ejF"
      },
      "source": [
        "Create the iterators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP5s8Vov9ejF"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "### Encoder\n",
        "\n",
        "First, we'll build the encoder. Similar to the previous model, we only use a single layer GRU, however we now use a *bidirectional RNN*. With a bidirectional RNN, we have two RNNs in each layer. A *forward RNN* going over the embedded sentence from left to right (shown below in green), and a *backward RNN* going over the embedded sentence from right to left (teal). All we need to do in code is set `bidirectional = True` and then pass the embedded sentence to the RNN as before. \n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq8.png?raw=1)\n",
        "\n",
        "We now have:\n",
        "\n",
        "$$\\begin{align*}\n",
        "h_t^\\rightarrow &= \\text{EncoderGRU}^\\rightarrow(e(x_t^\\rightarrow),h_{t-1}^\\rightarrow)\\\\\n",
        "h_t^\\leftarrow &= \\text{EncoderGRU}^\\leftarrow(e(x_t^\\leftarrow),h_{t-1}^\\leftarrow)\n",
        "\\end{align*}$$\n",
        "\n",
        "Where $x_0^\\rightarrow = \\text{<sos>}, x_1^\\rightarrow = \\text{guten}$ and $x_0^\\leftarrow = \\text{<eos>}, x_1^\\leftarrow = \\text{morgen}$.\n",
        "\n",
        "As before, we only pass an input (`embedded`) to the RNN, which tells PyTorch to initialize both the forward and backward initial hidden states ($h_0^\\rightarrow$ and $h_0^\\leftarrow$, respectively) to a tensor of all zeros. We'll also get two context vectors, one from the forward RNN after it has seen the final word in the sentence, $z^\\rightarrow=h_T^\\rightarrow$, and one from the backward RNN after it has seen the first word in the sentence, $z^\\leftarrow=h_T^\\leftarrow$.\n",
        "\n",
        "The RNN returns `outputs` and `hidden`. \n",
        "\n",
        "`outputs` is of size **[src len, batch size, hid dim * num directions]** where the first `hid_dim` elements in the third axis are the hidden states from the top layer forward RNN, and the last `hid_dim` elements are hidden states from the top layer backward RNN. We can think of the third axis as being the forward and backward hidden states concatenated together other, i.e. $h_1 = [h_1^\\rightarrow; h_{T}^\\leftarrow]$, $h_2 = [h_2^\\rightarrow; h_{T-1}^\\leftarrow]$ and we can denote all encoder hidden states (forward and backwards concatenated together) as $H=\\{ h_1, h_2, ..., h_T\\}$.\n",
        "\n",
        "`hidden` is of size **[n layers * num directions, batch size, hid dim]**, where **[-2, :, :]** gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and **[-1, :, :]** gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\n",
        "\n",
        "As the decoder is not bidirectional, it only needs a single context vector, $z$, to use as its initial hidden state, $s_0$, and we currently have two, a forward and a backward one ($z^\\rightarrow=h_T^\\rightarrow$ and $z^\\leftarrow=h_T^\\leftarrow$, respectively). We solve this by concatenating the two context vectors together, passing them through a linear layer, $g$, and applying the $\\tanh$ activation function. \n",
        "\n",
        "$$z=\\tanh(g(h_T^\\rightarrow, h_T^\\leftarrow)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$$\n",
        "\n",
        "**Note**: this is actually a deviation from the paper. Instead, they feed only the first backward RNN hidden state through a linear layer to get the context vector/decoder initial hidden state. This doesn't seem to make sense to me, so we have changed it.\n",
        "\n",
        "As we want our model to look back over the whole of the source sentence we return `outputs`, the stacked forward and backward hidden states for every token in the source sentence. We also return `hidden`, which acts as our initial hidden state in the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8ovMJ3_9ejF"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "                \n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9WNKZ4D9ejF"
      },
      "source": [
        "### Attention\n",
        "\n",
        "Next up is the attention layer. This will take in the previous hidden state of the decoder, $s_{t-1}$, and all of the stacked forward and backward hidden states from the encoder, $H$. The layer will output an attention vector, $a_t$, that is the length of the source sentence, each element is between 0 and 1 and the entire vector sums to 1.\n",
        "\n",
        "Intuitively, this layer takes what we have decoded so far, $s_{t-1}$, and all of what we have encoded, $H$, to produce a vector, $a_t$, that represents which words in the source sentence we should pay the most attention to in order to correctly predict the next word to decode, $\\hat{y}_{t+1}$. \n",
        "\n",
        "First, we calculate the *energy* between the previous decoder hidden state and the encoder hidden states. As our encoder hidden states are a sequence of $T$ tensors, and our previous decoder hidden state is a single tensor, the first thing we do is `repeat` the previous decoder hidden state $T$ times. We then calculate the energy, $E_t$, between them by concatenating them together and passing them through a linear layer (`attn`) and a $\\tanh$ activation function. \n",
        "\n",
        "$$E_t = \\tanh(\\text{attn}(s_{t-1}, H))$$ \n",
        "\n",
        "This can be thought of as calculating how well each encoder hidden state \"matches\" the previous decoder hidden state.\n",
        "\n",
        "We currently have a **[dec hid dim, src len]** tensor for each example in the batch. We want this to be **[src len]** for each example in the batch as the attention should be over the length of the source sentence. This is achieved by multiplying the `energy` by a **[1, dec hid dim]** tensor, $v$.\n",
        "\n",
        "$$\\hat{a}_t = v E_t$$\n",
        "\n",
        "We can think of $v$ as the weights for a weighted sum of the energy across all encoder hidden states. These weights tell us how much we should attend to each token in the source sequence. The parameters of $v$ are initialized randomly, but learned with the rest of the model via backpropagation. Note how $v$ is not dependent on time, and the same $v$ is used for each time-step of the decoding. We implement $v$ as a linear layer without a bias.\n",
        "\n",
        "Finally, we ensure the attention vector fits the constraints of having all elements between 0 and 1 and the vector summing to 1 by passing it through a $\\text{softmax}$ layer.\n",
        "\n",
        "$$a_t = \\text{softmax}(\\hat{a_t})$$\n",
        "\n",
        "This gives us the attention over the source sentence!\n",
        "\n",
        "Graphically, this looks something like below. This is for calculating the very first attention vector, where $s_{t-1} = s_0 = z$. The green/teal blocks represent the hidden states from both the forward and backward RNNs, and the attention computation is all done within the pink block.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq9.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yXR6Qbg9ejF"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        #attention= [batch size, src len]\n",
        "        \n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb-p1mhO9ejF"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "Next up is the decoder. \n",
        "\n",
        "The decoder contains the attention layer, `attention`, which takes the previous hidden state, $s_{t-1}$, all of the encoder hidden states, $H$, and returns the attention vector, $a_t$.\n",
        "\n",
        "We then use this attention vector to create a weighted source vector, $w_t$, denoted by `weighted`, which is a weighted sum of the encoder hidden states, $H$, using $a_t$ as the weights.\n",
        "\n",
        "$$w_t = a_t H$$\n",
        "\n",
        "The embedded input word, $d(y_t)$, the weighted source vector, $w_t$, and the previous decoder hidden state, $s_{t-1}$, are then all passed into the decoder RNN, with $d(y_t)$ and $w_t$ being concatenated together.\n",
        "\n",
        "$$s_t = \\text{DecoderGRU}(d(y_t), w_t, s_{t-1})$$\n",
        "\n",
        "We then pass $d(y_t)$, $w_t$ and $s_t$ through the linear layer, $f$, to make a prediction of the next word in the target sentence, $\\hat{y}_{t+1}$. This is done by concatenating them all together.\n",
        "\n",
        "$$\\hat{y}_{t+1} = f(d(y_t), w_t, s_t)$$\n",
        "\n",
        "The image below shows decoding the first word in an example translation.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq10.png?raw=1)\n",
        "\n",
        "The green/teal blocks show the forward/backward encoder RNNs which output $H$, the red block shows the context vector, $z = h_T = \\tanh(g(h^\\rightarrow_T,h^\\leftarrow_T)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$, the blue block shows the decoder RNN which outputs $s_t$, the purple block shows the linear layer, $f$, which outputs $\\hat{y}_{t+1}$ and the orange block shows the calculation of the weighted sum over $H$ by $a_t$ and outputs $w_t$. Not shown is the calculation of $a_t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wg3PolD9ejF"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj7DZYQD9ejF"
      },
      "source": [
        "### Seq2Seq\n",
        "\n",
        "This is the first model where we don't have to have the encoder RNN and decoder RNN have the same hidden dimensions, however the encoder has to be bidirectional. This requirement can be removed by changing all occurences of `enc_dim * 2` to `enc_dim * 2 if encoder_is_bidirectional else enc_dim`. \n",
        "\n",
        "This seq2seq encapsulator is similar to the last two. The only difference is that the `encoder` returns both the final hidden state (which is the final hidden state from both the forward and backward encoder RNNs passed through a linear layer) to be used as the initial hidden state for the decoder, as well as every hidden state (which are the forward and backward hidden states stacked on top of each other). We also need to ensure that `hidden` and `encoder_outputs` are passed to the decoder. \n",
        "\n",
        "Briefly going over all of the steps:\n",
        "- the `outputs` tensor is created to hold all predictions, $\\hat{Y}$\n",
        "- the source sequence, $X$, is fed into the encoder to receive $z$ and $H$\n",
        "- the initial decoder hidden state is set to be the `context` vector, $s_0 = z = h_T$\n",
        "- we use a batch of `<sos>` tokens as the first `input`, $y_1$\n",
        "- we then decode within a loop:\n",
        "  - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and all encoder outputs, $H$, into the decoder\n",
        "  - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n",
        "  - we then decide if we are going to teacher force or not, setting the next input as appropriate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEkTIixA9ejF"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXyLKEkh9ejF"
      },
      "source": [
        "## Training the Seq2Seq Model\n",
        "\n",
        "The rest of this session is very similar to the previous one.\n",
        "\n",
        "We initialise our parameters, encoder, decoder and seq2seq model (placing it on the GPU if we have one). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7FRQ8UR9ejF"
      },
      "source": [
        "# INPUT_DIM = len(SRC.vocab)\n",
        "# OUTPUT_DIM = len(TRG.vocab)\n",
        "\n",
        "INPUT_DIM = len(Sentence.vocab)\n",
        "OUTPUT_DIM = len(Label.vocab)\n",
        "\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux1sNqPk9ejF"
      },
      "source": [
        "We use a simplified version of the weight initialization scheme used in the paper. Here, we will initialize all biases to zero and all weights from $\\mathcal{N}(0, 0.01)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXaq2o559ejF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d89d68e-9d2f-42c8-df45-7e29aecf3f27"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(4450, 256)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(5708, 256)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=5708, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij9nRbgx9ejH"
      },
      "source": [
        "Calculate the number of parameters. We get an increase of almost 50% in the amount of parameters from the last model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oAsDCtJ9ejH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d734e5-0648-4a83-9e20-6b48203cf484"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 19,268,172 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-C9PF1d9ejH"
      },
      "source": [
        "We create an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox4JAgEE9ejH"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5FzwRlz9ejH"
      },
      "source": [
        "We initialize the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iAv-6As9ejH"
      },
      "source": [
        "# TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "TRG_PAD_IDX = Label.vocab.stoi[Label.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bzv6baC9ejH"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYPFqH6a9ejH"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        # src = batch.src\n",
        "        # trg = batch.trg\n",
        "        src = batch.sentence\n",
        "        trg = batch.label\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNawOMJD9ejH"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQotLtCi9ejH"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            # src = batch.src\n",
        "            # trg = batch.trg\n",
        "            src = batch.sentence\n",
        "            trg = batch.label\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_TUjr1O9ejH"
      },
      "source": [
        "Finally, define a timing function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UMyHEYS9ejH"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYKIfYvY9ejH"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2-2b9-W9ejH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c673d815-9019-437c-a804-640ca5fd2bab"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    # valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 31s\n",
            "\tTrain Loss: 5.466 | Train PPL: 236.482\n",
            "\t Val. Loss: 5.712 |  Val. PPL: 302.455\n",
            "Epoch: 02 | Time: 0m 31s\n",
            "\tTrain Loss: 4.120 | Train PPL:  61.588\n",
            "\t Val. Loss: 5.679 |  Val. PPL: 292.736\n",
            "Epoch: 03 | Time: 0m 32s\n",
            "\tTrain Loss: 3.579 | Train PPL:  35.840\n",
            "\t Val. Loss: 5.619 |  Val. PPL: 275.518\n",
            "Epoch: 04 | Time: 0m 32s\n",
            "\tTrain Loss: 3.051 | Train PPL:  21.138\n",
            "\t Val. Loss: 5.485 |  Val. PPL: 240.979\n",
            "Epoch: 05 | Time: 0m 32s\n",
            "\tTrain Loss: 2.512 | Train PPL:  12.323\n",
            "\t Val. Loss: 5.458 |  Val. PPL: 234.530\n",
            "Epoch: 06 | Time: 0m 32s\n",
            "\tTrain Loss: 2.004 | Train PPL:   7.421\n",
            "\t Val. Loss: 5.292 |  Val. PPL: 198.768\n",
            "Epoch: 07 | Time: 0m 32s\n",
            "\tTrain Loss: 1.661 | Train PPL:   5.264\n",
            "\t Val. Loss: 5.156 |  Val. PPL: 173.396\n",
            "Epoch: 08 | Time: 0m 32s\n",
            "\tTrain Loss: 1.414 | Train PPL:   4.113\n",
            "\t Val. Loss: 5.293 |  Val. PPL: 198.993\n",
            "Epoch: 09 | Time: 0m 32s\n",
            "\tTrain Loss: 1.238 | Train PPL:   3.450\n",
            "\t Val. Loss: 5.189 |  Val. PPL: 179.292\n",
            "Epoch: 10 | Time: 0m 32s\n",
            "\tTrain Loss: 1.090 | Train PPL:   2.974\n",
            "\t Val. Loss: 4.987 |  Val. PPL: 146.451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1Yy2D-AAGzU",
        "outputId": "72e891b5-9948-485f-c93f-d32383f0f12b"
      },
      "source": [
        "optimizer"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    eps: 1e-08\n",
              "    lr: 0.001\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPHt21sA9l1W",
        "outputId": "93bbb05b-dc6a-47c4-d563-d2a19e72d4cf"
      },
      "source": [
        "N_EPOCHS = 30\r\n",
        "CLIP = 1\r\n",
        "model.load_state_dict(torch.load('tut3-model.pt'))\r\n",
        "best_valid_loss = 4.9866932503720545\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    # valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 32s\n",
            "\tTrain Loss: 1.011 | Train PPL:   2.749\n",
            "\t Val. Loss: 4.860 |  Val. PPL: 128.971\n",
            "Epoch: 02 | Time: 0m 32s\n",
            "\tTrain Loss: 0.898 | Train PPL:   2.455\n",
            "\t Val. Loss: 4.669 |  Val. PPL: 106.548\n",
            "Epoch: 03 | Time: 0m 32s\n",
            "\tTrain Loss: 0.820 | Train PPL:   2.270\n",
            "\t Val. Loss: 4.567 |  Val. PPL:  96.222\n",
            "Epoch: 04 | Time: 0m 32s\n",
            "\tTrain Loss: 0.781 | Train PPL:   2.183\n",
            "\t Val. Loss: 4.513 |  Val. PPL:  91.229\n",
            "Epoch: 05 | Time: 0m 32s\n",
            "\tTrain Loss: 0.727 | Train PPL:   2.069\n",
            "\t Val. Loss: 4.488 |  Val. PPL:  88.941\n",
            "Epoch: 06 | Time: 0m 32s\n",
            "\tTrain Loss: 0.693 | Train PPL:   1.999\n",
            "\t Val. Loss: 4.436 |  Val. PPL:  84.466\n",
            "Epoch: 07 | Time: 0m 32s\n",
            "\tTrain Loss: 0.668 | Train PPL:   1.949\n",
            "\t Val. Loss: 4.415 |  Val. PPL:  82.642\n",
            "Epoch: 08 | Time: 0m 32s\n",
            "\tTrain Loss: 0.645 | Train PPL:   1.907\n",
            "\t Val. Loss: 4.404 |  Val. PPL:  81.779\n",
            "Epoch: 09 | Time: 0m 32s\n",
            "\tTrain Loss: 0.617 | Train PPL:   1.853\n",
            "\t Val. Loss: 4.371 |  Val. PPL:  79.144\n",
            "Epoch: 10 | Time: 0m 32s\n",
            "\tTrain Loss: 0.584 | Train PPL:   1.793\n",
            "\t Val. Loss: 4.325 |  Val. PPL:  75.532\n",
            "Epoch: 11 | Time: 0m 32s\n",
            "\tTrain Loss: 0.573 | Train PPL:   1.773\n",
            "\t Val. Loss: 4.309 |  Val. PPL:  74.362\n",
            "Epoch: 12 | Time: 0m 32s\n",
            "\tTrain Loss: 0.547 | Train PPL:   1.728\n",
            "\t Val. Loss: 4.292 |  Val. PPL:  73.114\n",
            "Epoch: 13 | Time: 0m 32s\n",
            "\tTrain Loss: 0.548 | Train PPL:   1.731\n",
            "\t Val. Loss: 4.261 |  Val. PPL:  70.846\n",
            "Epoch: 14 | Time: 0m 32s\n",
            "\tTrain Loss: 0.532 | Train PPL:   1.703\n",
            "\t Val. Loss: 4.238 |  Val. PPL:  69.244\n",
            "Epoch: 15 | Time: 0m 32s\n",
            "\tTrain Loss: 0.522 | Train PPL:   1.685\n",
            "\t Val. Loss: 4.230 |  Val. PPL:  68.724\n",
            "Epoch: 16 | Time: 0m 32s\n",
            "\tTrain Loss: 0.507 | Train PPL:   1.660\n",
            "\t Val. Loss: 4.215 |  Val. PPL:  67.687\n",
            "Epoch: 17 | Time: 0m 32s\n",
            "\tTrain Loss: 0.492 | Train PPL:   1.636\n",
            "\t Val. Loss: 4.199 |  Val. PPL:  66.607\n",
            "Epoch: 18 | Time: 0m 32s\n",
            "\tTrain Loss: 0.479 | Train PPL:   1.615\n",
            "\t Val. Loss: 4.159 |  Val. PPL:  63.981\n",
            "Epoch: 19 | Time: 0m 32s\n",
            "\tTrain Loss: 0.464 | Train PPL:   1.590\n",
            "\t Val. Loss: 4.141 |  Val. PPL:  62.892\n",
            "Epoch: 20 | Time: 0m 32s\n",
            "\tTrain Loss: 0.460 | Train PPL:   1.585\n",
            "\t Val. Loss: 4.141 |  Val. PPL:  62.835\n",
            "Epoch: 21 | Time: 0m 32s\n",
            "\tTrain Loss: 0.459 | Train PPL:   1.583\n",
            "\t Val. Loss: 4.102 |  Val. PPL:  60.455\n",
            "Epoch: 22 | Time: 0m 32s\n",
            "\tTrain Loss: 0.436 | Train PPL:   1.546\n",
            "\t Val. Loss: 4.076 |  Val. PPL:  58.906\n",
            "Epoch: 23 | Time: 0m 32s\n",
            "\tTrain Loss: 0.449 | Train PPL:   1.567\n",
            "\t Val. Loss: 4.091 |  Val. PPL:  59.802\n",
            "Epoch: 24 | Time: 0m 32s\n",
            "\tTrain Loss: 0.436 | Train PPL:   1.547\n",
            "\t Val. Loss: 4.090 |  Val. PPL:  59.720\n",
            "Epoch: 25 | Time: 0m 32s\n",
            "\tTrain Loss: 0.416 | Train PPL:   1.515\n",
            "\t Val. Loss: 4.082 |  Val. PPL:  59.289\n",
            "Epoch: 26 | Time: 0m 32s\n",
            "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
            "\t Val. Loss: 4.069 |  Val. PPL:  58.503\n",
            "Epoch: 27 | Time: 0m 32s\n",
            "\tTrain Loss: 0.406 | Train PPL:   1.501\n",
            "\t Val. Loss: 4.059 |  Val. PPL:  57.919\n",
            "Epoch: 28 | Time: 0m 32s\n",
            "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
            "\t Val. Loss: 4.065 |  Val. PPL:  58.281\n",
            "Epoch: 29 | Time: 0m 32s\n",
            "\tTrain Loss: 0.401 | Train PPL:   1.494\n",
            "\t Val. Loss: 4.024 |  Val. PPL:  55.908\n",
            "Epoch: 30 | Time: 0m 32s\n",
            "\tTrain Loss: 0.390 | Train PPL:   1.476\n",
            "\t Val. Loss: 4.024 |  Val. PPL:  55.911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY5B4McO9jQX"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHPn31mBpo--",
        "outputId": "a33cdfda-ae34-4da8-d13e-6d111385ce43"
      },
      "source": [
        "best_valid_loss, optimizer"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.023710258463596, Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     eps: 1e-08\n",
              "     lr: 1e-05\n",
              "     weight_decay: 0.0001\n",
              " ))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xak8dJXRBnqK",
        "outputId": "8164ec05-5de8-46e6-a067-e925c136be85"
      },
      "source": [
        "#optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.01)\r\n",
        "N_EPOCHS = 30\r\n",
        "CLIP = 1\r\n",
        "model.load_state_dict(torch.load('tut3-model.pt'))\r\n",
        "best_valid_loss = 4.023710258463596\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    # valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 32s\n",
            "\tTrain Loss: 0.406 | Train PPL:   1.501\n",
            "\t Val. Loss: 3.998 |  Val. PPL:  54.497\n",
            "Epoch: 02 | Time: 0m 32s\n",
            "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
            "\t Val. Loss: 3.994 |  Val. PPL:  54.276\n",
            "Epoch: 03 | Time: 0m 32s\n",
            "\tTrain Loss: 0.377 | Train PPL:   1.459\n",
            "\t Val. Loss: 3.982 |  Val. PPL:  53.641\n",
            "Epoch: 04 | Time: 0m 32s\n",
            "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
            "\t Val. Loss: 3.959 |  Val. PPL:  52.389\n",
            "Epoch: 05 | Time: 0m 32s\n",
            "\tTrain Loss: 0.390 | Train PPL:   1.476\n",
            "\t Val. Loss: 3.957 |  Val. PPL:  52.317\n",
            "Epoch: 06 | Time: 0m 32s\n",
            "\tTrain Loss: 0.380 | Train PPL:   1.462\n",
            "\t Val. Loss: 3.940 |  Val. PPL:  51.425\n",
            "Epoch: 07 | Time: 0m 32s\n",
            "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
            "\t Val. Loss: 3.917 |  Val. PPL:  50.255\n",
            "Epoch: 08 | Time: 0m 32s\n",
            "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
            "\t Val. Loss: 3.924 |  Val. PPL:  50.610\n",
            "Epoch: 09 | Time: 0m 32s\n",
            "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
            "\t Val. Loss: 3.899 |  Val. PPL:  49.376\n",
            "Epoch: 10 | Time: 0m 32s\n",
            "\tTrain Loss: 0.364 | Train PPL:   1.440\n",
            "\t Val. Loss: 3.883 |  Val. PPL:  48.558\n",
            "Epoch: 11 | Time: 0m 32s\n",
            "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
            "\t Val. Loss: 3.909 |  Val. PPL:  49.874\n",
            "Epoch: 12 | Time: 0m 32s\n",
            "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
            "\t Val. Loss: 3.892 |  Val. PPL:  49.030\n",
            "Epoch: 13 | Time: 0m 32s\n",
            "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
            "\t Val. Loss: 3.854 |  Val. PPL:  47.175\n",
            "Epoch: 14 | Time: 0m 32s\n",
            "\tTrain Loss: 0.356 | Train PPL:   1.427\n",
            "\t Val. Loss: 3.844 |  Val. PPL:  46.709\n",
            "Epoch: 15 | Time: 0m 32s\n",
            "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
            "\t Val. Loss: 3.830 |  Val. PPL:  46.053\n",
            "Epoch: 16 | Time: 0m 32s\n",
            "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
            "\t Val. Loss: 3.813 |  Val. PPL:  45.309\n",
            "Epoch: 17 | Time: 0m 32s\n",
            "\tTrain Loss: 0.354 | Train PPL:   1.424\n",
            "\t Val. Loss: 3.793 |  Val. PPL:  44.368\n",
            "Epoch: 18 | Time: 0m 32s\n",
            "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
            "\t Val. Loss: 3.835 |  Val. PPL:  46.284\n",
            "Epoch: 19 | Time: 0m 32s\n",
            "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
            "\t Val. Loss: 3.816 |  Val. PPL:  45.418\n",
            "Epoch: 20 | Time: 0m 32s\n",
            "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
            "\t Val. Loss: 3.806 |  Val. PPL:  44.948\n",
            "Epoch: 21 | Time: 0m 32s\n",
            "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
            "\t Val. Loss: 3.808 |  Val. PPL:  45.069\n",
            "Epoch: 22 | Time: 0m 32s\n",
            "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
            "\t Val. Loss: 3.800 |  Val. PPL:  44.694\n",
            "Epoch: 23 | Time: 0m 32s\n",
            "\tTrain Loss: 0.337 | Train PPL:   1.400\n",
            "\t Val. Loss: 3.807 |  Val. PPL:  44.999\n",
            "Epoch: 24 | Time: 0m 32s\n",
            "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
            "\t Val. Loss: 3.799 |  Val. PPL:  44.663\n",
            "Epoch: 25 | Time: 0m 32s\n",
            "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
            "\t Val. Loss: 3.804 |  Val. PPL:  44.889\n",
            "Epoch: 26 | Time: 0m 32s\n",
            "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
            "\t Val. Loss: 3.793 |  Val. PPL:  44.400\n",
            "Epoch: 27 | Time: 0m 32s\n",
            "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
            "\t Val. Loss: 3.803 |  Val. PPL:  44.822\n",
            "Epoch: 28 | Time: 0m 32s\n",
            "\tTrain Loss: 0.340 | Train PPL:   1.404\n",
            "\t Val. Loss: 3.805 |  Val. PPL:  44.905\n",
            "Epoch: 29 | Time: 0m 32s\n",
            "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
            "\t Val. Loss: 3.807 |  Val. PPL:  45.010\n",
            "Epoch: 30 | Time: 0m 32s\n",
            "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
            "\t Val. Loss: 3.761 |  Val. PPL:  42.985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5_tkdtzCGRT",
        "outputId": "d7a88b3e-a615-448f-f212-66f51ca7ac75"
      },
      "source": [
        "best_valid_loss, optimizer"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.760860301078634, Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     eps: 1e-08\n",
              "     lr: 1e-05\n",
              "     weight_decay: 0.0001\n",
              " ))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j8GENr8HLQJ"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.001)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYRR8HFBDDQj",
        "outputId": "683dbdce-39a2-4a77-885c-8b94666471cd"
      },
      "source": [
        "N_EPOCHS = 50\r\n",
        "CLIP = 1\r\n",
        "model.load_state_dict(torch.load('tut3-model.pt'))\r\n",
        "best_valid_loss = 3.760860301078634\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    # valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 32s\n",
            "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
            "\t Val. Loss: 3.761 |  Val. PPL:  42.982\n",
            "Epoch: 02 | Time: 0m 32s\n",
            "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
            "\t Val. Loss: 3.742 |  Val. PPL:  42.177\n",
            "Epoch: 03 | Time: 0m 32s\n",
            "\tTrain Loss: 0.368 | Train PPL:   1.444\n",
            "\t Val. Loss: 3.751 |  Val. PPL:  42.558\n",
            "Epoch: 04 | Time: 0m 32s\n",
            "\tTrain Loss: 0.397 | Train PPL:   1.487\n",
            "\t Val. Loss: 3.703 |  Val. PPL:  40.586\n",
            "Epoch: 05 | Time: 0m 32s\n",
            "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
            "\t Val. Loss: 3.705 |  Val. PPL:  40.653\n",
            "Epoch: 06 | Time: 0m 32s\n",
            "\tTrain Loss: 0.438 | Train PPL:   1.550\n",
            "\t Val. Loss: 3.686 |  Val. PPL:  39.873\n",
            "Epoch: 07 | Time: 0m 32s\n",
            "\tTrain Loss: 0.449 | Train PPL:   1.566\n",
            "\t Val. Loss: 3.658 |  Val. PPL:  38.781\n",
            "Epoch: 08 | Time: 0m 32s\n",
            "\tTrain Loss: 0.471 | Train PPL:   1.601\n",
            "\t Val. Loss: 3.673 |  Val. PPL:  39.355\n",
            "Epoch: 09 | Time: 0m 32s\n",
            "\tTrain Loss: 0.498 | Train PPL:   1.645\n",
            "\t Val. Loss: 3.665 |  Val. PPL:  39.060\n",
            "Epoch: 10 | Time: 0m 32s\n",
            "\tTrain Loss: 0.510 | Train PPL:   1.665\n",
            "\t Val. Loss: 3.676 |  Val. PPL:  39.476\n",
            "Epoch: 11 | Time: 0m 32s\n",
            "\tTrain Loss: 0.531 | Train PPL:   1.701\n",
            "\t Val. Loss: 3.657 |  Val. PPL:  38.734\n",
            "Epoch: 12 | Time: 0m 32s\n",
            "\tTrain Loss: 0.557 | Train PPL:   1.745\n",
            "\t Val. Loss: 3.659 |  Val. PPL:  38.824\n",
            "Epoch: 13 | Time: 0m 32s\n",
            "\tTrain Loss: 0.564 | Train PPL:   1.757\n",
            "\t Val. Loss: 3.673 |  Val. PPL:  39.378\n",
            "Epoch: 14 | Time: 0m 32s\n",
            "\tTrain Loss: 0.581 | Train PPL:   1.788\n",
            "\t Val. Loss: 3.664 |  Val. PPL:  39.031\n",
            "Epoch: 15 | Time: 0m 32s\n",
            "\tTrain Loss: 0.601 | Train PPL:   1.824\n",
            "\t Val. Loss: 3.686 |  Val. PPL:  39.878\n",
            "Epoch: 16 | Time: 0m 32s\n",
            "\tTrain Loss: 0.624 | Train PPL:   1.867\n",
            "\t Val. Loss: 3.712 |  Val. PPL:  40.927\n",
            "Epoch: 17 | Time: 0m 32s\n",
            "\tTrain Loss: 0.649 | Train PPL:   1.913\n",
            "\t Val. Loss: 3.709 |  Val. PPL:  40.795\n",
            "Epoch: 18 | Time: 0m 32s\n",
            "\tTrain Loss: 0.655 | Train PPL:   1.926\n",
            "\t Val. Loss: 3.725 |  Val. PPL:  41.467\n",
            "Epoch: 19 | Time: 0m 32s\n",
            "\tTrain Loss: 0.683 | Train PPL:   1.979\n",
            "\t Val. Loss: 3.726 |  Val. PPL:  41.522\n",
            "Epoch: 20 | Time: 0m 32s\n",
            "\tTrain Loss: 0.693 | Train PPL:   2.000\n",
            "\t Val. Loss: 3.762 |  Val. PPL:  43.028\n",
            "Epoch: 21 | Time: 0m 32s\n",
            "\tTrain Loss: 0.721 | Train PPL:   2.056\n",
            "\t Val. Loss: 3.770 |  Val. PPL:  43.399\n",
            "Epoch: 22 | Time: 0m 32s\n",
            "\tTrain Loss: 0.727 | Train PPL:   2.069\n",
            "\t Val. Loss: 3.758 |  Val. PPL:  42.861\n",
            "Epoch: 23 | Time: 0m 32s\n",
            "\tTrain Loss: 0.742 | Train PPL:   2.099\n",
            "\t Val. Loss: 3.787 |  Val. PPL:  44.138\n",
            "Epoch: 24 | Time: 0m 32s\n",
            "\tTrain Loss: 0.762 | Train PPL:   2.143\n",
            "\t Val. Loss: 3.804 |  Val. PPL:  44.894\n",
            "Epoch: 25 | Time: 0m 32s\n",
            "\tTrain Loss: 0.784 | Train PPL:   2.191\n",
            "\t Val. Loss: 3.814 |  Val. PPL:  45.329\n",
            "Epoch: 26 | Time: 0m 32s\n",
            "\tTrain Loss: 0.791 | Train PPL:   2.206\n",
            "\t Val. Loss: 3.834 |  Val. PPL:  46.232\n",
            "Epoch: 27 | Time: 0m 32s\n",
            "\tTrain Loss: 0.800 | Train PPL:   2.226\n",
            "\t Val. Loss: 3.848 |  Val. PPL:  46.920\n",
            "Epoch: 28 | Time: 0m 32s\n",
            "\tTrain Loss: 0.829 | Train PPL:   2.292\n",
            "\t Val. Loss: 3.838 |  Val. PPL:  46.410\n",
            "Epoch: 29 | Time: 0m 32s\n",
            "\tTrain Loss: 0.831 | Train PPL:   2.296\n",
            "\t Val. Loss: 3.844 |  Val. PPL:  46.732\n",
            "Epoch: 30 | Time: 0m 32s\n",
            "\tTrain Loss: 0.870 | Train PPL:   2.387\n",
            "\t Val. Loss: 3.860 |  Val. PPL:  47.447\n",
            "Epoch: 31 | Time: 0m 32s\n",
            "\tTrain Loss: 0.871 | Train PPL:   2.390\n",
            "\t Val. Loss: 3.853 |  Val. PPL:  47.136\n",
            "Epoch: 32 | Time: 0m 32s\n",
            "\tTrain Loss: 0.890 | Train PPL:   2.436\n",
            "\t Val. Loss: 3.878 |  Val. PPL:  48.309\n",
            "Epoch: 33 | Time: 0m 32s\n",
            "\tTrain Loss: 0.896 | Train PPL:   2.450\n",
            "\t Val. Loss: 3.898 |  Val. PPL:  49.298\n",
            "Epoch: 34 | Time: 0m 32s\n",
            "\tTrain Loss: 0.927 | Train PPL:   2.527\n",
            "\t Val. Loss: 3.909 |  Val. PPL:  49.862\n",
            "Epoch: 35 | Time: 0m 32s\n",
            "\tTrain Loss: 0.928 | Train PPL:   2.530\n",
            "\t Val. Loss: 3.904 |  Val. PPL:  49.622\n",
            "Epoch: 36 | Time: 0m 32s\n",
            "\tTrain Loss: 0.936 | Train PPL:   2.549\n",
            "\t Val. Loss: 3.944 |  Val. PPL:  51.603\n",
            "Epoch: 37 | Time: 0m 32s\n",
            "\tTrain Loss: 0.962 | Train PPL:   2.617\n",
            "\t Val. Loss: 3.912 |  Val. PPL:  49.985\n",
            "Epoch: 38 | Time: 0m 32s\n",
            "\tTrain Loss: 0.976 | Train PPL:   2.653\n",
            "\t Val. Loss: 3.960 |  Val. PPL:  52.453\n",
            "Epoch: 39 | Time: 0m 32s\n",
            "\tTrain Loss: 0.983 | Train PPL:   2.672\n",
            "\t Val. Loss: 3.950 |  Val. PPL:  51.931\n",
            "Epoch: 40 | Time: 0m 32s\n",
            "\tTrain Loss: 1.007 | Train PPL:   2.739\n",
            "\t Val. Loss: 3.974 |  Val. PPL:  53.198\n",
            "Epoch: 41 | Time: 0m 32s\n",
            "\tTrain Loss: 1.025 | Train PPL:   2.787\n",
            "\t Val. Loss: 3.996 |  Val. PPL:  54.376\n",
            "Epoch: 42 | Time: 0m 32s\n",
            "\tTrain Loss: 1.033 | Train PPL:   2.810\n",
            "\t Val. Loss: 4.002 |  Val. PPL:  54.696\n",
            "Epoch: 43 | Time: 0m 32s\n",
            "\tTrain Loss: 1.050 | Train PPL:   2.859\n",
            "\t Val. Loss: 4.030 |  Val. PPL:  56.266\n",
            "Epoch: 44 | Time: 0m 32s\n",
            "\tTrain Loss: 1.059 | Train PPL:   2.882\n",
            "\t Val. Loss: 4.057 |  Val. PPL:  57.778\n",
            "Epoch: 45 | Time: 0m 32s\n",
            "\tTrain Loss: 1.068 | Train PPL:   2.909\n",
            "\t Val. Loss: 4.043 |  Val. PPL:  57.020\n",
            "Epoch: 46 | Time: 0m 32s\n",
            "\tTrain Loss: 1.071 | Train PPL:   2.917\n",
            "\t Val. Loss: 4.041 |  Val. PPL:  56.883\n",
            "Epoch: 47 | Time: 0m 32s\n",
            "\tTrain Loss: 1.105 | Train PPL:   3.020\n",
            "\t Val. Loss: 4.048 |  Val. PPL:  57.292\n",
            "Epoch: 48 | Time: 0m 32s\n",
            "\tTrain Loss: 1.122 | Train PPL:   3.070\n",
            "\t Val. Loss: 4.034 |  Val. PPL:  56.487\n",
            "Epoch: 49 | Time: 0m 32s\n",
            "\tTrain Loss: 1.139 | Train PPL:   3.122\n",
            "\t Val. Loss: 4.053 |  Val. PPL:  57.596\n",
            "Epoch: 50 | Time: 0m 32s\n",
            "\tTrain Loss: 1.141 | Train PPL:   3.130\n",
            "\t Val. Loss: 4.092 |  Val. PPL:  59.840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgovsaLSwYeI",
        "outputId": "36bb53d7-872f-4455-a659-de2a30ab65de"
      },
      "source": [
        "optimizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    eps: 1e-08\n",
              "    lr: 1e-05\n",
              "    weight_decay: 0.01\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1lhFNSAD-oj",
        "outputId": "453c211c-7054-4ce4-d58d-2ab84f868496"
      },
      "source": [
        "best_valid_loss"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.656712980980569"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJm2KZKW9ejI"
      },
      "source": [
        "Finally, we test the model on the test set using these \"best\" parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZlTvUHI9ejI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e087b9ea-dce0-4675-b51c-fdc7cd69fd5f"
      },
      "source": [
        "model.load_state_dict(torch.load('tut3-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.657 | Test PPL:  38.734 |\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
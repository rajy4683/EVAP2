{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "ENDS9_TF_OpenBook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yUQMjegvDkpX"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EVAP2/blob/master/ENDS9_TF_OpenBook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzxOJwN7EzFO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1YC7Z0guYGX",
        "outputId": "8d6abfa6-bfc3-4128-d935-3e316b225bb3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jan  7 14:55:03 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUnMLdevEzFT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og-MDQc4hKL7",
        "outputId": "61cca084-131b-48cd-fd93-9dabddb8b7fb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzErHAsEzFU"
      },
      "source": [
        "Then set a random seed for deterministic results/reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_wP4J4LEzFX"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyaSxNdzxPE8",
        "outputId": "df6acc24-bbd0-45ef-ec9b-0be389e6d2b0"
      },
      "source": [
        "!pip install jsonlines\r\n",
        "import jsonlines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.15.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NciIxvgjzHwM"
      },
      "source": [
        "sentence = []\r\n",
        "label = []\r\n",
        "with open('/content/OpenBookQa/train.jsonl') as h:\r\n",
        "    for line in h:\r\n",
        "        example = json.loads(line)\r\n",
        "        print(example.keys())\r\n",
        "        # scores = []\r\n",
        "        merged_choices = ' A: '.join([choice['text'] for choice in example['question']['choices']])\r\n",
        "        input = 'Q: ' + example['question']['stem'] + ' A: ' + merged_choices\r\n",
        "        correct_answer = [ choice['text'] for choice in example['question']['choices'] if choice['label'] == example['answerKey'] ][0]\r\n",
        "        sentence.append(input)\r\n",
        "        label.append(correct_answer)\r\n",
        "        #print(input, correct_answer)\r\n",
        "dataset_df = pd.DataFrame({'sentence':sentence, 'label':label})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "DphX8AeVXmCz",
        "outputId": "a76c37f9-5cab-40c8-b7f9-07f67548c023"
      },
      "source": [
        "dataset_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q: The sun is responsible for A: puppies learn...</td>\n",
              "      <td>plants sprouting, blooming and wilting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q: When standing miles away from Mount Rushmor...</td>\n",
              "      <td>the mountains seem smaller than in photographs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q: When food is reduced in the stomach A: the ...</td>\n",
              "      <td>nutrients are being deconstructed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q: Stars are A: warm lights that float A: made...</td>\n",
              "      <td>great balls of gas burning billions of miles away</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q: You can make a telescope with a A: straw A:...</td>\n",
              "      <td>mailing tube</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3409</th>\n",
              "      <td>Q: Bears instinctively know when its time to h...</td>\n",
              "      <td>Their parents</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3410</th>\n",
              "      <td>Q: An example of a fossil is a paw print in wh...</td>\n",
              "      <td>hard stones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3411</th>\n",
              "      <td>Q: Which is likeliest to make light pass throu...</td>\n",
              "      <td>any kind of tangible object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3412</th>\n",
              "      <td>Q: What most likely caused the castle built by...</td>\n",
              "      <td>very strong eddies of fast moving air carried ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3413</th>\n",
              "      <td>Q: Splitting and fusing billions of atoms at t...</td>\n",
              "      <td>illumination</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3414 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence                                              label\n",
              "0     Q: The sun is responsible for A: puppies learn...             plants sprouting, blooming and wilting\n",
              "1     Q: When standing miles away from Mount Rushmor...     the mountains seem smaller than in photographs\n",
              "2     Q: When food is reduced in the stomach A: the ...                  nutrients are being deconstructed\n",
              "3     Q: Stars are A: warm lights that float A: made...  great balls of gas burning billions of miles away\n",
              "4     Q: You can make a telescope with a A: straw A:...                                       mailing tube\n",
              "...                                                 ...                                                ...\n",
              "3409  Q: Bears instinctively know when its time to h...                                      Their parents\n",
              "3410  Q: An example of a fossil is a paw print in wh...                                        hard stones\n",
              "3411  Q: Which is likeliest to make light pass throu...                        any kind of tangible object\n",
              "3412  Q: What most likely caused the castle built by...  very strong eddies of fast moving air carried ...\n",
              "3413  Q: Splitting and fusing billions of atoms at t...                                       illumination\n",
              "\n",
              "[3414 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hawianoFEzFY"
      },
      "source": [
        "Instantiate our German and English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLSPj13L2yAr",
        "outputId": "6f453734-214a-4d7c-83bd-a7d772227e0a"
      },
      "source": [
        "!python3 -m spacy download de_core_news_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907057 sha256=bef19d049a4a328fb8aa383e8d700b51dab3d254d44552ab104bb67962df8744\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-29sy714y/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVVbBjSb3MO1"
      },
      "source": [
        "import de_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BP3YSvJEzFY"
      },
      "source": [
        "spacy_de =  de_core_news_sm.load()\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bEkyPt5EzFY"
      },
      "source": [
        "Previously we reversed the source (German) sentence, however in the paper we are implementing they don't do this, so neither will we."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KaGEZ45EzFZ"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7rbQLYHEzFZ"
      },
      "source": [
        "Create our fields to process our data. This will append the \"start of sentence\" and \"end of sentence\" tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sytzu5pTnSCa"
      },
      "source": [
        "from torchtext import data \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "Sentence = data.Field(sequential = True, tokenize = 'spacy', batch_first =False, include_lengths=False, lower=False)\r\n",
        "Label = data.Field(sequential =True, tokenize ='spacy', is_target=False, batch_first =False, include_lengths=False, lower=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDuSMFrQwrXC"
      },
      "source": [
        "dataset = dataset_df\r\n",
        "fields = [('sentence', Sentence),('label',Label)]\r\n",
        "example = [data.Example.fromlist([dataset.sentence[i],dataset.label[i]], fields) for i in range(dataset.shape[0])] \r\n",
        "commonqa_ds = data.Dataset(example, fields)\r\n",
        "(train, valid) = commonqa_ds.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97K5lZwYx0BB"
      },
      "source": [
        "Sentence.build_vocab(train)\r\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZlXYwYyyz46",
        "outputId": "aa57e64b-73fd-449d-afa5-d8782b932507"
      },
      "source": [
        "print('Size of input vocab : ', len(Sentence.vocab))\r\n",
        "print('Size of label vocab : ', len(Label.vocab))\r\n",
        "print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\r\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  9462\n",
            "Size of label vocab :  3610\n",
            "Top 10 words appreared repeatedly : [(')', 55314), ('(', 55313), ('A', 7331), ('H', 6944), ('C', 6923), ('B', 6921), ('D', 6916), ('\\\\n', 6914), ('E', 6914), ('F', 6914)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7f6c2fa93f28>, {'<unk>': 0, '<pad>': 1, 'the': 2, 'of': 3, 'a': 4, 'and': 5, 'energy': 6, 'water': 7, '.': 8, 'to': 9, 'heat': 10, 'in': 11, 'plants': 12, 'it': 13, 'food': 14, 'animals': 15, 'cells': 16, 'light': 17, 'humans': 18, 'soil': 19, 'air': 20, 'body': 21, 'The': 22, 'A': 23, 'bacteria': 24, \"'s\": 25, 'Something': 26, 'chemical': 27, 'them': 28, ',': 29, 'an': 30, 'rain': 31, 'wind': 32, '-': 33, 'oxygen': 34, 'It': 35, 'insects': 36, 'on': 37, 'sunlight': 38, 'blood': 39, 'life': 40, 'their': 41, 'system': 42, 'organisms': 43, 'reproduction': 44, 'sun': 45, 'electricity': 46, 'is': 47, 'temperature': 48, 'acid': 49, 'viruses': 50, 'skin': 51, 'trees': 52, 'cancer': 53, 'from': 54, 'pressure': 55, 'carbon': 56, 'cold': 57, 'that': 58, 'warm': 59, 'waves': 60, 'weathering': 61, 'death': 62, 'electrons': 63, 'or': 64, 'Earth': 65, 'disease': 66, 'gas': 67, 'growth': 68, 'mitosis': 69, 'by': 70, 'sound': 71, 'DNA': 72, 'cell': 73, 'environment': 74, 'flow': 75, 'liquid': 76, 'weather': 77, 'They': 78, 'fire': 79, 'hormones': 80, 'living': 81, 'move': 82, 'birds': 83, 'changes': 84, 'eggs': 85, 'gametes': 86, 'molecules': 87, 'negative': 88, 'pollution': 89, 'dioxide': 90, 'fish': 91, 'hot': 92, 'kinetic': 93, 'mechanical': 94, 'precipitation': 95, 'protein': 96, 'proteins': 97, 'rocks': 98, 'sperm': 99, 'sugar': 100, 'survive': 101, 'they': 102, 'bees': 103, 'bushes': 104, 'can': 105, 'change': 106, 'fuel': 107, 'green': 108, 'into': 109, 'plant': 110, 'salt': 111, 'erosion': 112, 'flowers': 113, 'fuels': 114, 'genes': 115, 'grow': 116, 'h2o': 117, 'hair': 118, 'leaves': 119, 'mass': 120, 'with': 121, 'Water': 122, 'animal': 123, 'chemicals': 124, 'down': 125, 'dry': 126, 'for': 127, 'force': 128, 'frogs': 129, 'fungi': 130, 'genetic': 131, 'heart': 132, 'mammals': 133, 'natural': 134, 'photons': 135, 'rock': 136, 'starfish': 137, 'things': 138, 'In': 139, 'To': 140, 'backbone': 141, 'bamboo': 142, 'cooking': 143, 'dead': 144, 'deoxyribonucleic': 145, 'dogs': 146, 'electrical': 147, 'evaporation': 148, 'evolution': 149, 'fossil': 150, 'fur': 151, 'ice': 152, 'matter': 153, 'more': 154, 'nutrients': 155, 'organs': 156, 'some': 157, 'survival': 158, 'warming': 159, 'Plants': 160, 'are': 161, 'burning': 162, 'division': 163, 'fat': 164, 'female': 165, 'influenza': 166, 'its': 167, 'material': 168, 'motion': 169, 'moving': 170, 'population': 171, 'rays': 172, 'sex': 173, 'species': 174, 'sponges': 175, 'spores': 176, 'streams': 177, 'up': 178, 'Heat': 179, 'Light': 180, 'all': 181, 'at': 182, 'atmosphere': 183, 'black': 184, 'colors': 185, 'decreases': 186, 'dirt': 187, 'dormant': 188, 'earth': 189, 'electron': 190, 'exercise': 191, 'ferns': 192, 'fertilization': 193, 'global': 194, 'hydrogen': 195, 'nitrogen': 196, 'sea': 197, 'small': 198, 'through': 199, 'Food': 200, 'HIV': 201, 'adult': 202, 'be': 203, 'bonds': 204, 'cars': 205, 'cats': 206, 'clouds': 207, 'cycles': 208, 'earthquakes': 209, 'egg': 210, 'habitats': 211, 'harm': 212, 'have': 213, 'heating': 214, 'impact': 215, 'increase': 216, 'live': 217, 'most': 218, 'not': 219, 'object': 220, 'objects': 221, 'ocean': 222, 'oil': 223, 'out': 224, 'paper': 225, 'people': 226, 'positive': 227, 'rivers': 228, 'size': 229, 'temperatures': 230, 'time': 231, 'too': 232, 'winter': 233, 'RNA': 234, 'Sun': 235, 'acids': 236, 'antibodies': 237, 'antigens': 238, 'away': 239, 'axis': 240, 'bulbs': 241, 'cause': 242, 'cigarettes': 243, 'circuit': 244, 'coal': 245, 'cows': 246, 'damage': 247, 'decrease': 248, 'expands': 249, 'fats': 250, 'get': 251, 'glass': 252, 'ground': 253, 'high': 254, 'human': 255, 'invertebrates': 256, 'limestone': 257, 'long': 258, 'made': 259, 'magnetism': 260, 'male': 261, 'microorganisms': 262, 'microscope': 263, 'movement': 264, 'mutation': 265, 'nucleus': 266, 'oceans': 267, 'organism': 268, 'play': 269, 'produce': 270, 'roots': 271, 'seeds': 272, 'solid': 273, 'sweat': 274, 'together': 275, 'vapor': 276, 'warmth': 277, 'Bacteria': 278, 'Carbon': 279, 'bears': 280, 'breathing': 281, 'chlorophyll': 282, 'color': 283, 'decomposition': 284, 'die': 285, 'fatty': 286, 'fresh': 287, 'friction': 288, 'harmful': 289, 'heavy': 290, 'hurricanes': 291, 'immune': 292, 'level': 293, 'measure': 294, 'minerals': 295, 'mutations': 296, 'nervous': 297, 'ova': 298, 'pathogens': 299, 'power': 300, 'production': 301, 'radiation': 302, 'reaction': 303, 'remains': 304, 'reproduce': 305, 'river': 306, 'sense': 307, 'sexual': 308, 'shelter': 309, 'smell': 310, 'smoking': 311, 'space': 312, 'squids': 313, 'store': 314, 'structure': 315, 'structures': 316, 'substances': 317, 'tilt': 318, 'vibrations': 319, 'waste': 320, 'An': 321, 'Birds': 322, 'Energy': 323, 'Fungi': 324, 'H2O': 325, 'Helps': 326, 'Mechanical': 327, 'Oxygen': 328, 'Two': 329, 'Wind': 330, 'Winter': 331, 'aggression': 332, 'aging': 333, 'arteries': 334, 'batteries': 335, 'beams': 336, 'being': 337, 'bodily': 338, 'building': 339, 'car': 340, 'carbohydrates': 341, 'cellulose': 342, 'circulatory': 343, 'clams': 344, 'closed': 345, 'competition': 346, 'control': 347, 'copper': 348, 'coral': 349, 'current': 350, 'day': 351, 'different': 352, 'diversity': 353, 'during': 354, 'electric': 355, 'engines': 356, 'eukaryotes': 357, 'evergreens': 358, 'feet': 359, 'floods': 360, 'follicles': 361, 'fossils': 362, 'gene': 363, 'gets': 364, 'glucose': 365, 'habitat': 366, 'haploid': 367, 'homes': 368, 'immunity': 369, 'information': 370, 'liquids': 371, 'longer': 372, 'looseness': 373, 'lungs': 374, 'make': 375, 'metal': 376, 'neurons': 377, 'orchids': 378, 'organic': 379, 'other': 380, 'our': 381, 'planet': 382, 'plates': 383, 'preserved': 384, 'producers': 385, 'rotating': 386, 'sexually': 387, 'sleep': 388, 'slow': 389, 'smoke': 390, 'stove': 391, 'summer': 392, 'thunderstorms': 393, 'tobacco': 394, 'transfer': 395, 'travel': 396, 'underground': 397, 'weight': 398, 'will': 399, 'wood': 400, 'worms': 401, 'Air': 402, 'Cell': 403, 'Change': 404, 'Increase': 405, 'Sea': 406, 'Sediment': 407, 'Temperature': 408, 'Warm': 409, 'When': 410, 'absorb': 411, 'adaptation': 412, 'amount': 413, 'another': 414, 'ants': 415, 'around': 416, 'autumn': 417, 'balance': 418, 'bats': 419, 'bending': 420, 'bike': 421, 'boiling': 422, 'cacti': 423, 'celled': 424, 'children': 425, 'chromosomes': 426, 'climate': 427, 'complex': 428, 'compost': 429, 'daisies': 430, 'damaged': 431, 'darkness': 432, 'daylight': 433, 'depletion': 434, 'deposition': 435, 'deserts': 436, 'detritus': 437, 'deuterostomes': 438, 'differences': 439, 'digestive': 440, 'divide': 441, 'do': 442, 'eating': 443, 'ecosystems': 444, 'effect': 445, 'environments': 446, 'expand': 447, 'feathers': 448, 'find': 449, 'flooding': 450, 'form': 451, 'formed': 452, 'fronts': 453, 'gases': 454, 'goats': 455, 'health': 456, 'insulin': 457, 'iron': 458, 'keep': 459, 'less': 460, 'low': 461, 'lung': 462, 'marine': 463, 'messengers': 464, 'moist': 465, 'mold': 466, 'molds': 467, 'muscle': 468, 'mushrooms': 469, 'nematodes': 470, 'new': 471, 'night': 472, 'no': 473, 'nonliving': 474, 'oaks': 475, 'offspring': 476, 'over': 477, 'pH': 478, 'pain': 479, 'pancreas': 480, 'particles': 481, 'pesticides': 482, 'playing': 483, 'prokaryotes': 484, 'seasons': 485, 'sediment': 486, 'see': 487, 'seed': 488, 'shape': 489, 'shells': 490, 'shock': 491, 'sight': 492, 'single': 493, 'sodium': 494, 'solar': 495, 'spermatozoa': 496, 'star': 497, 'state': 498, 'stationary': 499, 'stem': 500, 'sweating': 501, 'systems': 502, 'tectonic': 503, 'thermal': 504, 'tilted': 505, 'tissue': 506, 'traits': 507, 'transport': 508, 'two': 509, 'used': 510, 'using': 511, 'varied': 512, 'vegetation': 513, 'vessels': 514, 'volume': 515, 'whales': 516, 'when': 517, \"'\": 518, '2': 519, 'Animals': 520, 'Carbohydrates': 521, 'Cells': 522, 'Coal': 523, 'Dehydration': 524, 'Evaporation': 525, 'Fossil': 526, 'Global': 527, 'Great': 528, 'Influenza': 529, 'Insects': 530, 'Jellyfish': 531, 'Joules': 532, 'Negative': 533, 'Nematoda': 534, 'Pacific': 535, 'Pollution': 536, 'Positive': 537, 'Power': 538, 'Rocks': 539, 'Roots': 540, 'Sand': 541, 'Sulfur': 542, 'Tobacco': 543, 'With': 544, 'absorbing': 545, 'activities': 546, 'adapt': 547, 'add': 548, 'algae': 549, 'alternation': 550, 'aquatic': 551, 'area': 552, 'atmospheric': 553, 'atoms': 554, 'become': 555, 'biodiversity': 556, 'blocks': 557, 'cartilage': 558, 'causes': 559, 'causing': 560, 'charge': 561, 'chlorine': 562, 'chloroplasts': 563, 'class': 564, 'coiled': 565, 'column': 566, 'complete': 567, 'compound': 568, 'consumers': 569, 'cooling': 570, 'cools': 571, 'courtship': 572, 'dark': 573, 'dating': 574, 'deadly': 575, 'decomposers': 576, 'density': 577, 'deposit': 578, 'destruction': 579, 'diploid': 580, 'dispersal': 581, 'drought': 582, 'drying': 583, 'earthworms': 584, 'ejaculation': 585, 'electromagnetic': 586, 'engine': 587, 'eras': 588, 'everywhere': 589, 'excretion': 590, 'expansion': 591, 'eyes': 592, 'fanning': 593, 'flows': 594, 'fuse': 595, 'gasoline': 596, 'generations': 597, 'gills': 598, 'gland': 599, 'go': 600, 'good': 601, 'gravity': 602, 'groundwater': 603, 'head': 604, 'hearing': 605, 'heated': 606, 'heats': 607, 'higher': 608, 'host': 609, 'illness': 610, 'increased': 611, 'increases': 612, 'injury': 613, 'inorganic': 614, 'jaws': 615, 'kill': 616, 'lack': 617, 'large': 618, 'layers': 619, 'laying': 620, 'leeches': 621, 'like': 622, 'loss': 623, 'lower': 624, 'magma': 625, 'making': 626, 'mate': 627, 'mating': 628, 'meat': 629, 'melting': 630, 'memory': 631, 'microscopic': 632, 'moisture': 633, 'multicellular': 634, 'muscles': 635, 'off': 636, 'olfaction': 637, 'organ': 638, 'outside': 639, 'ozone': 640, 'parasites': 641, 'permeable': 642, 'pheromones': 643, 'photosynthesis': 644, 'plankton': 645, 'planting': 646, 'plastic': 647, 'prism': 648, 'problems': 649, 'properly': 650, 'properties': 651, 'rainbow': 652, 'rainfall': 653, 'rains': 654, 'recycling': 655, 'release': 656, 'repair': 657, 'reptiles': 658, 'resources': 659, 'respiratory': 660, 'revolving': 661, 'rich': 662, 'ruler': 663, 'secondary': 664, 'seeping': 665, 'sequences': 666, 'sharks': 667, 'shell': 668, 'slate': 669, 'smaller': 670, 'smallest': 671, 'smallpox': 672, 'snail': 673, 'snails': 674, 'snow': 675, 'soils': 676, 'source': 677, 'speciation': 678, 'speed': 679, 'sports': 680, 'spring': 681, 'squirts': 682, 'stay': 683, 'stored': 684, 'storms': 685, 'stress': 686, 'support': 687, 'surfaces': 688, 'tape': 689, 'telomeres': 690, 'themselves': 691, 'tides': 692, 'transportation': 693, 'tropical': 694, 'turns': 695, 'urchins': 696, 'use': 697, 'vinegar': 698, 'vision': 699, 'within': 700, 'without': 701, 'Absorbing': 702, 'Animal': 703, 'Beavers': 704, 'Blood': 705, 'Break': 706, 'Burning': 707, 'By': 708, 'Chemical': 709, 'Condensation': 710, 'Decomposition': 711, 'Destroy': 712, 'Electric': 713, 'Electrical': 714, 'Fish': 715, 'Flowers': 716, 'Force': 717, 'Gas': 718, 'Gravity': 719, 'Grow': 720, 'H20': 721, 'Harm': 722, 'Heating': 723, 'Help': 724, 'Hot': 725, 'Human': 726, 'Lizards': 727, 'Male': 728, 'Mitosis': 729, 'Move': 730, 'Movement': 731, 'Mutations': 732, 'New': 733, 'Oil': 734, 'Physical': 735, 'Pituitary': 736, 'Plant': 737, 'Plastic': 738, 'Reproduce': 739, 'Salt': 740, 'San': 741, 'Skin': 742, 'Solar': 743, 'Starfish': 744, 'Summer': 745, 'Sunlight': 746, 'Through': 747, 'Thyroid': 748, 'Using': 749, 'Vapor': 750, 'Weathering': 751, 'Weight': 752, 'abalone': 753, 'ability': 754, 'absorbs': 755, 'aerobic': 756, 'age': 757, 'alleles': 758, 'amounts': 759, 'amphibians': 760, 'androgens': 761, 'annelids': 762, 'antibiotics': 763, 'apoptosis': 764, 'arthropods': 765, 'asexual': 766, 'automobile': 767, 'backbones': 768, 'barrel': 769, 'battery': 770, 'beavers': 771, 'becomes': 772, 'behave': 773, 'behavior': 774, 'beings': 775, 'better': 776, 'biomes': 777, 'blue': 778, 'bone': 779, 'bones': 780, 'bottleneck': 781, 'brain': 782, 'breaking': 783, 'breathe': 784, 'bright': 785, 'bubbles': 786, 'budding': 787, 'burned': 788, 'cactus': 789, 'calcium': 790, 'calories': 791, 'canyons': 792, 'carbonate': 793, 'carcinogens': 794, 'centralized': 795, 'chain': 796, 'changing': 797, 'chloride': 798, 'chromosome': 799, 'cilia': 800, 'clean': 801, 'climates': 802, 'colder': 803, 'compounds': 804, 'compressional': 805, 'condensation': 806, 'conditions': 807, 'conserve': 808, 'contact': 809, 'contractions': 810, 'converting': 811, 'cool': 812, 'cooled': 813, 'core': 814, 'corn': 815, 'crash': 816, 'crust': 817, 'cup': 818, 'currents': 819, 'cylinder': 820, 'cytoplasm': 821, 'deeper': 822, 'deforestation': 823, 'degrees': 824, 'dehydration': 825, 'dermis': 826, 'desert': 827, 'destroying': 828, 'devastating': 829, 'diabetes': 830, 'digestion': 831, 'dinosaurs': 832, 'diseases': 833, 'distance': 834, 'distracted': 835, 'dolphins': 836, 'doorbell': 837, 'drinking': 838, 'ducks': 839, 'ear': 840, 'ears': 841, 'elephants': 842, 'elms': 843, 'enzymes': 844, 'eroding': 845, 'excretory': 846, 'exposure': 847, 'extend': 848, 'extinction': 849, 'extreme': 850, 'fall': 851, 'family': 852, 'faulting': 853, 'females': 854, 'fingerprint': 855, 'flashlight': 856, 'fly': 857, 'forest': 858, 'found': 859, 'four': 860, 'free': 861, 'freezing': 862, 'frequency': 863, 'fruit': 864, 'function': 865, 'fusion': 866, 'gammaglobulins': 867, 'garbage': 868, 'geologic': 869, 'gold': 870, 'graduated': 871, 'grass': 872, 'gravitational': 873, 'greenhouse': 874, 'growing': 875, 'grows': 876, 'has': 877, 'hazard': 878, 'healing': 879, 'helps': 880, 'honeybees': 881, 'humidity': 882, 'hydration': 883, 'ill': 884, 'infection': 885, 'inside': 886, 'insulated': 887, 'insulation': 888, 'irradiation': 889, 'jellyfish': 890, 'keeps': 891, 'keratin': 892, 'kidneys': 893, 'lakes': 894, 'largest': 895, 'leave': 896, 'legumes': 897, 'lens': 898, 'leukemia': 899, 'lipids': 900, 'lizards': 901, 'location': 902, 'loose': 903, 'lose': 904, 'macroevolution': 905, 'makes': 906, 'measuring': 907, 'membranes': 908, 'mercury': 909, 'meristems': 910, 'metals': 911, 'microbes': 912, 'middle': 913, 'migration': 914, 'minute': 915, 'mirror': 916, 'monkeys': 917, 'monoxide': 918, 'moon': 919, 'moss': 920, 'mountain': 921, 'mountains': 922, 'mussels': 923, 'nerve': 924, 'nerves': 925, 'noise': 926, 'nuclear': 927, 'nucleic': 928, 'nutrient': 929, 'oak': 930, 'ovaries': 931, 'own': 932, 'oxidation': 933, 'oysters': 934, 'peat': 935, 'perspiration': 936, 'phospholipids': 937, 'photosynthetic': 938, 'phototropism': 939, 'pine': 940, 'platypus': 941, 'point': 942, 'position': 943, 'predators': 944, 'pregnancy': 945, 'prevent': 946, 'producing': 947, 'protection': 948, 'pugs': 949, 'quills': 950, 'rabies': 951, 'random': 952, 'reactions': 953, 'reduced': 954, 'reefs': 955, 'resistance': 956, 'respiration': 957, 'response': 958, 'rest': 959, 'rise': 960, 'rotation': 961, 'running': 962, 'salamanders': 963, 'salinity': 964, 'salmon': 965, 'sand': 966, 'sandstone': 967, 'saturated': 968, 'season': 969, 'segmented': 970, 'senses': 971, 'serious': 972, 'shorter': 973, 'sick': 974, 'side': 975, 'simple': 976, 'simplest': 977, 'snakes': 978, 'solute': 979, 'specialization': 980, 'squid': 981, 'stamina': 982, 'started': 983, 'steam': 984, 'stomach': 985, 'stops': 986, 'stream': 987, 'strength': 988, 'sulfur': 989, 'surface': 990, 'surgery': 991, 'swell': 992, 'tapeworms': 993, 'than': 994, 'thick': 995, 'thirst': 996, 'thrive': 997, 'thyroid': 998, 'tissues': 999, 'translation': 1000, 'tree': 1001, 'trout': 1002, 'trunk': 1003, 'tulips': 1004, 'turn': 1005, 'type': 1006, 'types': 1007, 'uncontrolled': 1008, 'under': 1009, 'urine': 1010, 'uterus': 1011, 'valley': 1012, 'vascular': 1013, 'vibrate': 1014, 'vibrating': 1015, 'vibration': 1016, 'visible': 1017, 'waters': 1018, 'wave': 1019, 'ways': 1020, 'wet': 1021, 'wheat': 1022, 'windward': 1023, 'wings': 1024, 'wiring': 1025, 'years': 1026, \"'re\": 1027, 'AIDS': 1028, 'Adult': 1029, 'Aging': 1030, 'Amphibians': 1031, 'Aves': 1032, 'Balance': 1033, 'Bees': 1034, 'Being': 1035, 'Breaks': 1036, 'Breathing': 1037, 'C': 1038, 'CO': 1039, 'CO2': 1040, 'Cancer': 1041, 'Carpooling': 1042, 'Changing': 1043, 'Chlorophyll': 1044, 'Clouds': 1045, 'Cold': 1046, 'Communication': 1047, 'Cooking': 1048, 'Copper': 1049, 'Coral': 1050, 'Create': 1051, 'Damage': 1052, 'Dead': 1053, 'Deforestation': 1054, 'Deoxyribonucleicacid': 1055, 'Division': 1056, 'Drought': 1057, 'Earthquakes': 1058, 'Eating': 1059, 'Electricity': 1060, 'Electricty': 1061, 'Electromagnetic': 1062, 'Embryo': 1063, 'Environment': 1064, 'Enzymes': 1065, 'Estrogen': 1066, 'Evolution': 1067, 'Fat': 1068, 'Fertilization': 1069, 'Fertilizer': 1070, 'Francisco': 1071, 'Freezing': 1072, 'Genetic': 1073, 'Granite': 1074, 'Green': 1075, 'Greenhouse': 1076, 'Having': 1077, 'Head': 1078, 'Heart': 1079, 'Heavy': 1080, 'Heterotrophs': 1081, 'Higher': 1082, 'Humans': 1083, 'Hydrogen': 1084, 'Hypothermia': 1085, 'If': 1086, 'Inches': 1087, 'Keeps': 1088, 'LEDs': 1089, 'Legionnaires': 1090, 'Life': 1091, 'Logging': 1092, 'Looking': 1093, 'Lungs': 1094, 'Makes': 1095, 'Males': 1096, 'Marble': 1097, 'Mass': 1098, 'Measuring': 1099, 'Moss': 1100, 'NaCl': 1101, 'Newton': 1102, 'Organelles': 1103, 'Organisms': 1104, 'Organs': 1105, 'People': 1106, 'Pesticides': 1107, 'Plankton': 1108, 'Potassium': 1109, 'Rain': 1110, 'Rainbows': 1111, 'Reduces': 1112, 'Reducing': 1113, 'Removing': 1114, 'Reproduction': 1115, 'Riding': 1116, 'River': 1117, 'Roundworms': 1118, 'Sensing': 1119, 'Sheds': 1120, 'Short': 1121, 'Sight': 1122, 'Sleep': 1123, 'Small': 1124, 'Soil': 1125, 'Some': 1126, 'Sweating': 1127, 'Tail': 1128, 'Testosterone': 1129, 'Their': 1130, 'Tiny': 1131, 'Trees': 1132, 'Tropical': 1133, 'UV': 1134, 'Unequal': 1135, 'Uterus': 1136, 'Vanes': 1137, 'Venus': 1138, 'Viruses': 1139, 'Vomiting': 1140, 'Yeast': 1141, 'absorption': 1142, 'acidity': 1143, 'across': 1144, 'activation': 1145, 'active': 1146, 'adaptations': 1147, 'adaptive': 1148, 'adding': 1149, 'adrenaline': 1150, 'adults': 1151, 'agent': 1152, 'agents': 1153, 'aggressive': 1154, 'alcohol': 1155, 'alligators': 1156, 'amoebae': 1157, 'amphibian': 1158, 'anchors': 1159, 'apart': 1160, 'archegonium': 1161, 'areas': 1162, 'arsenic': 1163, 'attract': 1164, 'baby': 1165, 'bacterial': 1166, 'bags': 1167, 'bark': 1168, 'basalt': 1169, 'bases': 1170, 'basic': 1171, 'bass': 1172, 'beaks': 1173, 'before': 1174, 'behind': 1175, 'between': 1176, 'biceps': 1177, 'bigger': 1178, 'biology': 1179, 'bird': 1180, 'blastoids': 1181, 'bleach': 1182, 'bloodstream': 1183, 'bottom': 1184, 'break': 1185, 'breaks': 1186, 'breezes': 1187, 'broken': 1188, 'bulb': 1189, 'buoyancy': 1190, 'burn': 1191, 'burns': 1192, 'burrows': 1193, 'burst': 1194, 'canyon': 1195, 'cavern': 1196, 'caves': 1197, 'centrioles': 1198, 'chance': 1199, 'charges': 1200, 'chickenpox': 1201, 'child': 1202, 'chill': 1203, 'chromatic': 1204, 'chronic': 1205, 'cigars': 1206, 'coastal': 1207, 'coat': 1208, 'coats': 1209, 'code': 1210, 'colonies': 1211, 'colored': 1212, 'communication': 1213, 'communities': 1214, 'complexity': 1215, 'computers': 1216, 'condensing': 1217, 'conducting': 1218, 'conduction': 1219, 'conservation': 1220, 'constriction': 1221, 'contaminated': 1222, 'covered': 1223, 'covering': 1224, 'creating': 1225, 'creatures': 1226, 'creeks': 1227, 'cross': 1228, 'cutting': 1229, 'cyclones': 1230, 'daily': 1231, 'damming': 1232, 'dangerous': 1233, 'decay': 1234, 'decaying': 1235, 'decomposing': 1236, 'deep': 1237, 'delta': 1238, 'desertification': 1239, 'details': 1240, 'detect': 1241, 'developments': 1242, 'devices': 1243, 'dew': 1244, 'dies': 1245, 'diet': 1246, 'differentiation': 1247, 'diffusion': 1248, 'dilation': 1249, 'direct': 1250, 'dispersion': 1251, 'dissolve': 1252, 'distances': 1253, 'disturb': 1254, 'diverse': 1255, 'divided': 1256, 'dolphin': 1257, 'dorsal': 1258, 'drink': 1259, 'driving': 1260, 'dromaeosaurs': 1261, 'easier': 1262, 'easily': 1263, 'eaten': 1264, 'echinoderms': 1265, 'echolocation': 1266, 'edible': 1267, 'eight': 1268, 'embryos': 1269, 'emissions': 1270, 'endangered': 1271, 'endoplasmic': 1272, 'entire': 1273, 'epidermis': 1274, 'equator': 1275, 'equilibrium': 1276, 'essential': 1277, 'estuaries': 1278, 'ethanol': 1279, 'evaporated': 1280, 'evolving': 1281, 'excess': 1282, 'exoskeleton': 1283, 'exoskeletons': 1284, 'explode': 1285, 'external': 1286, 'falling': 1287, 'feed': 1288, 'feeders': 1289, 'fermentation': 1290, 'fever': 1291, 'fiber': 1292, 'fibers': 1293, 'fibrous': 1294, 'filament': 1295, 'firecracker': 1296, 'fissures': 1297, 'flashlights': 1298, 'flood': 1299, 'flower': 1300, 'flowing': 1301, 'fluid': 1302, 'forces': 1303, 'foreign': 1304, 'forever': 1305, 'foxes': 1306, 'freezes': 1307, 'frog': 1308, 'fun': 1309, 'functions': 1310, 'fuzzy': 1311, 'gamete': 1312, 'gametophytes': 1313, 'gecko': 1314, 'generate': 1315, 'generating': 1316, 'generator': 1317, 'generators': 1318, 'genus': 1319, 'geometric': 1320, 'glands': 1321, 'glomerulus': 1322, 'goes': 1323, 'goggles': 1324, 'grams': 1325, 'granite': 1326, 'grapevines': 1327, 'grasses': 1328, 'greenhouses': 1329, 'groups': 1330, 'harming': 1331, 'hats': 1332, 'help': 1333, 'heterotrophs': 1334, 'hitting': 1335, 'hole': 1336, 'hormone': 1337, 'hosts': 1338, 'housing': 1339, 'hummingbirds': 1340, 'hydrophobic': 1341, 'hyperthyroidism': 1342, 'immobile': 1343, 'important': 1344, 'inches': 1345, 'incinerated': 1346, 'increasing': 1347, 'inner': 1348, 'instructions': 1349, 'insulator': 1350, 'intestine': 1351, 'irradiated': 1352, 'irradiating': 1353, 'isotopes': 1354, 'just': 1355, 'keeping': 1356, 'kicked': 1357, 'killed': 1358, 'killing': 1359, 'lake': 1360, 'landscape': 1361, 'landscapes': 1362, 'landslides': 1363, 'laser': 1364, 'last': 1365, 'lay': 1366, 'layer': 1367, 'learn': 1368, 'learning': 1369, 'length': 1370, 'lenses': 1371, 'lightning': 1372, 'limbs': 1373, 'lions': 1374, 'local': 1375, 'located': 1376, 'losing': 1377, 'lymphocytes': 1378, 'magnifying': 1379, 'manatees': 1380, 'many': 1381, 'marble': 1382, 'mask': 1383, 'massive': 1384, 'mates': 1385, 'mature': 1386, 'meiosis': 1387, 'members': 1388, 'menopause': 1389, 'menstruation': 1390, 'meristem': 1391, 'microscopes': 1392, 'migrate': 1393, 'mildew': 1394, 'milk': 1395, 'molecule': 1396, 'moles': 1397, 'mollusks': 1398, 'mosquitoes': 1399, 'mostly': 1400, 'motility': 1401, 'moves': 1402, 'mutated': 1403, \"n't\": 1404, 'native': 1405, 'near': 1406, 'nectar': 1407, 'nephrons': 1408, 'nesting': 1409, 'nucleotides': 1410, 'number': 1411, 'occurs': 1412, 'odors': 1413, 'old': 1414, 'omnivores': 1415, 'one': 1416, 'open': 1417, 'opposing': 1418, 'opposite': 1419, 'orbit': 1420, 'orthomyxoviruses': 1421, 'ovum': 1422, 'p53': 1423, 'palm': 1424, 'pan': 1425, 'panels': 1426, 'parasitic': 1427, 'parts': 1428, 'pattern': 1429, 'periods': 1430, 'person': 1431, 'petals': 1432, 'phase': 1433, 'pheremones': 1434, 'phone': 1435, 'phones': 1436, 'photosynthesize': 1437, 'phylum': 1438, 'physical': 1439, 'phytoplankton': 1440, 'pigs': 1441, 'plains': 1442, 'planktonic': 1443, 'plasma': 1444, 'plucking': 1445, 'poisoning': 1446, 'pollen': 1447, 'pollination': 1448, 'pollutants': 1449, 'polyps': 1450, 'ponds': 1451, 'potassium': 1452, 'potatoes': 1453, 'prehistoric': 1454, 'preservatives': 1455, 'prey': 1456, 'pride': 1457, 'process': 1458, 'produced': 1459, 'produces': 1460, 'prolactin': 1461, 'protect': 1462, 'protected': 1463, 'protects': 1464, 'pupils': 1465, 'quality': 1466, 'radio': 1467, 'rainbows': 1468, 'rainy': 1469, 'rapid': 1470, 'rate': 1471, 'ray': 1472, 'receptors': 1473, 'red': 1474, 'reduce': 1475, 'reduces': 1476, 'reducing': 1477, 'refract': 1478, 'refrigerators': 1479, 'regions': 1480, 'regulated': 1481, 'reindeer': 1482, 'relaxation': 1483, 'remove': 1484, 'repel': 1485, 'reproductive': 1486, 'requirement': 1487, 'reservoirs': 1488, 'reticulum': 1489, 'riding': 1490, 'rings': 1491, 'rises': 1492, 'roads': 1493, 'rodent': 1494, 'root': 1495, 'roses': 1496, 'rotting': 1497, 'roundworms': 1498, 'rubbing': 1499, 'runoff': 1500, 'safety': 1501, 'salts': 1502, 'scales': 1503, 'secretion': 1504, 'sedimentary': 1505, 'seeing': 1506, 'seen': 1507, 'selection': 1508, 'self': 1509, 'sensory': 1510, 'sessile': 1511, 'shaking': 1512, 'shallow': 1513, 'sheep': 1514, 'shelf': 1515, 'shivering': 1516, 'similar': 1517, 'six': 1518, 'skeleton': 1519, 'slime': 1520, 'smells': 1521, 'snort': 1522, 'soft': 1523, 'solidifies': 1524, 'spectrum': 1525, 'sphincter': 1526, 'spinal': 1527, 'spinning': 1528, 'stage': 1529, 'stars': 1530, 'steak': 1531, 'steel': 1532, 'stems': 1533, 'stopping': 1534, 'stroke': 1535, 'supplies': 1536, 'sustain': 1537, 'swim': 1538, 'swimming': 1539, 'symmetry': 1540, 'tRNA': 1541, 'tail': 1542, 'tar': 1543, 'taste': 1544, 'telephone': 1545, 'telephones': 1546, 'territory': 1547, 'texture': 1548, 'thawing': 1549, 'three': 1550, 'tigers': 1551, 'tiny': 1552, 'toxic': 1553, 'trails': 1554, 'trait': 1555, 'trophic': 1556, 'tumor': 1557, 'turbines': 1558, 'turning': 1559, 'turtles': 1560, 'unequal': 1561, 'units': 1562, 'unstable': 1563, 'urinary': 1564, 'vaccination': 1565, 'vapors': 1566, 'variation': 1567, 'veins': 1568, 'vertebral': 1569, 'vertebrates': 1570, 'very': 1571, 'vines': 1572, 'violence': 1573, 'visibility': 1574, 'vitamin': 1575, 'walking': 1576, 'walls': 1577, 'waterproof': 1578, 'watts': 1579, 'wearing': 1580, 'well': 1581, 'wells': 1582, 'wetlands': 1583, 'whale': 1584, 'where': 1585, 'white': 1586, 'wider': 1587, 'wildlife': 1588, 'willow': 1589, 'woody': 1590, 'work': 1591, 'worldwide': 1592, 'wounds': 1593, 'xerophytes': 1594, 'yes': 1595, 'zygote': 1596, 'zygotes': 1597, ' ': 1598, '&': 1599, '12': 1600, '14': 1601, '180': 1602, '25': 1603, '32': 1604, '35': 1605, '5.6': 1606, '65': 1607, '747': 1608, '?': 1609, 'AA': 1610, 'Abalone': 1611, 'Abnormal': 1612, 'Acidic': 1613, 'Acids': 1614, 'Active': 1615, 'Adatation': 1616, 'Adding': 1617, 'After': 1618, 'Alcohol': 1619, 'Alcohols': 1620, 'Allow': 1621, 'Aminoacylation': 1622, 'Andreas': 1623, 'Antarctica': 1624, 'Antennae': 1625, 'Anthrax': 1626, 'Antibiotics': 1627, 'Antibodies': 1628, 'Ants': 1629, 'Applying': 1630, 'Arteries': 1631, 'As': 1632, 'Asbestos': 1633, 'Asexual': 1634, 'At': 1635, 'Atlantic': 1636, 'Autumn': 1637, 'Away': 1638, 'Azathioprine': 1639, 'B': 1640, 'Barometer': 1641, 'Barrel': 1642, 'Barrier': 1643, 'Bats': 1644, 'Batteries': 1645, 'Beauty': 1646, 'Bend': 1647, 'Bending': 1648, 'Bermuda': 1649, 'Better': 1650, 'Biologists': 1651, 'Biomass': 1652, 'Biomolecules': 1653, 'Bird': 1654, 'Black': 1655, 'Blankets': 1656, 'Blowing': 1657, 'Blubber': 1658, 'Blue': 1659, 'Bodily': 1660, 'Boeing': 1661, 'Boiling': 1662, 'Boils': 1663, 'Both': 1664, 'Botulism': 1665, 'Brittle': 1666, 'Broken': 1667, 'Brutal': 1668, 'Budding': 1669, 'Bundles': 1670, 'Burma': 1671, 'Burying': 1672, 'CFCs': 1673, 'Cadillac': 1674, 'Calcium': 1675, 'Candida': 1676, 'Cane': 1677, 'Carrots': 1678, 'Cars': 1679, 'Caspian': 1680, 'Catch': 1681, 'Cause': 1682, 'Causes': 1683, 'Celsius': 1684, 'Cephalopod': 1685, 'Cephalopods': 1686, 'Changes': 1687, 'Chemicals': 1688, 'Chemotherapy': 1689, 'Chimneys': 1690, 'Chlorine': 1691, 'Chlorophyl': 1692, 'Cigarettes': 1693, 'Circuits': 1694, 'Clams': 1695, 'Clear': 1696, 'Closed': 1697, 'Columbia': 1698, 'Competing': 1699, 'Competition': 1700, 'Complete': 1701, 'Complex': 1702, 'Conception': 1703, 'Cones': 1704, 'Conifers': 1705, 'Constant': 1706, 'Constrict': 1707, 'Constriction': 1708, 'Consumers': 1709, 'Consuming': 1710, 'Contact': 1711, 'Contaminated': 1712, 'Continents': 1713, 'Contract': 1714, 'Converted': 1715, 'Converting': 1716, 'Converts': 1717, 'Cook': 1718, 'Cooling': 1719, 'Copepods': 1720, 'Corn': 1721, 'Created': 1722, 'Creating': 1723, 'Crewmembers': 1724, 'Crops': 1725, 'Currents': 1726, 'Cytokinesis': 1727, 'D': 1728, 'Daisy': 1729, 'Damaged': 1730, 'Damages': 1731, 'Dance': 1732, 'Darkness': 1733, 'Darwinism': 1734, 'David': 1735, 'Deaf': 1736, 'Decibels': 1737, 'Decomposers': 1738, 'Decrease': 1739, 'Decreased': 1740, 'Decreases': 1741, 'Depleting': 1742, 'Deposition': 1743, 'Depositions': 1744, 'Desertification': 1745, 'Deserts': 1746, 'Dew': 1747, 'Differences': 1748, 'Digesting': 1749, 'Dilution': 1750, 'Dioxide': 1751, 'Direct': 1752, 'Discrete': 1753, 'Dissolved': 1754, 'Distance': 1755, 'Diurnal': 1756, 'Diversity': 1757, 'Drink': 1758, 'Drips': 1759, 'Dry': 1760, 'Due': 1761, 'Duracell': 1762, 'Ears': 1763, 'Earthworm': 1764, 'Eat': 1765, 'Echinoderm': 1766, 'Echinoderms': 1767, 'Ecosystems': 1768, 'Edible': 1769, 'Electron': 1770, 'Electrons': 1771, 'Emit': 1772, 'Encoded': 1773, 'Endocrine': 1774, 'Engine': 1775, 'Eons': 1776, 'Erode': 1777, 'Erosion': 1778, 'Ethologist': 1779, 'Euphrates': 1780, 'Evolving': 1781, 'Excision': 1782, 'Excretion': 1783, 'Excretory': 1784, 'Exfoliation': 1785, 'Expandable': 1786, 'Expands': 1787, 'Extend': 1788, 'Eye': 1789, 'Eyes': 1790, 'Fahrenheit': 1791, 'Fall': 1792, 'Famine': 1793, 'Faraday': 1794, 'Fe': 1795, 'Female': 1796, 'Fiber': 1797, 'Fight': 1798, 'Final': 1799, 'Fine': 1800, 'Firecrackers': 1801, 'Five': 1802, 'Flask': 1803, 'Flowing': 1804, 'Fog': 1805, 'For': 1806, 'Fresh': 1807, 'Freshwater': 1808, 'Frog': 1809, 'Frogs': 1810, 'From': 1811, 'Frostbite': 1812, 'Fruits': 1813, 'Fuel': 1814, 'Fully': 1815, 'Fun': 1816, 'Fungus': 1817, 'Gamete': 1818, 'Gases': 1819, 'Gasoline': 1820, 'Germy': 1821, 'Get': 1822, 'Give': 1823, 'Gladioluses': 1824, 'Glass': 1825, 'Glucose': 1826, 'Go': 1827, 'Gold': 1828, 'Gorge': 1829, 'Grass': 1830, 'Graves': 1831, 'Gunshots': 1832, 'H': 1833, 'HPV': 1834, 'Habitat': 1835, 'Hail': 1836, 'Hair': 1837, 'Haploid': 1838, 'Harming': 1839, 'Harms': 1840, 'Hearing': 1841, 'Hemoglobin': 1842, 'Herpes': 1843, 'High': 1844, 'Himalayas': 1845, 'Holding': 1846, 'Honeybees': 1847, 'Hormone': 1848, 'Hormones': 1849, 'Humidity': 1850, 'Hummingbirds': 1851, 'Ice': 1852, 'Igneous': 1853, 'Impurities': 1854, 'Increases': 1855, 'Inertial': 1856, 'Infection': 1857, 'Inheriting': 1858, 'Injury': 1859, 'Instructions': 1860, 'Insulation': 1861, 'Invertebrates': 1862, 'Irradiated': 1863, 'Jet': 1864, 'Jupiter': 1865, 'Keep': 1866, 'Keeping': 1867, 'Kidneys': 1868, 'Kill': 1869, 'Kills': 1870, 'Kinetic': 1871, 'Kite': 1872, 'Lake': 1873, 'Lakes': 1874, 'Large': 1875, 'Laying': 1876, 'Lead': 1877, 'Leaving': 1878, 'Leeches': 1879, 'Leeuwenhoek': 1880, 'Length': 1881, 'Leo': 1882, 'Leptospirosis': 1883, 'Less': 1884, 'Leukemia': 1885, 'Lift': 1886, 'Lighting': 1887, 'Lilies': 1888, 'Limbs': 1889, 'Limestone': 1890, 'Link': 1891, 'Liters': 1892, 'Live': 1893, 'Loamy': 1894, 'Localized': 1895, 'Loonhaunt': 1896, 'Loose': 1897, 'Looseness': 1898, 'Losing': 1899, 'Loud': 1900, 'Lower': 1901, 'Lymphocytes': 1902, 'Machines': 1903, 'Magnetism': 1904, 'Maintain': 1905, 'Malignant': 1906, 'Malnutrition': 1907, 'Malpighian': 1908, 'Mammal': 1909, 'Man': 1910, 'Mantle': 1911, 'Many': 1912, 'Marine': 1913, 'Mars': 1914, 'Material': 1915, 'Measles': 1916, 'Meiosis': 1917, 'Melts': 1918, 'Memory': 1919, 'Menstruation': 1920, 'Meristems': 1921, 'Merkel': 1922, 'Metal': 1923, 'Metling': 1924, 'Mexico': 1925, 'Michael': 1926, 'Microscopic': 1927, 'Migration': 1928, 'Misell': 1929, 'Mixing': 1930, 'Moist': 1931, 'Moisture': 1932, 'Molasses': 1933, 'Molds': 1934, 'Molecular': 1935, 'Mollusks': 1936, 'Monosaccharides': 1937, 'Monotremes': 1938, 'Moon': 1939, 'Most': 1940, 'Motility': 1941, 'Mountains': 1942, 'Mulberry': 1943, 'Muscle': 1944, 'Muscles': 1945, 'Mushrooms': 1946, 'Mutagenic': 1947, 'Napa': 1948, 'Natural': 1949, 'Near': 1950, 'Nectar': 1951, 'Nematodes': 1952, 'Nervous': 1953, 'Nesco': 1954, 'Neurotransmitters': 1955, 'Nitrogen': 1956, 'Nonliving': 1957, 'Nucleotide': 1958, 'Nurseries': 1959, 'O': 1960, 'O2': 1961, 'O3': 1962, 'Oak': 1963, 'Object': 1964, 'Objects': 1965, 'One': 1966, 'Operating': 1967, 'Orange': 1968, 'Orchids': 1969, 'Organic': 1970, 'Origami': 1971, 'Our': 1972, 'Ozone': 1973, 'PDAs': 1974, 'Pairs': 1975, 'Pancreas': 1976, 'Papilloma': 1977, 'Part': 1978, 'Particulate': 1979, 'Pathogens': 1980, 'Person': 1981, 'Perspiration': 1982, 'Pesticide': 1983, 'Pheromones': 1984, 'Photons': 1985, 'Photosynthesis': 1986, 'Physiology': 1987, 'Pines': 1988, 'Pins': 1989, 'Pipets': 1990, 'Plastics': 1991, 'Plucking': 1992, 'Poison': 1993, 'Poisoning': 1994, 'Pollen': 1995, 'Polynucleotide': 1996, 'Population': 1997, 'Possibly': 1998, 'Predators': 1999, 'Premature': 2000, 'Preserves': 2001, 'Pressure': 2002, 'Preventing': 2003, 'Primary': 2004, 'Prolactin': 2005, 'Propubic': 2006, 'Protect': 2007, 'Protection': 2008, 'Protons': 2009, 'Provide': 2010, 'Pulmonologists': 2011, 'Quality': 2012, 'Radio': 2013, 'Radioactive': 2014, 'Raft': 2015, 'Rainforests': 2016, 'Raven': 2017, 'Receptor': 2018, 'Reef': 2019, 'Reefs': 2020, 'Refract': 2021, 'Regrows': 2022, 'Release': 2023, 'Remove': 2024, 'Reproducing': 2025, 'Reproductive': 2026, 'Reptiles': 2027, 'Reptilia': 2028, 'Respiration': 2029, 'Retina': 2030, 'Reusing': 2031, 'Revolution': 2032, 'Ribosomes': 2033, 'Ribs': 2034, 'Rio': 2035, 'Rise': 2036, 'Roads': 2037, 'Rob': 2038, 'Room': 2039, 'SNPs': 2040, 'SO2': 2041, 'Salamander': 2042, 'Salinity': 2043, 'Same': 2044, 'Samsung': 2045, 'Scent': 2046, 'Schwinn': 2047, 'Scrambling': 2048, 'Seasons': 2049, 'Sedimentary': 2050, 'Seeds': 2051, 'Seeing': 2052, 'Seesaw': 2053, 'Seismometer': 2054, 'September': 2055, 'Severe': 2056, 'Sexual': 2057, 'Sexually': 2058, 'Shallow': 2059, 'Shark': 2060, 'Shasta': 2061, 'Slow': 2062, 'Slowing': 2063, 'Smoking': 2064, 'Smooth': 2065, 'Snake': 2066, 'Social': 2067, 'Sodium': 2068, 'Soft': 2069, 'Softwoods': 2070, 'Somethign': 2071, 'Sonar': 2072, 'Sonoma': 2073, 'Sound': 2074, 'Sounds': 2075, 'Specialized': 2076, 'Species': 2077, 'Sperm': 2078, 'Split': 2079, 'Spread': 2080, 'Spring': 2081, 'Stay': 2082, 'Steam': 2083, 'Steroid': 2084, 'Store': 2085, 'Storing': 2086, 'Streams': 2087, 'Stress': 2088, 'Sudden': 2089, 'Sustain': 2090, 'Sweet': 2091, 'Syngamy': 2092, 'TRH': 2093, 'Tectonic': 2094, 'Thames': 2095, 'Thyroxine': 2096, 'Tibet': 2097, 'Time': 2098, 'Titanic': 2099, 'Towards': 2100, 'Toxicity': 2101, 'Toxins': 2102, 'Toyota': 2103, 'Transfers': 2104, 'Transplanted': 2105, 'Trophic': 2106, 'Tungsten': 2107, 'Turning': 2108, 'Turns': 2109, 'UVA': 2110, 'UVB': 2111, 'Ultraviolet': 2112, 'Under': 2113, 'Uneven': 2114, 'Uplift': 2115, 'Urea': 2116, 'Urethra': 2117, 'Use': 2118, 'Valleys': 2119, 'Veins': 2120, 'Vertebrates': 2121, 'Very': 2122, 'Vibrates': 2123, 'Vibrations': 2124, 'Vitamin': 2125, 'Volcanic': 2126, 'Walls': 2127, 'Warmth': 2128, 'Warriors': 2129, 'Whales': 2130, 'Where': 2131, 'Wild': 2132, 'Windward': 2133, 'Wither': 2134, 'Worldwide': 2135, 'Your': 2136, 'Yukon': 2137, 'Zinc': 2138, 'abcess': 2139, 'abdomen': 2140, 'about': 2141, 'above': 2142, 'abrasion': 2143, 'absolute': 2144, 'absorbed': 2145, 'abundant': 2146, 'abusive': 2147, 'acceleration': 2148, 'accident': 2149, 'accommodate': 2150, 'accumulate': 2151, 'accumulated': 2152, 'acetic': 2153, 'acidic': 2154, 'action': 2155, 'activated': 2156, 'adapted': 2157, 'adhesives': 2158, 'adjustable': 2159, 'adulthood': 2160, 'advances': 2161, 'advancing': 2162, 'advantageous': 2163, 'aerate': 2164, 'aeration': 2165, 'aerodynamics': 2166, 'after': 2167, 'agriculture': 2168, 'ahead': 2169, 'airplane': 2170, 'airplanes': 2171, 'alarm': 2172, 'albino': 2173, 'albumin': 2174, 'alive': 2175, 'allergic': 2176, 'allergy': 2177, 'allow': 2178, 'allows': 2179, 'alluvial': 2180, 'almost': 2181, 'alpacas': 2182, 'alter': 2183, 'aluminum': 2184, 'alveoli': 2185, 'amniotes': 2186, 'amnotic': 2187, 'amoeba': 2188, 'amputation': 2189, 'anabolic': 2190, 'analysis': 2191, 'anemia': 2192, 'aneurisms': 2193, 'angle': 2194, 'anion': 2195, 'anostrans': 2196, 'antennae': 2197, 'antigen': 2198, 'aphrodisiacs': 2199, 'apocymarin': 2200, 'appendages': 2201, 'apple': 2202, 'apples': 2203, 'appliance': 2204, 'application': 2205, 'aqueous': 2206, 'arms': 2207, 'arterioles': 2208, 'artery': 2209, 'arthropoda': 2210, 'as': 2211, 'asbestos': 2212, 'assassin': 2213, 'asses': 2214, 'asteroid': 2215, 'asthma': 2216, 'astroglial': 2217, 'astronauts': 2218, 'athletes': 2219, 'attach': 2220, 'attached': 2221, 'attacks': 2222, 'attracted': 2223, 'attracting': 2224, 'attractive': 2225, 'attracts': 2226, 'auburn': 2227, 'audible': 2228, 'audio': 2229, 'autotrophs': 2230, 'autumnal': 2231, 'average': 2232, 'aves': 2233, 'avoid': 2234, 'avoiding': 2235, 'aware': 2236, 'axons': 2237, 'b': 2238, 'babies': 2239, 'back': 2240, 'bacterium': 2241, 'badge': 2242, 'bag': 2243, 'baking': 2244, 'balanced': 2245, 'balloon': 2246, 'barks': 2247, 'barn': 2248, 'barnacles': 2249, 'barometric': 2250, 'barrier': 2251, 'beads': 2252, 'beam': 2253, 'beans': 2254, 'beds': 2255, 'bee': 2256, 'beef': 2257, 'beetles': 2258, 'behavioral': 2259, 'behaviorists': 2260, 'below': 2261, 'bend': 2262, 'bends': 2263, 'beneficial': 2264, 'benign': 2265, 'bent': 2266, 'benthic': 2267, 'big': 2268, 'bilateral': 2269, 'bilateraly': 2270, 'bill': 2271, 'biloba': 2272, 'binary': 2273, 'bind': 2274, 'bioaerosols': 2275, 'biofuel': 2276, 'biological': 2277, 'biomass': 2278, 'biome': 2279, 'birches': 2280, 'bisexual': 2281, 'bite': 2282, 'bladder': 2283, 'blisters': 2284, 'bloom': 2285, 'blubber': 2286, 'blueprints': 2287, 'boats': 2288, 'bobo': 2289, 'bodied': 2290, 'bodies': 2291, 'bog': 2292, 'bogs': 2293, 'bomb': 2294, 'bond': 2295, 'booster': 2296, 'border': 2297, 'bored': 2298, 'bottles': 2299, 'bouncing': 2300, 'boundaries': 2301, 'bowel': 2302, 'boxes': 2303, 'brains': 2304, 'brakes': 2305, 'bread': 2306, 'breakdown': 2307, 'breast': 2308, 'breastfeeding': 2309, 'breed': 2310, 'bridges': 2311, 'brightens': 2312, 'bronchodilatory': 2313, 'brown': 2314, 'bryophytes': 2315, 'bucks': 2316, 'bugs': 2317, 'buildings': 2318, 'bullfrogs': 2319, 'bumper': 2320, 'bundles': 2321, 'buried': 2322, 'burrow': 2323, 'burrs': 2324, 'buses': 2325, 'bush': 2326, 'butterflies': 2327, 'cake': 2328, 'calcite': 2329, 'called': 2330, 'calm': 2331, 'camels': 2332, 'cameras': 2333, 'camouflage': 2334, 'cancerous': 2335, 'cancers': 2336, 'canned': 2337, 'capability': 2338, 'capacity': 2339, 'caperata': 2340, 'capture': 2341, 'carbonic': 2342, 'carcinogen': 2343, 'carcinoma': 2344, 'carpets': 2345, 'carpooling': 2346, 'carrier': 2347, 'carrying': 2348, 'catabolism': 2349, 'cations': 2350, 'cattle': 2351, 'caused': 2352, 'cave': 2353, 'caverns': 2354, 'celcius': 2355, 'cellphone': 2356, 'cellular': 2357, 'centers': 2358, 'central': 2359, 'cephalopods': 2360, 'certain': 2361, 'chairs': 2362, 'chambered': 2363, 'chemistry': 2364, 'chickens': 2365, 'chime': 2366, 'chinooks': 2367, 'chlorofluorocarbons': 2368, 'chlorophy': 2369, 'cholesterol': 2370, 'chordate': 2371, 'circulation': 2372, 'cities': 2373, 'citokinesis': 2374, 'clam': 2375, 'clear': 2376, 'clearly': 2377, 'close': 2378, 'cloudy': 2379, 'cluster': 2380, 'clusters': 2381, 'coastlines': 2382, 'cobra': 2383, 'coded': 2384, 'coffee': 2385, 'coherent': 2386, 'coin': 2387, 'coldest': 2388, 'coliding': 2389, 'coliforms': 2390, 'collagen': 2391, 'collapse': 2392, 'collapsing': 2393, 'collide': 2394, 'colloidal': 2395, 'colon': 2396, 'colorless': 2397, 'combining': 2398, 'combust': 2399, 'combustion': 2400, 'come': 2401, 'comfort': 2402, 'comfortable': 2403, 'coming': 2404, 'common': 2405, 'commonly': 2406, 'communicate': 2407, 'communications': 2408, 'commuting': 2409, 'compacted': 2410, 'compaction': 2411, 'competing': 2412, 'components': 2413, 'composting': 2414, 'compresses': 2415, 'computer': 2416, 'concentrations': 2417, 'condense': 2418, 'condenses': 2419, 'conductor': 2420, 'conducts': 2421, 'cones': 2422, 'conifer': 2423, 'conncectivity': 2424, 'considerably': 2425, 'constant': 2426, 'construction': 2427, 'consume': 2428, 'consuming': 2429, 'contains': 2430, 'contamination': 2431, 'content': 2432, 'continual': 2433, 'continue': 2434, 'continues': 2435, 'continuously': 2436, 'contract': 2437, 'contracting': 2438, 'contraction': 2439, 'contracts': 2440, 'controls': 2441, 'converted': 2442, 'converts': 2443, 'conveying': 2444, 'cooler': 2445, 'coolness': 2446, 'copepods': 2447, 'corals': 2448, 'cord': 2449, 'cosmetic': 2450, 'cost': 2451, 'crack': 2452, 'cracking': 2453, 'cracks': 2454, 'cranes': 2455, 'crappie': 2456, 'crashes': 2457, 'create': 2458, 'created': 2459, 'crinoids': 2460, 'crippling': 2461, 'critical': 2462, 'crocodile': 2463, 'crocodiles': 2464, 'crop': 2465, 'crops': 2466, 'crows': 2467, 'crunchy': 2468, 'cubes': 2469, 'cups': 2470, 'curve': 2471, 'cut': 2472, 'cuticle': 2473, 'cutters': 2474, 'cyanide': 2475, 'cyanobacteria': 2476, 'cycle': 2477, 'cyclone': 2478, 'cytokinesis': 2479, 'cytoplasmic': 2480, 'daffodils': 2481, 'dam': 2482, 'damaging': 2483, 'days': 2484, 'deaths': 2485, 'debris': 2486, 'decompose': 2487, 'decreased': 2488, 'deer': 2489, 'defense': 2490, 'define': 2491, 'deformation': 2492, 'dehydrated': 2493, 'dehydrating': 2494, 'deltas': 2495, 'dense': 2496, 'densities': 2497, 'deoxygenated': 2498, 'deposited': 2499, 'depositon': 2500, 'deposits': 2501, 'depostion': 2502, 'depression': 2503, 'depressive': 2504, 'destroyed': 2505, 'detectors': 2506, 'determine': 2507, 'devastation': 2508, 'device': 2509, 'dexyribonucleic': 2510, 'diatomes': 2511, 'diatoms': 2512, 'differential': 2513, 'diffraction': 2514, 'digest': 2515, 'dinosaur': 2516, 'dioxins': 2517, 'dirtier': 2518, 'disasters': 2519, 'discouraged': 2520, 'discs': 2521, 'disinfectants': 2522, 'disintegrate': 2523, 'disintegration': 2524, 'disperse': 2525, 'displacement': 2526, 'displays': 2527, 'dissolving': 2528, 'distract': 2529, 'distractions': 2530, 'disturbance': 2531, 'diveristy': 2532, 'dividing': 2533, 'diving': 2534, 'doctor': 2535, 'does': 2536, 'dog': 2537, 'dollars': 2538, 'dolls': 2539, 'dominant': 2540, 'donates': 2541, 'dormancy': 2542, 'double': 2543, 'drains': 2544, 'dramatically': 2545, 'drifting': 2546, 'drivers': 2547, 'drug': 2548, 'drum': 2549, 'ductile': 2550, 'ducts': 2551, 'dunes': 2552, 'duplicate': 2553, 'duplication': 2554, 'dust': 2555, 'dying': 2556, 'each': 2557, 'eagles': 2558, 'eardrum': 2559, 'earthworm': 2560, 'easy': 2561, 'echidna': 2562, 'echidnas': 2563, 'eclipse': 2564, 'economy': 2565, 'ecosystem': 2566, 'ecotourism': 2567, 'edema': 2568, 'effects': 2569, 'efficiency': 2570, 'ejaculate': 2571, 'electrically': 2572, 'electro': 2573, 'electrolytes': 2574, 'electromagnetism': 2575, 'electrostatic': 2576, 'elevation': 2577, 'elevations': 2578, 'elimination': 2579, 'elliptical': 2580, 'else': 2581, 'emergency': 2582, 'encoded': 2583, 'end': 2584, 'endangering': 2585, 'endings': 2586, 'endogenous': 2587, 'energetic': 2588, 'enormous': 2589, 'enough': 2590, 'enriched': 2591, 'enviroment': 2592, 'environmental': 2593, 'eons': 2594, 'epidermal': 2595, 'epididymes': 2596, 'epidiymes': 2597, 'equations': 2598, 'equinox': 2599, 'eroded': 2600, 'essentials': 2601, 'estimate': 2602, 'ethologists': 2603, 'eukyarotes': 2604, 'evaporates': 2605, 'evaporating': 2606, 'evaporative': 2607, 'events': 2608, 'evergeeen': 2609, 'evergreen': 2610, 'everybody': 2611, 'evolved': 2612, 'exactly': 2613, 'excellent': 2614, 'exchange': 2615, 'excreted': 2616, 'expanding': 2617, 'expelled': 2618, 'expode': 2619, 'extinct': 2620, 'extracted': 2621, 'extrapolated': 2622, 'extremely': 2623, 'eye': 2624, 'fallen': 2625, 'falls': 2626, 'fan': 2627, 'far': 2628, 'farm': 2629, 'fast': 2630, 'faster': 2631, 'father': 2632, 'fault': 2633, 'faulty': 2634, 'feces': 2635, 'fed': 2636, 'feeder': 2637, 'feel': 2638, 'fens': 2639, 'fermented': 2640, 'fern': 2641, 'fertile': 2642, 'fertilize': 2643, 'fertilizer': 2644, 'fetch': 2645, 'few': 2646, 'fiberglass': 2647, 'fibrils': 2648, 'fields': 2649, 'fighters': 2650, 'filaments': 2651, 'final': 2652, 'fingers': 2653, 'finite': 2654, 'fins': 2655, 'firecrackers': 2656, 'firework': 2657, 'fireworks': 2658, 'firm': 2659, 'first': 2660, 'fissile': 2661, 'fission': 2662, 'fitness': 2663, 'flagella': 2664, 'flash': 2665, 'flashes': 2666, 'flatworms': 2667, 'fleas': 2668, 'flight': 2669, 'flights': 2670, 'float': 2671, 'floats': 2672, 'floor': 2673, 'flowerless': 2674, 'flu': 2675, 'fluctuate': 2676, 'fluctuates': 2677, 'fluids': 2678, 'flying': 2679, 'flytrap': 2680, 'focusing': 2681, 'foil': 2682, 'folding': 2683, 'foliage': 2684, 'formations': 2685, 'forward': 2686, 'fractions': 2687, 'framework': 2688, 'freckles': 2689, 'freesheet': 2690, 'freeze': 2691, 'freshwater': 2692, 'friendship': 2693, 'front': 2694, 'frontal': 2695, 'frostbite': 2696, 'frozen': 2697, 'fruiting': 2698, 'fruits': 2699, 'fumes': 2700, 'functional': 2701, 'functioning': 2702, 'fungus': 2703, 'future': 2704, 'gain': 2705, 'games': 2706, 'gardening': 2707, 'gasses': 2708, 'geckos': 2709, 'gender': 2710, 'generally': 2711, 'generates': 2712, 'genitalia': 2713, 'genitals': 2714, 'genotypes': 2715, 'geological': 2716, 'geothermal': 2717, 'germany': 2718, 'germs': 2719, 'getting': 2720, 'ginkgo': 2721, 'girl': 2722, 'glisten': 2723, 'gloves': 2724, 'glow': 2725, 'glycogen': 2726, 'goat': 2727, 'gone': 2728, 'goods': 2729, 'gorges': 2730, 'gourdin': 2731, 'graduations': 2732, 'grain': 2733, 'grains': 2734, 'graptolites': 2735, 'gravel': 2736, 'grazing': 2737, 'great': 2738, 'group': 2739, 'growths': 2740, 'guitars': 2741, 'guns': 2742, 'habitual': 2743, 'haircoat': 2744, 'hammer': 2745, 'hand': 2746, 'hands': 2747, 'hard': 2748, 'hardened': 2749, 'harmed': 2750, 'hatch': 2751, 'hatched': 2752, 'havoc': 2753, 'hawks': 2754, 'haze': 2755, 'headaches': 2756, 'headlights': 2757, 'heal': 2758, 'healthier': 2759, 'hearts': 2760, 'heightened': 2761, 'helping': 2762, 'hemisphere': 2763, 'hemp': 2764, 'herbicide': 2765, 'herbivores': 2766, 'heredity': 2767, 'heriditary': 2768, 'hermaphrodite': 2769, 'hermaphrodites': 2770, 'herons': 2771, 'hexazinone': 2772, 'hibernate': 2773, 'hibernation': 2774, 'hidden': 2775, 'histone': 2776, 'history': 2777, 'hockey': 2778, 'holes': 2779, 'home': 2780, 'homogenous': 2781, 'honey': 2782, 'hooded': 2783, 'horses': 2784, 'hospitals': 2785, 'hotter': 2786, 'hours': 2787, 'household': 2788, 'houses': 2789, 'how': 2790, 'huddle': 2791, 'hugs': 2792, 'humankind': 2793, 'humid': 2794, 'hunting': 2795, 'hurt': 2796, 'hydrate': 2797, 'hydraulic': 2798, 'hydrocarbon': 2799, 'hydrocarbons': 2800, 'hypothermia': 2801, 'hypothyroidism': 2802, 'iPhones': 2803, 'if': 2804, 'ignite': 2805, 'igniting': 2806, 'ignition': 2807, 'illnesses': 2808, 'illumination': 2809, 'image': 2810, 'immediate': 2811, 'immunization': 2812, 'immunodeficiency': 2813, 'immunogens': 2814, 'immunological': 2815, 'impacts': 2816, 'impelling': 2817, 'implanted': 2818, 'improve': 2819, 'improves': 2820, 'inactive': 2821, 'incandescent': 2822, 'incineration': 2823, 'incubation': 2824, 'indeterminate': 2825, 'indirect': 2826, 'individuals': 2827, 'inducible': 2828, 'industry': 2829, 'inefficient': 2830, 'inertia': 2831, 'infarction': 2832, 'infected': 2833, 'infections': 2834, 'infertility': 2835, 'infrared': 2836, 'ingested': 2837, 'inhalation': 2838, 'inherited': 2839, 'injuries': 2840, 'innovative': 2841, 'insect': 2842, 'instincts': 2843, 'insulators': 2844, 'intelligence': 2845, 'intense': 2846, 'interact': 2847, 'intermediate': 2848, 'internal': 2849, 'interstellar': 2850, 'intervertebrates': 2851, 'intrinsic': 2852, 'intrusion': 2853, 'invented': 2854, 'invertebrate': 2855, 'ion': 2856, 'ionization': 2857, 'ionized': 2858, 'iris': 2859, 'islands': 2860, 'itself': 2861, 'ivy': 2862, 'jaw': 2863, 'jerky': 2864, 'jet': 2865, 'jets': 2866, 'joining': 2867, 'joules': 2868, 'jungles': 2869, 'kalenchoes': 2870, 'karyogamy': 2871, 'kernel': 2872, 'keystone': 2873, 'killer': 2874, 'kills': 2875, 'kilogram': 2876, 'kilometers': 2877, 'kitchen': 2878, 'known': 2879, 'koalas': 2880, 'laboratory': 2881, 'lacking': 2882, 'lamps': 2883, 'land': 2884, 'landfills': 2885, 'lands': 2886, 'larger': 2887, 'larvae': 2888, 'late': 2889, 'later': 2890, 'lateral': 2891, 'latitude': 2892, 'latitudes': 2893, 'lava': 2894, 'lawn': 2895, 'lead': 2896, 'leaf': 2897, 'leaving': 2898, 'lemmons': 2899, 'leopard': 2900, 'lethal': 2901, 'lie': 2902, 'lifeforms': 2903, 'lifting': 2904, 'lightbulb': 2905, 'lighting': 2906, 'lights': 2907, 'lignin': 2908, 'ligt': 2909, 'lillies': 2910, 'limb': 2911, 'linguistically': 2912, 'linked': 2913, 'liters': 2914, 'lithium': 2915, 'litter': 2916, 'little': 2917, 'livers': 2918, 'lives': 2919, 'lmestone': 2920, 'lobes': 2921, 'lobsters': 2922, 'locations': 2923, 'loch': 2924, 'loggers': 2925, 'logging': 2926, 'logs': 2927, 'look': 2928, 'looking': 2929, 'loop': 2930, 'looser': 2931, 'loses': 2932, 'lot': 2933, 'loud': 2934, 'lowered': 2935, 'lowering': 2936, 'lowest': 2937, 'lumber': 2938, 'lumberjacks': 2939, 'luminometer': 2940, 'lungworms': 2941, 'lush': 2942, 'luxury': 2943, 'lymphoma': 2944, 'lysing': 2945, 'mL': 2946, 'macaques': 2947, 'machines': 2948, 'magnet': 2949, 'magnified': 2950, 'magnitude': 2951, 'major': 2952, 'malnutrition': 2953, 'mammal': 2954, 'manufacturing': 2955, 'manures': 2956, 'maples': 2957, 'margins': 2958, 'marker': 2959, 'marrow': 2960, 'marshes': 2961, 'marshy': 2962, 'matches': 2963, 'may': 2964, 'meals': 2965, 'measurement': 2966, 'meats': 2967, 'mechanism': 2968, 'mechanoreceptors': 2969, 'media': 2970, 'medical': 2971, 'melanistic': 2972, 'melanoma': 2973, 'messages': 2974, 'metabolic': 2975, 'metamorphizing': 2976, 'meters': 2977, 'methane': 2978, 'mice': 2979, 'microbial': 2980, 'microevolution': 2981, 'microtubules': 2982, 'microwave': 2983, 'mild': 2984, 'mill': 2985, 'mineral': 2986, 'mines': 2987, 'minutes': 2988, 'mirrors': 2989, 'mistake': 2990, 'mites': 2991, 'mitochondria': 2992, 'mixing': 2993, 'moldy': 2994, 'mole': 2995, 'molluscs': 2996, 'monomers': 2997, 'monotreme': 2998, 'mortality': 2999, 'mosaic': 3000, 'mosiac': 3001, 'mother': 3002, 'motor': 3003, 'motors': 3004, 'mouth': 3005, 'moved': 3006, 'movements': 3007, 'movemnet': 3008, 'mowers': 3009, 'much': 3010, 'mud': 3011, 'multiply': 3012, 'municipal': 3013, 'mushroom': 3014, 'must': 3015, 'mutagens': 3016, 'mutant': 3017, 'nature': 3018, 'nearest': 3019, 'need': 3020, 'needs': 3021, 'nektar': 3022, 'nest': 3023, 'newsprint': 3024, 'newtons': 3025, 'nickel': 3026, 'nicotine': 3027, 'nitrile': 3028, 'noises': 3029, 'nokia': 3030, 'normal': 3031, 'nose': 3032, 'nucelotide': 3033, 'nuclei': 3034, 'nucleoli': 3035, 'numbers': 3036, 'oats': 3037, 'objectstowards': 3038, 'octopus': 3039, 'odorless': 3040, 'often': 3041, 'omnivore': 3042, 'once': 3043, 'oncogenes': 3044, 'ones': 3045, 'oneself': 3046, 'operated': 3047, 'operating': 3048, 'opossums': 3049, 'opportunistic': 3050, 'optic': 3051, 'orange': 3052, 'orbital': 3053, 'orbiting': 3054, 'organics': 3055, 'origami': 3056, 'oscillations': 3057, 'osmosis': 3058, 'osteogenic': 3059, 'others': 3060, 'otters': 3061, 'outbursts': 3062, 'outer': 3063, 'oven': 3064, 'overheating': 3065, 'overheats': 3066, 'ovules': 3067, 'oxides': 3068, 'oxidiser': 3069, 'oxidize': 3070, 'pHs': 3071, 'pack': 3072, 'pained': 3073, 'painful': 3074, 'painting': 3075, 'panther': 3076, 'panting': 3077, 'papaya': 3078, 'parent': 3079, 'parisitic': 3080, 'partner': 3081, 'partners': 3082, 'passed': 3083, 'passing': 3084, 'passive': 3085, 'past': 3086, 'patch': 3087, 'path': 3088, 'pathogenic': 3089, 'peachleaf': 3090, 'peak': 3091, 'pelt': 3092, 'pelvis': 3093, 'penguins': 3094, 'penis': 3095, 'per': 3096, 'perceiving': 3097, 'perennials': 3098, 'period': 3099, 'permineralization': 3100, 'perspire': 3101, 'pest': 3102, 'pesticide': 3103, 'petrified': 3104, 'petroleum': 3105, 'phenomenon': 3106, 'phenotypes': 3107, 'pheremon': 3108, 'phosphorous': 3109, 'photosyntehsis': 3110, 'photosynthetics': 3111, 'physiology': 3112, 'piano': 3113, 'picks': 3114, 'pigeons': 3115, 'pigment': 3116, 'pigments': 3117, 'pines': 3118, 'pipes': 3119, 'pistons': 3120, 'place': 3121, 'places': 3122, 'planets': 3123, 'planted': 3124, 'plastids': 3125, 'plate': 3126, 'plated': 3127, 'platypuses': 3128, 'pneumonia': 3129, 'poison': 3130, 'polar': 3131, 'polarized': 3132, 'poles': 3133, 'pollinate': 3134, 'pollinated': 3135, 'polluted': 3136, 'polyethylene': 3137, 'polymers': 3138, 'polynucleotide': 3139, 'polynucleotides': 3140, 'polyp': 3141, 'pond': 3142, 'ponding': 3143, 'pools': 3144, 'populations': 3145, 'porcinis': 3146, 'porous': 3147, 'positively': 3148, 'possible': 3149, 'potash': 3150, 'potential': 3151, 'pottery': 3152, 'precious': 3153, 'predatory': 3154, 'preventing': 3155, 'prevention': 3156, 'prevents': 3157, 'prides': 3158, 'primarily': 3159, 'primitive': 3160, 'principals': 3161, 'printed': 3162, 'problem': 3163, 'processes': 3164, 'procreation': 3165, 'producer': 3166, 'product': 3167, 'products': 3168, 'programmed': 3169, 'progress': 3170, 'projections': 3171, 'prolonged': 3172, 'prone': 3173, 'propagate': 3174, 'prophase': 3175, 'proprioceptive': 3176, 'prosperity': 3177, 'protective': 3178, 'protist': 3179, 'protists': 3180, 'proton': 3181, 'protons': 3182, 'provides': 3183, 'pull': 3184, 'pulls': 3185, 'pulmonologist': 3186, 'pulmonologists': 3187, 'pulped': 3188, 'pupil': 3189, 'push': 3190, 'pushes': 3191, 'pyramid': 3192, 'pyrite': 3193, 'quadriceps': 3194, 'quart': 3195, 'quartz': 3196, 'rabbit': 3197, 'race': 3198, 'radial': 3199, 'radiator': 3200, 'radicals': 3201, 'radioactive': 3202, 'radiography': 3203, 'radios': 3204, 'rainwater': 3205, 'raising': 3206, 'range': 3207, 'rapidly': 3208, 'rat': 3209, 'rather': 3210, 'ravens': 3211, 'reaches': 3212, 'reactive': 3213, 'reacts': 3214, 'real': 3215, 'received': 3216, 'receives': 3217, 'receiving': 3218, 'recessive': 3219, 'recovery': 3220, 'recpetors': 3221, 'recycled': 3222, 'redness': 3223, 'reduction': 3224, 'refining': 3225, 'reflect': 3226, 'reflecting': 3227, 'reflex': 3228, 'refraction': 3229, 'regeneration': 3230, 'regulation': 3231, 'rejected': 3232, 'relationships': 3233, 'relatives': 3234, 'relax': 3235, 'remain': 3236, 'remaining': 3237, 'remarkable': 3238, 'removal': 3239, 'removing': 3240, 'renewable': 3241, 'repaired': 3242, 'repellent': 3243, 'replicating': 3244, 'reproduces': 3245, 'reptilia': 3246, 'reptilians': 3247, 'repulsive': 3248, 'research': 3249, 'resevoir': 3250, 'reshaped': 3251, 'resistor': 3252, 'resource': 3253, 'respirate': 3254, 'resting': 3255, 'retention': 3256, 'retina': 3257, 'reusing': 3258, 'reverse': 3259, 'revolutions': 3260, 'ribonucleic': 3261, 'ribosomes': 3262, 'ribs': 3263, 'ricce': 3264, 'rice': 3265, 'ripples': 3266, 'rockets': 3267, 'rodents': 3268, 'roof': 3269, 'roofs': 3270, 'rooms': 3271, 'roost': 3272, 'rotate': 3273, 'rotten': 3274, 'rough': 3275, 'rozites': 3276, 'run': 3277, 'rust': 3278, 'safely': 3279, 'sailplanes': 3280, 'saliva': 3281, 'salting': 3282, 'same': 3283, 'sandy': 3284, 'satellite': 3285, 'saving': 3286, 'scale': 3287, 'scarce': 3288, 'scattered': 3289, 'scavenged': 3290, 'scent': 3291, 'scientists': 3292, 'scraping': 3293, 'seals': 3294, 'seasonal': 3295, 'seasonally': 3296, 'seawater': 3297, 'secrete': 3298, 'secreted': 3299, 'sediments': 3300, 'seeding': 3301, 'seep': 3302, 'semen': 3303, 'semiochemicals': 3304, 'sensing': 3305, 'separatin': 3306, 'separation': 3307, 'set': 3308, 'seven': 3309, 'several': 3310, 'severely': 3311, 'shake': 3312, 'shale': 3313, 'shaped': 3314, 'shapes': 3315, 'share': 3316, 'sharing': 3317, 'shark': 3318, 'shelled': 3319, 'shine': 3320, 'ship': 3321, 'shiver': 3322, 'shoreline': 3323, 'short': 3324, 'shorten': 3325, 'shortening': 3326, 'shots': 3327, 'show': 3328, 'shrews': 3329, 'shrub': 3330, 'sickness': 3331, 'signal': 3332, 'silicate': 3333, 'sing': 3334, 'singing': 3335, 'sink': 3336, 'sinkholes': 3337, 'sinking': 3338, 'sinks': 3339, 'sizes': 3340, 'ski': 3341, 'skull': 3342, 'sky': 3343, 'sleepwalking': 3344, 'slowly': 3345, 'slows': 3346, 'smog': 3347, 'smoothing': 3348, 'smother': 3349, 'snarls': 3350, 'so': 3351, 'social': 3352, 'soda': 3353, 'softwoods': 3354, 'solids': 3355, 'solution': 3356, 'someone': 3357, 'something': 3358, 'somewhere': 3359, 'soundwaves': 3360, 'sources': 3361, 'sparrows': 3362, 'special': 3363, 'specialized': 3364, 'specific': 3365, 'spillways': 3366, 'spin': 3367, 'split': 3368, 'splitting': 3369, 'spoilage': 3370, 'spore': 3371, 'sprawl': 3372, 'spread': 3373, 'springs': 3374, 'sprout': 3375, 'spruce': 3376, 'sqeezing': 3377, 'squeeze': 3378, 'squirrels': 3379, 'stability': 3380, 'stagnant': 3381, 'stalactites': 3382, 'standing': 3383, 'starch': 3384, 'startle': 3385, 'stations': 3386, 'staying': 3387, 'stays': 3388, 'step': 3389, 'sterilization': 3390, 'steroids': 3391, 'stigma': 3392, 'stimulating': 3393, 'stomata': 3394, 'stomates': 3395, 'storage': 3396, 'stores': 3397, 'storing': 3398, 'storm': 3399, 'straight': 3400, 'straightens': 3401, 'strands': 3402, 'strawberries': 3403, 'strays': 3404, 'string': 3405, 'strong': 3406, 'struck': 3407, 'studied': 3408, 'studies': 3409, 'submarines': 3410, 'subrounded': 3411, 'substance': 3412, 'substrate': 3413, 'subtropical': 3414, 'successive': 3415, 'sugary': 3416, 'sulfate': 3417, 'sulfuric': 3418, 'supersonic': 3419, 'surge': 3420, 'surges': 3421, 'surgical': 3422, 'surrounding': 3423, 'susceptible': 3424, 'sustainable': 3425, 'sustaining': 3426, 'swallow': 3427, 'swamps': 3428, 'swea': 3429, 'sweet': 3430, 'sweeter': 3431, 'swelling': 3432, 'switch': 3433, 'symbol': 3434, 'symmetrical': 3435, 'sympathetic': 3436, 'symptoms': 3437, 'syngamy': 3438, 'synthetic': 3439, 'systolic': 3440, 'take': 3441, 'tapeworm': 3442, 'taxes': 3443, 'teats': 3444, 'technology': 3445, 'tectonics': 3446, 'televisions': 3447, 'tempting': 3448, 'termites': 3449, 'terrain': 3450, 'terrapins': 3451, 'testing': 3452, 'testosterone': 3453, 'theme': 3454, 'there': 3455, 'thermodynamically': 3456, 'thermogenesis': 3457, 'thorax': 3458, 'thousands': 3459, 'threat': 3460, 'threatened': 3461, 'threatening': 3462, 'threatens': 3463, 'thresholds': 3464, 'thunder': 3465, 'ticks': 3466, 'tidal': 3467, 'tiger': 3468, 'tight': 3469, 'tips': 3470, 'tire': 3471, 'tired': 3472, 'tires': 3473, 'toads': 3474, 'toaster': 3475, 'tongue': 3476, 'topsoil': 3477, 'tornadoes': 3478, 'tortoises': 3479, 'touch': 3480, 'tough': 3481, 'toughness': 3482, 'toxins': 3483, 'traffic': 3484, 'trains': 3485, 'transform': 3486, 'transistors': 3487, 'transition': 3488, 'transpiration': 3489, 'transported': 3490, 'trash': 3491, 'travelers': 3492, 'treatment': 3493, 'trellised': 3494, 'tremors': 3495, 'triceps': 3496, 'trisomy': 3497, 'trochophore': 3498, 'trunks': 3499, 'tsunami': 3500, 'tube': 3501, 'tubes': 3502, 'tuna': 3503, 'tunicates': 3504, 'turbine': 3505, 'uncoated': 3506, 'uncontrollably': 3507, 'underwater': 3508, 'unidirectional': 3509, 'unique': 3510, 'unisexual': 3511, 'unpredicatbale': 3512, 'unpredictable': 3513, 'unwashed': 3514, 'uranium': 3515, 'urban': 3516, 'urea': 3517, 'urethra': 3518, 'urination': 3519, 'usage': 3520, 'user': 3521, 'usually': 3522, 'vacuum': 3523, 'vagina': 3524, 'valleys': 3525, 'vanes': 3526, 'vanpooling': 3527, 'variable': 3528, 'variations': 3529, 'variety': 3530, 'vary': 3531, 'varying': 3532, 'vasoconstriction': 3533, 'vehicles': 3534, 'vein': 3535, 'velocity': 3536, 'venom': 3537, 'verbal': 3538, 'vertebrae': 3539, 'vertebrate': 3540, 'vertically': 3541, 'vesicle': 3542, 'vibrates': 3543, 'vibrational': 3544, 'video': 3545, 'vinyl': 3546, 'violent': 3547, 'virus': 3548, 'visitation': 3549, 'vital': 3550, 'vitamins': 3551, 'vocalizing': 3552, 'voice': 3553, 'volatile': 3554, 'volcanoes': 3555, 'voles': 3556, 'volkswagen': 3557, 'voltage': 3558, 'voluntary': 3559, 'vomiting': 3560, 'vultures': 3561, 'walruses': 3562, 'warblers': 3563, 'was': 3564, 'wash': 3565, 'wasps': 3566, 'wasted': 3567, 'watching': 3568, 'waterfall': 3569, 'watershed': 3570, 'watershred': 3571, 'wavelengths': 3572, 'way': 3573, 'webbed': 3574, 'wedging': 3575, 'weighs': 3576, 'weightlifting': 3577, 'wetland': 3578, 'wetter': 3579, 'what': 3580, 'while': 3581, 'whistle': 3582, 'whose': 3583, 'widely': 3584, 'widen': 3585, 'wild': 3586, 'wildfire': 3587, 'wildfires': 3588, 'windpower': 3589, 'wire': 3590, 'wires': 3591, 'wo': 3592, 'woman': 3593, 'wooly': 3594, 'working': 3595, 'workouts': 3596, 'works': 3597, 'world': 3598, 'worsened': 3599, 'would': 3600, 'year': 3601, 'yeast': 3602, 'yeasts': 3603, 'yellow': 3604, 'yields': 3605, 'young': 3606, 'zero': 3607, 'zinc': 3608, 'zones': 3609})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGyqmiNlyAiR",
        "outputId": "c8c3b86e-69b7-453b-945b-019dd762b08a"
      },
      "source": [
        "Sentence.vocab.stoi['new construction'], Label.vocab.stoi['new construction'] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsycLH0DEzFa"
      },
      "source": [
        "Load our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fen1DHy6EzFb"
      },
      "source": [
        "We'll also print out an example just to double check they're not reversed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRDnbA09EzFc"
      },
      "source": [
        "Then create our vocabulary, converting all tokens appearing less than twice into `<unk>` tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2hiioCLEzFd"
      },
      "source": [
        "Finally, define the `device` and create our iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4davOyYEzFd"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YoFk7hgzQ1h"
      },
      "source": [
        "BATCH_SIZE=128\r\n",
        "train_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train, valid), \r\n",
        "    batch_size = BATCH_SIZE, \r\n",
        "    device = device, sort=False, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeK3RzjvzZSg"
      },
      "source": [
        "iter_one = next(iter(train_iterator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cid31jyj-cuR",
        "outputId": "c7fab463-e809-4487-c903-506690a3abe0"
      },
      "source": [
        "iter_one.label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([14, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NEKYsXD0HK2"
      },
      "source": [
        "for entry in (iter_one.sentence[0][0,:]):\r\n",
        "    print(entry.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqR66FM2EzFj"
      },
      "source": [
        "## Seq2Seq Model\n",
        "\n",
        "Putting the encoder and decoder together, we get:\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq7.png?raw=1)\n",
        "\n",
        "Again, in this implementation we need to ensure the hidden dimensions in both the encoder and the decoder are the same.\n",
        "\n",
        "Briefly going over all of the steps:\n",
        "- the `outputs` tensor is created to hold all predictions, $\\hat{Y}$\n",
        "- the source sequence, $X$, is fed into the encoder to receive a `context` vector\n",
        "- the initial decoder hidden state is set to be the `context` vector, $s_0 = z = h_T$\n",
        "- we use a batch of `<sos>` tokens as the first `input`, $y_1$\n",
        "- we then decode within a loop:\n",
        "  - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and the context vector, $z$, into the decoder\n",
        "  - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n",
        "  - we then decide if we are going to teacher force or not, setting the next input as appropriate (either the ground truth next token in the target sequence or the highest predicted next token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUQMjegvDkpX"
      },
      "source": [
        "## Model Checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEs1lsz62Cbl",
        "outputId": "8711c066-00a6-4b79-b5c4-a23c0df8389c"
      },
      "source": [
        "next(iter(train_iterator_de_en)), iter_one"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\n",
              " [torchtext.data.batch.Batch of size 128 from MULTI30K]\n",
              " \t[.src]:[torch.LongTensor of size 34x128]\n",
              " \t[.trg]:[torch.LongTensor of size 35x128], \n",
              " [torchtext.data.batch.Batch of size 128]\n",
              " \t[.sentence]:('[torch.LongTensor of size 128x64]', '[torch.LongTensor of size 128]')\n",
              " \t[.label]:('[torch.LongTensor of size 128x4]', '[torch.LongTensor of size 128]'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "ox7ymRsn-ggb",
        "outputId": "59c0e070-c953-4801-a6bc-a8aedbfc23a3"
      },
      "source": [
        "iter_one.sentence[1],iter_one.sentence[0][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6fc72cdf7d19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miter_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'iter_one' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX6OvwfE_PEz"
      },
      "source": [
        "text, text_lengths = iter_one.sentence\r\n",
        "embedded = nn.Embedding(len(Sentence.vocab), 256)(text)\r\n",
        "packed_seq = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False) \r\n",
        "outputs, hidden = nn.GRU(256, 512)(packed_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aEzHD7kSPpk"
      },
      "source": [
        "outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\r\n",
        "hidden, hidden_lengths = torch.nn.utils.rnn.pad_packed_sequence(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_uQydwySoFd"
      },
      "source": [
        "hidden, outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeWWqzIRSUZX",
        "outputId": "d1dd886a-6d7c-4ec3-f22e-3991aa6532e4"
      },
      "source": [
        "hidden.shape,dec_embed.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 128, 512]), torch.Size([1, 128, 256]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPdIucFuQw3q"
      },
      "source": [
        "label_text, label_text_lengths = iter_one.label\r\n",
        "label_embedded = nn.Embedding(len(Label.vocab), 256)(label_text)\r\n",
        "label_packed_seq = nn.utils.rnn.pack_padded_sequence(label_embedded, label_text_lengths.cpu(), batch_first=False, enforce_sorted=False) \r\n",
        "label_outputs, label_hidden = nn.GRU(256, 512)(label_packed_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPfuqe19L1b1"
      },
      "source": [
        "embedded.shape, text.shape\r\n",
        "non_padded_output = nn.GRU(256, 512)(embedded)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW1jXbkCPjm5",
        "outputId": "3d985a48-c7cc-44bf-d351-616e8ff80f99"
      },
      "source": [
        "print(non_padded_output[0].shape, non_padded_output[1].shape, outputs.data.shape, hidden.shape)\r\n",
        "print(label_outputs.data.shape, label_hidden.data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 64, 512]) torch.Size([1, 64, 512]) torch.Size([4986, 512]) torch.Size([1, 128, 512])\n",
            "torch.Size([207, 512]) torch.Size([1, 128, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlshEaYtCl_l"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCqHVG8WAa_M",
        "outputId": "ea0c6931-b59d-4cf9-d8ed-7bc18bf085ca"
      },
      "source": [
        "len(Sentence.vocab),len(Label.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4344, 986)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPYVujzqBVlM"
      },
      "source": [
        "iter_de_en = next(iter(train_iterator_de_en))\r\n",
        "embedded = nn.Embedding(len(SRC.vocab), 256)(iter_de_en.src)\r\n",
        "\r\n",
        "output_deen, hidden_deen = nn.GRU(256, 512)(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jVRY76CCjsB",
        "outputId": "083d8891-a253-4332-a02e-0403eb4a4b46"
      },
      "source": [
        "hidden_deen.shape, hidden.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 128, 512]), torch.Size([1, 128, 512]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym1rJxtjDg02"
      },
      "source": [
        "label_text, label_text_lengths =iter_one.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS1L66NMDlC0",
        "outputId": "db17467f-7dd9-4863-b88f-c1da71a9c677"
      },
      "source": [
        "print(label_text.shape, label_text_lengths.shape, iter_de_en.trg.shape)\r\n",
        "label_packed_seq = nn.utils.rnn.pack_padded_sequence(label_text, label_text_lengths.cpu(), batch_first=True, enforce_sorted=False) \r\n",
        "label_packed_seq.sorted_indices.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 4]) torch.Size([128]) torch.Size([31, 128])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QuxMVCDPU91",
        "outputId": "52b07782-cab1-45ad-bb38-2187c043e592"
      },
      "source": [
        "label_text.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOr5l2TfIxg9",
        "outputId": "5ccc3ab7-928f-4cc0-a498-e0a16047c6c9"
      },
      "source": [
        "label_packed_seq.data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([207])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7D36CMHGo6G",
        "outputId": "13c4f456-7878-4916-929b-8506b961e841"
      },
      "source": [
        "iter_de_en.src.shape, iter_de_en.trg.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([36, 128]), torch.Size([31, 128]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i9hoRWSGM31"
      },
      "source": [
        "label_text_reshaped =  label_text.reshape(4, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuQ5G0vJHRIS",
        "outputId": "229d5e15-50d8-419c-a5f4-cfba316cad1f"
      },
      "source": [
        "input_to_decoder.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGxidG4pHAY4"
      },
      "source": [
        "# input_to_decoder = label_text_reshaped[0,:]\r\n",
        "# input_to_decoder = input_to_decoder.unsqueeze(0)\r\n",
        "\r\n",
        "# INPUT_DIM = len(Sentence.vocab)\r\n",
        "# OUTPUT_DIM = len(Label.vocab)\r\n",
        "# ENC_EMB_DIM = 256\r\n",
        "# DEC_EMB_DIM = 256\r\n",
        "# HID_DIM = 512\r\n",
        "# ENC_DROPOUT = 0.5\r\n",
        "# DEC_DROPOUT = 0.5\r\n",
        "\r\n",
        "# enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\r\n",
        "# dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\r\n",
        "\r\n",
        "\r\n",
        "dec_embed = nn.Embedding(len(Label.vocab), 256)(input_to_decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwjR8rsPMgc7"
      },
      "source": [
        "dec_embed.shape\r\n",
        "dec_output, dec_hidden = nn.GRU(256, 512)(dec_embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX-2KkaNNqN_",
        "outputId": "f79c601f-b1dd-4692-a2e0-0422656da0cd"
      },
      "source": [
        "dec_embed.squeeze(0).shape,hidden.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([128, 256]), torch.Size([1, 128, 512]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0b0fGhONhXw",
        "outputId": "17f6412b-1fc0-4908-e60d-01338f146ea8"
      },
      "source": [
        "torch.cat((dec_embed, hidden), dim=2).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX8T2msbNDqu",
        "outputId": "a0a3b080-b3b9-4808-f7be-768cfdc388fa"
      },
      "source": [
        "dec_output.shape, dec_hidden.shape, hidden.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 128, 512]),\n",
              " torch.Size([1, 128, 512]),\n",
              " torch.Size([1, 128, 512]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUTUQnTBFcn9"
      },
      "source": [
        "for entry in label_packed_seq.data:\r\n",
        "    print(Label.vocab.itos[entry.item()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSH6CKKzCw3l"
      },
      "source": [
        "iter_de_en.trg[0,:].shape, iter_one.label[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT00q_JvjdGE",
        "outputId": "f16af9bb-44fb-4412-a7a4-6c317a2d87f4"
      },
      "source": [
        "label_text.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo0QowlGj1LI",
        "outputId": "7c08fd81-7c4c-4715-8840-4f39b92cb7c7"
      },
      "source": [
        "label_text_reshaped[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([196,  36,   3,   1, 393,   1,   1,   1,  35,  82,   1,   1, 971,   1,\n",
              "          1,   1, 888,   1,   1,   1,  28,   1,   1,   1,  17, 300, 551,   1,\n",
              "        791,  21,   1,   1, 605,   1,   1,   1, 215, 304,   1,   1,  58,  11,\n",
              "          5,   1, 428,  52,   1,   1, 108,  64,   1,   1, 705,   1,   1,   1,\n",
              "        549,   1,   1,   1,  51,   3,  16,   1, 148, 701,   1,   1,   6,   1,\n",
              "          1,   1,  44,   1,   1,   1,  12,   1,   1,   1, 410,   1,   1,   1,\n",
              "        795, 772,   1,   1, 688,   1,   1,   1, 611,   1,   1,   1, 592, 719,\n",
              "          1,   1, 188,   1,   1,   1,  34, 200, 497,   1, 563, 857,   1,   1,\n",
              "        601,   1,   1,   1, 771, 951,   1,   1, 665, 974,   1,   1,  25,   1,\n",
              "          1,   1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11CnDhTkEzFd"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        text, text_lengths = src\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # packed_seq = nn.utils.rnn.pack_padded_sequence(embedded, \n",
        "        #                                                text_lengths.cpu(),\n",
        "        #                                                batch_first=False,\n",
        "        #                                                enforce_sorted=False)        \n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded) #no cell state! \n",
        "        ## outputs is a packed sequence but since it is not used we will not \n",
        "        ## unpack it         \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Hb0dXWHLsG"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRA8hkiLEzFh"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded) #no cell state!\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, context):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #context = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        #context = [1, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # print(embedded.shape, hidden.shape, context.shape)\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "            \n",
        "        #emb_con = [1, batch size, emb dim + hid dim]\n",
        "            \n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \n",
        "                           dim = 1)\n",
        "        \n",
        "        #output = [batch size, emb dim + hid dim * 2]\n",
        "        \n",
        "        prediction = self.fc_out(output)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDyNrQ8VEzFk"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is the context\n",
        "        context = self.encoder(src)\n",
        "        \n",
        "        #context also used as the initial hidden state of the decoder\n",
        "        hidden = context\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state and the context state\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDG6jOSuEzFk"
      },
      "source": [
        "# INPUT_DIM = len(SRC.vocab)\n",
        "# OUTPUT_DIM = len(TRG.vocab)\n",
        "\n",
        "INPUT_DIM = len(Sentence.vocab)\n",
        "OUTPUT_DIM = len(Label.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 256\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIc1l6CEzFk"
      },
      "source": [
        "Next, we initialize our parameters. The paper states the parameters are initialized from a normal distribution with a mean of 0 and a standard deviation of 0.01, i.e. $\\mathcal{N}(0, 0.01)$. \n",
        "\n",
        "It also states we should initialize the recurrent parameters to a special initialization, however to keep things simple we'll also initialize them to $\\mathcal{N}(0, 0.01)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgqMqq-oEzFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720656f1-3662-47b1-dc2b-07e022e230dd"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(8645, 256)\n",
              "    (rnn): GRU(256, 256)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(3482, 256)\n",
              "    (rnn): GRU(512, 256)\n",
              "    (fc_out): Linear(in_features=768, out_features=3482, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IggCwIBgEzFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa4d50f-490f-4b17-ee9d-7fa7fa912d6a"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 6,768,282 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiH54qzHEzFl"
      },
      "source": [
        "We initiaize our optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO1_eoG7EzFl"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR, OneCycleLR, MultiStepLR, CyclicLR, ReduceLROnPlateau\r\n",
        "\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0009893272, weight_decay=0.0001)\r\n",
        "scheduler = OneCycleLR(optimizer, \r\n",
        "                        0.001, \r\n",
        "                        epochs=100, \r\n",
        "                        cycle_momentum=False, \r\n",
        "                        steps_per_epoch=len(train_iterator), \r\n",
        "                        #base_momentum=config.momentum,\r\n",
        "                        #max_momentum=0.95, \r\n",
        "                        pct_start=0.208,\r\n",
        "                        # anneal_strategy=config.anneal_strategy,\r\n",
        "                        div_factor=100,\r\n",
        "                        # final_div_factor=config.final_div_factor\r\n",
        "                        )\r\n",
        "# 0.000981989011942079"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwkV3FKlEzFl"
      },
      "source": [
        "We also initialize the loss function, making sure to ignore the loss on `<pad>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0DAbGbcEzFl"
      },
      "source": [
        "#TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "TRG_PAD_IDX = Label.vocab.stoi[Label.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTay7E9rEzFm"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OYuoFdEzFm"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.sentence\n",
        "        trg = batch.label\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        # print(\"output_dim:\", output.shape)\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        # print(\"output_dim before loss:\", output.shape, trg.shape)\n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEAia1wGvv9K",
        "outputId": "96ef3473-a60b-4568-a088-e9ba0437f088"
      },
      "source": [
        "iter_one.label[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfey_MRivGVO"
      },
      "source": [
        "iter_one = next(iter(train_iterator))\r\n",
        "iter_one.label[0][1:].view(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rfUx5lhEzFm"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw022pw0EzFm"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            # src = batch.src\n",
        "            # trg = batch.trg\n",
        "            src = batch.sentence\n",
        "            trg = batch.label\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E43h8dnQEzFm"
      },
      "source": [
        "We'll also define the function that calculates how long an epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTAmu3-EEzFm"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_cbEVPWadt5",
        "outputId": "98bf9688-4cd4-4e98-aa47-bffe9fd9fba3"
      },
      "source": [
        "optimizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    eps: 1e-08\n",
              "    initial_lr: 1e-06\n",
              "    lr: 9.999999999999159e-07\n",
              "    max_lr: 0.001\n",
              "    min_lr: 9.999999999999999e-11\n",
              "    weight_decay: 0.0001\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbCGyO4ZEzFm"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5fzUqG-nkhn"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjFyRUK9EzFm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64a0ef95-a99b-40a2-aa94-0d17822383b8"
      },
      "source": [
        "N_EPOCHS = 100\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "# model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\n",
        "    \n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | LR: {scheduler.get_last_lr()}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 1s\n",
            "\tTrain Loss: 8.154 | Train PPL: 3478.227 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.150 |  Val. PPL: 3462.884\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 8.154 | Train PPL: 3475.632 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.149 |  Val. PPL: 3460.940\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 8.153 | Train PPL: 3472.959 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.149 |  Val. PPL: 3458.951\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 8.152 | Train PPL: 3470.415 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.148 |  Val. PPL: 3456.900\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 8.151 | Train PPL: 3467.501 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.147 |  Val. PPL: 3454.503\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 8.150 | Train PPL: 3464.282 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.147 |  Val. PPL: 3451.783\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 8.149 | Train PPL: 3460.747 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.146 |  Val. PPL: 3448.774\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 8.148 | Train PPL: 3456.685 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.145 |  Val. PPL: 3445.484\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 8.147 | Train PPL: 3451.996 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.144 |  Val. PPL: 3441.612\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 8.145 | Train PPL: 3445.687 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.142 |  Val. PPL: 3435.892\n",
            "Epoch: 11 | Time: 0m 1s\n",
            "\tTrain Loss: 8.142 | Train PPL: 3437.365 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.140 |  Val. PPL: 3428.326\n",
            "Epoch: 12 | Time: 0m 1s\n",
            "\tTrain Loss: 8.139 | Train PPL: 3426.367 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.137 |  Val. PPL: 3417.886\n",
            "Epoch: 13 | Time: 0m 1s\n",
            "\tTrain Loss: 8.135 | Train PPL: 3410.887 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.132 |  Val. PPL: 3403.131\n",
            "Epoch: 14 | Time: 0m 1s\n",
            "\tTrain Loss: 8.128 | Train PPL: 3388.248 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.126 |  Val. PPL: 3381.324\n",
            "Epoch: 15 | Time: 0m 1s\n",
            "\tTrain Loss: 8.118 | Train PPL: 3353.692 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.116 |  Val. PPL: 3347.555\n",
            "Epoch: 16 | Time: 0m 1s\n",
            "\tTrain Loss: 8.101 | Train PPL: 3298.515 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.099 |  Val. PPL: 3292.434\n",
            "Epoch: 17 | Time: 0m 1s\n",
            "\tTrain Loss: 8.072 | Train PPL: 3203.029 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.070 |  Val. PPL: 3195.631\n",
            "Epoch: 18 | Time: 0m 1s\n",
            "\tTrain Loss: 8.015 | Train PPL: 3025.307 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 8.009 |  Val. PPL: 3006.805\n",
            "Epoch: 19 | Time: 0m 1s\n",
            "\tTrain Loss: 7.879 | Train PPL: 2640.667 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.849 |  Val. PPL: 2562.113\n",
            "Epoch: 20 | Time: 0m 1s\n",
            "\tTrain Loss: 7.496 | Train PPL: 1801.263 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.487 |  Val. PPL: 1784.501\n",
            "Epoch: 21 | Time: 0m 1s\n",
            "\tTrain Loss: 7.039 | Train PPL: 1140.155 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.396 |  Val. PPL: 1630.185\n",
            "Epoch: 22 | Time: 0m 1s\n",
            "\tTrain Loss: 6.927 | Train PPL: 1019.897 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.409 |  Val. PPL: 1650.238\n",
            "Epoch: 23 | Time: 0m 1s\n",
            "\tTrain Loss: 6.892 | Train PPL: 984.821 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.433 |  Val. PPL: 1690.646\n",
            "Epoch: 24 | Time: 0m 1s\n",
            "\tTrain Loss: 6.876 | Train PPL: 969.177 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.457 |  Val. PPL: 1731.603\n",
            "Epoch: 25 | Time: 0m 1s\n",
            "\tTrain Loss: 6.867 | Train PPL: 960.199 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.478 |  Val. PPL: 1769.378\n",
            "Epoch: 26 | Time: 0m 1s\n",
            "\tTrain Loss: 6.861 | Train PPL: 954.008 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.497 |  Val. PPL: 1802.586\n",
            "Epoch: 27 | Time: 0m 1s\n",
            "\tTrain Loss: 6.856 | Train PPL: 949.303 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.513 |  Val. PPL: 1832.606\n",
            "Epoch: 28 | Time: 0m 1s\n",
            "\tTrain Loss: 6.852 | Train PPL: 945.443 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.528 |  Val. PPL: 1858.872\n",
            "Epoch: 29 | Time: 0m 1s\n",
            "\tTrain Loss: 6.848 | Train PPL: 942.093 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.540 |  Val. PPL: 1882.407\n",
            "Epoch: 30 | Time: 0m 1s\n",
            "\tTrain Loss: 6.845 | Train PPL: 939.213 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.552 |  Val. PPL: 1903.602\n",
            "Epoch: 31 | Time: 0m 1s\n",
            "\tTrain Loss: 6.842 | Train PPL: 936.590 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.562 |  Val. PPL: 1922.767\n",
            "Epoch: 32 | Time: 0m 1s\n",
            "\tTrain Loss: 6.840 | Train PPL: 934.210 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.571 |  Val. PPL: 1940.156\n",
            "Epoch: 33 | Time: 0m 1s\n",
            "\tTrain Loss: 6.837 | Train PPL: 932.027 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.579 |  Val. PPL: 1955.974\n",
            "Epoch: 34 | Time: 0m 1s\n",
            "\tTrain Loss: 6.835 | Train PPL: 929.920 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.586 |  Val. PPL: 1970.411\n",
            "Epoch: 35 | Time: 0m 1s\n",
            "\tTrain Loss: 6.833 | Train PPL: 928.023 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.593 |  Val. PPL: 1983.609\n",
            "Epoch: 36 | Time: 0m 1s\n",
            "\tTrain Loss: 6.831 | Train PPL: 926.146 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.599 |  Val. PPL: 1995.708\n",
            "Epoch: 37 | Time: 0m 1s\n",
            "\tTrain Loss: 6.829 | Train PPL: 924.464 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.604 |  Val. PPL: 2006.801\n",
            "Epoch: 38 | Time: 0m 1s\n",
            "\tTrain Loss: 6.827 | Train PPL: 922.790 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.609 |  Val. PPL: 2016.998\n",
            "Epoch: 39 | Time: 0m 1s\n",
            "\tTrain Loss: 6.826 | Train PPL: 921.168 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.614 |  Val. PPL: 2026.381\n",
            "Epoch: 40 | Time: 0m 1s\n",
            "\tTrain Loss: 6.824 | Train PPL: 919.606 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.618 |  Val. PPL: 2035.036\n",
            "Epoch: 41 | Time: 0m 1s\n",
            "\tTrain Loss: 6.822 | Train PPL: 918.047 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.622 |  Val. PPL: 2042.815\n",
            "Epoch: 42 | Time: 0m 1s\n",
            "\tTrain Loss: 6.821 | Train PPL: 916.615 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.626 |  Val. PPL: 2049.964\n",
            "Epoch: 43 | Time: 0m 1s\n",
            "\tTrain Loss: 6.819 | Train PPL: 915.196 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.629 |  Val. PPL: 2056.797\n",
            "Epoch: 44 | Time: 0m 1s\n",
            "\tTrain Loss: 6.818 | Train PPL: 913.790 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.632 |  Val. PPL: 2063.127\n",
            "Epoch: 45 | Time: 0m 1s\n",
            "\tTrain Loss: 6.816 | Train PPL: 912.367 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.635 |  Val. PPL: 2068.987\n",
            "Epoch: 46 | Time: 0m 1s\n",
            "\tTrain Loss: 6.815 | Train PPL: 911.012 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.637 |  Val. PPL: 2074.424\n",
            "Epoch: 47 | Time: 0m 1s\n",
            "\tTrain Loss: 6.813 | Train PPL: 909.666 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.640 |  Val. PPL: 2079.485\n",
            "Epoch: 48 | Time: 0m 1s\n",
            "\tTrain Loss: 6.812 | Train PPL: 908.285 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.642 |  Val. PPL: 2084.191\n",
            "Epoch: 49 | Time: 0m 1s\n",
            "\tTrain Loss: 6.810 | Train PPL: 907.040 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.644 |  Val. PPL: 2088.568\n",
            "Epoch: 50 | Time: 0m 1s\n",
            "\tTrain Loss: 6.809 | Train PPL: 905.734 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.646 |  Val. PPL: 2092.656\n",
            "Epoch: 51 | Time: 0m 1s\n",
            "\tTrain Loss: 6.807 | Train PPL: 904.421 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.648 |  Val. PPL: 2096.477\n",
            "Epoch: 52 | Time: 0m 1s\n",
            "\tTrain Loss: 6.806 | Train PPL: 903.181 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.650 |  Val. PPL: 2100.054\n",
            "Epoch: 53 | Time: 0m 1s\n",
            "\tTrain Loss: 6.804 | Train PPL: 901.868 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.651 |  Val. PPL: 2103.407\n",
            "Epoch: 54 | Time: 0m 1s\n",
            "\tTrain Loss: 6.803 | Train PPL: 900.688 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.653 |  Val. PPL: 2106.565\n",
            "Epoch: 55 | Time: 0m 1s\n",
            "\tTrain Loss: 6.802 | Train PPL: 899.410 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.654 |  Val. PPL: 2109.378\n",
            "Epoch: 56 | Time: 0m 1s\n",
            "\tTrain Loss: 6.800 | Train PPL: 898.185 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.655 |  Val. PPL: 2112.189\n",
            "Epoch: 57 | Time: 0m 1s\n",
            "\tTrain Loss: 6.799 | Train PPL: 896.934 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.657 |  Val. PPL: 2114.844\n",
            "Epoch: 58 | Time: 0m 1s\n",
            "\tTrain Loss: 6.798 | Train PPL: 895.724 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.658 |  Val. PPL: 2117.369\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-c4871709d5f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-6dec24c3e9a3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYIRZh9vLH_4",
        "outputId": "adf81b69-079e-4034-c29c-5e92889817fa"
      },
      "source": [
        "best_valid_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.396448850631714"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXHg1YUjEnXz",
        "outputId": "bfec0152-958f-4b41-adfe-73c128e571ae"
      },
      "source": [
        "N_EPOCHS = 100\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = 7.396448850631714\r\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 1s\n",
            "\tTrain Loss: 6.949 | Train PPL: 1041.755\n",
            "\t Val. Loss: 7.394 |  Val. PPL: 1626.104\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 6.946 | Train PPL: 1038.512\n",
            "\t Val. Loss: 7.392 |  Val. PPL: 1623.474\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 6.943 | Train PPL: 1035.957\n",
            "\t Val. Loss: 7.391 |  Val. PPL: 1620.960\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 6.941 | Train PPL: 1033.612\n",
            "\t Val. Loss: 7.389 |  Val. PPL: 1618.544\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 6.939 | Train PPL: 1031.422\n",
            "\t Val. Loss: 7.388 |  Val. PPL: 1616.215\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 6.937 | Train PPL: 1029.345\n",
            "\t Val. Loss: 7.386 |  Val. PPL: 1613.964\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 6.935 | Train PPL: 1027.347\n",
            "\t Val. Loss: 7.385 |  Val. PPL: 1611.787\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 6.933 | Train PPL: 1025.482\n",
            "\t Val. Loss: 7.384 |  Val. PPL: 1609.675\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 6.931 | Train PPL: 1023.749\n",
            "\t Val. Loss: 7.383 |  Val. PPL: 1607.624\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 6.930 | Train PPL: 1022.038\n",
            "\t Val. Loss: 7.381 |  Val. PPL: 1605.628\n",
            "Epoch: 11 | Time: 0m 1s\n",
            "\tTrain Loss: 6.928 | Train PPL: 1020.454\n",
            "\t Val. Loss: 7.380 |  Val. PPL: 1603.723\n",
            "Epoch: 12 | Time: 0m 1s\n",
            "\tTrain Loss: 6.927 | Train PPL: 1018.926\n",
            "\t Val. Loss: 7.379 |  Val. PPL: 1601.871\n",
            "Epoch: 13 | Time: 0m 1s\n",
            "\tTrain Loss: 6.925 | Train PPL: 1017.478\n",
            "\t Val. Loss: 7.378 |  Val. PPL: 1600.038\n",
            "Epoch: 14 | Time: 0m 1s\n",
            "\tTrain Loss: 6.924 | Train PPL: 1016.074\n",
            "\t Val. Loss: 7.377 |  Val. PPL: 1598.267\n",
            "Epoch: 15 | Time: 0m 1s\n",
            "\tTrain Loss: 6.922 | Train PPL: 1014.804\n",
            "\t Val. Loss: 7.376 |  Val. PPL: 1596.579\n",
            "Epoch: 16 | Time: 0m 1s\n",
            "\tTrain Loss: 6.921 | Train PPL: 1013.553\n",
            "\t Val. Loss: 7.375 |  Val. PPL: 1594.879\n",
            "Epoch: 17 | Time: 0m 1s\n",
            "\tTrain Loss: 6.920 | Train PPL: 1012.388\n",
            "\t Val. Loss: 7.374 |  Val. PPL: 1593.218\n",
            "Epoch: 18 | Time: 0m 1s\n",
            "\tTrain Loss: 6.919 | Train PPL: 1011.245\n",
            "\t Val. Loss: 7.372 |  Val. PPL: 1591.597\n",
            "Epoch: 19 | Time: 0m 1s\n",
            "\tTrain Loss: 6.918 | Train PPL: 1010.136\n",
            "\t Val. Loss: 7.372 |  Val. PPL: 1590.020\n",
            "Epoch: 20 | Time: 0m 1s\n",
            "\tTrain Loss: 6.917 | Train PPL: 1009.134\n",
            "\t Val. Loss: 7.371 |  Val. PPL: 1588.545\n",
            "Epoch: 21 | Time: 0m 1s\n",
            "\tTrain Loss: 6.916 | Train PPL: 1008.115\n",
            "\t Val. Loss: 7.370 |  Val. PPL: 1587.029\n",
            "Epoch: 22 | Time: 0m 1s\n",
            "\tTrain Loss: 6.915 | Train PPL: 1007.118\n",
            "\t Val. Loss: 7.369 |  Val. PPL: 1585.544\n",
            "Epoch: 23 | Time: 0m 1s\n",
            "\tTrain Loss: 6.914 | Train PPL: 1006.271\n",
            "\t Val. Loss: 7.368 |  Val. PPL: 1584.091\n",
            "Epoch: 24 | Time: 0m 1s\n",
            "\tTrain Loss: 6.913 | Train PPL: 1005.346\n",
            "\t Val. Loss: 7.367 |  Val. PPL: 1582.666\n",
            "Epoch: 25 | Time: 0m 1s\n",
            "\tTrain Loss: 6.912 | Train PPL: 1004.519\n",
            "\t Val. Loss: 7.366 |  Val. PPL: 1581.271\n",
            "Epoch: 26 | Time: 0m 1s\n",
            "\tTrain Loss: 6.911 | Train PPL: 1003.719\n",
            "\t Val. Loss: 7.365 |  Val. PPL: 1579.904\n",
            "Epoch: 27 | Time: 0m 1s\n",
            "\tTrain Loss: 6.911 | Train PPL: 1002.938\n",
            "\t Val. Loss: 7.364 |  Val. PPL: 1578.563\n",
            "Epoch: 28 | Time: 0m 1s\n",
            "\tTrain Loss: 6.910 | Train PPL: 1002.199\n",
            "\t Val. Loss: 7.363 |  Val. PPL: 1577.248\n",
            "Epoch: 29 | Time: 0m 1s\n",
            "\tTrain Loss: 6.909 | Train PPL: 1001.471\n",
            "\t Val. Loss: 7.363 |  Val. PPL: 1575.958\n",
            "Epoch: 30 | Time: 0m 1s\n",
            "\tTrain Loss: 6.909 | Train PPL: 1000.765\n",
            "\t Val. Loss: 7.362 |  Val. PPL: 1574.691\n",
            "Epoch: 31 | Time: 0m 1s\n",
            "\tTrain Loss: 6.908 | Train PPL: 1000.136\n",
            "\t Val. Loss: 7.361 |  Val. PPL: 1573.448\n",
            "Epoch: 32 | Time: 0m 1s\n",
            "\tTrain Loss: 6.907 | Train PPL: 999.480\n",
            "\t Val. Loss: 7.360 |  Val. PPL: 1572.226\n",
            "Epoch: 33 | Time: 0m 1s\n",
            "\tTrain Loss: 6.907 | Train PPL: 998.946\n",
            "\t Val. Loss: 7.359 |  Val. PPL: 1571.022\n",
            "Epoch: 34 | Time: 0m 1s\n",
            "\tTrain Loss: 6.906 | Train PPL: 998.309\n",
            "\t Val. Loss: 7.359 |  Val. PPL: 1569.851\n",
            "Epoch: 35 | Time: 0m 1s\n",
            "\tTrain Loss: 6.905 | Train PPL: 997.708\n",
            "\t Val. Loss: 7.358 |  Val. PPL: 1568.666\n",
            "Epoch: 36 | Time: 0m 1s\n",
            "\tTrain Loss: 6.905 | Train PPL: 997.174\n",
            "\t Val. Loss: 7.357 |  Val. PPL: 1567.514\n",
            "Epoch: 37 | Time: 0m 1s\n",
            "\tTrain Loss: 6.904 | Train PPL: 996.676\n",
            "\t Val. Loss: 7.357 |  Val. PPL: 1566.389\n",
            "Epoch: 38 | Time: 0m 1s\n",
            "\tTrain Loss: 6.904 | Train PPL: 996.123\n",
            "\t Val. Loss: 7.356 |  Val. PPL: 1565.286\n",
            "Epoch: 39 | Time: 0m 1s\n",
            "\tTrain Loss: 6.903 | Train PPL: 995.691\n",
            "\t Val. Loss: 7.355 |  Val. PPL: 1564.200\n",
            "Epoch: 40 | Time: 0m 1s\n",
            "\tTrain Loss: 6.903 | Train PPL: 995.235\n",
            "\t Val. Loss: 7.354 |  Val. PPL: 1563.133\n",
            "Epoch: 41 | Time: 0m 1s\n",
            "\tTrain Loss: 6.903 | Train PPL: 994.775\n",
            "\t Val. Loss: 7.354 |  Val. PPL: 1562.082\n",
            "Epoch: 42 | Time: 0m 1s\n",
            "\tTrain Loss: 6.902 | Train PPL: 994.322\n",
            "\t Val. Loss: 7.353 |  Val. PPL: 1561.048\n",
            "Epoch: 43 | Time: 0m 1s\n",
            "\tTrain Loss: 6.902 | Train PPL: 993.905\n",
            "\t Val. Loss: 7.352 |  Val. PPL: 1560.028\n",
            "Epoch: 44 | Time: 0m 1s\n",
            "\tTrain Loss: 6.901 | Train PPL: 993.545\n",
            "\t Val. Loss: 7.352 |  Val. PPL: 1559.024\n",
            "Epoch: 45 | Time: 0m 1s\n",
            "\tTrain Loss: 6.901 | Train PPL: 993.111\n",
            "\t Val. Loss: 7.351 |  Val. PPL: 1558.037\n",
            "Epoch: 46 | Time: 0m 1s\n",
            "\tTrain Loss: 6.900 | Train PPL: 992.741\n",
            "\t Val. Loss: 7.351 |  Val. PPL: 1557.063\n",
            "Epoch: 47 | Time: 0m 1s\n",
            "\tTrain Loss: 6.900 | Train PPL: 992.401\n",
            "\t Val. Loss: 7.350 |  Val. PPL: 1556.103\n",
            "Epoch: 48 | Time: 0m 1s\n",
            "\tTrain Loss: 6.900 | Train PPL: 992.022\n",
            "\t Val. Loss: 7.349 |  Val. PPL: 1555.159\n",
            "Epoch: 49 | Time: 0m 1s\n",
            "\tTrain Loss: 6.899 | Train PPL: 991.643\n",
            "\t Val. Loss: 7.349 |  Val. PPL: 1554.228\n",
            "Epoch: 50 | Time: 0m 1s\n",
            "\tTrain Loss: 6.899 | Train PPL: 991.365\n",
            "\t Val. Loss: 7.348 |  Val. PPL: 1553.310\n",
            "Epoch: 51 | Time: 0m 1s\n",
            "\tTrain Loss: 6.899 | Train PPL: 991.022\n",
            "\t Val. Loss: 7.348 |  Val. PPL: 1552.405\n",
            "Epoch: 52 | Time: 0m 1s\n",
            "\tTrain Loss: 6.898 | Train PPL: 990.708\n",
            "\t Val. Loss: 7.347 |  Val. PPL: 1551.514\n",
            "Epoch: 53 | Time: 0m 1s\n",
            "\tTrain Loss: 6.898 | Train PPL: 990.432\n",
            "\t Val. Loss: 7.346 |  Val. PPL: 1550.635\n",
            "Epoch: 54 | Time: 0m 1s\n",
            "\tTrain Loss: 6.898 | Train PPL: 990.168\n",
            "\t Val. Loss: 7.346 |  Val. PPL: 1549.767\n",
            "Epoch: 55 | Time: 0m 1s\n",
            "\tTrain Loss: 6.898 | Train PPL: 989.825\n",
            "\t Val. Loss: 7.345 |  Val. PPL: 1548.913\n",
            "Epoch: 56 | Time: 0m 1s\n",
            "\tTrain Loss: 6.897 | Train PPL: 989.583\n",
            "\t Val. Loss: 7.345 |  Val. PPL: 1548.069\n",
            "Epoch: 57 | Time: 0m 1s\n",
            "\tTrain Loss: 6.897 | Train PPL: 989.328\n",
            "\t Val. Loss: 7.344 |  Val. PPL: 1547.238\n",
            "Epoch: 58 | Time: 0m 1s\n",
            "\tTrain Loss: 6.897 | Train PPL: 989.066\n",
            "\t Val. Loss: 7.344 |  Val. PPL: 1546.417\n",
            "Epoch: 59 | Time: 0m 1s\n",
            "\tTrain Loss: 6.897 | Train PPL: 988.813\n",
            "\t Val. Loss: 7.343 |  Val. PPL: 1545.609\n",
            "Epoch: 60 | Time: 0m 1s\n",
            "\tTrain Loss: 6.896 | Train PPL: 988.552\n",
            "\t Val. Loss: 7.343 |  Val. PPL: 1544.811\n",
            "Epoch: 61 | Time: 0m 1s\n",
            "\tTrain Loss: 6.896 | Train PPL: 988.348\n",
            "\t Val. Loss: 7.342 |  Val. PPL: 1544.024\n",
            "Epoch: 62 | Time: 0m 1s\n",
            "\tTrain Loss: 6.896 | Train PPL: 988.082\n",
            "\t Val. Loss: 7.342 |  Val. PPL: 1543.248\n",
            "Epoch: 63 | Time: 0m 1s\n",
            "\tTrain Loss: 6.896 | Train PPL: 987.900\n",
            "\t Val. Loss: 7.341 |  Val. PPL: 1542.483\n",
            "Epoch: 64 | Time: 0m 1s\n",
            "\tTrain Loss: 6.895 | Train PPL: 987.675\n",
            "\t Val. Loss: 7.341 |  Val. PPL: 1541.726\n",
            "Epoch: 65 | Time: 0m 1s\n",
            "\tTrain Loss: 6.895 | Train PPL: 987.470\n",
            "\t Val. Loss: 7.340 |  Val. PPL: 1540.981\n",
            "Epoch: 66 | Time: 0m 1s\n",
            "\tTrain Loss: 6.895 | Train PPL: 987.250\n",
            "\t Val. Loss: 7.340 |  Val. PPL: 1540.246\n",
            "Epoch: 67 | Time: 0m 1s\n",
            "\tTrain Loss: 6.895 | Train PPL: 987.092\n",
            "\t Val. Loss: 7.339 |  Val. PPL: 1539.520\n",
            "Epoch: 68 | Time: 0m 1s\n",
            "\tTrain Loss: 6.895 | Train PPL: 986.869\n",
            "\t Val. Loss: 7.339 |  Val. PPL: 1538.804\n",
            "Epoch: 69 | Time: 0m 1s\n",
            "\tTrain Loss: 6.894 | Train PPL: 986.673\n",
            "\t Val. Loss: 7.338 |  Val. PPL: 1538.098\n",
            "Epoch: 70 | Time: 0m 1s\n",
            "\tTrain Loss: 6.894 | Train PPL: 986.500\n",
            "\t Val. Loss: 7.338 |  Val. PPL: 1537.401\n",
            "Epoch: 71 | Time: 0m 1s\n",
            "\tTrain Loss: 6.894 | Train PPL: 986.318\n",
            "\t Val. Loss: 7.337 |  Val. PPL: 1536.713\n",
            "Epoch: 72 | Time: 0m 1s\n",
            "\tTrain Loss: 6.894 | Train PPL: 986.156\n",
            "\t Val. Loss: 7.337 |  Val. PPL: 1536.036\n",
            "Epoch: 73 | Time: 0m 1s\n",
            "\tTrain Loss: 6.894 | Train PPL: 985.957\n",
            "\t Val. Loss: 7.337 |  Val. PPL: 1535.365\n",
            "Epoch: 74 | Time: 0m 1s\n",
            "\tTrain Loss: 6.893 | Train PPL: 985.799\n",
            "\t Val. Loss: 7.336 |  Val. PPL: 1534.706\n",
            "Epoch: 75 | Time: 0m 1s\n",
            "\tTrain Loss: 6.893 | Train PPL: 985.669\n",
            "\t Val. Loss: 7.336 |  Val. PPL: 1534.053\n",
            "Epoch: 76 | Time: 0m 1s\n",
            "\tTrain Loss: 6.893 | Train PPL: 985.482\n",
            "\t Val. Loss: 7.335 |  Val. PPL: 1533.411\n",
            "Epoch: 77 | Time: 0m 1s\n",
            "\tTrain Loss: 6.893 | Train PPL: 985.340\n",
            "\t Val. Loss: 7.335 |  Val. PPL: 1532.777\n",
            "Epoch: 78 | Time: 0m 1s\n",
            "\tTrain Loss: 6.893 | Train PPL: 985.188\n",
            "\t Val. Loss: 7.334 |  Val. PPL: 1532.152\n",
            "Epoch: 79 | Time: 0m 1s\n",
            "\tTrain Loss: 6.893 | Train PPL: 985.024\n",
            "\t Val. Loss: 7.334 |  Val. PPL: 1531.534\n",
            "Epoch: 80 | Time: 0m 1s\n",
            "\tTrain Loss: 6.892 | Train PPL: 984.858\n",
            "\t Val. Loss: 7.334 |  Val. PPL: 1530.926\n",
            "Epoch: 81 | Time: 0m 1s\n",
            "\tTrain Loss: 6.892 | Train PPL: 984.706\n",
            "\t Val. Loss: 7.333 |  Val. PPL: 1530.326\n",
            "Epoch: 82 | Time: 0m 1s\n",
            "\tTrain Loss: 6.892 | Train PPL: 984.585\n",
            "\t Val. Loss: 7.333 |  Val. PPL: 1529.732\n",
            "Epoch: 83 | Time: 0m 1s\n",
            "\tTrain Loss: 6.892 | Train PPL: 984.458\n",
            "\t Val. Loss: 7.332 |  Val. PPL: 1529.141\n",
            "Epoch: 84 | Time: 0m 1s\n",
            "\tTrain Loss: 6.892 | Train PPL: 984.330\n",
            "\t Val. Loss: 7.332 |  Val. PPL: 1528.582\n",
            "Epoch: 85 | Time: 0m 1s\n",
            "\tTrain Loss: 6.892 | Train PPL: 984.154\n",
            "\t Val. Loss: 7.332 |  Val. PPL: 1528.151\n",
            "Epoch: 86 | Time: 0m 1s\n",
            "\tTrain Loss: 6.892 | Train PPL: 984.028\n",
            "\t Val. Loss: 7.332 |  Val. PPL: 1527.712\n",
            "Epoch: 87 | Time: 0m 1s\n",
            "\tTrain Loss: 6.892 | Train PPL: 983.933\n",
            "\t Val. Loss: 7.331 |  Val. PPL: 1527.189\n",
            "Epoch: 88 | Time: 0m 1s\n",
            "\tTrain Loss: 6.891 | Train PPL: 983.811\n",
            "\t Val. Loss: 7.331 |  Val. PPL: 1526.666\n",
            "Epoch: 89 | Time: 0m 1s\n",
            "\tTrain Loss: 6.891 | Train PPL: 983.683\n",
            "\t Val. Loss: 7.330 |  Val. PPL: 1526.124\n",
            "Epoch: 90 | Time: 0m 1s\n",
            "\tTrain Loss: 6.891 | Train PPL: 983.535\n",
            "\t Val. Loss: 7.330 |  Val. PPL: 1525.588\n",
            "Epoch: 91 | Time: 0m 1s\n",
            "\tTrain Loss: 6.891 | Train PPL: 983.450\n",
            "\t Val. Loss: 7.330 |  Val. PPL: 1525.061\n",
            "Epoch: 92 | Time: 0m 1s\n",
            "\tTrain Loss: 6.891 | Train PPL: 983.301\n",
            "\t Val. Loss: 7.329 |  Val. PPL: 1524.541\n",
            "Epoch: 93 | Time: 0m 1s\n",
            "\tTrain Loss: 6.891 | Train PPL: 983.209\n",
            "\t Val. Loss: 7.329 |  Val. PPL: 1524.027\n",
            "Epoch: 94 | Time: 0m 1s\n",
            "\tTrain Loss: 6.891 | Train PPL: 983.113\n",
            "\t Val. Loss: 7.329 |  Val. PPL: 1523.521\n",
            "Epoch: 95 | Time: 0m 1s\n",
            "\tTrain Loss: 6.891 | Train PPL: 982.996\n",
            "\t Val. Loss: 7.328 |  Val. PPL: 1523.021\n",
            "Epoch: 96 | Time: 0m 1s\n",
            "\tTrain Loss: 6.891 | Train PPL: 982.894\n",
            "\t Val. Loss: 7.328 |  Val. PPL: 1522.530\n",
            "Epoch: 97 | Time: 0m 1s\n",
            "\tTrain Loss: 6.890 | Train PPL: 982.768\n",
            "\t Val. Loss: 7.328 |  Val. PPL: 1522.044\n",
            "Epoch: 98 | Time: 0m 1s\n",
            "\tTrain Loss: 6.890 | Train PPL: 982.660\n",
            "\t Val. Loss: 7.327 |  Val. PPL: 1521.565\n",
            "Epoch: 99 | Time: 0m 1s\n",
            "\tTrain Loss: 6.890 | Train PPL: 982.524\n",
            "\t Val. Loss: 7.327 |  Val. PPL: 1521.092\n",
            "Epoch: 100 | Time: 0m 1s\n",
            "\tTrain Loss: 6.890 | Train PPL: 982.437\n",
            "\t Val. Loss: 7.327 |  Val. PPL: 1520.626\n",
            "7.326877593994141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq3-W2A5rVLn",
        "outputId": "b2f44107-e0ce-441f-9193-666e1d907f22"
      },
      "source": [
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.326877593994141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uGl0qfzjKcs",
        "outputId": "e878f063-6678-4c23-fff0-a9ae9c679c56"
      },
      "source": [
        "N_EPOCHS = 50\r\n",
        "CLIP = 1\r\n",
        "print(best_valid_loss)\r\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\r\n",
        "\r\n",
        "best_valid_loss = 7.326877593994141 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.326877593994141\n",
            "Epoch: 01 | Time: 0m 1s\n",
            "\tTrain Loss: 6.890 | Train PPL: 982.353\n",
            "\t Val. Loss: 7.327 |  Val. PPL: 1520.165\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 6.890 | Train PPL: 982.263\n",
            "\t Val. Loss: 7.326 |  Val. PPL: 1519.710\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 6.890 | Train PPL: 982.148\n",
            "\t Val. Loss: 7.326 |  Val. PPL: 1519.261\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 6.890 | Train PPL: 982.069\n",
            "\t Val. Loss: 7.326 |  Val. PPL: 1518.820\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 6.890 | Train PPL: 981.943\n",
            "\t Val. Loss: 7.325 |  Val. PPL: 1518.384\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 6.889 | Train PPL: 981.854\n",
            "\t Val. Loss: 7.325 |  Val. PPL: 1517.954\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 6.889 | Train PPL: 981.782\n",
            "\t Val. Loss: 7.325 |  Val. PPL: 1517.529\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 6.889 | Train PPL: 981.647\n",
            "\t Val. Loss: 7.325 |  Val. PPL: 1517.110\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 6.889 | Train PPL: 981.560\n",
            "\t Val. Loss: 7.324 |  Val. PPL: 1516.697\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 6.889 | Train PPL: 981.495\n",
            "\t Val. Loss: 7.324 |  Val. PPL: 1516.289\n",
            "Epoch: 11 | Time: 0m 1s\n",
            "\tTrain Loss: 6.889 | Train PPL: 981.376\n",
            "\t Val. Loss: 7.324 |  Val. PPL: 1515.888\n",
            "Epoch: 12 | Time: 0m 1s\n",
            "\tTrain Loss: 6.889 | Train PPL: 981.276\n",
            "\t Val. Loss: 7.323 |  Val. PPL: 1515.491\n",
            "Epoch: 13 | Time: 0m 1s\n",
            "\tTrain Loss: 6.889 | Train PPL: 981.198\n",
            "\t Val. Loss: 7.323 |  Val. PPL: 1515.099\n",
            "Epoch: 14 | Time: 0m 1s\n",
            "\tTrain Loss: 6.889 | Train PPL: 981.110\n",
            "\t Val. Loss: 7.323 |  Val. PPL: 1514.714\n",
            "Epoch: 15 | Time: 0m 1s\n",
            "\tTrain Loss: 6.889 | Train PPL: 981.042\n",
            "\t Val. Loss: 7.323 |  Val. PPL: 1514.333\n",
            "Epoch: 16 | Time: 0m 1s\n",
            "\tTrain Loss: 6.889 | Train PPL: 980.949\n",
            "\t Val. Loss: 7.322 |  Val. PPL: 1513.956\n",
            "Epoch: 17 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 980.845\n",
            "\t Val. Loss: 7.322 |  Val. PPL: 1513.584\n",
            "Epoch: 18 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 980.765\n",
            "\t Val. Loss: 7.322 |  Val. PPL: 1513.217\n",
            "Epoch: 19 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 980.692\n",
            "\t Val. Loss: 7.322 |  Val. PPL: 1512.981\n",
            "Epoch: 20 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 980.595\n",
            "\t Val. Loss: 7.322 |  Val. PPL: 1512.621\n",
            "Epoch: 21 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 980.521\n",
            "\t Val. Loss: 7.321 |  Val. PPL: 1512.267\n",
            "Epoch: 22 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 980.439\n",
            "\t Val. Loss: 7.321 |  Val. PPL: 1511.917\n",
            "Epoch: 23 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 980.356\n",
            "\t Val. Loss: 7.321 |  Val. PPL: 1511.572\n",
            "Epoch: 24 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 980.219\n",
            "\t Val. Loss: 7.321 |  Val. PPL: 1511.244\n",
            "Epoch: 25 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 980.196\n",
            "\t Val. Loss: 7.321 |  Val. PPL: 1510.975\n",
            "Epoch: 26 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 980.101\n",
            "\t Val. Loss: 7.320 |  Val. PPL: 1510.728\n",
            "Epoch: 27 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 980.033\n",
            "\t Val. Loss: 7.320 |  Val. PPL: 1510.465\n",
            "Epoch: 28 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 979.950\n",
            "\t Val. Loss: 7.320 |  Val. PPL: 1510.147\n",
            "Epoch: 29 | Time: 0m 1s\n",
            "\tTrain Loss: 6.887 | Train PPL: 979.834\n",
            "\t Val. Loss: 7.320 |  Val. PPL: 1509.824\n",
            "Epoch: 30 | Time: 0m 1s\n",
            "\tTrain Loss: 6.887 | Train PPL: 979.783\n",
            "\t Val. Loss: 7.320 |  Val. PPL: 1509.508\n",
            "Epoch: 31 | Time: 0m 1s\n",
            "\tTrain Loss: 6.887 | Train PPL: 979.653\n",
            "\t Val. Loss: 7.319 |  Val. PPL: 1509.195\n",
            "Epoch: 32 | Time: 0m 1s\n",
            "\tTrain Loss: 6.887 | Train PPL: 979.626\n",
            "\t Val. Loss: 7.319 |  Val. PPL: 1508.885\n",
            "Epoch: 33 | Time: 0m 1s\n",
            "\tTrain Loss: 6.887 | Train PPL: 979.538\n",
            "\t Val. Loss: 7.319 |  Val. PPL: 1508.581\n",
            "Epoch: 34 | Time: 0m 1s\n",
            "\tTrain Loss: 6.887 | Train PPL: 979.465\n",
            "\t Val. Loss: 7.319 |  Val. PPL: 1508.281\n",
            "Epoch: 35 | Time: 0m 1s\n",
            "\tTrain Loss: 6.887 | Train PPL: 979.367\n",
            "\t Val. Loss: 7.319 |  Val. PPL: 1507.983\n",
            "Epoch: 36 | Time: 0m 1s\n",
            "\tTrain Loss: 6.887 | Train PPL: 979.295\n",
            "\t Val. Loss: 7.318 |  Val. PPL: 1507.690\n",
            "Epoch: 37 | Time: 0m 1s\n",
            "\tTrain Loss: 6.887 | Train PPL: 979.200\n",
            "\t Val. Loss: 7.318 |  Val. PPL: 1507.400\n",
            "Epoch: 38 | Time: 0m 1s\n",
            "\tTrain Loss: 6.887 | Train PPL: 979.129\n",
            "\t Val. Loss: 7.318 |  Val. PPL: 1507.116\n",
            "Epoch: 39 | Time: 0m 1s\n",
            "\tTrain Loss: 6.887 | Train PPL: 979.048\n",
            "\t Val. Loss: 7.318 |  Val. PPL: 1506.835\n",
            "Epoch: 40 | Time: 0m 1s\n",
            "\tTrain Loss: 6.887 | Train PPL: 978.982\n",
            "\t Val. Loss: 7.318 |  Val. PPL: 1506.557\n",
            "Epoch: 41 | Time: 0m 1s\n",
            "\tTrain Loss: 6.886 | Train PPL: 978.916\n",
            "\t Val. Loss: 7.317 |  Val. PPL: 1506.282\n",
            "Epoch: 42 | Time: 0m 1s\n",
            "\tTrain Loss: 6.886 | Train PPL: 978.842\n",
            "\t Val. Loss: 7.317 |  Val. PPL: 1506.011\n",
            "Epoch: 43 | Time: 0m 1s\n",
            "\tTrain Loss: 6.886 | Train PPL: 978.753\n",
            "\t Val. Loss: 7.317 |  Val. PPL: 1505.743\n",
            "Epoch: 44 | Time: 0m 1s\n",
            "\tTrain Loss: 6.886 | Train PPL: 978.675\n",
            "\t Val. Loss: 7.317 |  Val. PPL: 1505.480\n",
            "Epoch: 45 | Time: 0m 1s\n",
            "\tTrain Loss: 6.886 | Train PPL: 978.621\n",
            "\t Val. Loss: 7.317 |  Val. PPL: 1505.217\n",
            "Epoch: 46 | Time: 0m 1s\n",
            "\tTrain Loss: 6.886 | Train PPL: 978.528\n",
            "\t Val. Loss: 7.317 |  Val. PPL: 1504.959\n",
            "Epoch: 47 | Time: 0m 1s\n",
            "\tTrain Loss: 6.886 | Train PPL: 978.437\n",
            "\t Val. Loss: 7.316 |  Val. PPL: 1504.705\n",
            "Epoch: 48 | Time: 0m 1s\n",
            "\tTrain Loss: 6.886 | Train PPL: 978.368\n",
            "\t Val. Loss: 7.316 |  Val. PPL: 1504.454\n",
            "Epoch: 49 | Time: 0m 1s\n",
            "\tTrain Loss: 6.886 | Train PPL: 978.336\n",
            "\t Val. Loss: 7.316 |  Val. PPL: 1504.206\n",
            "Epoch: 50 | Time: 0m 1s\n",
            "\tTrain Loss: 6.886 | Train PPL: 978.263\n",
            "\t Val. Loss: 7.316 |  Val. PPL: 1503.960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJbL1n2XMgob",
        "outputId": "ef620664-8042-42b2-cd96-2b0f7591418d"
      },
      "source": [
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.315857172012329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i5ZxyI-1J3t",
        "outputId": "fd6ec32f-3912-4afe-d6cd-46311b56a56d"
      },
      "source": [
        "optimizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    eps: 1e-08\n",
              "    lr: 1e-06\n",
              "    weight_decay: 0.1\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB85z8dKM8cS"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-06, weight_decay=0.1)\r\n",
        "# scheduler = OneCycleLR(optimizer, \r\n",
        "#                         config.ocp_max_lr, \r\n",
        "#                         epochs=config.epochs, \r\n",
        "#                         cycle_momentum=cycle_momentum, \r\n",
        "#                         steps_per_epoch=len(trainloader), \r\n",
        "#                         base_momentum=config.momentum,\r\n",
        "#                         max_momentum=0.95, \r\n",
        "#                         pct_start=0.208,\r\n",
        "#                         anneal_strategy=config.anneal_strategy,\r\n",
        "#                         div_factor=config.div_factor,\r\n",
        "#                         final_div_factor=config.final_div_factor\r\n",
        "#                         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDT6kRLijzEZ",
        "outputId": "5fe17812-4228-4f40-e656-9e38e88c5981"
      },
      "source": [
        "N_EPOCHS = 50\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = 7.315857172012329 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 1s\n",
            "\tTrain Loss: 6.886 | Train PPL: 978.660\n",
            "\t Val. Loss: 7.314 |  Val. PPL: 1501.789\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 6.887 | Train PPL: 979.204\n",
            "\t Val. Loss: 7.313 |  Val. PPL: 1499.645\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 979.963\n",
            "\t Val. Loss: 7.312 |  Val. PPL: 1497.530\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 6.888 | Train PPL: 980.739\n",
            "\t Val. Loss: 7.310 |  Val. PPL: 1495.444\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 6.889 | Train PPL: 981.437\n",
            "\t Val. Loss: 7.309 |  Val. PPL: 1493.384\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 6.890 | Train PPL: 982.210\n",
            "\t Val. Loss: 7.307 |  Val. PPL: 1491.352\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 6.891 | Train PPL: 982.978\n",
            "\t Val. Loss: 7.306 |  Val. PPL: 1489.343\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 6.891 | Train PPL: 983.760\n",
            "\t Val. Loss: 7.305 |  Val. PPL: 1487.360\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 6.892 | Train PPL: 984.524\n",
            "\t Val. Loss: 7.303 |  Val. PPL: 1485.400\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 6.893 | Train PPL: 985.341\n",
            "\t Val. Loss: 7.302 |  Val. PPL: 1483.468\n",
            "Epoch: 11 | Time: 0m 1s\n",
            "\tTrain Loss: 6.894 | Train PPL: 986.122\n",
            "\t Val. Loss: 7.301 |  Val. PPL: 1481.557\n",
            "Epoch: 12 | Time: 0m 1s\n",
            "\tTrain Loss: 6.895 | Train PPL: 986.884\n",
            "\t Val. Loss: 7.300 |  Val. PPL: 1479.668\n",
            "Epoch: 13 | Time: 0m 1s\n",
            "\tTrain Loss: 6.895 | Train PPL: 987.745\n",
            "\t Val. Loss: 7.298 |  Val. PPL: 1477.804\n",
            "Epoch: 14 | Time: 0m 1s\n",
            "\tTrain Loss: 6.896 | Train PPL: 988.542\n",
            "\t Val. Loss: 7.297 |  Val. PPL: 1475.962\n",
            "Epoch: 15 | Time: 0m 1s\n",
            "\tTrain Loss: 6.897 | Train PPL: 989.384\n",
            "\t Val. Loss: 7.296 |  Val. PPL: 1474.140\n",
            "Epoch: 16 | Time: 0m 1s\n",
            "\tTrain Loss: 6.898 | Train PPL: 990.223\n",
            "\t Val. Loss: 7.295 |  Val. PPL: 1472.340\n",
            "Epoch: 17 | Time: 0m 1s\n",
            "\tTrain Loss: 6.899 | Train PPL: 991.010\n",
            "\t Val. Loss: 7.293 |  Val. PPL: 1470.563\n",
            "Epoch: 18 | Time: 0m 1s\n",
            "\tTrain Loss: 6.900 | Train PPL: 991.876\n",
            "\t Val. Loss: 7.292 |  Val. PPL: 1468.805\n",
            "Epoch: 19 | Time: 0m 1s\n",
            "\tTrain Loss: 6.900 | Train PPL: 992.715\n",
            "\t Val. Loss: 7.291 |  Val. PPL: 1467.068\n",
            "Epoch: 20 | Time: 0m 1s\n",
            "\tTrain Loss: 6.901 | Train PPL: 993.556\n",
            "\t Val. Loss: 7.290 |  Val. PPL: 1465.351\n",
            "Epoch: 21 | Time: 0m 1s\n",
            "\tTrain Loss: 6.902 | Train PPL: 994.409\n",
            "\t Val. Loss: 7.289 |  Val. PPL: 1463.653\n",
            "Epoch: 22 | Time: 0m 1s\n",
            "\tTrain Loss: 6.903 | Train PPL: 995.250\n",
            "\t Val. Loss: 7.288 |  Val. PPL: 1461.976\n",
            "Epoch: 23 | Time: 0m 1s\n",
            "\tTrain Loss: 6.904 | Train PPL: 996.129\n",
            "\t Val. Loss: 7.286 |  Val. PPL: 1460.318\n",
            "Epoch: 24 | Time: 0m 1s\n",
            "\tTrain Loss: 6.905 | Train PPL: 996.974\n",
            "\t Val. Loss: 7.285 |  Val. PPL: 1458.679\n",
            "Epoch: 25 | Time: 0m 1s\n",
            "\tTrain Loss: 6.906 | Train PPL: 997.876\n",
            "\t Val. Loss: 7.284 |  Val. PPL: 1457.058\n",
            "Epoch: 26 | Time: 0m 1s\n",
            "\tTrain Loss: 6.906 | Train PPL: 998.719\n",
            "\t Val. Loss: 7.283 |  Val. PPL: 1455.456\n",
            "Epoch: 27 | Time: 0m 1s\n",
            "\tTrain Loss: 6.907 | Train PPL: 999.589\n",
            "\t Val. Loss: 7.282 |  Val. PPL: 1453.871\n",
            "Epoch: 28 | Time: 0m 1s\n",
            "\tTrain Loss: 6.908 | Train PPL: 1000.447\n",
            "\t Val. Loss: 7.281 |  Val. PPL: 1452.306\n",
            "Epoch: 29 | Time: 0m 1s\n",
            "\tTrain Loss: 6.909 | Train PPL: 1001.318\n",
            "\t Val. Loss: 7.280 |  Val. PPL: 1450.758\n",
            "Epoch: 30 | Time: 0m 1s\n",
            "\tTrain Loss: 6.910 | Train PPL: 1002.188\n",
            "\t Val. Loss: 7.279 |  Val. PPL: 1449.228\n",
            "Epoch: 31 | Time: 0m 1s\n",
            "\tTrain Loss: 6.911 | Train PPL: 1003.044\n",
            "\t Val. Loss: 7.278 |  Val. PPL: 1447.714\n",
            "Epoch: 32 | Time: 0m 1s\n",
            "\tTrain Loss: 6.912 | Train PPL: 1003.942\n",
            "\t Val. Loss: 7.277 |  Val. PPL: 1446.217\n",
            "Epoch: 33 | Time: 0m 1s\n",
            "\tTrain Loss: 6.913 | Train PPL: 1004.801\n",
            "\t Val. Loss: 7.276 |  Val. PPL: 1444.736\n",
            "Epoch: 34 | Time: 0m 1s\n",
            "\tTrain Loss: 6.913 | Train PPL: 1005.678\n",
            "\t Val. Loss: 7.275 |  Val. PPL: 1443.273\n",
            "Epoch: 35 | Time: 0m 1s\n",
            "\tTrain Loss: 6.914 | Train PPL: 1006.574\n",
            "\t Val. Loss: 7.274 |  Val. PPL: 1441.824\n",
            "Epoch: 36 | Time: 0m 1s\n",
            "\tTrain Loss: 6.915 | Train PPL: 1007.435\n",
            "\t Val. Loss: 7.273 |  Val. PPL: 1440.392\n",
            "Epoch: 37 | Time: 0m 1s\n",
            "\tTrain Loss: 6.916 | Train PPL: 1008.288\n",
            "\t Val. Loss: 7.272 |  Val. PPL: 1438.975\n",
            "Epoch: 38 | Time: 0m 1s\n",
            "\tTrain Loss: 6.917 | Train PPL: 1009.167\n",
            "\t Val. Loss: 7.271 |  Val. PPL: 1437.573\n",
            "Epoch: 39 | Time: 0m 1s\n",
            "\tTrain Loss: 6.918 | Train PPL: 1010.007\n",
            "\t Val. Loss: 7.270 |  Val. PPL: 1436.187\n",
            "Epoch: 40 | Time: 0m 1s\n",
            "\tTrain Loss: 6.919 | Train PPL: 1010.897\n",
            "\t Val. Loss: 7.269 |  Val. PPL: 1434.815\n",
            "Epoch: 41 | Time: 0m 1s\n",
            "\tTrain Loss: 6.919 | Train PPL: 1011.767\n",
            "\t Val. Loss: 7.268 |  Val. PPL: 1433.455\n",
            "Epoch: 42 | Time: 0m 1s\n",
            "\tTrain Loss: 6.920 | Train PPL: 1012.637\n",
            "\t Val. Loss: 7.267 |  Val. PPL: 1432.111\n",
            "Epoch: 43 | Time: 0m 1s\n",
            "\tTrain Loss: 6.921 | Train PPL: 1013.484\n",
            "\t Val. Loss: 7.266 |  Val. PPL: 1430.780\n",
            "Epoch: 44 | Time: 0m 1s\n",
            "\tTrain Loss: 6.922 | Train PPL: 1014.353\n",
            "\t Val. Loss: 7.265 |  Val. PPL: 1429.463\n",
            "Epoch: 45 | Time: 0m 1s\n",
            "\tTrain Loss: 6.923 | Train PPL: 1015.186\n",
            "\t Val. Loss: 7.264 |  Val. PPL: 1428.158\n",
            "Epoch: 46 | Time: 0m 1s\n",
            "\tTrain Loss: 6.924 | Train PPL: 1016.055\n",
            "\t Val. Loss: 7.263 |  Val. PPL: 1426.866\n",
            "Epoch: 47 | Time: 0m 1s\n",
            "\tTrain Loss: 6.925 | Train PPL: 1016.911\n",
            "\t Val. Loss: 7.262 |  Val. PPL: 1425.587\n",
            "Epoch: 48 | Time: 0m 1s\n",
            "\tTrain Loss: 6.925 | Train PPL: 1017.731\n",
            "\t Val. Loss: 7.261 |  Val. PPL: 1424.320\n",
            "Epoch: 49 | Time: 0m 1s\n",
            "\tTrain Loss: 6.926 | Train PPL: 1018.594\n",
            "\t Val. Loss: 7.261 |  Val. PPL: 1423.065\n",
            "Epoch: 50 | Time: 0m 1s\n",
            "\tTrain Loss: 6.927 | Train PPL: 1019.421\n",
            "\t Val. Loss: 7.260 |  Val. PPL: 1421.822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElnQtHVqlkSf",
        "outputId": "acd0200b-4bcf-4819-afa8-9f9dd5717882"
      },
      "source": [
        "best_valid_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.259694695472717"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGCUoX7gkfQF",
        "outputId": "f318da33-62a9-4958-b0dd-8b274e5735e2"
      },
      "source": [
        "N_EPOCHS = 50\r\n",
        "CLIP = 1\r\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\r\n",
        "best_valid_loss = 7.259694695472717 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 1s\n",
            "\tTrain Loss: 6.928 | Train PPL: 1020.265\n",
            "\t Val. Loss: 7.259 |  Val. PPL: 1420.590\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 6.929 | Train PPL: 1021.090\n",
            "\t Val. Loss: 7.258 |  Val. PPL: 1419.370\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 6.929 | Train PPL: 1021.905\n",
            "\t Val. Loss: 7.257 |  Val. PPL: 1418.161\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 6.930 | Train PPL: 1022.741\n",
            "\t Val. Loss: 7.256 |  Val. PPL: 1416.960\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 6.931 | Train PPL: 1023.542\n",
            "\t Val. Loss: 7.255 |  Val. PPL: 1415.773\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 6.932 | Train PPL: 1024.350\n",
            "\t Val. Loss: 7.255 |  Val. PPL: 1414.595\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 6.933 | Train PPL: 1025.141\n",
            "\t Val. Loss: 7.254 |  Val. PPL: 1413.426\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 6.933 | Train PPL: 1025.946\n",
            "\t Val. Loss: 7.253 |  Val. PPL: 1412.270\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 6.934 | Train PPL: 1026.753\n",
            "\t Val. Loss: 7.252 |  Val. PPL: 1411.120\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 6.935 | Train PPL: 1027.537\n",
            "\t Val. Loss: 7.251 |  Val. PPL: 1409.979\n",
            "Epoch: 11 | Time: 0m 1s\n",
            "\tTrain Loss: 6.936 | Train PPL: 1028.348\n",
            "\t Val. Loss: 7.251 |  Val. PPL: 1408.851\n",
            "Epoch: 12 | Time: 0m 1s\n",
            "\tTrain Loss: 6.936 | Train PPL: 1029.113\n",
            "\t Val. Loss: 7.250 |  Val. PPL: 1407.729\n",
            "Epoch: 13 | Time: 0m 1s\n",
            "\tTrain Loss: 6.937 | Train PPL: 1029.879\n",
            "\t Val. Loss: 7.249 |  Val. PPL: 1406.617\n",
            "Epoch: 14 | Time: 0m 1s\n",
            "\tTrain Loss: 6.938 | Train PPL: 1030.656\n",
            "\t Val. Loss: 7.248 |  Val. PPL: 1405.513\n",
            "Epoch: 15 | Time: 0m 1s\n",
            "\tTrain Loss: 6.939 | Train PPL: 1031.444\n",
            "\t Val. Loss: 7.247 |  Val. PPL: 1404.418\n",
            "Epoch: 16 | Time: 0m 1s\n",
            "\tTrain Loss: 6.939 | Train PPL: 1032.193\n",
            "\t Val. Loss: 7.247 |  Val. PPL: 1403.331\n",
            "Epoch: 17 | Time: 0m 1s\n",
            "\tTrain Loss: 6.940 | Train PPL: 1032.957\n",
            "\t Val. Loss: 7.246 |  Val. PPL: 1402.253\n",
            "Epoch: 18 | Time: 0m 1s\n",
            "\tTrain Loss: 6.941 | Train PPL: 1033.706\n",
            "\t Val. Loss: 7.245 |  Val. PPL: 1401.182\n",
            "Epoch: 19 | Time: 0m 1s\n",
            "\tTrain Loss: 6.942 | Train PPL: 1034.434\n",
            "\t Val. Loss: 7.244 |  Val. PPL: 1400.119\n",
            "Epoch: 20 | Time: 0m 1s\n",
            "\tTrain Loss: 6.942 | Train PPL: 1035.177\n",
            "\t Val. Loss: 7.244 |  Val. PPL: 1399.065\n",
            "Epoch: 21 | Time: 0m 1s\n",
            "\tTrain Loss: 6.943 | Train PPL: 1035.936\n",
            "\t Val. Loss: 7.243 |  Val. PPL: 1398.018\n",
            "Epoch: 22 | Time: 0m 1s\n",
            "\tTrain Loss: 6.944 | Train PPL: 1036.647\n",
            "\t Val. Loss: 7.242 |  Val. PPL: 1396.978\n",
            "Epoch: 23 | Time: 0m 1s\n",
            "\tTrain Loss: 6.944 | Train PPL: 1037.401\n",
            "\t Val. Loss: 7.241 |  Val. PPL: 1395.946\n",
            "Epoch: 24 | Time: 0m 1s\n",
            "\tTrain Loss: 6.945 | Train PPL: 1038.093\n",
            "\t Val. Loss: 7.241 |  Val. PPL: 1394.923\n",
            "Epoch: 25 | Time: 0m 1s\n",
            "\tTrain Loss: 6.946 | Train PPL: 1038.799\n",
            "\t Val. Loss: 7.240 |  Val. PPL: 1393.905\n",
            "Epoch: 26 | Time: 0m 1s\n",
            "\tTrain Loss: 6.947 | Train PPL: 1039.507\n",
            "\t Val. Loss: 7.239 |  Val. PPL: 1392.894\n",
            "Epoch: 27 | Time: 0m 1s\n",
            "\tTrain Loss: 6.947 | Train PPL: 1040.215\n",
            "\t Val. Loss: 7.238 |  Val. PPL: 1391.891\n",
            "Epoch: 28 | Time: 0m 1s\n",
            "\tTrain Loss: 6.948 | Train PPL: 1040.914\n",
            "\t Val. Loss: 7.238 |  Val. PPL: 1390.895\n",
            "Epoch: 29 | Time: 0m 1s\n",
            "\tTrain Loss: 6.949 | Train PPL: 1041.620\n",
            "\t Val. Loss: 7.237 |  Val. PPL: 1389.907\n",
            "Epoch: 30 | Time: 0m 1s\n",
            "\tTrain Loss: 6.949 | Train PPL: 1042.286\n",
            "\t Val. Loss: 7.236 |  Val. PPL: 1388.924\n",
            "Epoch: 31 | Time: 0m 1s\n",
            "\tTrain Loss: 6.950 | Train PPL: 1043.007\n",
            "\t Val. Loss: 7.236 |  Val. PPL: 1387.947\n",
            "Epoch: 32 | Time: 0m 1s\n",
            "\tTrain Loss: 6.950 | Train PPL: 1043.668\n",
            "\t Val. Loss: 7.235 |  Val. PPL: 1386.979\n",
            "Epoch: 33 | Time: 0m 1s\n",
            "\tTrain Loss: 6.951 | Train PPL: 1044.356\n",
            "\t Val. Loss: 7.234 |  Val. PPL: 1386.017\n",
            "Epoch: 34 | Time: 0m 1s\n",
            "\tTrain Loss: 6.952 | Train PPL: 1045.019\n",
            "\t Val. Loss: 7.233 |  Val. PPL: 1385.062\n",
            "Epoch: 35 | Time: 0m 1s\n",
            "\tTrain Loss: 6.952 | Train PPL: 1045.699\n",
            "\t Val. Loss: 7.233 |  Val. PPL: 1384.112\n",
            "Epoch: 36 | Time: 0m 1s\n",
            "\tTrain Loss: 6.953 | Train PPL: 1046.329\n",
            "\t Val. Loss: 7.232 |  Val. PPL: 1383.170\n",
            "Epoch: 37 | Time: 0m 1s\n",
            "\tTrain Loss: 6.954 | Train PPL: 1046.998\n",
            "\t Val. Loss: 7.231 |  Val. PPL: 1382.234\n",
            "Epoch: 38 | Time: 0m 1s\n",
            "\tTrain Loss: 6.954 | Train PPL: 1047.645\n",
            "\t Val. Loss: 7.231 |  Val. PPL: 1381.304\n",
            "Epoch: 39 | Time: 0m 1s\n",
            "\tTrain Loss: 6.955 | Train PPL: 1048.277\n",
            "\t Val. Loss: 7.230 |  Val. PPL: 1380.381\n",
            "Epoch: 40 | Time: 0m 1s\n",
            "\tTrain Loss: 6.956 | Train PPL: 1048.924\n",
            "\t Val. Loss: 7.229 |  Val. PPL: 1379.464\n",
            "Epoch: 41 | Time: 0m 1s\n",
            "\tTrain Loss: 6.956 | Train PPL: 1049.590\n",
            "\t Val. Loss: 7.229 |  Val. PPL: 1378.554\n",
            "Epoch: 42 | Time: 0m 1s\n",
            "\tTrain Loss: 6.957 | Train PPL: 1050.210\n",
            "\t Val. Loss: 7.228 |  Val. PPL: 1377.648\n",
            "Epoch: 43 | Time: 0m 1s\n",
            "\tTrain Loss: 6.957 | Train PPL: 1050.865\n",
            "\t Val. Loss: 7.227 |  Val. PPL: 1376.751\n",
            "Epoch: 44 | Time: 0m 1s\n",
            "\tTrain Loss: 6.958 | Train PPL: 1051.472\n",
            "\t Val. Loss: 7.227 |  Val. PPL: 1375.859\n",
            "Epoch: 45 | Time: 0m 1s\n",
            "\tTrain Loss: 6.959 | Train PPL: 1052.113\n",
            "\t Val. Loss: 7.226 |  Val. PPL: 1374.973\n",
            "Epoch: 46 | Time: 0m 1s\n",
            "\tTrain Loss: 6.959 | Train PPL: 1052.722\n",
            "\t Val. Loss: 7.226 |  Val. PPL: 1374.092\n",
            "Epoch: 47 | Time: 0m 1s\n",
            "\tTrain Loss: 6.960 | Train PPL: 1053.346\n",
            "\t Val. Loss: 7.225 |  Val. PPL: 1373.218\n",
            "Epoch: 48 | Time: 0m 1s\n",
            "\tTrain Loss: 6.960 | Train PPL: 1053.955\n",
            "\t Val. Loss: 7.224 |  Val. PPL: 1372.350\n",
            "Epoch: 49 | Time: 0m 1s\n",
            "\tTrain Loss: 6.961 | Train PPL: 1054.540\n",
            "\t Val. Loss: 7.224 |  Val. PPL: 1371.488\n",
            "Epoch: 50 | Time: 0m 1s\n",
            "\tTrain Loss: 6.961 | Train PPL: 1055.158\n",
            "\t Val. Loss: 7.223 |  Val. PPL: 1370.632\n",
            "7.223027586936951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6WKCHQNrw1i"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=5e-8, weight_decay=0.005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNivIH5ir1HI",
        "outputId": "d9eeacf4-0ebd-408c-ed61-27f1729ffd24"
      },
      "source": [
        "N_EPOCHS = 50\r\n",
        "CLIP = 1\r\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\r\n",
        "best_valid_loss = 7.211369276046753 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 1s\n",
            "\tTrain Loss: 6.973 | Train PPL: 1067.101\n",
            "\t Val. Loss: 7.211 |  Val. PPL: 1354.013\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 6.973 | Train PPL: 1067.649\n",
            "\t Val. Loss: 7.210 |  Val. PPL: 1353.283\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 6.974 | Train PPL: 1068.173\n",
            "\t Val. Loss: 7.210 |  Val. PPL: 1352.560\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 6.974 | Train PPL: 1068.695\n",
            "\t Val. Loss: 7.209 |  Val. PPL: 1351.843\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 6.975 | Train PPL: 1069.220\n",
            "\t Val. Loss: 7.209 |  Val. PPL: 1351.131\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 6.975 | Train PPL: 1069.757\n",
            "\t Val. Loss: 7.208 |  Val. PPL: 1350.424\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 6.976 | Train PPL: 1070.293\n",
            "\t Val. Loss: 7.208 |  Val. PPL: 1349.724\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 6.976 | Train PPL: 1070.809\n",
            "\t Val. Loss: 7.207 |  Val. PPL: 1349.027\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 6.977 | Train PPL: 1071.332\n",
            "\t Val. Loss: 7.207 |  Val. PPL: 1348.338\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 6.977 | Train PPL: 1071.839\n",
            "\t Val. Loss: 7.206 |  Val. PPL: 1347.653\n",
            "Epoch: 11 | Time: 0m 1s\n",
            "\tTrain Loss: 6.978 | Train PPL: 1072.363\n",
            "\t Val. Loss: 7.206 |  Val. PPL: 1346.974\n",
            "Epoch: 12 | Time: 0m 1s\n",
            "\tTrain Loss: 6.978 | Train PPL: 1072.870\n",
            "\t Val. Loss: 7.205 |  Val. PPL: 1346.299\n",
            "Epoch: 13 | Time: 0m 1s\n",
            "\tTrain Loss: 6.979 | Train PPL: 1073.374\n",
            "\t Val. Loss: 7.205 |  Val. PPL: 1345.631\n",
            "Epoch: 14 | Time: 0m 1s\n",
            "\tTrain Loss: 6.979 | Train PPL: 1073.879\n",
            "\t Val. Loss: 7.204 |  Val. PPL: 1344.967\n",
            "Epoch: 15 | Time: 0m 1s\n",
            "\tTrain Loss: 6.980 | Train PPL: 1074.400\n",
            "\t Val. Loss: 7.204 |  Val. PPL: 1344.310\n",
            "Epoch: 16 | Time: 0m 1s\n",
            "\tTrain Loss: 6.980 | Train PPL: 1074.873\n",
            "\t Val. Loss: 7.203 |  Val. PPL: 1343.656\n",
            "Epoch: 17 | Time: 0m 1s\n",
            "\tTrain Loss: 6.980 | Train PPL: 1075.387\n",
            "\t Val. Loss: 7.203 |  Val. PPL: 1343.010\n",
            "Epoch: 18 | Time: 0m 1s\n",
            "\tTrain Loss: 6.981 | Train PPL: 1075.879\n",
            "\t Val. Loss: 7.202 |  Val. PPL: 1342.368\n",
            "Epoch: 19 | Time: 0m 1s\n",
            "\tTrain Loss: 6.981 | Train PPL: 1076.386\n",
            "\t Val. Loss: 7.202 |  Val. PPL: 1341.730\n",
            "Epoch: 20 | Time: 0m 1s\n",
            "\tTrain Loss: 6.982 | Train PPL: 1076.879\n",
            "\t Val. Loss: 7.201 |  Val. PPL: 1341.098\n",
            "Epoch: 21 | Time: 0m 1s\n",
            "\tTrain Loss: 6.982 | Train PPL: 1077.371\n",
            "\t Val. Loss: 7.201 |  Val. PPL: 1340.471\n",
            "Epoch: 22 | Time: 0m 1s\n",
            "\tTrain Loss: 6.983 | Train PPL: 1077.857\n",
            "\t Val. Loss: 7.200 |  Val. PPL: 1339.849\n",
            "Epoch: 23 | Time: 0m 1s\n",
            "\tTrain Loss: 6.983 | Train PPL: 1078.337\n",
            "\t Val. Loss: 7.200 |  Val. PPL: 1339.233\n",
            "Epoch: 24 | Time: 0m 1s\n",
            "\tTrain Loss: 6.984 | Train PPL: 1078.816\n",
            "\t Val. Loss: 7.199 |  Val. PPL: 1338.622\n",
            "Epoch: 25 | Time: 0m 1s\n",
            "\tTrain Loss: 6.984 | Train PPL: 1079.278\n",
            "\t Val. Loss: 7.199 |  Val. PPL: 1338.017\n",
            "Epoch: 26 | Time: 0m 1s\n",
            "\tTrain Loss: 6.985 | Train PPL: 1079.778\n",
            "\t Val. Loss: 7.198 |  Val. PPL: 1337.416\n",
            "Epoch: 27 | Time: 0m 1s\n",
            "\tTrain Loss: 6.985 | Train PPL: 1080.257\n",
            "\t Val. Loss: 7.198 |  Val. PPL: 1336.820\n",
            "Epoch: 28 | Time: 0m 1s\n",
            "\tTrain Loss: 6.985 | Train PPL: 1080.722\n",
            "\t Val. Loss: 7.198 |  Val. PPL: 1336.228\n",
            "Epoch: 29 | Time: 0m 1s\n",
            "\tTrain Loss: 6.986 | Train PPL: 1081.187\n",
            "\t Val. Loss: 7.197 |  Val. PPL: 1335.643\n",
            "Epoch: 30 | Time: 0m 1s\n",
            "\tTrain Loss: 6.986 | Train PPL: 1081.650\n",
            "\t Val. Loss: 7.197 |  Val. PPL: 1335.062\n",
            "Epoch: 31 | Time: 0m 1s\n",
            "\tTrain Loss: 6.987 | Train PPL: 1082.142\n",
            "\t Val. Loss: 7.196 |  Val. PPL: 1334.486\n",
            "Epoch: 32 | Time: 0m 1s\n",
            "\tTrain Loss: 6.987 | Train PPL: 1082.599\n",
            "\t Val. Loss: 7.196 |  Val. PPL: 1333.916\n",
            "Epoch: 33 | Time: 0m 1s\n",
            "\tTrain Loss: 6.988 | Train PPL: 1083.059\n",
            "\t Val. Loss: 7.195 |  Val. PPL: 1333.350\n",
            "Epoch: 34 | Time: 0m 1s\n",
            "\tTrain Loss: 6.988 | Train PPL: 1083.501\n",
            "\t Val. Loss: 7.195 |  Val. PPL: 1332.789\n",
            "Epoch: 35 | Time: 0m 1s\n",
            "\tTrain Loss: 6.988 | Train PPL: 1083.981\n",
            "\t Val. Loss: 7.195 |  Val. PPL: 1332.233\n",
            "Epoch: 36 | Time: 0m 1s\n",
            "\tTrain Loss: 6.989 | Train PPL: 1084.412\n",
            "\t Val. Loss: 7.194 |  Val. PPL: 1331.682\n",
            "Epoch: 37 | Time: 0m 1s\n",
            "\tTrain Loss: 6.989 | Train PPL: 1084.863\n",
            "\t Val. Loss: 7.194 |  Val. PPL: 1331.136\n",
            "Epoch: 38 | Time: 0m 1s\n",
            "\tTrain Loss: 6.990 | Train PPL: 1085.320\n",
            "\t Val. Loss: 7.193 |  Val. PPL: 1330.594\n",
            "Epoch: 39 | Time: 0m 1s\n",
            "\tTrain Loss: 6.990 | Train PPL: 1085.772\n",
            "\t Val. Loss: 7.193 |  Val. PPL: 1330.059\n",
            "Epoch: 40 | Time: 0m 1s\n",
            "\tTrain Loss: 6.990 | Train PPL: 1086.222\n",
            "\t Val. Loss: 7.193 |  Val. PPL: 1329.526\n",
            "Epoch: 41 | Time: 0m 1s\n",
            "\tTrain Loss: 6.991 | Train PPL: 1086.651\n",
            "\t Val. Loss: 7.192 |  Val. PPL: 1328.998\n",
            "Epoch: 42 | Time: 0m 1s\n",
            "\tTrain Loss: 6.991 | Train PPL: 1087.109\n",
            "\t Val. Loss: 7.192 |  Val. PPL: 1328.476\n",
            "Epoch: 43 | Time: 0m 1s\n",
            "\tTrain Loss: 6.992 | Train PPL: 1087.531\n",
            "\t Val. Loss: 7.191 |  Val. PPL: 1327.960\n",
            "Epoch: 44 | Time: 0m 1s\n",
            "\tTrain Loss: 6.992 | Train PPL: 1087.966\n",
            "\t Val. Loss: 7.191 |  Val. PPL: 1327.447\n",
            "Epoch: 45 | Time: 0m 1s\n",
            "\tTrain Loss: 6.992 | Train PPL: 1088.405\n",
            "\t Val. Loss: 7.191 |  Val. PPL: 1326.939\n",
            "Epoch: 46 | Time: 0m 1s\n",
            "\tTrain Loss: 6.993 | Train PPL: 1088.831\n",
            "\t Val. Loss: 7.190 |  Val. PPL: 1326.436\n",
            "Epoch: 47 | Time: 0m 1s\n",
            "\tTrain Loss: 6.993 | Train PPL: 1089.261\n",
            "\t Val. Loss: 7.190 |  Val. PPL: 1325.937\n",
            "Epoch: 48 | Time: 0m 1s\n",
            "\tTrain Loss: 6.994 | Train PPL: 1089.679\n",
            "\t Val. Loss: 7.190 |  Val. PPL: 1325.443\n",
            "Epoch: 49 | Time: 0m 1s\n",
            "\tTrain Loss: 6.994 | Train PPL: 1090.123\n",
            "\t Val. Loss: 7.189 |  Val. PPL: 1324.955\n",
            "Epoch: 50 | Time: 0m 1s\n",
            "\tTrain Loss: 6.994 | Train PPL: 1090.539\n",
            "\t Val. Loss: 7.189 |  Val. PPL: 1324.470\n",
            "7.188767671585083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcsr26pardUo",
        "outputId": "5310c140-1d67-41e7-f62d-1e43431ca3d6"
      },
      "source": [
        "N_EPOCHS = 100\r\n",
        "CLIP = 1\r\n",
        "model.load_state_dict(torch.load('/content/tut2-model.pt'))\r\n",
        "best_valid_loss = 7.188767671585083 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 1s\n",
            "\tTrain Loss: 6.995 | Train PPL: 1090.953\n",
            "\t Val. Loss: 7.188 |  Val. PPL: 1323.990\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 6.995 | Train PPL: 1091.381\n",
            "\t Val. Loss: 7.188 |  Val. PPL: 1323.515\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 6.996 | Train PPL: 1091.789\n",
            "\t Val. Loss: 7.188 |  Val. PPL: 1323.043\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 6.996 | Train PPL: 1092.202\n",
            "\t Val. Loss: 7.187 |  Val. PPL: 1322.578\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 6.996 | Train PPL: 1092.605\n",
            "\t Val. Loss: 7.187 |  Val. PPL: 1322.116\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 6.997 | Train PPL: 1093.027\n",
            "\t Val. Loss: 7.187 |  Val. PPL: 1321.659\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 6.997 | Train PPL: 1093.425\n",
            "\t Val. Loss: 7.186 |  Val. PPL: 1321.206\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 6.997 | Train PPL: 1093.830\n",
            "\t Val. Loss: 7.186 |  Val. PPL: 1320.757\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 6.998 | Train PPL: 1094.228\n",
            "\t Val. Loss: 7.186 |  Val. PPL: 1320.313\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 6.998 | Train PPL: 1094.624\n",
            "\t Val. Loss: 7.185 |  Val. PPL: 1319.873\n",
            "Epoch: 11 | Time: 0m 1s\n",
            "\tTrain Loss: 6.999 | Train PPL: 1095.017\n",
            "\t Val. Loss: 7.185 |  Val. PPL: 1319.439\n",
            "Epoch: 12 | Time: 0m 1s\n",
            "\tTrain Loss: 6.999 | Train PPL: 1095.417\n",
            "\t Val. Loss: 7.185 |  Val. PPL: 1319.007\n",
            "Epoch: 13 | Time: 0m 1s\n",
            "\tTrain Loss: 6.999 | Train PPL: 1095.807\n",
            "\t Val. Loss: 7.184 |  Val. PPL: 1318.582\n",
            "Epoch: 14 | Time: 0m 1s\n",
            "\tTrain Loss: 7.000 | Train PPL: 1096.189\n",
            "\t Val. Loss: 7.184 |  Val. PPL: 1318.160\n",
            "Epoch: 15 | Time: 0m 1s\n",
            "\tTrain Loss: 7.000 | Train PPL: 1096.586\n",
            "\t Val. Loss: 7.184 |  Val. PPL: 1317.742\n",
            "Epoch: 16 | Time: 0m 1s\n",
            "\tTrain Loss: 7.000 | Train PPL: 1096.956\n",
            "\t Val. Loss: 7.183 |  Val. PPL: 1317.329\n",
            "Epoch: 17 | Time: 0m 1s\n",
            "\tTrain Loss: 7.001 | Train PPL: 1097.345\n",
            "\t Val. Loss: 7.183 |  Val. PPL: 1316.920\n",
            "Epoch: 18 | Time: 0m 1s\n",
            "\tTrain Loss: 7.001 | Train PPL: 1097.724\n",
            "\t Val. Loss: 7.183 |  Val. PPL: 1316.514\n",
            "Epoch: 19 | Time: 0m 1s\n",
            "\tTrain Loss: 7.001 | Train PPL: 1098.104\n",
            "\t Val. Loss: 7.182 |  Val. PPL: 1316.113\n",
            "Epoch: 20 | Time: 0m 1s\n",
            "\tTrain Loss: 7.002 | Train PPL: 1098.469\n",
            "\t Val. Loss: 7.182 |  Val. PPL: 1315.717\n",
            "Epoch: 21 | Time: 0m 1s\n",
            "\tTrain Loss: 7.002 | Train PPL: 1098.842\n",
            "\t Val. Loss: 7.182 |  Val. PPL: 1315.325\n",
            "Epoch: 22 | Time: 0m 1s\n",
            "\tTrain Loss: 7.002 | Train PPL: 1099.216\n",
            "\t Val. Loss: 7.182 |  Val. PPL: 1314.936\n",
            "Epoch: 23 | Time: 0m 1s\n",
            "\tTrain Loss: 7.003 | Train PPL: 1099.594\n",
            "\t Val. Loss: 7.181 |  Val. PPL: 1314.551\n",
            "Epoch: 24 | Time: 0m 1s\n",
            "\tTrain Loss: 7.003 | Train PPL: 1099.948\n",
            "\t Val. Loss: 7.181 |  Val. PPL: 1314.171\n",
            "Epoch: 25 | Time: 0m 1s\n",
            "\tTrain Loss: 7.003 | Train PPL: 1100.307\n",
            "\t Val. Loss: 7.181 |  Val. PPL: 1313.796\n",
            "Epoch: 26 | Time: 0m 1s\n",
            "\tTrain Loss: 7.004 | Train PPL: 1100.680\n",
            "\t Val. Loss: 7.180 |  Val. PPL: 1313.423\n",
            "Epoch: 27 | Time: 0m 1s\n",
            "\tTrain Loss: 7.004 | Train PPL: 1101.035\n",
            "\t Val. Loss: 7.180 |  Val. PPL: 1313.055\n",
            "Epoch: 28 | Time: 0m 1s\n",
            "\tTrain Loss: 7.004 | Train PPL: 1101.381\n",
            "\t Val. Loss: 7.180 |  Val. PPL: 1312.690\n",
            "Epoch: 29 | Time: 0m 1s\n",
            "\tTrain Loss: 7.005 | Train PPL: 1101.730\n",
            "\t Val. Loss: 7.180 |  Val. PPL: 1312.330\n",
            "Epoch: 30 | Time: 0m 1s\n",
            "\tTrain Loss: 7.005 | Train PPL: 1102.082\n",
            "\t Val. Loss: 7.179 |  Val. PPL: 1311.974\n",
            "Epoch: 31 | Time: 0m 1s\n",
            "\tTrain Loss: 7.005 | Train PPL: 1102.439\n",
            "\t Val. Loss: 7.179 |  Val. PPL: 1311.623\n",
            "Epoch: 32 | Time: 0m 1s\n",
            "\tTrain Loss: 7.006 | Train PPL: 1102.780\n",
            "\t Val. Loss: 7.179 |  Val. PPL: 1311.274\n",
            "Epoch: 33 | Time: 0m 1s\n",
            "\tTrain Loss: 7.006 | Train PPL: 1103.130\n",
            "\t Val. Loss: 7.178 |  Val. PPL: 1310.929\n",
            "Epoch: 34 | Time: 0m 1s\n",
            "\tTrain Loss: 7.006 | Train PPL: 1103.458\n",
            "\t Val. Loss: 7.178 |  Val. PPL: 1310.588\n",
            "Epoch: 35 | Time: 0m 1s\n",
            "\tTrain Loss: 7.007 | Train PPL: 1103.800\n",
            "\t Val. Loss: 7.178 |  Val. PPL: 1310.251\n",
            "Epoch: 36 | Time: 0m 1s\n",
            "\tTrain Loss: 7.007 | Train PPL: 1104.143\n",
            "\t Val. Loss: 7.178 |  Val. PPL: 1309.919\n",
            "Epoch: 37 | Time: 0m 1s\n",
            "\tTrain Loss: 7.007 | Train PPL: 1104.467\n",
            "\t Val. Loss: 7.177 |  Val. PPL: 1309.589\n",
            "Epoch: 38 | Time: 0m 1s\n",
            "\tTrain Loss: 7.007 | Train PPL: 1104.806\n",
            "\t Val. Loss: 7.177 |  Val. PPL: 1309.264\n",
            "Epoch: 39 | Time: 0m 1s\n",
            "\tTrain Loss: 7.008 | Train PPL: 1105.133\n",
            "\t Val. Loss: 7.177 |  Val. PPL: 1308.942\n",
            "Epoch: 40 | Time: 0m 1s\n",
            "\tTrain Loss: 7.008 | Train PPL: 1105.454\n",
            "\t Val. Loss: 7.177 |  Val. PPL: 1308.622\n",
            "Epoch: 41 | Time: 0m 1s\n",
            "\tTrain Loss: 7.008 | Train PPL: 1105.784\n",
            "\t Val. Loss: 7.176 |  Val. PPL: 1308.308\n",
            "Epoch: 42 | Time: 0m 1s\n",
            "\tTrain Loss: 7.009 | Train PPL: 1106.101\n",
            "\t Val. Loss: 7.176 |  Val. PPL: 1307.996\n",
            "Epoch: 43 | Time: 0m 1s\n",
            "\tTrain Loss: 7.009 | Train PPL: 1106.417\n",
            "\t Val. Loss: 7.176 |  Val. PPL: 1307.689\n",
            "Epoch: 44 | Time: 0m 1s\n",
            "\tTrain Loss: 7.009 | Train PPL: 1106.732\n",
            "\t Val. Loss: 7.176 |  Val. PPL: 1307.386\n",
            "Epoch: 45 | Time: 0m 1s\n",
            "\tTrain Loss: 7.009 | Train PPL: 1107.054\n",
            "\t Val. Loss: 7.176 |  Val. PPL: 1307.084\n",
            "Epoch: 46 | Time: 0m 1s\n",
            "\tTrain Loss: 7.010 | Train PPL: 1107.370\n",
            "\t Val. Loss: 7.175 |  Val. PPL: 1306.788\n",
            "Epoch: 47 | Time: 0m 1s\n",
            "\tTrain Loss: 7.010 | Train PPL: 1107.680\n",
            "\t Val. Loss: 7.175 |  Val. PPL: 1306.494\n",
            "Epoch: 48 | Time: 0m 1s\n",
            "\tTrain Loss: 7.010 | Train PPL: 1107.990\n",
            "\t Val. Loss: 7.175 |  Val. PPL: 1306.203\n",
            "Epoch: 49 | Time: 0m 1s\n",
            "\tTrain Loss: 7.011 | Train PPL: 1108.294\n",
            "\t Val. Loss: 7.175 |  Val. PPL: 1305.916\n",
            "Epoch: 50 | Time: 0m 1s\n",
            "\tTrain Loss: 7.011 | Train PPL: 1108.592\n",
            "\t Val. Loss: 7.174 |  Val. PPL: 1305.632\n",
            "Epoch: 51 | Time: 0m 1s\n",
            "\tTrain Loss: 7.011 | Train PPL: 1108.898\n",
            "\t Val. Loss: 7.174 |  Val. PPL: 1305.352\n",
            "Epoch: 52 | Time: 0m 1s\n",
            "\tTrain Loss: 7.011 | Train PPL: 1109.187\n",
            "\t Val. Loss: 7.174 |  Val. PPL: 1305.075\n",
            "Epoch: 53 | Time: 0m 1s\n",
            "\tTrain Loss: 7.012 | Train PPL: 1109.480\n",
            "\t Val. Loss: 7.174 |  Val. PPL: 1304.801\n",
            "Epoch: 54 | Time: 0m 1s\n",
            "\tTrain Loss: 7.012 | Train PPL: 1109.779\n",
            "\t Val. Loss: 7.174 |  Val. PPL: 1304.531\n",
            "Epoch: 55 | Time: 0m 1s\n",
            "\tTrain Loss: 7.012 | Train PPL: 1110.074\n",
            "\t Val. Loss: 7.173 |  Val. PPL: 1304.263\n",
            "Epoch: 56 | Time: 0m 1s\n",
            "\tTrain Loss: 7.012 | Train PPL: 1110.363\n",
            "\t Val. Loss: 7.173 |  Val. PPL: 1303.999\n",
            "Epoch: 57 | Time: 0m 1s\n",
            "\tTrain Loss: 7.013 | Train PPL: 1110.659\n",
            "\t Val. Loss: 7.173 |  Val. PPL: 1303.738\n",
            "Epoch: 58 | Time: 0m 1s\n",
            "\tTrain Loss: 7.013 | Train PPL: 1110.920\n",
            "\t Val. Loss: 7.173 |  Val. PPL: 1303.480\n",
            "Epoch: 59 | Time: 0m 1s\n",
            "\tTrain Loss: 7.013 | Train PPL: 1111.213\n",
            "\t Val. Loss: 7.173 |  Val. PPL: 1303.224\n",
            "Epoch: 60 | Time: 0m 1s\n",
            "\tTrain Loss: 7.013 | Train PPL: 1111.493\n",
            "\t Val. Loss: 7.172 |  Val. PPL: 1302.973\n",
            "Epoch: 61 | Time: 0m 1s\n",
            "\tTrain Loss: 7.014 | Train PPL: 1111.770\n",
            "\t Val. Loss: 7.172 |  Val. PPL: 1302.723\n",
            "Epoch: 62 | Time: 0m 1s\n",
            "\tTrain Loss: 7.014 | Train PPL: 1112.057\n",
            "\t Val. Loss: 7.172 |  Val. PPL: 1302.478\n",
            "Epoch: 63 | Time: 0m 1s\n",
            "\tTrain Loss: 7.014 | Train PPL: 1112.329\n",
            "\t Val. Loss: 7.172 |  Val. PPL: 1302.234\n",
            "Epoch: 64 | Time: 0m 1s\n",
            "\tTrain Loss: 7.014 | Train PPL: 1112.596\n",
            "\t Val. Loss: 7.172 |  Val. PPL: 1301.993\n",
            "Epoch: 65 | Time: 0m 1s\n",
            "\tTrain Loss: 7.015 | Train PPL: 1112.872\n",
            "\t Val. Loss: 7.171 |  Val. PPL: 1301.756\n",
            "Epoch: 66 | Time: 0m 1s\n",
            "\tTrain Loss: 7.015 | Train PPL: 1113.138\n",
            "\t Val. Loss: 7.171 |  Val. PPL: 1301.521\n",
            "Epoch: 67 | Time: 0m 1s\n",
            "\tTrain Loss: 7.015 | Train PPL: 1113.398\n",
            "\t Val. Loss: 7.171 |  Val. PPL: 1301.289\n",
            "Epoch: 68 | Time: 0m 1s\n",
            "\tTrain Loss: 7.015 | Train PPL: 1113.661\n",
            "\t Val. Loss: 7.171 |  Val. PPL: 1301.060\n",
            "Epoch: 69 | Time: 0m 1s\n",
            "\tTrain Loss: 7.016 | Train PPL: 1113.933\n",
            "\t Val. Loss: 7.171 |  Val. PPL: 1300.833\n",
            "Epoch: 70 | Time: 0m 1s\n",
            "\tTrain Loss: 7.016 | Train PPL: 1114.193\n",
            "\t Val. Loss: 7.171 |  Val. PPL: 1300.610\n",
            "Epoch: 71 | Time: 0m 1s\n",
            "\tTrain Loss: 7.016 | Train PPL: 1114.446\n",
            "\t Val. Loss: 7.170 |  Val. PPL: 1300.388\n",
            "Epoch: 72 | Time: 0m 1s\n",
            "\tTrain Loss: 7.016 | Train PPL: 1114.704\n",
            "\t Val. Loss: 7.170 |  Val. PPL: 1300.169\n",
            "Epoch: 73 | Time: 0m 1s\n",
            "\tTrain Loss: 7.017 | Train PPL: 1114.956\n",
            "\t Val. Loss: 7.170 |  Val. PPL: 1299.952\n",
            "Epoch: 74 | Time: 0m 1s\n",
            "\tTrain Loss: 7.017 | Train PPL: 1115.200\n",
            "\t Val. Loss: 7.170 |  Val. PPL: 1299.739\n",
            "Epoch: 75 | Time: 0m 1s\n",
            "\tTrain Loss: 7.017 | Train PPL: 1115.447\n",
            "\t Val. Loss: 7.170 |  Val. PPL: 1299.528\n",
            "Epoch: 76 | Time: 0m 1s\n",
            "\tTrain Loss: 7.017 | Train PPL: 1115.693\n",
            "\t Val. Loss: 7.170 |  Val. PPL: 1299.320\n",
            "Epoch: 77 | Time: 0m 1s\n",
            "\tTrain Loss: 7.017 | Train PPL: 1115.943\n",
            "\t Val. Loss: 7.169 |  Val. PPL: 1299.113\n",
            "Epoch: 78 | Time: 0m 1s\n",
            "\tTrain Loss: 7.018 | Train PPL: 1116.191\n",
            "\t Val. Loss: 7.169 |  Val. PPL: 1298.908\n",
            "Epoch: 79 | Time: 0m 1s\n",
            "\tTrain Loss: 7.018 | Train PPL: 1116.428\n",
            "\t Val. Loss: 7.169 |  Val. PPL: 1298.707\n",
            "Epoch: 80 | Time: 0m 1s\n",
            "\tTrain Loss: 7.018 | Train PPL: 1116.666\n",
            "\t Val. Loss: 7.169 |  Val. PPL: 1298.508\n",
            "Epoch: 81 | Time: 0m 1s\n",
            "\tTrain Loss: 7.018 | Train PPL: 1116.901\n",
            "\t Val. Loss: 7.169 |  Val. PPL: 1298.311\n",
            "Epoch: 82 | Time: 0m 1s\n",
            "\tTrain Loss: 7.019 | Train PPL: 1117.143\n",
            "\t Val. Loss: 7.169 |  Val. PPL: 1298.117\n",
            "Epoch: 83 | Time: 0m 1s\n",
            "\tTrain Loss: 7.019 | Train PPL: 1117.371\n",
            "\t Val. Loss: 7.169 |  Val. PPL: 1297.924\n",
            "Epoch: 84 | Time: 0m 1s\n",
            "\tTrain Loss: 7.019 | Train PPL: 1117.606\n",
            "\t Val. Loss: 7.168 |  Val. PPL: 1297.733\n",
            "Epoch: 85 | Time: 0m 1s\n",
            "\tTrain Loss: 7.019 | Train PPL: 1117.837\n",
            "\t Val. Loss: 7.168 |  Val. PPL: 1297.545\n",
            "Epoch: 86 | Time: 0m 1s\n",
            "\tTrain Loss: 7.019 | Train PPL: 1118.062\n",
            "\t Val. Loss: 7.168 |  Val. PPL: 1297.358\n",
            "Epoch: 87 | Time: 0m 1s\n",
            "\tTrain Loss: 7.020 | Train PPL: 1118.291\n",
            "\t Val. Loss: 7.168 |  Val. PPL: 1297.175\n",
            "Epoch: 88 | Time: 0m 1s\n",
            "\tTrain Loss: 7.020 | Train PPL: 1118.513\n",
            "\t Val. Loss: 7.168 |  Val. PPL: 1296.992\n",
            "Epoch: 89 | Time: 0m 1s\n",
            "\tTrain Loss: 7.020 | Train PPL: 1118.734\n",
            "\t Val. Loss: 7.168 |  Val. PPL: 1296.812\n",
            "Epoch: 90 | Time: 0m 1s\n",
            "\tTrain Loss: 7.020 | Train PPL: 1118.958\n",
            "\t Val. Loss: 7.168 |  Val. PPL: 1296.634\n",
            "Epoch: 91 | Time: 0m 1s\n",
            "\tTrain Loss: 7.020 | Train PPL: 1119.170\n",
            "\t Val. Loss: 7.167 |  Val. PPL: 1296.458\n",
            "Epoch: 92 | Time: 0m 1s\n",
            "\tTrain Loss: 7.021 | Train PPL: 1119.386\n",
            "\t Val. Loss: 7.167 |  Val. PPL: 1296.284\n",
            "Epoch: 93 | Time: 0m 1s\n",
            "\tTrain Loss: 7.021 | Train PPL: 1119.603\n",
            "\t Val. Loss: 7.167 |  Val. PPL: 1296.107\n",
            "Epoch: 94 | Time: 0m 1s\n",
            "\tTrain Loss: 7.021 | Train PPL: 1119.821\n",
            "\t Val. Loss: 7.167 |  Val. PPL: 1295.918\n",
            "Epoch: 95 | Time: 0m 1s\n",
            "\tTrain Loss: 7.021 | Train PPL: 1120.029\n",
            "\t Val. Loss: 7.167 |  Val. PPL: 1295.739\n",
            "Epoch: 96 | Time: 0m 1s\n",
            "\tTrain Loss: 7.021 | Train PPL: 1120.245\n",
            "\t Val. Loss: 7.167 |  Val. PPL: 1295.557\n",
            "Epoch: 97 | Time: 0m 1s\n",
            "\tTrain Loss: 7.021 | Train PPL: 1120.448\n",
            "\t Val. Loss: 7.167 |  Val. PPL: 1295.370\n",
            "Epoch: 98 | Time: 0m 1s\n",
            "\tTrain Loss: 7.022 | Train PPL: 1120.652\n",
            "\t Val. Loss: 7.166 |  Val. PPL: 1295.205\n",
            "Epoch: 99 | Time: 0m 1s\n",
            "\tTrain Loss: 7.022 | Train PPL: 1120.859\n",
            "\t Val. Loss: 7.166 |  Val. PPL: 1295.042\n",
            "Epoch: 100 | Time: 0m 1s\n",
            "\tTrain Loss: 7.022 | Train PPL: 1121.061\n",
            "\t Val. Loss: 7.166 |  Val. PPL: 1294.884\n",
            "7.1661763191223145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIGNFuTq3MNH"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-05, weight_decay=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s10zQenj3AiN",
        "outputId": "134fcb6a-9203-4058-cc4d-ec8281a1737b"
      },
      "source": [
        "N_EPOCHS = 50\r\n",
        "CLIP = 1\r\n",
        "model.load_state_dict(torch.load('/content/tut2-model.pt'))\r\n",
        "best_valid_loss = 7.150131702423096 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 1s\n",
            "\tTrain Loss: 7.049 | Train PPL: 1152.105\n",
            "\t Val. Loss: 7.149 |  Val. PPL: 1272.257\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 7.044 | Train PPL: 1145.868\n",
            "\t Val. Loss: 7.148 |  Val. PPL: 1271.496\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 7.044 | Train PPL: 1145.672\n",
            "\t Val. Loss: 7.148 |  Val. PPL: 1272.085\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 7.043 | Train PPL: 1145.171\n",
            "\t Val. Loss: 7.149 |  Val. PPL: 1272.417\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 7.043 | Train PPL: 1144.855\n",
            "\t Val. Loss: 7.149 |  Val. PPL: 1272.848\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 7.043 | Train PPL: 1144.437\n",
            "\t Val. Loss: 7.149 |  Val. PPL: 1273.244\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 7.042 | Train PPL: 1144.029\n",
            "\t Val. Loss: 7.150 |  Val. PPL: 1273.638\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 7.042 | Train PPL: 1143.605\n",
            "\t Val. Loss: 7.150 |  Val. PPL: 1274.014\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 7.042 | Train PPL: 1143.176\n",
            "\t Val. Loss: 7.150 |  Val. PPL: 1274.373\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 7.041 | Train PPL: 1142.744\n",
            "\t Val. Loss: 7.150 |  Val. PPL: 1274.715\n",
            "Epoch: 11 | Time: 0m 1s\n",
            "\tTrain Loss: 7.041 | Train PPL: 1142.309\n",
            "\t Val. Loss: 7.151 |  Val. PPL: 1275.038\n",
            "Epoch: 12 | Time: 0m 1s\n",
            "\tTrain Loss: 7.040 | Train PPL: 1141.875\n",
            "\t Val. Loss: 7.151 |  Val. PPL: 1275.344\n",
            "Epoch: 13 | Time: 0m 1s\n",
            "\tTrain Loss: 7.040 | Train PPL: 1141.441\n",
            "\t Val. Loss: 7.151 |  Val. PPL: 1275.634\n",
            "Epoch: 14 | Time: 0m 1s\n",
            "\tTrain Loss: 7.040 | Train PPL: 1141.011\n",
            "\t Val. Loss: 7.151 |  Val. PPL: 1275.906\n",
            "Epoch: 15 | Time: 0m 1s\n",
            "\tTrain Loss: 7.039 | Train PPL: 1140.582\n",
            "\t Val. Loss: 7.152 |  Val. PPL: 1276.161\n",
            "Epoch: 16 | Time: 0m 1s\n",
            "\tTrain Loss: 7.039 | Train PPL: 1140.156\n",
            "\t Val. Loss: 7.152 |  Val. PPL: 1276.402\n",
            "Epoch: 17 | Time: 0m 1s\n",
            "\tTrain Loss: 7.039 | Train PPL: 1139.734\n",
            "\t Val. Loss: 7.152 |  Val. PPL: 1276.629\n",
            "Epoch: 18 | Time: 0m 1s\n",
            "\tTrain Loss: 7.038 | Train PPL: 1139.317\n",
            "\t Val. Loss: 7.152 |  Val. PPL: 1276.839\n",
            "Epoch: 19 | Time: 0m 1s\n",
            "\tTrain Loss: 7.038 | Train PPL: 1138.903\n",
            "\t Val. Loss: 7.152 |  Val. PPL: 1277.038\n",
            "Epoch: 20 | Time: 0m 1s\n",
            "\tTrain Loss: 7.037 | Train PPL: 1138.495\n",
            "\t Val. Loss: 7.152 |  Val. PPL: 1277.222\n",
            "Epoch: 21 | Time: 0m 1s\n",
            "\tTrain Loss: 7.037 | Train PPL: 1138.092\n",
            "\t Val. Loss: 7.153 |  Val. PPL: 1277.395\n",
            "Epoch: 22 | Time: 0m 1s\n",
            "\tTrain Loss: 7.037 | Train PPL: 1137.693\n",
            "\t Val. Loss: 7.153 |  Val. PPL: 1277.559\n",
            "Epoch: 23 | Time: 0m 1s\n",
            "\tTrain Loss: 7.036 | Train PPL: 1137.301\n",
            "\t Val. Loss: 7.153 |  Val. PPL: 1277.710\n",
            "Epoch: 24 | Time: 0m 1s\n",
            "\tTrain Loss: 7.036 | Train PPL: 1136.913\n",
            "\t Val. Loss: 7.153 |  Val. PPL: 1277.851\n",
            "Epoch: 25 | Time: 0m 1s\n",
            "\tTrain Loss: 7.036 | Train PPL: 1136.531\n",
            "\t Val. Loss: 7.153 |  Val. PPL: 1277.983\n",
            "Epoch: 26 | Time: 0m 1s\n",
            "\tTrain Loss: 7.035 | Train PPL: 1136.155\n",
            "\t Val. Loss: 7.153 |  Val. PPL: 1278.105\n",
            "Epoch: 27 | Time: 0m 1s\n",
            "\tTrain Loss: 7.035 | Train PPL: 1135.786\n",
            "\t Val. Loss: 7.153 |  Val. PPL: 1278.220\n",
            "Epoch: 28 | Time: 0m 1s\n",
            "\tTrain Loss: 7.035 | Train PPL: 1135.422\n",
            "\t Val. Loss: 7.153 |  Val. PPL: 1278.326\n",
            "Epoch: 29 | Time: 0m 1s\n",
            "\tTrain Loss: 7.034 | Train PPL: 1135.063\n",
            "\t Val. Loss: 7.153 |  Val. PPL: 1278.425\n",
            "Epoch: 30 | Time: 0m 1s\n",
            "\tTrain Loss: 7.034 | Train PPL: 1134.711\n",
            "\t Val. Loss: 7.153 |  Val. PPL: 1278.518\n",
            "Epoch: 31 | Time: 0m 1s\n",
            "\tTrain Loss: 7.034 | Train PPL: 1134.365\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1278.603\n",
            "Epoch: 32 | Time: 0m 1s\n",
            "\tTrain Loss: 7.034 | Train PPL: 1134.025\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1278.681\n",
            "Epoch: 33 | Time: 0m 1s\n",
            "\tTrain Loss: 7.033 | Train PPL: 1133.691\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1278.755\n",
            "Epoch: 34 | Time: 0m 1s\n",
            "\tTrain Loss: 7.033 | Train PPL: 1133.364\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1278.822\n",
            "Epoch: 35 | Time: 0m 1s\n",
            "\tTrain Loss: 7.033 | Train PPL: 1133.042\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1278.885\n",
            "Epoch: 36 | Time: 0m 1s\n",
            "\tTrain Loss: 7.032 | Train PPL: 1132.727\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1278.942\n",
            "Epoch: 37 | Time: 0m 1s\n",
            "\tTrain Loss: 7.032 | Train PPL: 1132.417\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1278.994\n",
            "Epoch: 38 | Time: 0m 1s\n",
            "\tTrain Loss: 7.032 | Train PPL: 1132.113\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1279.042\n",
            "Epoch: 39 | Time: 0m 1s\n",
            "\tTrain Loss: 7.032 | Train PPL: 1131.815\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1279.087\n",
            "Epoch: 40 | Time: 0m 1s\n",
            "\tTrain Loss: 7.031 | Train PPL: 1131.523\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1279.127\n",
            "Epoch: 41 | Time: 0m 1s\n",
            "\tTrain Loss: 7.031 | Train PPL: 1131.237\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1279.164\n",
            "Epoch: 42 | Time: 0m 1s\n",
            "\tTrain Loss: 7.031 | Train PPL: 1130.956\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1279.198\n",
            "Epoch: 43 | Time: 0m 1s\n",
            "\tTrain Loss: 7.031 | Train PPL: 1130.682\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1279.228\n",
            "Epoch: 44 | Time: 0m 1s\n",
            "\tTrain Loss: 7.030 | Train PPL: 1130.413\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1279.254\n",
            "Epoch: 45 | Time: 0m 1s\n",
            "\tTrain Loss: 7.030 | Train PPL: 1130.150\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1279.280\n",
            "Epoch: 46 | Time: 0m 1s\n",
            "\tTrain Loss: 7.030 | Train PPL: 1129.892\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1279.301\n",
            "Epoch: 47 | Time: 0m 1s\n",
            "\tTrain Loss: 7.030 | Train PPL: 1129.640\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1279.320\n",
            "Epoch: 48 | Time: 0m 1s\n",
            "\tTrain Loss: 7.029 | Train PPL: 1129.392\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1279.336\n",
            "Epoch: 49 | Time: 0m 1s\n",
            "\tTrain Loss: 7.029 | Train PPL: 1129.151\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1279.351\n",
            "Epoch: 50 | Time: 0m 1s\n",
            "\tTrain Loss: 7.029 | Train PPL: 1128.914\n",
            "\t Val. Loss: 7.154 |  Val. PPL: 1279.363\n",
            "7.14794921875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HZketc3gJdq",
        "outputId": "436a2ada-ee31-4bb9-e226-3448fa065cfa"
      },
      "source": [
        "iter_one = next(iter(train_iterator))\r\n",
        "iter_one.label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztR5mNm8EzFn"
      },
      "source": [
        "Finally, we test the model on the test set using these \"best\" parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaJo3X9aEzFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e02fec-6f49-4a92-8619-4c7a3547c9a7"
      },
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 10.525 | Test PPL: 37253.098 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY7SsC8TEzFn"
      },
      "source": [
        "Just looking at the test loss, we get better performance. This is a pretty good sign that this model architecture is doing something right! Relieving the information compression seems like the way forard, and in the next tutorial we'll expand on this even further with *attention*."
      ]
    }
  ]
}
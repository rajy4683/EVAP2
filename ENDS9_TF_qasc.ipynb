{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "ENDS9_TF_qasc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EVAP2/blob/master/ENDS9_TF_qasc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzxOJwN7EzFO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1YC7Z0guYGX",
        "outputId": "66fd5660-c8d5-4d7a-8bc3-25e728de772e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jan  4 06:19:22 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    12W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUnMLdevEzFT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og-MDQc4hKL7",
        "outputId": "ac374171-664a-4c92-cce6-bde7c6c5b413"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzErHAsEzFU"
      },
      "source": [
        "Then set a random seed for deterministic results/reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_wP4J4LEzFX"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oazmnaraL0A7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7337da9d-18e2-41fd-e076-5390eea3e831"
      },
      "source": [
        "!gsutil cp -r \\\r\n",
        "  gs://unifiedqa/data/qasc/ \\\r\n",
        "  gs://unifiedqa/data/qasc_test/ \\\r\n",
        "  gs://unifiedqa/data/qasc_with_ir/ \\\r\n",
        "  gs://unifiedqa/data/qasc_with_ir_test/ \\\r\n",
        "  ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://unifiedqa/data/qasc/counts.json...\n",
            "Copying gs://unifiedqa/data/qasc/dev.tsv...\n",
            "Copying gs://unifiedqa/data/qasc/dev_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc/test.tsv...\n",
            "- [4 files][352.9 KiB/352.9 KiB]                                                \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://unifiedqa/data/qasc/test_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc/train.tsv...\n",
            "Copying gs://unifiedqa/data/qasc/train_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_test/counts.json...\n",
            "Copying gs://unifiedqa/data/qasc_test/dev.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_test/dev_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_test/test.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_test/test_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_test/train.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_test/train_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/counts.json...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/dev.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/dev_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/test.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/test_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/train.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir/train_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/counts.json...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/dev.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/dev_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/test.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/test_meta.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/train.tsv...\n",
            "Copying gs://unifiedqa/data/qasc_with_ir_test/train_meta.tsv...\n",
            "| [28 files][ 38.7 MiB/ 38.7 MiB]                                               \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "\n",
            "Operation completed over 28 objects/38.7 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC_7SAqaBu3M"
      },
      "source": [
        "def read_file(file, split):\r\n",
        "    fout = open(f\"./{split}.tsv\", \"w\")\r\n",
        "    fout_meta = open(f\"./{split}_meta.tsv\", \"w\")\r\n",
        "    with open(file) as f:\r\n",
        "        for line in f.readlines():\r\n",
        "            json_line = json.loads(line)\r\n",
        "\r\n",
        "            candidates_str = \" \".join([f\"({x['label']}) {x['text']}\" for x in json_line['question']['choices']])\r\n",
        "            if split != \"test\":\r\n",
        "                selected_ans_string = [x['text'] for x in json_line['question']['choices'] if\r\n",
        "                                        json_line['answerKey'] == x['label']]\r\n",
        "                assert len(selected_ans_string) == 1, f\"{len(selected_ans_string)} -- {json_line['answerKey']}\"\r\n",
        "            json_line['question']['stem'] = json_line['question']['stem'].replace(\"\\t\", \" \").replace(\"\\n\", \"\")\r\n",
        "            candidates_str = candidates_str.replace(\"\\t\", \" \").replace(\"\\n\", \"\")\r\n",
        "\r\n",
        "            if split == \"test\":\r\n",
        "                fout_meta.write(f\"{json_line['id']}\\t-\\n\")\r\n",
        "                fout.write(f\"{json_line['question']['stem']} \\\\n {candidates_str}\\t-\\n\")\r\n",
        "            else:\r\n",
        "                fout_meta.write(f\"{json_line['id']}\\t{json_line['answerKey']}\\n\")\r\n",
        "                selected_ans_string[0] = selected_ans_string[0].replace(\"\\t\", \" \").replace(\"\\n\", \"\")\r\n",
        "                fout.write(f\"{json_line['question']['stem']} \\\\n {candidates_str}\\t{selected_ans_string[0]}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGYiDLywFypD"
      },
      "source": [
        "#dataset = pd.read_csv(\"/content/dev.tsv\", sep='\\t', header=None)\r\n",
        "dataset = pd.read_csv(\"/content/qasc/train.tsv\", sep='\\t', header=None)\r\n",
        "dataset.columns = [\"sentence\", \"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-7zLo-ZyqwS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "16077ca5-d57b-4eed-f3df-ff8da803fec8"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What type of water formation is formed by clou...</td>\n",
              "      <td>beads</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Where do beads of water come from? \\n (A) Too ...</td>\n",
              "      <td>Vapor turning into a liquid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What forms beads of water?  \\n (A) Necklaces. ...</td>\n",
              "      <td>Steam.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what kind of beads are formed from vapor conde...</td>\n",
              "      <td>h2o</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what kind of beads are formed by their vapor c...</td>\n",
              "      <td>h2o</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8129</th>\n",
              "      <td>Chitin can be used for protection by whom? \\n ...</td>\n",
              "      <td>Fish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8130</th>\n",
              "      <td>Which type of animal uses plates for protectio...</td>\n",
              "      <td>reptiles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8131</th>\n",
              "      <td>What are used for protection by fish? \\n (A) s...</td>\n",
              "      <td>scales</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8132</th>\n",
              "      <td>What are pangolins covered in? \\n (A) tunicate...</td>\n",
              "      <td>protection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8133</th>\n",
              "      <td>What are covered with protection? \\n (A) apple...</td>\n",
              "      <td>fish</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8134 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence                        label\n",
              "0     What type of water formation is formed by clou...                        beads\n",
              "1     Where do beads of water come from? \\n (A) Too ...  Vapor turning into a liquid\n",
              "2     What forms beads of water?  \\n (A) Necklaces. ...                       Steam.\n",
              "3     what kind of beads are formed from vapor conde...                          h2o\n",
              "4     what kind of beads are formed by their vapor c...                          h2o\n",
              "...                                                 ...                          ...\n",
              "8129  Chitin can be used for protection by whom? \\n ...                         Fish\n",
              "8130  Which type of animal uses plates for protectio...                     reptiles\n",
              "8131  What are used for protection by fish? \\n (A) s...                       scales\n",
              "8132  What are pangolins covered in? \\n (A) tunicate...                   protection\n",
              "8133  What are covered with protection? \\n (A) apple...                         fish\n",
              "\n",
              "[8134 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQGi1iSYTBjj",
        "outputId": "c1bebf9d-9954-4c6f-d1c7-0c846089cd15"
      },
      "source": [
        "!gsutil -m cp -r gs://unifiedqa/data/squad2/dev.tsv ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://unifiedqa/data/squad2/dev.tsv...\n",
            "/ [0/1 files][    0.0 B/ 10.3 MiB]   0% Done                                    \r/ [1/1 files][ 10.3 MiB/ 10.3 MiB] 100% Done                                    \r\n",
            "Operation completed over 1 objects/10.3 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH9ZJ6ZVMzWG",
        "outputId": "d3723f75-1851-4b08-ed6f-c505a1060f22"
      },
      "source": [
        "!gsutil -m cp -r \\\r\n",
        "  gs://unifiedqa/data/ambigqa/ \\\r\n",
        "  ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://unifiedqa/data/ambigqa/counts.json...\n",
            "/ [0/5 files][    0.0 B/  2.8 MiB]   0% Done                                    \rCopying gs://unifiedqa/data/ambigqa/dev.tsv...\n",
            "/ [0/5 files][    0.0 B/  2.8 MiB]   0% Done                                    \rCopying gs://unifiedqa/data/ambigqa/train_meta.tsv...\n",
            "/ [0/5 files][    0.0 B/  2.8 MiB]   0% Done                                    \rCopying gs://unifiedqa/data/ambigqa/train.tsv...\n",
            "Copying gs://unifiedqa/data/ambigqa/dev_meta.tsv...\n",
            "/ [5/5 files][  2.8 MiB/  2.8 MiB] 100% Done                                    \n",
            "Operation completed over 5 objects/2.8 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87RC_1U5BwuC"
      },
      "source": [
        "read_file(\"/content/dev_rand_split.jsonl\", \"dev\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hawianoFEzFY"
      },
      "source": [
        "Instantiate our German and English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLSPj13L2yAr",
        "outputId": "d0deaa46-dda4-4ddf-c40f-198691314635"
      },
      "source": [
        "!python3 -m spacy download de_core_news_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 14.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907057 sha256=db821aad79b4fff44bc3f6f93013824542eeaa2f6955efdd9e116ac8d6dc17bf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v0mzfpvk/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVVbBjSb3MO1"
      },
      "source": [
        "import de_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BP3YSvJEzFY"
      },
      "source": [
        "spacy_de =  de_core_news_sm.load()\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bEkyPt5EzFY"
      },
      "source": [
        "Previously we reversed the source (German) sentence, however in the paper we are implementing they don't do this, so neither will we."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KaGEZ45EzFZ"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7rbQLYHEzFZ"
      },
      "source": [
        "Create our fields to process our data. This will append the \"start of sentence\" and \"end of sentence\" tokens as well as converting all words to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "BuC9C7w6wf1w",
        "outputId": "6d6672b4-5b33-499c-efbe-e24421ae639d"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What type of water formation is formed by clou...</td>\n",
              "      <td>beads</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Where do beads of water come from? \\n (A) Too ...</td>\n",
              "      <td>Vapor turning into a liquid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What forms beads of water?  \\n (A) Necklaces. ...</td>\n",
              "      <td>Steam.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what kind of beads are formed from vapor conde...</td>\n",
              "      <td>h2o</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what kind of beads are formed by their vapor c...</td>\n",
              "      <td>h2o</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8129</th>\n",
              "      <td>Chitin can be used for protection by whom? \\n ...</td>\n",
              "      <td>Fish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8130</th>\n",
              "      <td>Which type of animal uses plates for protectio...</td>\n",
              "      <td>reptiles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8131</th>\n",
              "      <td>What are used for protection by fish? \\n (A) s...</td>\n",
              "      <td>scales</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8132</th>\n",
              "      <td>What are pangolins covered in? \\n (A) tunicate...</td>\n",
              "      <td>protection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8133</th>\n",
              "      <td>What are covered with protection? \\n (A) apple...</td>\n",
              "      <td>fish</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8134 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence                        label\n",
              "0     What type of water formation is formed by clou...                        beads\n",
              "1     Where do beads of water come from? \\n (A) Too ...  Vapor turning into a liquid\n",
              "2     What forms beads of water?  \\n (A) Necklaces. ...                       Steam.\n",
              "3     what kind of beads are formed from vapor conde...                          h2o\n",
              "4     what kind of beads are formed by their vapor c...                          h2o\n",
              "...                                                 ...                          ...\n",
              "8129  Chitin can be used for protection by whom? \\n ...                         Fish\n",
              "8130  Which type of animal uses plates for protectio...                     reptiles\n",
              "8131  What are used for protection by fish? \\n (A) s...                       scales\n",
              "8132  What are pangolins covered in? \\n (A) tunicate...                   protection\n",
              "8133  What are covered with protection? \\n (A) apple...                         fish\n",
              "\n",
              "[8134 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sytzu5pTnSCa"
      },
      "source": [
        "from torchtext import data \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "Sentence = data.Field(sequential = True, tokenize = 'spacy', batch_first =False, include_lengths=False, lower=False)\r\n",
        "Label = data.Field(sequential =True, tokenize ='spacy', is_target=False, batch_first =False, include_lengths=False, lower=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDuSMFrQwrXC"
      },
      "source": [
        "#dataset = dataset_df\r\n",
        "fields = [('sentence', Sentence),('label',Label)]\r\n",
        "example = [data.Example.fromlist([dataset.sentence[i],dataset.label[i]], fields) for i in range(dataset.shape[0])] \r\n",
        "commonqa_ds = data.Dataset(example, fields)\r\n",
        "(train, valid) = commonqa_ds.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZlXYwYyyz46",
        "outputId": "aa57e64b-73fd-449d-afa5-d8782b932507"
      },
      "source": [
        "print('Size of input vocab : ', len(Sentence.vocab))\r\n",
        "print('Size of label vocab : ', len(Label.vocab))\r\n",
        "print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\r\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  9462\n",
            "Size of label vocab :  3610\n",
            "Top 10 words appreared repeatedly : [(')', 55314), ('(', 55313), ('A', 7331), ('H', 6944), ('C', 6923), ('B', 6921), ('D', 6916), ('\\\\n', 6914), ('E', 6914), ('F', 6914)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7f6c2fa93f28>, {'<unk>': 0, '<pad>': 1, 'the': 2, 'of': 3, 'a': 4, 'and': 5, 'energy': 6, 'water': 7, '.': 8, 'to': 9, 'heat': 10, 'in': 11, 'plants': 12, 'it': 13, 'food': 14, 'animals': 15, 'cells': 16, 'light': 17, 'humans': 18, 'soil': 19, 'air': 20, 'body': 21, 'The': 22, 'A': 23, 'bacteria': 24, \"'s\": 25, 'Something': 26, 'chemical': 27, 'them': 28, ',': 29, 'an': 30, 'rain': 31, 'wind': 32, '-': 33, 'oxygen': 34, 'It': 35, 'insects': 36, 'on': 37, 'sunlight': 38, 'blood': 39, 'life': 40, 'their': 41, 'system': 42, 'organisms': 43, 'reproduction': 44, 'sun': 45, 'electricity': 46, 'is': 47, 'temperature': 48, 'acid': 49, 'viruses': 50, 'skin': 51, 'trees': 52, 'cancer': 53, 'from': 54, 'pressure': 55, 'carbon': 56, 'cold': 57, 'that': 58, 'warm': 59, 'waves': 60, 'weathering': 61, 'death': 62, 'electrons': 63, 'or': 64, 'Earth': 65, 'disease': 66, 'gas': 67, 'growth': 68, 'mitosis': 69, 'by': 70, 'sound': 71, 'DNA': 72, 'cell': 73, 'environment': 74, 'flow': 75, 'liquid': 76, 'weather': 77, 'They': 78, 'fire': 79, 'hormones': 80, 'living': 81, 'move': 82, 'birds': 83, 'changes': 84, 'eggs': 85, 'gametes': 86, 'molecules': 87, 'negative': 88, 'pollution': 89, 'dioxide': 90, 'fish': 91, 'hot': 92, 'kinetic': 93, 'mechanical': 94, 'precipitation': 95, 'protein': 96, 'proteins': 97, 'rocks': 98, 'sperm': 99, 'sugar': 100, 'survive': 101, 'they': 102, 'bees': 103, 'bushes': 104, 'can': 105, 'change': 106, 'fuel': 107, 'green': 108, 'into': 109, 'plant': 110, 'salt': 111, 'erosion': 112, 'flowers': 113, 'fuels': 114, 'genes': 115, 'grow': 116, 'h2o': 117, 'hair': 118, 'leaves': 119, 'mass': 120, 'with': 121, 'Water': 122, 'animal': 123, 'chemicals': 124, 'down': 125, 'dry': 126, 'for': 127, 'force': 128, 'frogs': 129, 'fungi': 130, 'genetic': 131, 'heart': 132, 'mammals': 133, 'natural': 134, 'photons': 135, 'rock': 136, 'starfish': 137, 'things': 138, 'In': 139, 'To': 140, 'backbone': 141, 'bamboo': 142, 'cooking': 143, 'dead': 144, 'deoxyribonucleic': 145, 'dogs': 146, 'electrical': 147, 'evaporation': 148, 'evolution': 149, 'fossil': 150, 'fur': 151, 'ice': 152, 'matter': 153, 'more': 154, 'nutrients': 155, 'organs': 156, 'some': 157, 'survival': 158, 'warming': 159, 'Plants': 160, 'are': 161, 'burning': 162, 'division': 163, 'fat': 164, 'female': 165, 'influenza': 166, 'its': 167, 'material': 168, 'motion': 169, 'moving': 170, 'population': 171, 'rays': 172, 'sex': 173, 'species': 174, 'sponges': 175, 'spores': 176, 'streams': 177, 'up': 178, 'Heat': 179, 'Light': 180, 'all': 181, 'at': 182, 'atmosphere': 183, 'black': 184, 'colors': 185, 'decreases': 186, 'dirt': 187, 'dormant': 188, 'earth': 189, 'electron': 190, 'exercise': 191, 'ferns': 192, 'fertilization': 193, 'global': 194, 'hydrogen': 195, 'nitrogen': 196, 'sea': 197, 'small': 198, 'through': 199, 'Food': 200, 'HIV': 201, 'adult': 202, 'be': 203, 'bonds': 204, 'cars': 205, 'cats': 206, 'clouds': 207, 'cycles': 208, 'earthquakes': 209, 'egg': 210, 'habitats': 211, 'harm': 212, 'have': 213, 'heating': 214, 'impact': 215, 'increase': 216, 'live': 217, 'most': 218, 'not': 219, 'object': 220, 'objects': 221, 'ocean': 222, 'oil': 223, 'out': 224, 'paper': 225, 'people': 226, 'positive': 227, 'rivers': 228, 'size': 229, 'temperatures': 230, 'time': 231, 'too': 232, 'winter': 233, 'RNA': 234, 'Sun': 235, 'acids': 236, 'antibodies': 237, 'antigens': 238, 'away': 239, 'axis': 240, 'bulbs': 241, 'cause': 242, 'cigarettes': 243, 'circuit': 244, 'coal': 245, 'cows': 246, 'damage': 247, 'decrease': 248, 'expands': 249, 'fats': 250, 'get': 251, 'glass': 252, 'ground': 253, 'high': 254, 'human': 255, 'invertebrates': 256, 'limestone': 257, 'long': 258, 'made': 259, 'magnetism': 260, 'male': 261, 'microorganisms': 262, 'microscope': 263, 'movement': 264, 'mutation': 265, 'nucleus': 266, 'oceans': 267, 'organism': 268, 'play': 269, 'produce': 270, 'roots': 271, 'seeds': 272, 'solid': 273, 'sweat': 274, 'together': 275, 'vapor': 276, 'warmth': 277, 'Bacteria': 278, 'Carbon': 279, 'bears': 280, 'breathing': 281, 'chlorophyll': 282, 'color': 283, 'decomposition': 284, 'die': 285, 'fatty': 286, 'fresh': 287, 'friction': 288, 'harmful': 289, 'heavy': 290, 'hurricanes': 291, 'immune': 292, 'level': 293, 'measure': 294, 'minerals': 295, 'mutations': 296, 'nervous': 297, 'ova': 298, 'pathogens': 299, 'power': 300, 'production': 301, 'radiation': 302, 'reaction': 303, 'remains': 304, 'reproduce': 305, 'river': 306, 'sense': 307, 'sexual': 308, 'shelter': 309, 'smell': 310, 'smoking': 311, 'space': 312, 'squids': 313, 'store': 314, 'structure': 315, 'structures': 316, 'substances': 317, 'tilt': 318, 'vibrations': 319, 'waste': 320, 'An': 321, 'Birds': 322, 'Energy': 323, 'Fungi': 324, 'H2O': 325, 'Helps': 326, 'Mechanical': 327, 'Oxygen': 328, 'Two': 329, 'Wind': 330, 'Winter': 331, 'aggression': 332, 'aging': 333, 'arteries': 334, 'batteries': 335, 'beams': 336, 'being': 337, 'bodily': 338, 'building': 339, 'car': 340, 'carbohydrates': 341, 'cellulose': 342, 'circulatory': 343, 'clams': 344, 'closed': 345, 'competition': 346, 'control': 347, 'copper': 348, 'coral': 349, 'current': 350, 'day': 351, 'different': 352, 'diversity': 353, 'during': 354, 'electric': 355, 'engines': 356, 'eukaryotes': 357, 'evergreens': 358, 'feet': 359, 'floods': 360, 'follicles': 361, 'fossils': 362, 'gene': 363, 'gets': 364, 'glucose': 365, 'habitat': 366, 'haploid': 367, 'homes': 368, 'immunity': 369, 'information': 370, 'liquids': 371, 'longer': 372, 'looseness': 373, 'lungs': 374, 'make': 375, 'metal': 376, 'neurons': 377, 'orchids': 378, 'organic': 379, 'other': 380, 'our': 381, 'planet': 382, 'plates': 383, 'preserved': 384, 'producers': 385, 'rotating': 386, 'sexually': 387, 'sleep': 388, 'slow': 389, 'smoke': 390, 'stove': 391, 'summer': 392, 'thunderstorms': 393, 'tobacco': 394, 'transfer': 395, 'travel': 396, 'underground': 397, 'weight': 398, 'will': 399, 'wood': 400, 'worms': 401, 'Air': 402, 'Cell': 403, 'Change': 404, 'Increase': 405, 'Sea': 406, 'Sediment': 407, 'Temperature': 408, 'Warm': 409, 'When': 410, 'absorb': 411, 'adaptation': 412, 'amount': 413, 'another': 414, 'ants': 415, 'around': 416, 'autumn': 417, 'balance': 418, 'bats': 419, 'bending': 420, 'bike': 421, 'boiling': 422, 'cacti': 423, 'celled': 424, 'children': 425, 'chromosomes': 426, 'climate': 427, 'complex': 428, 'compost': 429, 'daisies': 430, 'damaged': 431, 'darkness': 432, 'daylight': 433, 'depletion': 434, 'deposition': 435, 'deserts': 436, 'detritus': 437, 'deuterostomes': 438, 'differences': 439, 'digestive': 440, 'divide': 441, 'do': 442, 'eating': 443, 'ecosystems': 444, 'effect': 445, 'environments': 446, 'expand': 447, 'feathers': 448, 'find': 449, 'flooding': 450, 'form': 451, 'formed': 452, 'fronts': 453, 'gases': 454, 'goats': 455, 'health': 456, 'insulin': 457, 'iron': 458, 'keep': 459, 'less': 460, 'low': 461, 'lung': 462, 'marine': 463, 'messengers': 464, 'moist': 465, 'mold': 466, 'molds': 467, 'muscle': 468, 'mushrooms': 469, 'nematodes': 470, 'new': 471, 'night': 472, 'no': 473, 'nonliving': 474, 'oaks': 475, 'offspring': 476, 'over': 477, 'pH': 478, 'pain': 479, 'pancreas': 480, 'particles': 481, 'pesticides': 482, 'playing': 483, 'prokaryotes': 484, 'seasons': 485, 'sediment': 486, 'see': 487, 'seed': 488, 'shape': 489, 'shells': 490, 'shock': 491, 'sight': 492, 'single': 493, 'sodium': 494, 'solar': 495, 'spermatozoa': 496, 'star': 497, 'state': 498, 'stationary': 499, 'stem': 500, 'sweating': 501, 'systems': 502, 'tectonic': 503, 'thermal': 504, 'tilted': 505, 'tissue': 506, 'traits': 507, 'transport': 508, 'two': 509, 'used': 510, 'using': 511, 'varied': 512, 'vegetation': 513, 'vessels': 514, 'volume': 515, 'whales': 516, 'when': 517, \"'\": 518, '2': 519, 'Animals': 520, 'Carbohydrates': 521, 'Cells': 522, 'Coal': 523, 'Dehydration': 524, 'Evaporation': 525, 'Fossil': 526, 'Global': 527, 'Great': 528, 'Influenza': 529, 'Insects': 530, 'Jellyfish': 531, 'Joules': 532, 'Negative': 533, 'Nematoda': 534, 'Pacific': 535, 'Pollution': 536, 'Positive': 537, 'Power': 538, 'Rocks': 539, 'Roots': 540, 'Sand': 541, 'Sulfur': 542, 'Tobacco': 543, 'With': 544, 'absorbing': 545, 'activities': 546, 'adapt': 547, 'add': 548, 'algae': 549, 'alternation': 550, 'aquatic': 551, 'area': 552, 'atmospheric': 553, 'atoms': 554, 'become': 555, 'biodiversity': 556, 'blocks': 557, 'cartilage': 558, 'causes': 559, 'causing': 560, 'charge': 561, 'chlorine': 562, 'chloroplasts': 563, 'class': 564, 'coiled': 565, 'column': 566, 'complete': 567, 'compound': 568, 'consumers': 569, 'cooling': 570, 'cools': 571, 'courtship': 572, 'dark': 573, 'dating': 574, 'deadly': 575, 'decomposers': 576, 'density': 577, 'deposit': 578, 'destruction': 579, 'diploid': 580, 'dispersal': 581, 'drought': 582, 'drying': 583, 'earthworms': 584, 'ejaculation': 585, 'electromagnetic': 586, 'engine': 587, 'eras': 588, 'everywhere': 589, 'excretion': 590, 'expansion': 591, 'eyes': 592, 'fanning': 593, 'flows': 594, 'fuse': 595, 'gasoline': 596, 'generations': 597, 'gills': 598, 'gland': 599, 'go': 600, 'good': 601, 'gravity': 602, 'groundwater': 603, 'head': 604, 'hearing': 605, 'heated': 606, 'heats': 607, 'higher': 608, 'host': 609, 'illness': 610, 'increased': 611, 'increases': 612, 'injury': 613, 'inorganic': 614, 'jaws': 615, 'kill': 616, 'lack': 617, 'large': 618, 'layers': 619, 'laying': 620, 'leeches': 621, 'like': 622, 'loss': 623, 'lower': 624, 'magma': 625, 'making': 626, 'mate': 627, 'mating': 628, 'meat': 629, 'melting': 630, 'memory': 631, 'microscopic': 632, 'moisture': 633, 'multicellular': 634, 'muscles': 635, 'off': 636, 'olfaction': 637, 'organ': 638, 'outside': 639, 'ozone': 640, 'parasites': 641, 'permeable': 642, 'pheromones': 643, 'photosynthesis': 644, 'plankton': 645, 'planting': 646, 'plastic': 647, 'prism': 648, 'problems': 649, 'properly': 650, 'properties': 651, 'rainbow': 652, 'rainfall': 653, 'rains': 654, 'recycling': 655, 'release': 656, 'repair': 657, 'reptiles': 658, 'resources': 659, 'respiratory': 660, 'revolving': 661, 'rich': 662, 'ruler': 663, 'secondary': 664, 'seeping': 665, 'sequences': 666, 'sharks': 667, 'shell': 668, 'slate': 669, 'smaller': 670, 'smallest': 671, 'smallpox': 672, 'snail': 673, 'snails': 674, 'snow': 675, 'soils': 676, 'source': 677, 'speciation': 678, 'speed': 679, 'sports': 680, 'spring': 681, 'squirts': 682, 'stay': 683, 'stored': 684, 'storms': 685, 'stress': 686, 'support': 687, 'surfaces': 688, 'tape': 689, 'telomeres': 690, 'themselves': 691, 'tides': 692, 'transportation': 693, 'tropical': 694, 'turns': 695, 'urchins': 696, 'use': 697, 'vinegar': 698, 'vision': 699, 'within': 700, 'without': 701, 'Absorbing': 702, 'Animal': 703, 'Beavers': 704, 'Blood': 705, 'Break': 706, 'Burning': 707, 'By': 708, 'Chemical': 709, 'Condensation': 710, 'Decomposition': 711, 'Destroy': 712, 'Electric': 713, 'Electrical': 714, 'Fish': 715, 'Flowers': 716, 'Force': 717, 'Gas': 718, 'Gravity': 719, 'Grow': 720, 'H20': 721, 'Harm': 722, 'Heating': 723, 'Help': 724, 'Hot': 725, 'Human': 726, 'Lizards': 727, 'Male': 728, 'Mitosis': 729, 'Move': 730, 'Movement': 731, 'Mutations': 732, 'New': 733, 'Oil': 734, 'Physical': 735, 'Pituitary': 736, 'Plant': 737, 'Plastic': 738, 'Reproduce': 739, 'Salt': 740, 'San': 741, 'Skin': 742, 'Solar': 743, 'Starfish': 744, 'Summer': 745, 'Sunlight': 746, 'Through': 747, 'Thyroid': 748, 'Using': 749, 'Vapor': 750, 'Weathering': 751, 'Weight': 752, 'abalone': 753, 'ability': 754, 'absorbs': 755, 'aerobic': 756, 'age': 757, 'alleles': 758, 'amounts': 759, 'amphibians': 760, 'androgens': 761, 'annelids': 762, 'antibiotics': 763, 'apoptosis': 764, 'arthropods': 765, 'asexual': 766, 'automobile': 767, 'backbones': 768, 'barrel': 769, 'battery': 770, 'beavers': 771, 'becomes': 772, 'behave': 773, 'behavior': 774, 'beings': 775, 'better': 776, 'biomes': 777, 'blue': 778, 'bone': 779, 'bones': 780, 'bottleneck': 781, 'brain': 782, 'breaking': 783, 'breathe': 784, 'bright': 785, 'bubbles': 786, 'budding': 787, 'burned': 788, 'cactus': 789, 'calcium': 790, 'calories': 791, 'canyons': 792, 'carbonate': 793, 'carcinogens': 794, 'centralized': 795, 'chain': 796, 'changing': 797, 'chloride': 798, 'chromosome': 799, 'cilia': 800, 'clean': 801, 'climates': 802, 'colder': 803, 'compounds': 804, 'compressional': 805, 'condensation': 806, 'conditions': 807, 'conserve': 808, 'contact': 809, 'contractions': 810, 'converting': 811, 'cool': 812, 'cooled': 813, 'core': 814, 'corn': 815, 'crash': 816, 'crust': 817, 'cup': 818, 'currents': 819, 'cylinder': 820, 'cytoplasm': 821, 'deeper': 822, 'deforestation': 823, 'degrees': 824, 'dehydration': 825, 'dermis': 826, 'desert': 827, 'destroying': 828, 'devastating': 829, 'diabetes': 830, 'digestion': 831, 'dinosaurs': 832, 'diseases': 833, 'distance': 834, 'distracted': 835, 'dolphins': 836, 'doorbell': 837, 'drinking': 838, 'ducks': 839, 'ear': 840, 'ears': 841, 'elephants': 842, 'elms': 843, 'enzymes': 844, 'eroding': 845, 'excretory': 846, 'exposure': 847, 'extend': 848, 'extinction': 849, 'extreme': 850, 'fall': 851, 'family': 852, 'faulting': 853, 'females': 854, 'fingerprint': 855, 'flashlight': 856, 'fly': 857, 'forest': 858, 'found': 859, 'four': 860, 'free': 861, 'freezing': 862, 'frequency': 863, 'fruit': 864, 'function': 865, 'fusion': 866, 'gammaglobulins': 867, 'garbage': 868, 'geologic': 869, 'gold': 870, 'graduated': 871, 'grass': 872, 'gravitational': 873, 'greenhouse': 874, 'growing': 875, 'grows': 876, 'has': 877, 'hazard': 878, 'healing': 879, 'helps': 880, 'honeybees': 881, 'humidity': 882, 'hydration': 883, 'ill': 884, 'infection': 885, 'inside': 886, 'insulated': 887, 'insulation': 888, 'irradiation': 889, 'jellyfish': 890, 'keeps': 891, 'keratin': 892, 'kidneys': 893, 'lakes': 894, 'largest': 895, 'leave': 896, 'legumes': 897, 'lens': 898, 'leukemia': 899, 'lipids': 900, 'lizards': 901, 'location': 902, 'loose': 903, 'lose': 904, 'macroevolution': 905, 'makes': 906, 'measuring': 907, 'membranes': 908, 'mercury': 909, 'meristems': 910, 'metals': 911, 'microbes': 912, 'middle': 913, 'migration': 914, 'minute': 915, 'mirror': 916, 'monkeys': 917, 'monoxide': 918, 'moon': 919, 'moss': 920, 'mountain': 921, 'mountains': 922, 'mussels': 923, 'nerve': 924, 'nerves': 925, 'noise': 926, 'nuclear': 927, 'nucleic': 928, 'nutrient': 929, 'oak': 930, 'ovaries': 931, 'own': 932, 'oxidation': 933, 'oysters': 934, 'peat': 935, 'perspiration': 936, 'phospholipids': 937, 'photosynthetic': 938, 'phototropism': 939, 'pine': 940, 'platypus': 941, 'point': 942, 'position': 943, 'predators': 944, 'pregnancy': 945, 'prevent': 946, 'producing': 947, 'protection': 948, 'pugs': 949, 'quills': 950, 'rabies': 951, 'random': 952, 'reactions': 953, 'reduced': 954, 'reefs': 955, 'resistance': 956, 'respiration': 957, 'response': 958, 'rest': 959, 'rise': 960, 'rotation': 961, 'running': 962, 'salamanders': 963, 'salinity': 964, 'salmon': 965, 'sand': 966, 'sandstone': 967, 'saturated': 968, 'season': 969, 'segmented': 970, 'senses': 971, 'serious': 972, 'shorter': 973, 'sick': 974, 'side': 975, 'simple': 976, 'simplest': 977, 'snakes': 978, 'solute': 979, 'specialization': 980, 'squid': 981, 'stamina': 982, 'started': 983, 'steam': 984, 'stomach': 985, 'stops': 986, 'stream': 987, 'strength': 988, 'sulfur': 989, 'surface': 990, 'surgery': 991, 'swell': 992, 'tapeworms': 993, 'than': 994, 'thick': 995, 'thirst': 996, 'thrive': 997, 'thyroid': 998, 'tissues': 999, 'translation': 1000, 'tree': 1001, 'trout': 1002, 'trunk': 1003, 'tulips': 1004, 'turn': 1005, 'type': 1006, 'types': 1007, 'uncontrolled': 1008, 'under': 1009, 'urine': 1010, 'uterus': 1011, 'valley': 1012, 'vascular': 1013, 'vibrate': 1014, 'vibrating': 1015, 'vibration': 1016, 'visible': 1017, 'waters': 1018, 'wave': 1019, 'ways': 1020, 'wet': 1021, 'wheat': 1022, 'windward': 1023, 'wings': 1024, 'wiring': 1025, 'years': 1026, \"'re\": 1027, 'AIDS': 1028, 'Adult': 1029, 'Aging': 1030, 'Amphibians': 1031, 'Aves': 1032, 'Balance': 1033, 'Bees': 1034, 'Being': 1035, 'Breaks': 1036, 'Breathing': 1037, 'C': 1038, 'CO': 1039, 'CO2': 1040, 'Cancer': 1041, 'Carpooling': 1042, 'Changing': 1043, 'Chlorophyll': 1044, 'Clouds': 1045, 'Cold': 1046, 'Communication': 1047, 'Cooking': 1048, 'Copper': 1049, 'Coral': 1050, 'Create': 1051, 'Damage': 1052, 'Dead': 1053, 'Deforestation': 1054, 'Deoxyribonucleicacid': 1055, 'Division': 1056, 'Drought': 1057, 'Earthquakes': 1058, 'Eating': 1059, 'Electricity': 1060, 'Electricty': 1061, 'Electromagnetic': 1062, 'Embryo': 1063, 'Environment': 1064, 'Enzymes': 1065, 'Estrogen': 1066, 'Evolution': 1067, 'Fat': 1068, 'Fertilization': 1069, 'Fertilizer': 1070, 'Francisco': 1071, 'Freezing': 1072, 'Genetic': 1073, 'Granite': 1074, 'Green': 1075, 'Greenhouse': 1076, 'Having': 1077, 'Head': 1078, 'Heart': 1079, 'Heavy': 1080, 'Heterotrophs': 1081, 'Higher': 1082, 'Humans': 1083, 'Hydrogen': 1084, 'Hypothermia': 1085, 'If': 1086, 'Inches': 1087, 'Keeps': 1088, 'LEDs': 1089, 'Legionnaires': 1090, 'Life': 1091, 'Logging': 1092, 'Looking': 1093, 'Lungs': 1094, 'Makes': 1095, 'Males': 1096, 'Marble': 1097, 'Mass': 1098, 'Measuring': 1099, 'Moss': 1100, 'NaCl': 1101, 'Newton': 1102, 'Organelles': 1103, 'Organisms': 1104, 'Organs': 1105, 'People': 1106, 'Pesticides': 1107, 'Plankton': 1108, 'Potassium': 1109, 'Rain': 1110, 'Rainbows': 1111, 'Reduces': 1112, 'Reducing': 1113, 'Removing': 1114, 'Reproduction': 1115, 'Riding': 1116, 'River': 1117, 'Roundworms': 1118, 'Sensing': 1119, 'Sheds': 1120, 'Short': 1121, 'Sight': 1122, 'Sleep': 1123, 'Small': 1124, 'Soil': 1125, 'Some': 1126, 'Sweating': 1127, 'Tail': 1128, 'Testosterone': 1129, 'Their': 1130, 'Tiny': 1131, 'Trees': 1132, 'Tropical': 1133, 'UV': 1134, 'Unequal': 1135, 'Uterus': 1136, 'Vanes': 1137, 'Venus': 1138, 'Viruses': 1139, 'Vomiting': 1140, 'Yeast': 1141, 'absorption': 1142, 'acidity': 1143, 'across': 1144, 'activation': 1145, 'active': 1146, 'adaptations': 1147, 'adaptive': 1148, 'adding': 1149, 'adrenaline': 1150, 'adults': 1151, 'agent': 1152, 'agents': 1153, 'aggressive': 1154, 'alcohol': 1155, 'alligators': 1156, 'amoebae': 1157, 'amphibian': 1158, 'anchors': 1159, 'apart': 1160, 'archegonium': 1161, 'areas': 1162, 'arsenic': 1163, 'attract': 1164, 'baby': 1165, 'bacterial': 1166, 'bags': 1167, 'bark': 1168, 'basalt': 1169, 'bases': 1170, 'basic': 1171, 'bass': 1172, 'beaks': 1173, 'before': 1174, 'behind': 1175, 'between': 1176, 'biceps': 1177, 'bigger': 1178, 'biology': 1179, 'bird': 1180, 'blastoids': 1181, 'bleach': 1182, 'bloodstream': 1183, 'bottom': 1184, 'break': 1185, 'breaks': 1186, 'breezes': 1187, 'broken': 1188, 'bulb': 1189, 'buoyancy': 1190, 'burn': 1191, 'burns': 1192, 'burrows': 1193, 'burst': 1194, 'canyon': 1195, 'cavern': 1196, 'caves': 1197, 'centrioles': 1198, 'chance': 1199, 'charges': 1200, 'chickenpox': 1201, 'child': 1202, 'chill': 1203, 'chromatic': 1204, 'chronic': 1205, 'cigars': 1206, 'coastal': 1207, 'coat': 1208, 'coats': 1209, 'code': 1210, 'colonies': 1211, 'colored': 1212, 'communication': 1213, 'communities': 1214, 'complexity': 1215, 'computers': 1216, 'condensing': 1217, 'conducting': 1218, 'conduction': 1219, 'conservation': 1220, 'constriction': 1221, 'contaminated': 1222, 'covered': 1223, 'covering': 1224, 'creating': 1225, 'creatures': 1226, 'creeks': 1227, 'cross': 1228, 'cutting': 1229, 'cyclones': 1230, 'daily': 1231, 'damming': 1232, 'dangerous': 1233, 'decay': 1234, 'decaying': 1235, 'decomposing': 1236, 'deep': 1237, 'delta': 1238, 'desertification': 1239, 'details': 1240, 'detect': 1241, 'developments': 1242, 'devices': 1243, 'dew': 1244, 'dies': 1245, 'diet': 1246, 'differentiation': 1247, 'diffusion': 1248, 'dilation': 1249, 'direct': 1250, 'dispersion': 1251, 'dissolve': 1252, 'distances': 1253, 'disturb': 1254, 'diverse': 1255, 'divided': 1256, 'dolphin': 1257, 'dorsal': 1258, 'drink': 1259, 'driving': 1260, 'dromaeosaurs': 1261, 'easier': 1262, 'easily': 1263, 'eaten': 1264, 'echinoderms': 1265, 'echolocation': 1266, 'edible': 1267, 'eight': 1268, 'embryos': 1269, 'emissions': 1270, 'endangered': 1271, 'endoplasmic': 1272, 'entire': 1273, 'epidermis': 1274, 'equator': 1275, 'equilibrium': 1276, 'essential': 1277, 'estuaries': 1278, 'ethanol': 1279, 'evaporated': 1280, 'evolving': 1281, 'excess': 1282, 'exoskeleton': 1283, 'exoskeletons': 1284, 'explode': 1285, 'external': 1286, 'falling': 1287, 'feed': 1288, 'feeders': 1289, 'fermentation': 1290, 'fever': 1291, 'fiber': 1292, 'fibers': 1293, 'fibrous': 1294, 'filament': 1295, 'firecracker': 1296, 'fissures': 1297, 'flashlights': 1298, 'flood': 1299, 'flower': 1300, 'flowing': 1301, 'fluid': 1302, 'forces': 1303, 'foreign': 1304, 'forever': 1305, 'foxes': 1306, 'freezes': 1307, 'frog': 1308, 'fun': 1309, 'functions': 1310, 'fuzzy': 1311, 'gamete': 1312, 'gametophytes': 1313, 'gecko': 1314, 'generate': 1315, 'generating': 1316, 'generator': 1317, 'generators': 1318, 'genus': 1319, 'geometric': 1320, 'glands': 1321, 'glomerulus': 1322, 'goes': 1323, 'goggles': 1324, 'grams': 1325, 'granite': 1326, 'grapevines': 1327, 'grasses': 1328, 'greenhouses': 1329, 'groups': 1330, 'harming': 1331, 'hats': 1332, 'help': 1333, 'heterotrophs': 1334, 'hitting': 1335, 'hole': 1336, 'hormone': 1337, 'hosts': 1338, 'housing': 1339, 'hummingbirds': 1340, 'hydrophobic': 1341, 'hyperthyroidism': 1342, 'immobile': 1343, 'important': 1344, 'inches': 1345, 'incinerated': 1346, 'increasing': 1347, 'inner': 1348, 'instructions': 1349, 'insulator': 1350, 'intestine': 1351, 'irradiated': 1352, 'irradiating': 1353, 'isotopes': 1354, 'just': 1355, 'keeping': 1356, 'kicked': 1357, 'killed': 1358, 'killing': 1359, 'lake': 1360, 'landscape': 1361, 'landscapes': 1362, 'landslides': 1363, 'laser': 1364, 'last': 1365, 'lay': 1366, 'layer': 1367, 'learn': 1368, 'learning': 1369, 'length': 1370, 'lenses': 1371, 'lightning': 1372, 'limbs': 1373, 'lions': 1374, 'local': 1375, 'located': 1376, 'losing': 1377, 'lymphocytes': 1378, 'magnifying': 1379, 'manatees': 1380, 'many': 1381, 'marble': 1382, 'mask': 1383, 'massive': 1384, 'mates': 1385, 'mature': 1386, 'meiosis': 1387, 'members': 1388, 'menopause': 1389, 'menstruation': 1390, 'meristem': 1391, 'microscopes': 1392, 'migrate': 1393, 'mildew': 1394, 'milk': 1395, 'molecule': 1396, 'moles': 1397, 'mollusks': 1398, 'mosquitoes': 1399, 'mostly': 1400, 'motility': 1401, 'moves': 1402, 'mutated': 1403, \"n't\": 1404, 'native': 1405, 'near': 1406, 'nectar': 1407, 'nephrons': 1408, 'nesting': 1409, 'nucleotides': 1410, 'number': 1411, 'occurs': 1412, 'odors': 1413, 'old': 1414, 'omnivores': 1415, 'one': 1416, 'open': 1417, 'opposing': 1418, 'opposite': 1419, 'orbit': 1420, 'orthomyxoviruses': 1421, 'ovum': 1422, 'p53': 1423, 'palm': 1424, 'pan': 1425, 'panels': 1426, 'parasitic': 1427, 'parts': 1428, 'pattern': 1429, 'periods': 1430, 'person': 1431, 'petals': 1432, 'phase': 1433, 'pheremones': 1434, 'phone': 1435, 'phones': 1436, 'photosynthesize': 1437, 'phylum': 1438, 'physical': 1439, 'phytoplankton': 1440, 'pigs': 1441, 'plains': 1442, 'planktonic': 1443, 'plasma': 1444, 'plucking': 1445, 'poisoning': 1446, 'pollen': 1447, 'pollination': 1448, 'pollutants': 1449, 'polyps': 1450, 'ponds': 1451, 'potassium': 1452, 'potatoes': 1453, 'prehistoric': 1454, 'preservatives': 1455, 'prey': 1456, 'pride': 1457, 'process': 1458, 'produced': 1459, 'produces': 1460, 'prolactin': 1461, 'protect': 1462, 'protected': 1463, 'protects': 1464, 'pupils': 1465, 'quality': 1466, 'radio': 1467, 'rainbows': 1468, 'rainy': 1469, 'rapid': 1470, 'rate': 1471, 'ray': 1472, 'receptors': 1473, 'red': 1474, 'reduce': 1475, 'reduces': 1476, 'reducing': 1477, 'refract': 1478, 'refrigerators': 1479, 'regions': 1480, 'regulated': 1481, 'reindeer': 1482, 'relaxation': 1483, 'remove': 1484, 'repel': 1485, 'reproductive': 1486, 'requirement': 1487, 'reservoirs': 1488, 'reticulum': 1489, 'riding': 1490, 'rings': 1491, 'rises': 1492, 'roads': 1493, 'rodent': 1494, 'root': 1495, 'roses': 1496, 'rotting': 1497, 'roundworms': 1498, 'rubbing': 1499, 'runoff': 1500, 'safety': 1501, 'salts': 1502, 'scales': 1503, 'secretion': 1504, 'sedimentary': 1505, 'seeing': 1506, 'seen': 1507, 'selection': 1508, 'self': 1509, 'sensory': 1510, 'sessile': 1511, 'shaking': 1512, 'shallow': 1513, 'sheep': 1514, 'shelf': 1515, 'shivering': 1516, 'similar': 1517, 'six': 1518, 'skeleton': 1519, 'slime': 1520, 'smells': 1521, 'snort': 1522, 'soft': 1523, 'solidifies': 1524, 'spectrum': 1525, 'sphincter': 1526, 'spinal': 1527, 'spinning': 1528, 'stage': 1529, 'stars': 1530, 'steak': 1531, 'steel': 1532, 'stems': 1533, 'stopping': 1534, 'stroke': 1535, 'supplies': 1536, 'sustain': 1537, 'swim': 1538, 'swimming': 1539, 'symmetry': 1540, 'tRNA': 1541, 'tail': 1542, 'tar': 1543, 'taste': 1544, 'telephone': 1545, 'telephones': 1546, 'territory': 1547, 'texture': 1548, 'thawing': 1549, 'three': 1550, 'tigers': 1551, 'tiny': 1552, 'toxic': 1553, 'trails': 1554, 'trait': 1555, 'trophic': 1556, 'tumor': 1557, 'turbines': 1558, 'turning': 1559, 'turtles': 1560, 'unequal': 1561, 'units': 1562, 'unstable': 1563, 'urinary': 1564, 'vaccination': 1565, 'vapors': 1566, 'variation': 1567, 'veins': 1568, 'vertebral': 1569, 'vertebrates': 1570, 'very': 1571, 'vines': 1572, 'violence': 1573, 'visibility': 1574, 'vitamin': 1575, 'walking': 1576, 'walls': 1577, 'waterproof': 1578, 'watts': 1579, 'wearing': 1580, 'well': 1581, 'wells': 1582, 'wetlands': 1583, 'whale': 1584, 'where': 1585, 'white': 1586, 'wider': 1587, 'wildlife': 1588, 'willow': 1589, 'woody': 1590, 'work': 1591, 'worldwide': 1592, 'wounds': 1593, 'xerophytes': 1594, 'yes': 1595, 'zygote': 1596, 'zygotes': 1597, ' ': 1598, '&': 1599, '12': 1600, '14': 1601, '180': 1602, '25': 1603, '32': 1604, '35': 1605, '5.6': 1606, '65': 1607, '747': 1608, '?': 1609, 'AA': 1610, 'Abalone': 1611, 'Abnormal': 1612, 'Acidic': 1613, 'Acids': 1614, 'Active': 1615, 'Adatation': 1616, 'Adding': 1617, 'After': 1618, 'Alcohol': 1619, 'Alcohols': 1620, 'Allow': 1621, 'Aminoacylation': 1622, 'Andreas': 1623, 'Antarctica': 1624, 'Antennae': 1625, 'Anthrax': 1626, 'Antibiotics': 1627, 'Antibodies': 1628, 'Ants': 1629, 'Applying': 1630, 'Arteries': 1631, 'As': 1632, 'Asbestos': 1633, 'Asexual': 1634, 'At': 1635, 'Atlantic': 1636, 'Autumn': 1637, 'Away': 1638, 'Azathioprine': 1639, 'B': 1640, 'Barometer': 1641, 'Barrel': 1642, 'Barrier': 1643, 'Bats': 1644, 'Batteries': 1645, 'Beauty': 1646, 'Bend': 1647, 'Bending': 1648, 'Bermuda': 1649, 'Better': 1650, 'Biologists': 1651, 'Biomass': 1652, 'Biomolecules': 1653, 'Bird': 1654, 'Black': 1655, 'Blankets': 1656, 'Blowing': 1657, 'Blubber': 1658, 'Blue': 1659, 'Bodily': 1660, 'Boeing': 1661, 'Boiling': 1662, 'Boils': 1663, 'Both': 1664, 'Botulism': 1665, 'Brittle': 1666, 'Broken': 1667, 'Brutal': 1668, 'Budding': 1669, 'Bundles': 1670, 'Burma': 1671, 'Burying': 1672, 'CFCs': 1673, 'Cadillac': 1674, 'Calcium': 1675, 'Candida': 1676, 'Cane': 1677, 'Carrots': 1678, 'Cars': 1679, 'Caspian': 1680, 'Catch': 1681, 'Cause': 1682, 'Causes': 1683, 'Celsius': 1684, 'Cephalopod': 1685, 'Cephalopods': 1686, 'Changes': 1687, 'Chemicals': 1688, 'Chemotherapy': 1689, 'Chimneys': 1690, 'Chlorine': 1691, 'Chlorophyl': 1692, 'Cigarettes': 1693, 'Circuits': 1694, 'Clams': 1695, 'Clear': 1696, 'Closed': 1697, 'Columbia': 1698, 'Competing': 1699, 'Competition': 1700, 'Complete': 1701, 'Complex': 1702, 'Conception': 1703, 'Cones': 1704, 'Conifers': 1705, 'Constant': 1706, 'Constrict': 1707, 'Constriction': 1708, 'Consumers': 1709, 'Consuming': 1710, 'Contact': 1711, 'Contaminated': 1712, 'Continents': 1713, 'Contract': 1714, 'Converted': 1715, 'Converting': 1716, 'Converts': 1717, 'Cook': 1718, 'Cooling': 1719, 'Copepods': 1720, 'Corn': 1721, 'Created': 1722, 'Creating': 1723, 'Crewmembers': 1724, 'Crops': 1725, 'Currents': 1726, 'Cytokinesis': 1727, 'D': 1728, 'Daisy': 1729, 'Damaged': 1730, 'Damages': 1731, 'Dance': 1732, 'Darkness': 1733, 'Darwinism': 1734, 'David': 1735, 'Deaf': 1736, 'Decibels': 1737, 'Decomposers': 1738, 'Decrease': 1739, 'Decreased': 1740, 'Decreases': 1741, 'Depleting': 1742, 'Deposition': 1743, 'Depositions': 1744, 'Desertification': 1745, 'Deserts': 1746, 'Dew': 1747, 'Differences': 1748, 'Digesting': 1749, 'Dilution': 1750, 'Dioxide': 1751, 'Direct': 1752, 'Discrete': 1753, 'Dissolved': 1754, 'Distance': 1755, 'Diurnal': 1756, 'Diversity': 1757, 'Drink': 1758, 'Drips': 1759, 'Dry': 1760, 'Due': 1761, 'Duracell': 1762, 'Ears': 1763, 'Earthworm': 1764, 'Eat': 1765, 'Echinoderm': 1766, 'Echinoderms': 1767, 'Ecosystems': 1768, 'Edible': 1769, 'Electron': 1770, 'Electrons': 1771, 'Emit': 1772, 'Encoded': 1773, 'Endocrine': 1774, 'Engine': 1775, 'Eons': 1776, 'Erode': 1777, 'Erosion': 1778, 'Ethologist': 1779, 'Euphrates': 1780, 'Evolving': 1781, 'Excision': 1782, 'Excretion': 1783, 'Excretory': 1784, 'Exfoliation': 1785, 'Expandable': 1786, 'Expands': 1787, 'Extend': 1788, 'Eye': 1789, 'Eyes': 1790, 'Fahrenheit': 1791, 'Fall': 1792, 'Famine': 1793, 'Faraday': 1794, 'Fe': 1795, 'Female': 1796, 'Fiber': 1797, 'Fight': 1798, 'Final': 1799, 'Fine': 1800, 'Firecrackers': 1801, 'Five': 1802, 'Flask': 1803, 'Flowing': 1804, 'Fog': 1805, 'For': 1806, 'Fresh': 1807, 'Freshwater': 1808, 'Frog': 1809, 'Frogs': 1810, 'From': 1811, 'Frostbite': 1812, 'Fruits': 1813, 'Fuel': 1814, 'Fully': 1815, 'Fun': 1816, 'Fungus': 1817, 'Gamete': 1818, 'Gases': 1819, 'Gasoline': 1820, 'Germy': 1821, 'Get': 1822, 'Give': 1823, 'Gladioluses': 1824, 'Glass': 1825, 'Glucose': 1826, 'Go': 1827, 'Gold': 1828, 'Gorge': 1829, 'Grass': 1830, 'Graves': 1831, 'Gunshots': 1832, 'H': 1833, 'HPV': 1834, 'Habitat': 1835, 'Hail': 1836, 'Hair': 1837, 'Haploid': 1838, 'Harming': 1839, 'Harms': 1840, 'Hearing': 1841, 'Hemoglobin': 1842, 'Herpes': 1843, 'High': 1844, 'Himalayas': 1845, 'Holding': 1846, 'Honeybees': 1847, 'Hormone': 1848, 'Hormones': 1849, 'Humidity': 1850, 'Hummingbirds': 1851, 'Ice': 1852, 'Igneous': 1853, 'Impurities': 1854, 'Increases': 1855, 'Inertial': 1856, 'Infection': 1857, 'Inheriting': 1858, 'Injury': 1859, 'Instructions': 1860, 'Insulation': 1861, 'Invertebrates': 1862, 'Irradiated': 1863, 'Jet': 1864, 'Jupiter': 1865, 'Keep': 1866, 'Keeping': 1867, 'Kidneys': 1868, 'Kill': 1869, 'Kills': 1870, 'Kinetic': 1871, 'Kite': 1872, 'Lake': 1873, 'Lakes': 1874, 'Large': 1875, 'Laying': 1876, 'Lead': 1877, 'Leaving': 1878, 'Leeches': 1879, 'Leeuwenhoek': 1880, 'Length': 1881, 'Leo': 1882, 'Leptospirosis': 1883, 'Less': 1884, 'Leukemia': 1885, 'Lift': 1886, 'Lighting': 1887, 'Lilies': 1888, 'Limbs': 1889, 'Limestone': 1890, 'Link': 1891, 'Liters': 1892, 'Live': 1893, 'Loamy': 1894, 'Localized': 1895, 'Loonhaunt': 1896, 'Loose': 1897, 'Looseness': 1898, 'Losing': 1899, 'Loud': 1900, 'Lower': 1901, 'Lymphocytes': 1902, 'Machines': 1903, 'Magnetism': 1904, 'Maintain': 1905, 'Malignant': 1906, 'Malnutrition': 1907, 'Malpighian': 1908, 'Mammal': 1909, 'Man': 1910, 'Mantle': 1911, 'Many': 1912, 'Marine': 1913, 'Mars': 1914, 'Material': 1915, 'Measles': 1916, 'Meiosis': 1917, 'Melts': 1918, 'Memory': 1919, 'Menstruation': 1920, 'Meristems': 1921, 'Merkel': 1922, 'Metal': 1923, 'Metling': 1924, 'Mexico': 1925, 'Michael': 1926, 'Microscopic': 1927, 'Migration': 1928, 'Misell': 1929, 'Mixing': 1930, 'Moist': 1931, 'Moisture': 1932, 'Molasses': 1933, 'Molds': 1934, 'Molecular': 1935, 'Mollusks': 1936, 'Monosaccharides': 1937, 'Monotremes': 1938, 'Moon': 1939, 'Most': 1940, 'Motility': 1941, 'Mountains': 1942, 'Mulberry': 1943, 'Muscle': 1944, 'Muscles': 1945, 'Mushrooms': 1946, 'Mutagenic': 1947, 'Napa': 1948, 'Natural': 1949, 'Near': 1950, 'Nectar': 1951, 'Nematodes': 1952, 'Nervous': 1953, 'Nesco': 1954, 'Neurotransmitters': 1955, 'Nitrogen': 1956, 'Nonliving': 1957, 'Nucleotide': 1958, 'Nurseries': 1959, 'O': 1960, 'O2': 1961, 'O3': 1962, 'Oak': 1963, 'Object': 1964, 'Objects': 1965, 'One': 1966, 'Operating': 1967, 'Orange': 1968, 'Orchids': 1969, 'Organic': 1970, 'Origami': 1971, 'Our': 1972, 'Ozone': 1973, 'PDAs': 1974, 'Pairs': 1975, 'Pancreas': 1976, 'Papilloma': 1977, 'Part': 1978, 'Particulate': 1979, 'Pathogens': 1980, 'Person': 1981, 'Perspiration': 1982, 'Pesticide': 1983, 'Pheromones': 1984, 'Photons': 1985, 'Photosynthesis': 1986, 'Physiology': 1987, 'Pines': 1988, 'Pins': 1989, 'Pipets': 1990, 'Plastics': 1991, 'Plucking': 1992, 'Poison': 1993, 'Poisoning': 1994, 'Pollen': 1995, 'Polynucleotide': 1996, 'Population': 1997, 'Possibly': 1998, 'Predators': 1999, 'Premature': 2000, 'Preserves': 2001, 'Pressure': 2002, 'Preventing': 2003, 'Primary': 2004, 'Prolactin': 2005, 'Propubic': 2006, 'Protect': 2007, 'Protection': 2008, 'Protons': 2009, 'Provide': 2010, 'Pulmonologists': 2011, 'Quality': 2012, 'Radio': 2013, 'Radioactive': 2014, 'Raft': 2015, 'Rainforests': 2016, 'Raven': 2017, 'Receptor': 2018, 'Reef': 2019, 'Reefs': 2020, 'Refract': 2021, 'Regrows': 2022, 'Release': 2023, 'Remove': 2024, 'Reproducing': 2025, 'Reproductive': 2026, 'Reptiles': 2027, 'Reptilia': 2028, 'Respiration': 2029, 'Retina': 2030, 'Reusing': 2031, 'Revolution': 2032, 'Ribosomes': 2033, 'Ribs': 2034, 'Rio': 2035, 'Rise': 2036, 'Roads': 2037, 'Rob': 2038, 'Room': 2039, 'SNPs': 2040, 'SO2': 2041, 'Salamander': 2042, 'Salinity': 2043, 'Same': 2044, 'Samsung': 2045, 'Scent': 2046, 'Schwinn': 2047, 'Scrambling': 2048, 'Seasons': 2049, 'Sedimentary': 2050, 'Seeds': 2051, 'Seeing': 2052, 'Seesaw': 2053, 'Seismometer': 2054, 'September': 2055, 'Severe': 2056, 'Sexual': 2057, 'Sexually': 2058, 'Shallow': 2059, 'Shark': 2060, 'Shasta': 2061, 'Slow': 2062, 'Slowing': 2063, 'Smoking': 2064, 'Smooth': 2065, 'Snake': 2066, 'Social': 2067, 'Sodium': 2068, 'Soft': 2069, 'Softwoods': 2070, 'Somethign': 2071, 'Sonar': 2072, 'Sonoma': 2073, 'Sound': 2074, 'Sounds': 2075, 'Specialized': 2076, 'Species': 2077, 'Sperm': 2078, 'Split': 2079, 'Spread': 2080, 'Spring': 2081, 'Stay': 2082, 'Steam': 2083, 'Steroid': 2084, 'Store': 2085, 'Storing': 2086, 'Streams': 2087, 'Stress': 2088, 'Sudden': 2089, 'Sustain': 2090, 'Sweet': 2091, 'Syngamy': 2092, 'TRH': 2093, 'Tectonic': 2094, 'Thames': 2095, 'Thyroxine': 2096, 'Tibet': 2097, 'Time': 2098, 'Titanic': 2099, 'Towards': 2100, 'Toxicity': 2101, 'Toxins': 2102, 'Toyota': 2103, 'Transfers': 2104, 'Transplanted': 2105, 'Trophic': 2106, 'Tungsten': 2107, 'Turning': 2108, 'Turns': 2109, 'UVA': 2110, 'UVB': 2111, 'Ultraviolet': 2112, 'Under': 2113, 'Uneven': 2114, 'Uplift': 2115, 'Urea': 2116, 'Urethra': 2117, 'Use': 2118, 'Valleys': 2119, 'Veins': 2120, 'Vertebrates': 2121, 'Very': 2122, 'Vibrates': 2123, 'Vibrations': 2124, 'Vitamin': 2125, 'Volcanic': 2126, 'Walls': 2127, 'Warmth': 2128, 'Warriors': 2129, 'Whales': 2130, 'Where': 2131, 'Wild': 2132, 'Windward': 2133, 'Wither': 2134, 'Worldwide': 2135, 'Your': 2136, 'Yukon': 2137, 'Zinc': 2138, 'abcess': 2139, 'abdomen': 2140, 'about': 2141, 'above': 2142, 'abrasion': 2143, 'absolute': 2144, 'absorbed': 2145, 'abundant': 2146, 'abusive': 2147, 'acceleration': 2148, 'accident': 2149, 'accommodate': 2150, 'accumulate': 2151, 'accumulated': 2152, 'acetic': 2153, 'acidic': 2154, 'action': 2155, 'activated': 2156, 'adapted': 2157, 'adhesives': 2158, 'adjustable': 2159, 'adulthood': 2160, 'advances': 2161, 'advancing': 2162, 'advantageous': 2163, 'aerate': 2164, 'aeration': 2165, 'aerodynamics': 2166, 'after': 2167, 'agriculture': 2168, 'ahead': 2169, 'airplane': 2170, 'airplanes': 2171, 'alarm': 2172, 'albino': 2173, 'albumin': 2174, 'alive': 2175, 'allergic': 2176, 'allergy': 2177, 'allow': 2178, 'allows': 2179, 'alluvial': 2180, 'almost': 2181, 'alpacas': 2182, 'alter': 2183, 'aluminum': 2184, 'alveoli': 2185, 'amniotes': 2186, 'amnotic': 2187, 'amoeba': 2188, 'amputation': 2189, 'anabolic': 2190, 'analysis': 2191, 'anemia': 2192, 'aneurisms': 2193, 'angle': 2194, 'anion': 2195, 'anostrans': 2196, 'antennae': 2197, 'antigen': 2198, 'aphrodisiacs': 2199, 'apocymarin': 2200, 'appendages': 2201, 'apple': 2202, 'apples': 2203, 'appliance': 2204, 'application': 2205, 'aqueous': 2206, 'arms': 2207, 'arterioles': 2208, 'artery': 2209, 'arthropoda': 2210, 'as': 2211, 'asbestos': 2212, 'assassin': 2213, 'asses': 2214, 'asteroid': 2215, 'asthma': 2216, 'astroglial': 2217, 'astronauts': 2218, 'athletes': 2219, 'attach': 2220, 'attached': 2221, 'attacks': 2222, 'attracted': 2223, 'attracting': 2224, 'attractive': 2225, 'attracts': 2226, 'auburn': 2227, 'audible': 2228, 'audio': 2229, 'autotrophs': 2230, 'autumnal': 2231, 'average': 2232, 'aves': 2233, 'avoid': 2234, 'avoiding': 2235, 'aware': 2236, 'axons': 2237, 'b': 2238, 'babies': 2239, 'back': 2240, 'bacterium': 2241, 'badge': 2242, 'bag': 2243, 'baking': 2244, 'balanced': 2245, 'balloon': 2246, 'barks': 2247, 'barn': 2248, 'barnacles': 2249, 'barometric': 2250, 'barrier': 2251, 'beads': 2252, 'beam': 2253, 'beans': 2254, 'beds': 2255, 'bee': 2256, 'beef': 2257, 'beetles': 2258, 'behavioral': 2259, 'behaviorists': 2260, 'below': 2261, 'bend': 2262, 'bends': 2263, 'beneficial': 2264, 'benign': 2265, 'bent': 2266, 'benthic': 2267, 'big': 2268, 'bilateral': 2269, 'bilateraly': 2270, 'bill': 2271, 'biloba': 2272, 'binary': 2273, 'bind': 2274, 'bioaerosols': 2275, 'biofuel': 2276, 'biological': 2277, 'biomass': 2278, 'biome': 2279, 'birches': 2280, 'bisexual': 2281, 'bite': 2282, 'bladder': 2283, 'blisters': 2284, 'bloom': 2285, 'blubber': 2286, 'blueprints': 2287, 'boats': 2288, 'bobo': 2289, 'bodied': 2290, 'bodies': 2291, 'bog': 2292, 'bogs': 2293, 'bomb': 2294, 'bond': 2295, 'booster': 2296, 'border': 2297, 'bored': 2298, 'bottles': 2299, 'bouncing': 2300, 'boundaries': 2301, 'bowel': 2302, 'boxes': 2303, 'brains': 2304, 'brakes': 2305, 'bread': 2306, 'breakdown': 2307, 'breast': 2308, 'breastfeeding': 2309, 'breed': 2310, 'bridges': 2311, 'brightens': 2312, 'bronchodilatory': 2313, 'brown': 2314, 'bryophytes': 2315, 'bucks': 2316, 'bugs': 2317, 'buildings': 2318, 'bullfrogs': 2319, 'bumper': 2320, 'bundles': 2321, 'buried': 2322, 'burrow': 2323, 'burrs': 2324, 'buses': 2325, 'bush': 2326, 'butterflies': 2327, 'cake': 2328, 'calcite': 2329, 'called': 2330, 'calm': 2331, 'camels': 2332, 'cameras': 2333, 'camouflage': 2334, 'cancerous': 2335, 'cancers': 2336, 'canned': 2337, 'capability': 2338, 'capacity': 2339, 'caperata': 2340, 'capture': 2341, 'carbonic': 2342, 'carcinogen': 2343, 'carcinoma': 2344, 'carpets': 2345, 'carpooling': 2346, 'carrier': 2347, 'carrying': 2348, 'catabolism': 2349, 'cations': 2350, 'cattle': 2351, 'caused': 2352, 'cave': 2353, 'caverns': 2354, 'celcius': 2355, 'cellphone': 2356, 'cellular': 2357, 'centers': 2358, 'central': 2359, 'cephalopods': 2360, 'certain': 2361, 'chairs': 2362, 'chambered': 2363, 'chemistry': 2364, 'chickens': 2365, 'chime': 2366, 'chinooks': 2367, 'chlorofluorocarbons': 2368, 'chlorophy': 2369, 'cholesterol': 2370, 'chordate': 2371, 'circulation': 2372, 'cities': 2373, 'citokinesis': 2374, 'clam': 2375, 'clear': 2376, 'clearly': 2377, 'close': 2378, 'cloudy': 2379, 'cluster': 2380, 'clusters': 2381, 'coastlines': 2382, 'cobra': 2383, 'coded': 2384, 'coffee': 2385, 'coherent': 2386, 'coin': 2387, 'coldest': 2388, 'coliding': 2389, 'coliforms': 2390, 'collagen': 2391, 'collapse': 2392, 'collapsing': 2393, 'collide': 2394, 'colloidal': 2395, 'colon': 2396, 'colorless': 2397, 'combining': 2398, 'combust': 2399, 'combustion': 2400, 'come': 2401, 'comfort': 2402, 'comfortable': 2403, 'coming': 2404, 'common': 2405, 'commonly': 2406, 'communicate': 2407, 'communications': 2408, 'commuting': 2409, 'compacted': 2410, 'compaction': 2411, 'competing': 2412, 'components': 2413, 'composting': 2414, 'compresses': 2415, 'computer': 2416, 'concentrations': 2417, 'condense': 2418, 'condenses': 2419, 'conductor': 2420, 'conducts': 2421, 'cones': 2422, 'conifer': 2423, 'conncectivity': 2424, 'considerably': 2425, 'constant': 2426, 'construction': 2427, 'consume': 2428, 'consuming': 2429, 'contains': 2430, 'contamination': 2431, 'content': 2432, 'continual': 2433, 'continue': 2434, 'continues': 2435, 'continuously': 2436, 'contract': 2437, 'contracting': 2438, 'contraction': 2439, 'contracts': 2440, 'controls': 2441, 'converted': 2442, 'converts': 2443, 'conveying': 2444, 'cooler': 2445, 'coolness': 2446, 'copepods': 2447, 'corals': 2448, 'cord': 2449, 'cosmetic': 2450, 'cost': 2451, 'crack': 2452, 'cracking': 2453, 'cracks': 2454, 'cranes': 2455, 'crappie': 2456, 'crashes': 2457, 'create': 2458, 'created': 2459, 'crinoids': 2460, 'crippling': 2461, 'critical': 2462, 'crocodile': 2463, 'crocodiles': 2464, 'crop': 2465, 'crops': 2466, 'crows': 2467, 'crunchy': 2468, 'cubes': 2469, 'cups': 2470, 'curve': 2471, 'cut': 2472, 'cuticle': 2473, 'cutters': 2474, 'cyanide': 2475, 'cyanobacteria': 2476, 'cycle': 2477, 'cyclone': 2478, 'cytokinesis': 2479, 'cytoplasmic': 2480, 'daffodils': 2481, 'dam': 2482, 'damaging': 2483, 'days': 2484, 'deaths': 2485, 'debris': 2486, 'decompose': 2487, 'decreased': 2488, 'deer': 2489, 'defense': 2490, 'define': 2491, 'deformation': 2492, 'dehydrated': 2493, 'dehydrating': 2494, 'deltas': 2495, 'dense': 2496, 'densities': 2497, 'deoxygenated': 2498, 'deposited': 2499, 'depositon': 2500, 'deposits': 2501, 'depostion': 2502, 'depression': 2503, 'depressive': 2504, 'destroyed': 2505, 'detectors': 2506, 'determine': 2507, 'devastation': 2508, 'device': 2509, 'dexyribonucleic': 2510, 'diatomes': 2511, 'diatoms': 2512, 'differential': 2513, 'diffraction': 2514, 'digest': 2515, 'dinosaur': 2516, 'dioxins': 2517, 'dirtier': 2518, 'disasters': 2519, 'discouraged': 2520, 'discs': 2521, 'disinfectants': 2522, 'disintegrate': 2523, 'disintegration': 2524, 'disperse': 2525, 'displacement': 2526, 'displays': 2527, 'dissolving': 2528, 'distract': 2529, 'distractions': 2530, 'disturbance': 2531, 'diveristy': 2532, 'dividing': 2533, 'diving': 2534, 'doctor': 2535, 'does': 2536, 'dog': 2537, 'dollars': 2538, 'dolls': 2539, 'dominant': 2540, 'donates': 2541, 'dormancy': 2542, 'double': 2543, 'drains': 2544, 'dramatically': 2545, 'drifting': 2546, 'drivers': 2547, 'drug': 2548, 'drum': 2549, 'ductile': 2550, 'ducts': 2551, 'dunes': 2552, 'duplicate': 2553, 'duplication': 2554, 'dust': 2555, 'dying': 2556, 'each': 2557, 'eagles': 2558, 'eardrum': 2559, 'earthworm': 2560, 'easy': 2561, 'echidna': 2562, 'echidnas': 2563, 'eclipse': 2564, 'economy': 2565, 'ecosystem': 2566, 'ecotourism': 2567, 'edema': 2568, 'effects': 2569, 'efficiency': 2570, 'ejaculate': 2571, 'electrically': 2572, 'electro': 2573, 'electrolytes': 2574, 'electromagnetism': 2575, 'electrostatic': 2576, 'elevation': 2577, 'elevations': 2578, 'elimination': 2579, 'elliptical': 2580, 'else': 2581, 'emergency': 2582, 'encoded': 2583, 'end': 2584, 'endangering': 2585, 'endings': 2586, 'endogenous': 2587, 'energetic': 2588, 'enormous': 2589, 'enough': 2590, 'enriched': 2591, 'enviroment': 2592, 'environmental': 2593, 'eons': 2594, 'epidermal': 2595, 'epididymes': 2596, 'epidiymes': 2597, 'equations': 2598, 'equinox': 2599, 'eroded': 2600, 'essentials': 2601, 'estimate': 2602, 'ethologists': 2603, 'eukyarotes': 2604, 'evaporates': 2605, 'evaporating': 2606, 'evaporative': 2607, 'events': 2608, 'evergeeen': 2609, 'evergreen': 2610, 'everybody': 2611, 'evolved': 2612, 'exactly': 2613, 'excellent': 2614, 'exchange': 2615, 'excreted': 2616, 'expanding': 2617, 'expelled': 2618, 'expode': 2619, 'extinct': 2620, 'extracted': 2621, 'extrapolated': 2622, 'extremely': 2623, 'eye': 2624, 'fallen': 2625, 'falls': 2626, 'fan': 2627, 'far': 2628, 'farm': 2629, 'fast': 2630, 'faster': 2631, 'father': 2632, 'fault': 2633, 'faulty': 2634, 'feces': 2635, 'fed': 2636, 'feeder': 2637, 'feel': 2638, 'fens': 2639, 'fermented': 2640, 'fern': 2641, 'fertile': 2642, 'fertilize': 2643, 'fertilizer': 2644, 'fetch': 2645, 'few': 2646, 'fiberglass': 2647, 'fibrils': 2648, 'fields': 2649, 'fighters': 2650, 'filaments': 2651, 'final': 2652, 'fingers': 2653, 'finite': 2654, 'fins': 2655, 'firecrackers': 2656, 'firework': 2657, 'fireworks': 2658, 'firm': 2659, 'first': 2660, 'fissile': 2661, 'fission': 2662, 'fitness': 2663, 'flagella': 2664, 'flash': 2665, 'flashes': 2666, 'flatworms': 2667, 'fleas': 2668, 'flight': 2669, 'flights': 2670, 'float': 2671, 'floats': 2672, 'floor': 2673, 'flowerless': 2674, 'flu': 2675, 'fluctuate': 2676, 'fluctuates': 2677, 'fluids': 2678, 'flying': 2679, 'flytrap': 2680, 'focusing': 2681, 'foil': 2682, 'folding': 2683, 'foliage': 2684, 'formations': 2685, 'forward': 2686, 'fractions': 2687, 'framework': 2688, 'freckles': 2689, 'freesheet': 2690, 'freeze': 2691, 'freshwater': 2692, 'friendship': 2693, 'front': 2694, 'frontal': 2695, 'frostbite': 2696, 'frozen': 2697, 'fruiting': 2698, 'fruits': 2699, 'fumes': 2700, 'functional': 2701, 'functioning': 2702, 'fungus': 2703, 'future': 2704, 'gain': 2705, 'games': 2706, 'gardening': 2707, 'gasses': 2708, 'geckos': 2709, 'gender': 2710, 'generally': 2711, 'generates': 2712, 'genitalia': 2713, 'genitals': 2714, 'genotypes': 2715, 'geological': 2716, 'geothermal': 2717, 'germany': 2718, 'germs': 2719, 'getting': 2720, 'ginkgo': 2721, 'girl': 2722, 'glisten': 2723, 'gloves': 2724, 'glow': 2725, 'glycogen': 2726, 'goat': 2727, 'gone': 2728, 'goods': 2729, 'gorges': 2730, 'gourdin': 2731, 'graduations': 2732, 'grain': 2733, 'grains': 2734, 'graptolites': 2735, 'gravel': 2736, 'grazing': 2737, 'great': 2738, 'group': 2739, 'growths': 2740, 'guitars': 2741, 'guns': 2742, 'habitual': 2743, 'haircoat': 2744, 'hammer': 2745, 'hand': 2746, 'hands': 2747, 'hard': 2748, 'hardened': 2749, 'harmed': 2750, 'hatch': 2751, 'hatched': 2752, 'havoc': 2753, 'hawks': 2754, 'haze': 2755, 'headaches': 2756, 'headlights': 2757, 'heal': 2758, 'healthier': 2759, 'hearts': 2760, 'heightened': 2761, 'helping': 2762, 'hemisphere': 2763, 'hemp': 2764, 'herbicide': 2765, 'herbivores': 2766, 'heredity': 2767, 'heriditary': 2768, 'hermaphrodite': 2769, 'hermaphrodites': 2770, 'herons': 2771, 'hexazinone': 2772, 'hibernate': 2773, 'hibernation': 2774, 'hidden': 2775, 'histone': 2776, 'history': 2777, 'hockey': 2778, 'holes': 2779, 'home': 2780, 'homogenous': 2781, 'honey': 2782, 'hooded': 2783, 'horses': 2784, 'hospitals': 2785, 'hotter': 2786, 'hours': 2787, 'household': 2788, 'houses': 2789, 'how': 2790, 'huddle': 2791, 'hugs': 2792, 'humankind': 2793, 'humid': 2794, 'hunting': 2795, 'hurt': 2796, 'hydrate': 2797, 'hydraulic': 2798, 'hydrocarbon': 2799, 'hydrocarbons': 2800, 'hypothermia': 2801, 'hypothyroidism': 2802, 'iPhones': 2803, 'if': 2804, 'ignite': 2805, 'igniting': 2806, 'ignition': 2807, 'illnesses': 2808, 'illumination': 2809, 'image': 2810, 'immediate': 2811, 'immunization': 2812, 'immunodeficiency': 2813, 'immunogens': 2814, 'immunological': 2815, 'impacts': 2816, 'impelling': 2817, 'implanted': 2818, 'improve': 2819, 'improves': 2820, 'inactive': 2821, 'incandescent': 2822, 'incineration': 2823, 'incubation': 2824, 'indeterminate': 2825, 'indirect': 2826, 'individuals': 2827, 'inducible': 2828, 'industry': 2829, 'inefficient': 2830, 'inertia': 2831, 'infarction': 2832, 'infected': 2833, 'infections': 2834, 'infertility': 2835, 'infrared': 2836, 'ingested': 2837, 'inhalation': 2838, 'inherited': 2839, 'injuries': 2840, 'innovative': 2841, 'insect': 2842, 'instincts': 2843, 'insulators': 2844, 'intelligence': 2845, 'intense': 2846, 'interact': 2847, 'intermediate': 2848, 'internal': 2849, 'interstellar': 2850, 'intervertebrates': 2851, 'intrinsic': 2852, 'intrusion': 2853, 'invented': 2854, 'invertebrate': 2855, 'ion': 2856, 'ionization': 2857, 'ionized': 2858, 'iris': 2859, 'islands': 2860, 'itself': 2861, 'ivy': 2862, 'jaw': 2863, 'jerky': 2864, 'jet': 2865, 'jets': 2866, 'joining': 2867, 'joules': 2868, 'jungles': 2869, 'kalenchoes': 2870, 'karyogamy': 2871, 'kernel': 2872, 'keystone': 2873, 'killer': 2874, 'kills': 2875, 'kilogram': 2876, 'kilometers': 2877, 'kitchen': 2878, 'known': 2879, 'koalas': 2880, 'laboratory': 2881, 'lacking': 2882, 'lamps': 2883, 'land': 2884, 'landfills': 2885, 'lands': 2886, 'larger': 2887, 'larvae': 2888, 'late': 2889, 'later': 2890, 'lateral': 2891, 'latitude': 2892, 'latitudes': 2893, 'lava': 2894, 'lawn': 2895, 'lead': 2896, 'leaf': 2897, 'leaving': 2898, 'lemmons': 2899, 'leopard': 2900, 'lethal': 2901, 'lie': 2902, 'lifeforms': 2903, 'lifting': 2904, 'lightbulb': 2905, 'lighting': 2906, 'lights': 2907, 'lignin': 2908, 'ligt': 2909, 'lillies': 2910, 'limb': 2911, 'linguistically': 2912, 'linked': 2913, 'liters': 2914, 'lithium': 2915, 'litter': 2916, 'little': 2917, 'livers': 2918, 'lives': 2919, 'lmestone': 2920, 'lobes': 2921, 'lobsters': 2922, 'locations': 2923, 'loch': 2924, 'loggers': 2925, 'logging': 2926, 'logs': 2927, 'look': 2928, 'looking': 2929, 'loop': 2930, 'looser': 2931, 'loses': 2932, 'lot': 2933, 'loud': 2934, 'lowered': 2935, 'lowering': 2936, 'lowest': 2937, 'lumber': 2938, 'lumberjacks': 2939, 'luminometer': 2940, 'lungworms': 2941, 'lush': 2942, 'luxury': 2943, 'lymphoma': 2944, 'lysing': 2945, 'mL': 2946, 'macaques': 2947, 'machines': 2948, 'magnet': 2949, 'magnified': 2950, 'magnitude': 2951, 'major': 2952, 'malnutrition': 2953, 'mammal': 2954, 'manufacturing': 2955, 'manures': 2956, 'maples': 2957, 'margins': 2958, 'marker': 2959, 'marrow': 2960, 'marshes': 2961, 'marshy': 2962, 'matches': 2963, 'may': 2964, 'meals': 2965, 'measurement': 2966, 'meats': 2967, 'mechanism': 2968, 'mechanoreceptors': 2969, 'media': 2970, 'medical': 2971, 'melanistic': 2972, 'melanoma': 2973, 'messages': 2974, 'metabolic': 2975, 'metamorphizing': 2976, 'meters': 2977, 'methane': 2978, 'mice': 2979, 'microbial': 2980, 'microevolution': 2981, 'microtubules': 2982, 'microwave': 2983, 'mild': 2984, 'mill': 2985, 'mineral': 2986, 'mines': 2987, 'minutes': 2988, 'mirrors': 2989, 'mistake': 2990, 'mites': 2991, 'mitochondria': 2992, 'mixing': 2993, 'moldy': 2994, 'mole': 2995, 'molluscs': 2996, 'monomers': 2997, 'monotreme': 2998, 'mortality': 2999, 'mosaic': 3000, 'mosiac': 3001, 'mother': 3002, 'motor': 3003, 'motors': 3004, 'mouth': 3005, 'moved': 3006, 'movements': 3007, 'movemnet': 3008, 'mowers': 3009, 'much': 3010, 'mud': 3011, 'multiply': 3012, 'municipal': 3013, 'mushroom': 3014, 'must': 3015, 'mutagens': 3016, 'mutant': 3017, 'nature': 3018, 'nearest': 3019, 'need': 3020, 'needs': 3021, 'nektar': 3022, 'nest': 3023, 'newsprint': 3024, 'newtons': 3025, 'nickel': 3026, 'nicotine': 3027, 'nitrile': 3028, 'noises': 3029, 'nokia': 3030, 'normal': 3031, 'nose': 3032, 'nucelotide': 3033, 'nuclei': 3034, 'nucleoli': 3035, 'numbers': 3036, 'oats': 3037, 'objectstowards': 3038, 'octopus': 3039, 'odorless': 3040, 'often': 3041, 'omnivore': 3042, 'once': 3043, 'oncogenes': 3044, 'ones': 3045, 'oneself': 3046, 'operated': 3047, 'operating': 3048, 'opossums': 3049, 'opportunistic': 3050, 'optic': 3051, 'orange': 3052, 'orbital': 3053, 'orbiting': 3054, 'organics': 3055, 'origami': 3056, 'oscillations': 3057, 'osmosis': 3058, 'osteogenic': 3059, 'others': 3060, 'otters': 3061, 'outbursts': 3062, 'outer': 3063, 'oven': 3064, 'overheating': 3065, 'overheats': 3066, 'ovules': 3067, 'oxides': 3068, 'oxidiser': 3069, 'oxidize': 3070, 'pHs': 3071, 'pack': 3072, 'pained': 3073, 'painful': 3074, 'painting': 3075, 'panther': 3076, 'panting': 3077, 'papaya': 3078, 'parent': 3079, 'parisitic': 3080, 'partner': 3081, 'partners': 3082, 'passed': 3083, 'passing': 3084, 'passive': 3085, 'past': 3086, 'patch': 3087, 'path': 3088, 'pathogenic': 3089, 'peachleaf': 3090, 'peak': 3091, 'pelt': 3092, 'pelvis': 3093, 'penguins': 3094, 'penis': 3095, 'per': 3096, 'perceiving': 3097, 'perennials': 3098, 'period': 3099, 'permineralization': 3100, 'perspire': 3101, 'pest': 3102, 'pesticide': 3103, 'petrified': 3104, 'petroleum': 3105, 'phenomenon': 3106, 'phenotypes': 3107, 'pheremon': 3108, 'phosphorous': 3109, 'photosyntehsis': 3110, 'photosynthetics': 3111, 'physiology': 3112, 'piano': 3113, 'picks': 3114, 'pigeons': 3115, 'pigment': 3116, 'pigments': 3117, 'pines': 3118, 'pipes': 3119, 'pistons': 3120, 'place': 3121, 'places': 3122, 'planets': 3123, 'planted': 3124, 'plastids': 3125, 'plate': 3126, 'plated': 3127, 'platypuses': 3128, 'pneumonia': 3129, 'poison': 3130, 'polar': 3131, 'polarized': 3132, 'poles': 3133, 'pollinate': 3134, 'pollinated': 3135, 'polluted': 3136, 'polyethylene': 3137, 'polymers': 3138, 'polynucleotide': 3139, 'polynucleotides': 3140, 'polyp': 3141, 'pond': 3142, 'ponding': 3143, 'pools': 3144, 'populations': 3145, 'porcinis': 3146, 'porous': 3147, 'positively': 3148, 'possible': 3149, 'potash': 3150, 'potential': 3151, 'pottery': 3152, 'precious': 3153, 'predatory': 3154, 'preventing': 3155, 'prevention': 3156, 'prevents': 3157, 'prides': 3158, 'primarily': 3159, 'primitive': 3160, 'principals': 3161, 'printed': 3162, 'problem': 3163, 'processes': 3164, 'procreation': 3165, 'producer': 3166, 'product': 3167, 'products': 3168, 'programmed': 3169, 'progress': 3170, 'projections': 3171, 'prolonged': 3172, 'prone': 3173, 'propagate': 3174, 'prophase': 3175, 'proprioceptive': 3176, 'prosperity': 3177, 'protective': 3178, 'protist': 3179, 'protists': 3180, 'proton': 3181, 'protons': 3182, 'provides': 3183, 'pull': 3184, 'pulls': 3185, 'pulmonologist': 3186, 'pulmonologists': 3187, 'pulped': 3188, 'pupil': 3189, 'push': 3190, 'pushes': 3191, 'pyramid': 3192, 'pyrite': 3193, 'quadriceps': 3194, 'quart': 3195, 'quartz': 3196, 'rabbit': 3197, 'race': 3198, 'radial': 3199, 'radiator': 3200, 'radicals': 3201, 'radioactive': 3202, 'radiography': 3203, 'radios': 3204, 'rainwater': 3205, 'raising': 3206, 'range': 3207, 'rapidly': 3208, 'rat': 3209, 'rather': 3210, 'ravens': 3211, 'reaches': 3212, 'reactive': 3213, 'reacts': 3214, 'real': 3215, 'received': 3216, 'receives': 3217, 'receiving': 3218, 'recessive': 3219, 'recovery': 3220, 'recpetors': 3221, 'recycled': 3222, 'redness': 3223, 'reduction': 3224, 'refining': 3225, 'reflect': 3226, 'reflecting': 3227, 'reflex': 3228, 'refraction': 3229, 'regeneration': 3230, 'regulation': 3231, 'rejected': 3232, 'relationships': 3233, 'relatives': 3234, 'relax': 3235, 'remain': 3236, 'remaining': 3237, 'remarkable': 3238, 'removal': 3239, 'removing': 3240, 'renewable': 3241, 'repaired': 3242, 'repellent': 3243, 'replicating': 3244, 'reproduces': 3245, 'reptilia': 3246, 'reptilians': 3247, 'repulsive': 3248, 'research': 3249, 'resevoir': 3250, 'reshaped': 3251, 'resistor': 3252, 'resource': 3253, 'respirate': 3254, 'resting': 3255, 'retention': 3256, 'retina': 3257, 'reusing': 3258, 'reverse': 3259, 'revolutions': 3260, 'ribonucleic': 3261, 'ribosomes': 3262, 'ribs': 3263, 'ricce': 3264, 'rice': 3265, 'ripples': 3266, 'rockets': 3267, 'rodents': 3268, 'roof': 3269, 'roofs': 3270, 'rooms': 3271, 'roost': 3272, 'rotate': 3273, 'rotten': 3274, 'rough': 3275, 'rozites': 3276, 'run': 3277, 'rust': 3278, 'safely': 3279, 'sailplanes': 3280, 'saliva': 3281, 'salting': 3282, 'same': 3283, 'sandy': 3284, 'satellite': 3285, 'saving': 3286, 'scale': 3287, 'scarce': 3288, 'scattered': 3289, 'scavenged': 3290, 'scent': 3291, 'scientists': 3292, 'scraping': 3293, 'seals': 3294, 'seasonal': 3295, 'seasonally': 3296, 'seawater': 3297, 'secrete': 3298, 'secreted': 3299, 'sediments': 3300, 'seeding': 3301, 'seep': 3302, 'semen': 3303, 'semiochemicals': 3304, 'sensing': 3305, 'separatin': 3306, 'separation': 3307, 'set': 3308, 'seven': 3309, 'several': 3310, 'severely': 3311, 'shake': 3312, 'shale': 3313, 'shaped': 3314, 'shapes': 3315, 'share': 3316, 'sharing': 3317, 'shark': 3318, 'shelled': 3319, 'shine': 3320, 'ship': 3321, 'shiver': 3322, 'shoreline': 3323, 'short': 3324, 'shorten': 3325, 'shortening': 3326, 'shots': 3327, 'show': 3328, 'shrews': 3329, 'shrub': 3330, 'sickness': 3331, 'signal': 3332, 'silicate': 3333, 'sing': 3334, 'singing': 3335, 'sink': 3336, 'sinkholes': 3337, 'sinking': 3338, 'sinks': 3339, 'sizes': 3340, 'ski': 3341, 'skull': 3342, 'sky': 3343, 'sleepwalking': 3344, 'slowly': 3345, 'slows': 3346, 'smog': 3347, 'smoothing': 3348, 'smother': 3349, 'snarls': 3350, 'so': 3351, 'social': 3352, 'soda': 3353, 'softwoods': 3354, 'solids': 3355, 'solution': 3356, 'someone': 3357, 'something': 3358, 'somewhere': 3359, 'soundwaves': 3360, 'sources': 3361, 'sparrows': 3362, 'special': 3363, 'specialized': 3364, 'specific': 3365, 'spillways': 3366, 'spin': 3367, 'split': 3368, 'splitting': 3369, 'spoilage': 3370, 'spore': 3371, 'sprawl': 3372, 'spread': 3373, 'springs': 3374, 'sprout': 3375, 'spruce': 3376, 'sqeezing': 3377, 'squeeze': 3378, 'squirrels': 3379, 'stability': 3380, 'stagnant': 3381, 'stalactites': 3382, 'standing': 3383, 'starch': 3384, 'startle': 3385, 'stations': 3386, 'staying': 3387, 'stays': 3388, 'step': 3389, 'sterilization': 3390, 'steroids': 3391, 'stigma': 3392, 'stimulating': 3393, 'stomata': 3394, 'stomates': 3395, 'storage': 3396, 'stores': 3397, 'storing': 3398, 'storm': 3399, 'straight': 3400, 'straightens': 3401, 'strands': 3402, 'strawberries': 3403, 'strays': 3404, 'string': 3405, 'strong': 3406, 'struck': 3407, 'studied': 3408, 'studies': 3409, 'submarines': 3410, 'subrounded': 3411, 'substance': 3412, 'substrate': 3413, 'subtropical': 3414, 'successive': 3415, 'sugary': 3416, 'sulfate': 3417, 'sulfuric': 3418, 'supersonic': 3419, 'surge': 3420, 'surges': 3421, 'surgical': 3422, 'surrounding': 3423, 'susceptible': 3424, 'sustainable': 3425, 'sustaining': 3426, 'swallow': 3427, 'swamps': 3428, 'swea': 3429, 'sweet': 3430, 'sweeter': 3431, 'swelling': 3432, 'switch': 3433, 'symbol': 3434, 'symmetrical': 3435, 'sympathetic': 3436, 'symptoms': 3437, 'syngamy': 3438, 'synthetic': 3439, 'systolic': 3440, 'take': 3441, 'tapeworm': 3442, 'taxes': 3443, 'teats': 3444, 'technology': 3445, 'tectonics': 3446, 'televisions': 3447, 'tempting': 3448, 'termites': 3449, 'terrain': 3450, 'terrapins': 3451, 'testing': 3452, 'testosterone': 3453, 'theme': 3454, 'there': 3455, 'thermodynamically': 3456, 'thermogenesis': 3457, 'thorax': 3458, 'thousands': 3459, 'threat': 3460, 'threatened': 3461, 'threatening': 3462, 'threatens': 3463, 'thresholds': 3464, 'thunder': 3465, 'ticks': 3466, 'tidal': 3467, 'tiger': 3468, 'tight': 3469, 'tips': 3470, 'tire': 3471, 'tired': 3472, 'tires': 3473, 'toads': 3474, 'toaster': 3475, 'tongue': 3476, 'topsoil': 3477, 'tornadoes': 3478, 'tortoises': 3479, 'touch': 3480, 'tough': 3481, 'toughness': 3482, 'toxins': 3483, 'traffic': 3484, 'trains': 3485, 'transform': 3486, 'transistors': 3487, 'transition': 3488, 'transpiration': 3489, 'transported': 3490, 'trash': 3491, 'travelers': 3492, 'treatment': 3493, 'trellised': 3494, 'tremors': 3495, 'triceps': 3496, 'trisomy': 3497, 'trochophore': 3498, 'trunks': 3499, 'tsunami': 3500, 'tube': 3501, 'tubes': 3502, 'tuna': 3503, 'tunicates': 3504, 'turbine': 3505, 'uncoated': 3506, 'uncontrollably': 3507, 'underwater': 3508, 'unidirectional': 3509, 'unique': 3510, 'unisexual': 3511, 'unpredicatbale': 3512, 'unpredictable': 3513, 'unwashed': 3514, 'uranium': 3515, 'urban': 3516, 'urea': 3517, 'urethra': 3518, 'urination': 3519, 'usage': 3520, 'user': 3521, 'usually': 3522, 'vacuum': 3523, 'vagina': 3524, 'valleys': 3525, 'vanes': 3526, 'vanpooling': 3527, 'variable': 3528, 'variations': 3529, 'variety': 3530, 'vary': 3531, 'varying': 3532, 'vasoconstriction': 3533, 'vehicles': 3534, 'vein': 3535, 'velocity': 3536, 'venom': 3537, 'verbal': 3538, 'vertebrae': 3539, 'vertebrate': 3540, 'vertically': 3541, 'vesicle': 3542, 'vibrates': 3543, 'vibrational': 3544, 'video': 3545, 'vinyl': 3546, 'violent': 3547, 'virus': 3548, 'visitation': 3549, 'vital': 3550, 'vitamins': 3551, 'vocalizing': 3552, 'voice': 3553, 'volatile': 3554, 'volcanoes': 3555, 'voles': 3556, 'volkswagen': 3557, 'voltage': 3558, 'voluntary': 3559, 'vomiting': 3560, 'vultures': 3561, 'walruses': 3562, 'warblers': 3563, 'was': 3564, 'wash': 3565, 'wasps': 3566, 'wasted': 3567, 'watching': 3568, 'waterfall': 3569, 'watershed': 3570, 'watershred': 3571, 'wavelengths': 3572, 'way': 3573, 'webbed': 3574, 'wedging': 3575, 'weighs': 3576, 'weightlifting': 3577, 'wetland': 3578, 'wetter': 3579, 'what': 3580, 'while': 3581, 'whistle': 3582, 'whose': 3583, 'widely': 3584, 'widen': 3585, 'wild': 3586, 'wildfire': 3587, 'wildfires': 3588, 'windpower': 3589, 'wire': 3590, 'wires': 3591, 'wo': 3592, 'woman': 3593, 'wooly': 3594, 'working': 3595, 'workouts': 3596, 'works': 3597, 'world': 3598, 'worsened': 3599, 'would': 3600, 'year': 3601, 'yeast': 3602, 'yeasts': 3603, 'yellow': 3604, 'yields': 3605, 'young': 3606, 'zero': 3607, 'zinc': 3608, 'zones': 3609})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGyqmiNlyAiR",
        "outputId": "c8c3b86e-69b7-453b-945b-019dd762b08a"
      },
      "source": [
        "Sentence.vocab.stoi['new construction'], Label.vocab.stoi['new construction'] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsycLH0DEzFa"
      },
      "source": [
        "Load our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fen1DHy6EzFb"
      },
      "source": [
        "We'll also print out an example just to double check they're not reversed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRDnbA09EzFc"
      },
      "source": [
        "Then create our vocabulary, converting all tokens appearing less than twice into `<unk>` tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2hiioCLEzFd"
      },
      "source": [
        "Finally, define the `device` and create our iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4davOyYEzFd"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YoFk7hgzQ1h"
      },
      "source": [
        "BATCH_SIZE=128\r\n",
        "train_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train, valid), \r\n",
        "    batch_size = BATCH_SIZE, \r\n",
        "    device = device, sort=False, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeK3RzjvzZSg"
      },
      "source": [
        "iter_one = next(iter(train_iterator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cid31jyj-cuR",
        "outputId": "281b4aa4-a7d1-4c61-87c5-1a84a78cbfe5"
      },
      "source": [
        "iter_one.label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NEKYsXD0HK2"
      },
      "source": [
        "for entry in (iter_one.sentence[0][0,:]):\r\n",
        "    print(entry.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqR66FM2EzFj"
      },
      "source": [
        "## Seq2Seq Model\n",
        "\n",
        "Putting the encoder and decoder together, we get:\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq7.png?raw=1)\n",
        "\n",
        "Again, in this implementation we need to ensure the hidden dimensions in both the encoder and the decoder are the same.\n",
        "\n",
        "Briefly going over all of the steps:\n",
        "- the `outputs` tensor is created to hold all predictions, $\\hat{Y}$\n",
        "- the source sequence, $X$, is fed into the encoder to receive a `context` vector\n",
        "- the initial decoder hidden state is set to be the `context` vector, $s_0 = z = h_T$\n",
        "- we use a batch of `<sos>` tokens as the first `input`, $y_1$\n",
        "- we then decode within a loop:\n",
        "  - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and the context vector, $z$, into the decoder\n",
        "  - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n",
        "  - we then decide if we are going to teacher force or not, setting the next input as appropriate (either the ground truth next token in the target sequence or the highest predicted next token)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11CnDhTkEzFd"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        text, text_lengths = src\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # packed_seq = nn.utils.rnn.pack_padded_sequence(embedded, \n",
        "        #                                                text_lengths.cpu(),\n",
        "        #                                                batch_first=False,\n",
        "        #                                                enforce_sorted=False)        \n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded) #no cell state! \n",
        "        ## outputs is a packed sequence but since it is not used we will not \n",
        "        ## unpack it         \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Hb0dXWHLsG"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRA8hkiLEzFh"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded) #no cell state!\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, context):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #context = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        #context = [1, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # print(embedded.shape, hidden.shape, context.shape)\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "            \n",
        "        #emb_con = [1, batch size, emb dim + hid dim]\n",
        "            \n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \n",
        "                           dim = 1)\n",
        "        \n",
        "        #output = [batch size, emb dim + hid dim * 2]\n",
        "        \n",
        "        prediction = self.fc_out(output)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDyNrQ8VEzFk"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is the context\n",
        "        context = self.encoder(src)\n",
        "        \n",
        "        #context also used as the initial hidden state of the decoder\n",
        "        hidden = context\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state and the context state\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxVMMPUyEzFk"
      },
      "source": [
        "# Training the Seq2Seq Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDG6jOSuEzFk"
      },
      "source": [
        "# INPUT_DIM = len(SRC.vocab)\n",
        "# OUTPUT_DIM = len(TRG.vocab)\n",
        "\n",
        "INPUT_DIM = len(Sentence.vocab)\n",
        "OUTPUT_DIM = len(Label.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 256\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIc1l6CEzFk"
      },
      "source": [
        "Next, we initialize our parameters. The paper states the parameters are initialized from a normal distribution with a mean of 0 and a standard deviation of 0.01, i.e. $\\mathcal{N}(0, 0.01)$. \n",
        "\n",
        "It also states we should initialize the recurrent parameters to a special initialization, however to keep things simple we'll also initialize them to $\\mathcal{N}(0, 0.01)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgqMqq-oEzFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38f526ba-5ec0-4012-e611-6d39ab03d825"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(9462, 256)\n",
              "    (rnn): GRU(256, 256)\n",
              "    (dropout): Dropout(p=0.8, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(3610, 256)\n",
              "    (rnn): GRU(512, 256)\n",
              "    (fc_out): Linear(in_features=768, out_features=3610, bias=True)\n",
              "    (dropout): Dropout(p=0.8, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd1QoOsUEzFl"
      },
      "source": [
        "We print out the number of parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IggCwIBgEzFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2948785-d665-4ff7-eb64-e207726a334d"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 7,108,634 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiH54qzHEzFl"
      },
      "source": [
        "We initiaize our optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO1_eoG7EzFl"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR, OneCycleLR, MultiStepLR, CyclicLR, ReduceLROnPlateau\r\n",
        "\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0009893272, weight_decay=0.0001)\r\n",
        "scheduler = OneCycleLR(optimizer, \r\n",
        "                        0.001, \r\n",
        "                        epochs=100, \r\n",
        "                        cycle_momentum=False, \r\n",
        "                        steps_per_epoch=len(train_iterator), \r\n",
        "                        #base_momentum=config.momentum,\r\n",
        "                        #max_momentum=0.95, \r\n",
        "                        pct_start=0.208,\r\n",
        "                        # anneal_strategy=config.anneal_strategy,\r\n",
        "                        div_factor=100,\r\n",
        "                        # final_div_factor=config.final_div_factor\r\n",
        "                        )\r\n",
        "# 0.000981989011942079"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwkV3FKlEzFl"
      },
      "source": [
        "We also initialize the loss function, making sure to ignore the loss on `<pad>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0DAbGbcEzFl"
      },
      "source": [
        "#TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "TRG_PAD_IDX = Label.vocab.stoi[Label.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTay7E9rEzFm"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OYuoFdEzFm"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.sentence\n",
        "        trg = batch.label\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        # print(\"output_dim:\", output.shape)\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        # print(\"output_dim before loss:\", output.shape, trg.shape)\n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw022pw0EzFm"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            # src = batch.src\n",
        "            # trg = batch.trg\n",
        "            src = batch.sentence\n",
        "            trg = batch.label\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E43h8dnQEzFm"
      },
      "source": [
        "We'll also define the function that calculates how long an epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTAmu3-EEzFm"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_cbEVPWadt5",
        "outputId": "98bf9688-4cd4-4e98-aa47-bffe9fd9fba3"
      },
      "source": [
        "optimizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    eps: 1e-08\n",
              "    initial_lr: 1e-06\n",
              "    lr: 9.999999999999159e-07\n",
              "    max_lr: 0.001\n",
              "    min_lr: 9.999999999999999e-11\n",
              "    weight_decay: 0.0001\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbCGyO4ZEzFm"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5fzUqG-nkhn"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjFyRUK9EzFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15024502-1ff0-42ef-8890-4bac39d02705"
      },
      "source": [
        "N_EPOCHS = 100\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\n",
        "    \n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | LR: {scheduler.get_last_lr()}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 1s\n",
            "\tTrain Loss: 6.495 | Train PPL: 661.664 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.083 |  Val. PPL: 1191.807\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 6.465 | Train PPL: 642.095 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.080 |  Val. PPL: 1188.536\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 6.453 | Train PPL: 634.840 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.078 |  Val. PPL: 1185.041\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 6.446 | Train PPL: 629.997 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.075 |  Val. PPL: 1181.794\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 6.440 | Train PPL: 626.309 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.072 |  Val. PPL: 1178.952\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 6.435 | Train PPL: 623.474 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.070 |  Val. PPL: 1176.561\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 6.432 | Train PPL: 621.194 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.069 |  Val. PPL: 1174.570\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 6.428 | Train PPL: 619.206 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.067 |  Val. PPL: 1172.920\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 6.426 | Train PPL: 617.440 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.066 |  Val. PPL: 1171.544\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 6.423 | Train PPL: 615.929 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.065 |  Val. PPL: 1170.374\n",
            "Epoch: 11 | Time: 0m 1s\n",
            "\tTrain Loss: 6.421 | Train PPL: 614.633 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.064 |  Val. PPL: 1169.371\n",
            "Epoch: 12 | Time: 0m 1s\n",
            "\tTrain Loss: 6.419 | Train PPL: 613.435 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.063 |  Val. PPL: 1168.487\n",
            "Epoch: 13 | Time: 0m 1s\n",
            "\tTrain Loss: 6.417 | Train PPL: 612.220 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.063 |  Val. PPL: 1167.687\n",
            "Epoch: 14 | Time: 0m 1s\n",
            "\tTrain Loss: 6.415 | Train PPL: 611.153 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.062 |  Val. PPL: 1166.957\n",
            "Epoch: 15 | Time: 0m 1s\n",
            "\tTrain Loss: 6.414 | Train PPL: 610.144 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.062 |  Val. PPL: 1166.270\n",
            "Epoch: 16 | Time: 0m 1s\n",
            "\tTrain Loss: 6.412 | Train PPL: 609.275 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.061 |  Val. PPL: 1165.616\n",
            "Epoch: 17 | Time: 0m 1s\n",
            "\tTrain Loss: 6.411 | Train PPL: 608.399 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.060 |  Val. PPL: 1164.995\n",
            "Epoch: 18 | Time: 0m 1s\n",
            "\tTrain Loss: 6.410 | Train PPL: 607.613 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.060 |  Val. PPL: 1164.393\n",
            "Epoch: 19 | Time: 0m 1s\n",
            "\tTrain Loss: 6.408 | Train PPL: 606.786 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.059 |  Val. PPL: 1163.803\n",
            "Epoch: 20 | Time: 0m 1s\n",
            "\tTrain Loss: 6.407 | Train PPL: 606.007 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.059 |  Val. PPL: 1163.226\n",
            "Epoch: 21 | Time: 0m 1s\n",
            "\tTrain Loss: 6.406 | Train PPL: 605.292 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.058 |  Val. PPL: 1162.650\n",
            "Epoch: 22 | Time: 0m 1s\n",
            "\tTrain Loss: 6.404 | Train PPL: 604.539 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.058 |  Val. PPL: 1162.071\n",
            "Epoch: 23 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.903 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.057 |  Val. PPL: 1161.494\n",
            "Epoch: 24 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 603.194 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.057 |  Val. PPL: 1160.906\n",
            "Epoch: 25 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.495 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.056 |  Val. PPL: 1160.315\n",
            "Epoch: 26 | Time: 0m 1s\n",
            "\tTrain Loss: 6.400 | Train PPL: 601.869 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.056 |  Val. PPL: 1159.712\n",
            "Epoch: 27 | Time: 0m 1s\n",
            "\tTrain Loss: 6.399 | Train PPL: 601.197 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.055 |  Val. PPL: 1159.093\n",
            "Epoch: 28 | Time: 0m 1s\n",
            "\tTrain Loss: 6.398 | Train PPL: 600.567 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.055 |  Val. PPL: 1158.468\n",
            "Epoch: 29 | Time: 0m 1s\n",
            "\tTrain Loss: 6.397 | Train PPL: 599.863 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.054 |  Val. PPL: 1157.834\n",
            "Epoch: 30 | Time: 0m 1s\n",
            "\tTrain Loss: 6.396 | Train PPL: 599.172 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.054 |  Val. PPL: 1157.198\n",
            "Epoch: 31 | Time: 0m 1s\n",
            "\tTrain Loss: 6.394 | Train PPL: 598.508 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.053 |  Val. PPL: 1156.563\n",
            "Epoch: 32 | Time: 0m 1s\n",
            "\tTrain Loss: 6.393 | Train PPL: 597.857 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.053 |  Val. PPL: 1155.937\n",
            "Epoch: 33 | Time: 0m 1s\n",
            "\tTrain Loss: 6.392 | Train PPL: 597.178 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.052 |  Val. PPL: 1155.316\n",
            "Epoch: 34 | Time: 0m 1s\n",
            "\tTrain Loss: 6.391 | Train PPL: 596.514 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.052 |  Val. PPL: 1154.706\n",
            "Epoch: 35 | Time: 0m 1s\n",
            "\tTrain Loss: 6.390 | Train PPL: 595.814 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.051 |  Val. PPL: 1154.105\n",
            "Epoch: 36 | Time: 0m 1s\n",
            "\tTrain Loss: 6.389 | Train PPL: 595.210 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.051 |  Val. PPL: 1153.505\n",
            "Epoch: 37 | Time: 0m 1s\n",
            "\tTrain Loss: 6.388 | Train PPL: 594.516 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.050 |  Val. PPL: 1152.911\n",
            "Epoch: 38 | Time: 0m 1s\n",
            "\tTrain Loss: 6.387 | Train PPL: 593.867 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.050 |  Val. PPL: 1152.319\n",
            "Epoch: 39 | Time: 0m 1s\n",
            "\tTrain Loss: 6.386 | Train PPL: 593.218 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.049 |  Val. PPL: 1151.722\n",
            "Epoch: 40 | Time: 0m 1s\n",
            "\tTrain Loss: 6.384 | Train PPL: 592.552 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.049 |  Val. PPL: 1151.131\n",
            "Epoch: 41 | Time: 0m 1s\n",
            "\tTrain Loss: 6.383 | Train PPL: 591.846 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.048 |  Val. PPL: 1150.533\n",
            "Epoch: 42 | Time: 0m 1s\n",
            "\tTrain Loss: 6.382 | Train PPL: 591.153 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.047 |  Val. PPL: 1149.938\n",
            "Epoch: 43 | Time: 0m 1s\n",
            "\tTrain Loss: 6.381 | Train PPL: 590.457 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.047 |  Val. PPL: 1149.346\n",
            "Epoch: 44 | Time: 0m 1s\n",
            "\tTrain Loss: 6.380 | Train PPL: 589.734 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.046 |  Val. PPL: 1148.759\n",
            "Epoch: 45 | Time: 0m 1s\n",
            "\tTrain Loss: 6.379 | Train PPL: 589.105 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.046 |  Val. PPL: 1148.176\n",
            "Epoch: 46 | Time: 0m 1s\n",
            "\tTrain Loss: 6.377 | Train PPL: 588.370 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.045 |  Val. PPL: 1147.569\n",
            "Epoch: 47 | Time: 0m 1s\n",
            "\tTrain Loss: 6.376 | Train PPL: 587.658 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.045 |  Val. PPL: 1146.992\n",
            "Epoch: 48 | Time: 0m 1s\n",
            "\tTrain Loss: 6.375 | Train PPL: 586.941 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.044 |  Val. PPL: 1146.422\n",
            "Epoch: 49 | Time: 0m 1s\n",
            "\tTrain Loss: 6.374 | Train PPL: 586.211 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.044 |  Val. PPL: 1145.847\n",
            "Epoch: 50 | Time: 0m 1s\n",
            "\tTrain Loss: 6.373 | Train PPL: 585.521 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.043 |  Val. PPL: 1145.290\n",
            "Epoch: 51 | Time: 0m 1s\n",
            "\tTrain Loss: 6.371 | Train PPL: 584.803 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.043 |  Val. PPL: 1144.727\n",
            "Epoch: 52 | Time: 0m 1s\n",
            "\tTrain Loss: 6.370 | Train PPL: 584.072 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.042 |  Val. PPL: 1144.165\n",
            "Epoch: 53 | Time: 0m 1s\n",
            "\tTrain Loss: 6.369 | Train PPL: 583.362 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.042 |  Val. PPL: 1143.600\n",
            "Epoch: 54 | Time: 0m 1s\n",
            "\tTrain Loss: 6.368 | Train PPL: 582.662 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.041 |  Val. PPL: 1143.038\n",
            "Epoch: 55 | Time: 0m 1s\n",
            "\tTrain Loss: 6.366 | Train PPL: 581.976 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.041 |  Val. PPL: 1142.473\n",
            "Epoch: 56 | Time: 0m 1s\n",
            "\tTrain Loss: 6.365 | Train PPL: 581.268 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.040 |  Val. PPL: 1141.905\n",
            "Epoch: 57 | Time: 0m 1s\n",
            "\tTrain Loss: 6.364 | Train PPL: 580.555 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.040 |  Val. PPL: 1141.338\n",
            "Epoch: 58 | Time: 0m 1s\n",
            "\tTrain Loss: 6.363 | Train PPL: 579.877 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.039 |  Val. PPL: 1140.775\n",
            "Epoch: 59 | Time: 0m 1s\n",
            "\tTrain Loss: 6.362 | Train PPL: 579.147 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.039 |  Val. PPL: 1140.221\n",
            "Epoch: 60 | Time: 0m 1s\n",
            "\tTrain Loss: 6.360 | Train PPL: 578.456 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.038 |  Val. PPL: 1139.661\n",
            "Epoch: 61 | Time: 0m 1s\n",
            "\tTrain Loss: 6.359 | Train PPL: 577.732 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.038 |  Val. PPL: 1139.110\n",
            "Epoch: 62 | Time: 0m 1s\n",
            "\tTrain Loss: 6.358 | Train PPL: 577.019 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.038 |  Val. PPL: 1138.558\n",
            "Epoch: 63 | Time: 0m 1s\n",
            "\tTrain Loss: 6.357 | Train PPL: 576.260 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.037 |  Val. PPL: 1138.026\n",
            "Epoch: 64 | Time: 0m 1s\n",
            "\tTrain Loss: 6.355 | Train PPL: 575.564 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.037 |  Val. PPL: 1137.488\n",
            "Epoch: 65 | Time: 0m 1s\n",
            "\tTrain Loss: 6.354 | Train PPL: 574.820 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.036 |  Val. PPL: 1136.969\n",
            "Epoch: 66 | Time: 0m 1s\n",
            "\tTrain Loss: 6.353 | Train PPL: 574.189 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.036 |  Val. PPL: 1136.443\n",
            "Epoch: 67 | Time: 0m 1s\n",
            "\tTrain Loss: 6.352 | Train PPL: 573.477 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.035 |  Val. PPL: 1135.970\n",
            "Epoch: 68 | Time: 0m 1s\n",
            "\tTrain Loss: 6.350 | Train PPL: 572.715 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.035 |  Val. PPL: 1135.475\n",
            "Epoch: 69 | Time: 0m 1s\n",
            "\tTrain Loss: 6.349 | Train PPL: 572.042 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.034 |  Val. PPL: 1134.996\n",
            "Epoch: 70 | Time: 0m 1s\n",
            "\tTrain Loss: 6.348 | Train PPL: 571.362 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.034 |  Val. PPL: 1134.517\n",
            "Epoch: 71 | Time: 0m 1s\n",
            "\tTrain Loss: 6.347 | Train PPL: 570.658 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.034 |  Val. PPL: 1134.042\n",
            "Epoch: 72 | Time: 0m 1s\n",
            "\tTrain Loss: 6.345 | Train PPL: 569.890 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.033 |  Val. PPL: 1133.575\n",
            "Epoch: 73 | Time: 0m 1s\n",
            "\tTrain Loss: 6.344 | Train PPL: 569.216 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.033 |  Val. PPL: 1133.111\n",
            "Epoch: 74 | Time: 0m 1s\n",
            "\tTrain Loss: 6.343 | Train PPL: 568.513 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.032 |  Val. PPL: 1132.648\n",
            "Epoch: 75 | Time: 0m 1s\n",
            "\tTrain Loss: 6.342 | Train PPL: 567.836 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.032 |  Val. PPL: 1132.187\n",
            "Epoch: 76 | Time: 0m 1s\n",
            "\tTrain Loss: 6.341 | Train PPL: 567.101 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.032 |  Val. PPL: 1131.731\n",
            "Epoch: 77 | Time: 0m 1s\n",
            "\tTrain Loss: 6.339 | Train PPL: 566.361 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.031 |  Val. PPL: 1131.283\n",
            "Epoch: 78 | Time: 0m 1s\n",
            "\tTrain Loss: 6.338 | Train PPL: 565.672 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.031 |  Val. PPL: 1130.842\n",
            "Epoch: 79 | Time: 0m 1s\n",
            "\tTrain Loss: 6.337 | Train PPL: 565.047 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.030 |  Val. PPL: 1130.413\n",
            "Epoch: 80 | Time: 0m 1s\n",
            "\tTrain Loss: 6.336 | Train PPL: 564.269 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.030 |  Val. PPL: 1129.987\n",
            "Epoch: 81 | Time: 0m 1s\n",
            "\tTrain Loss: 6.334 | Train PPL: 563.636 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.030 |  Val. PPL: 1129.717\n",
            "Epoch: 82 | Time: 0m 1s\n",
            "\tTrain Loss: 6.333 | Train PPL: 562.945 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.029 |  Val. PPL: 1129.315\n",
            "Epoch: 83 | Time: 0m 1s\n",
            "\tTrain Loss: 6.332 | Train PPL: 562.254 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.029 |  Val. PPL: 1128.922\n",
            "Epoch: 84 | Time: 0m 1s\n",
            "\tTrain Loss: 6.331 | Train PPL: 561.576 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.029 |  Val. PPL: 1128.533\n",
            "Epoch: 85 | Time: 0m 1s\n",
            "\tTrain Loss: 6.329 | Train PPL: 560.836 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.028 |  Val. PPL: 1128.157\n",
            "Epoch: 86 | Time: 0m 1s\n",
            "\tTrain Loss: 6.328 | Train PPL: 560.210 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.028 |  Val. PPL: 1127.783\n",
            "Epoch: 87 | Time: 0m 1s\n",
            "\tTrain Loss: 6.327 | Train PPL: 559.516 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.028 |  Val. PPL: 1127.409\n",
            "Epoch: 88 | Time: 0m 1s\n",
            "\tTrain Loss: 6.326 | Train PPL: 558.801 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.027 |  Val. PPL: 1127.038\n",
            "Epoch: 89 | Time: 0m 1s\n",
            "\tTrain Loss: 6.325 | Train PPL: 558.185 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.027 |  Val. PPL: 1126.671\n",
            "Epoch: 90 | Time: 0m 1s\n",
            "\tTrain Loss: 6.323 | Train PPL: 557.492 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.027 |  Val. PPL: 1126.308\n",
            "Epoch: 91 | Time: 0m 1s\n",
            "\tTrain Loss: 6.322 | Train PPL: 556.775 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.026 |  Val. PPL: 1125.949\n",
            "Epoch: 92 | Time: 0m 1s\n",
            "\tTrain Loss: 6.321 | Train PPL: 556.153 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.026 |  Val. PPL: 1125.596\n",
            "Epoch: 93 | Time: 0m 1s\n",
            "\tTrain Loss: 6.320 | Train PPL: 555.514 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.026 |  Val. PPL: 1125.251\n",
            "Epoch: 94 | Time: 0m 1s\n",
            "\tTrain Loss: 6.319 | Train PPL: 554.839 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.025 |  Val. PPL: 1124.908\n",
            "Epoch: 95 | Time: 0m 1s\n",
            "\tTrain Loss: 6.318 | Train PPL: 554.202 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.025 |  Val. PPL: 1124.575\n",
            "Epoch: 96 | Time: 0m 1s\n",
            "\tTrain Loss: 6.316 | Train PPL: 553.500 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.025 |  Val. PPL: 1124.252\n",
            "Epoch: 97 | Time: 0m 1s\n",
            "\tTrain Loss: 6.315 | Train PPL: 552.924 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.025 |  Val. PPL: 1123.924\n",
            "Epoch: 98 | Time: 0m 1s\n",
            "\tTrain Loss: 6.314 | Train PPL: 552.247 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.024 |  Val. PPL: 1123.613\n",
            "Epoch: 99 | Time: 0m 1s\n",
            "\tTrain Loss: 6.313 | Train PPL: 551.592 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.024 |  Val. PPL: 1123.306\n",
            "Epoch: 100 | Time: 0m 1s\n",
            "\tTrain Loss: 6.312 | Train PPL: 550.968 | LR: [1.0000000000000026e-05]\n",
            "\t Val. Loss: 7.024 |  Val. PPL: 1123.008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRfuK1MHIg7v"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.01, amsgrad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYIRZh9vLH_4",
        "outputId": "70322d98-9e71-451a-9c22-b3f0785d50f8"
      },
      "source": [
        "optimizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    eps: 1e-08\n",
              "    lr: 1e-05\n",
              "    weight_decay: 0.01\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXHg1YUjEnXz",
        "outputId": "9c951277-47b9-43d4-8d32-e0d93e7142b7"
      },
      "source": [
        "N_EPOCHS = 100\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = 7.02376618385315\r\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 1s\n",
            "\tTrain Loss: 6.311 | Train PPL: 550.375\n",
            "\t Val. Loss: 7.024 |  Val. PPL: 1122.716\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 6.310 | Train PPL: 549.776\n",
            "\t Val. Loss: 7.023 |  Val. PPL: 1122.437\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 6.308 | Train PPL: 549.150\n",
            "\t Val. Loss: 7.023 |  Val. PPL: 1122.156\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 6.307 | Train PPL: 548.532\n",
            "\t Val. Loss: 7.023 |  Val. PPL: 1121.894\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 6.306 | Train PPL: 547.947\n",
            "\t Val. Loss: 7.023 |  Val. PPL: 1121.638\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 6.305 | Train PPL: 547.307\n",
            "\t Val. Loss: 7.022 |  Val. PPL: 1121.387\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 6.304 | Train PPL: 546.812\n",
            "\t Val. Loss: 7.022 |  Val. PPL: 1121.151\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 6.303 | Train PPL: 546.084\n",
            "\t Val. Loss: 7.022 |  Val. PPL: 1120.922\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 6.302 | Train PPL: 545.529\n",
            "\t Val. Loss: 7.022 |  Val. PPL: 1120.705\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 6.301 | Train PPL: 544.988\n",
            "\t Val. Loss: 7.022 |  Val. PPL: 1120.500\n",
            "Epoch: 11 | Time: 0m 1s\n",
            "\tTrain Loss: 6.300 | Train PPL: 544.329\n",
            "\t Val. Loss: 7.021 |  Val. PPL: 1120.296\n",
            "Epoch: 12 | Time: 0m 1s\n",
            "\tTrain Loss: 6.298 | Train PPL: 543.751\n",
            "\t Val. Loss: 7.021 |  Val. PPL: 1120.104\n",
            "Epoch: 13 | Time: 0m 1s\n",
            "\tTrain Loss: 6.297 | Train PPL: 543.161\n",
            "\t Val. Loss: 7.021 |  Val. PPL: 1119.919\n",
            "Epoch: 14 | Time: 0m 1s\n",
            "\tTrain Loss: 6.296 | Train PPL: 542.572\n",
            "\t Val. Loss: 7.021 |  Val. PPL: 1119.747\n",
            "Epoch: 15 | Time: 0m 1s\n",
            "\tTrain Loss: 6.295 | Train PPL: 542.034\n",
            "\t Val. Loss: 7.021 |  Val. PPL: 1119.581\n",
            "Epoch: 16 | Time: 0m 1s\n",
            "\tTrain Loss: 6.294 | Train PPL: 541.518\n",
            "\t Val. Loss: 7.021 |  Val. PPL: 1119.426\n",
            "Epoch: 17 | Time: 0m 1s\n",
            "\tTrain Loss: 6.293 | Train PPL: 540.919\n",
            "\t Val. Loss: 7.020 |  Val. PPL: 1119.275\n",
            "Epoch: 18 | Time: 0m 1s\n",
            "\tTrain Loss: 6.292 | Train PPL: 540.366\n",
            "\t Val. Loss: 7.020 |  Val. PPL: 1119.139\n",
            "Epoch: 19 | Time: 0m 1s\n",
            "\tTrain Loss: 6.291 | Train PPL: 539.819\n",
            "\t Val. Loss: 7.020 |  Val. PPL: 1118.995\n",
            "Epoch: 20 | Time: 0m 1s\n",
            "\tTrain Loss: 6.290 | Train PPL: 539.229\n",
            "\t Val. Loss: 7.020 |  Val. PPL: 1118.857\n",
            "Epoch: 21 | Time: 0m 1s\n",
            "\tTrain Loss: 6.289 | Train PPL: 538.657\n",
            "\t Val. Loss: 7.020 |  Val. PPL: 1118.713\n",
            "Epoch: 22 | Time: 0m 1s\n",
            "\tTrain Loss: 6.288 | Train PPL: 538.104\n",
            "\t Val. Loss: 7.020 |  Val. PPL: 1118.569\n",
            "Epoch: 23 | Time: 0m 1s\n",
            "\tTrain Loss: 6.287 | Train PPL: 537.602\n",
            "\t Val. Loss: 7.020 |  Val. PPL: 1118.426\n",
            "Epoch: 24 | Time: 0m 1s\n",
            "\tTrain Loss: 6.286 | Train PPL: 537.098\n",
            "\t Val. Loss: 7.020 |  Val. PPL: 1118.281\n",
            "Epoch: 25 | Time: 0m 1s\n",
            "\tTrain Loss: 6.285 | Train PPL: 536.501\n",
            "\t Val. Loss: 7.019 |  Val. PPL: 1118.139\n",
            "Epoch: 26 | Time: 0m 1s\n",
            "\tTrain Loss: 6.284 | Train PPL: 535.912\n",
            "\t Val. Loss: 7.019 |  Val. PPL: 1117.996\n",
            "Epoch: 27 | Time: 0m 1s\n",
            "\tTrain Loss: 6.283 | Train PPL: 535.482\n",
            "\t Val. Loss: 7.019 |  Val. PPL: 1117.855\n",
            "Epoch: 28 | Time: 0m 1s\n",
            "\tTrain Loss: 6.282 | Train PPL: 534.957\n",
            "\t Val. Loss: 7.019 |  Val. PPL: 1117.732\n",
            "Epoch: 29 | Time: 0m 1s\n",
            "\tTrain Loss: 6.281 | Train PPL: 534.378\n",
            "\t Val. Loss: 7.019 |  Val. PPL: 1117.282\n",
            "Epoch: 30 | Time: 0m 1s\n",
            "\tTrain Loss: 6.280 | Train PPL: 533.922\n",
            "\t Val. Loss: 7.019 |  Val. PPL: 1117.151\n",
            "Epoch: 31 | Time: 0m 1s\n",
            "\tTrain Loss: 6.279 | Train PPL: 533.424\n",
            "\t Val. Loss: 7.018 |  Val. PPL: 1117.025\n",
            "Epoch: 32 | Time: 0m 1s\n",
            "\tTrain Loss: 6.278 | Train PPL: 532.934\n",
            "\t Val. Loss: 7.018 |  Val. PPL: 1116.904\n",
            "Epoch: 33 | Time: 0m 1s\n",
            "\tTrain Loss: 6.277 | Train PPL: 532.368\n",
            "\t Val. Loss: 7.018 |  Val. PPL: 1116.750\n",
            "Epoch: 34 | Time: 0m 1s\n",
            "\tTrain Loss: 6.277 | Train PPL: 531.927\n",
            "\t Val. Loss: 7.018 |  Val. PPL: 1116.631\n",
            "Epoch: 35 | Time: 0m 1s\n",
            "\tTrain Loss: 6.276 | Train PPL: 531.400\n",
            "\t Val. Loss: 7.018 |  Val. PPL: 1116.487\n",
            "Epoch: 36 | Time: 0m 1s\n",
            "\tTrain Loss: 6.274 | Train PPL: 530.860\n",
            "\t Val. Loss: 7.018 |  Val. PPL: 1116.378\n",
            "Epoch: 37 | Time: 0m 1s\n",
            "\tTrain Loss: 6.274 | Train PPL: 530.396\n",
            "\t Val. Loss: 7.018 |  Val. PPL: 1116.239\n",
            "Epoch: 38 | Time: 0m 1s\n",
            "\tTrain Loss: 6.273 | Train PPL: 529.840\n",
            "\t Val. Loss: 7.018 |  Val. PPL: 1116.384\n",
            "Epoch: 39 | Time: 0m 1s\n",
            "\tTrain Loss: 6.272 | Train PPL: 529.396\n",
            "\t Val. Loss: 7.018 |  Val. PPL: 1116.199\n",
            "Epoch: 40 | Time: 0m 1s\n",
            "\tTrain Loss: 6.271 | Train PPL: 528.906\n",
            "\t Val. Loss: 7.018 |  Val. PPL: 1116.108\n",
            "Epoch: 41 | Time: 0m 1s\n",
            "\tTrain Loss: 6.270 | Train PPL: 528.461\n",
            "\t Val. Loss: 7.018 |  Val. PPL: 1116.016\n",
            "Epoch: 42 | Time: 0m 1s\n",
            "\tTrain Loss: 6.269 | Train PPL: 527.924\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.933\n",
            "Epoch: 43 | Time: 0m 1s\n",
            "\tTrain Loss: 6.268 | Train PPL: 527.462\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.845\n",
            "Epoch: 44 | Time: 0m 1s\n",
            "\tTrain Loss: 6.267 | Train PPL: 526.930\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.758\n",
            "Epoch: 45 | Time: 0m 1s\n",
            "\tTrain Loss: 6.266 | Train PPL: 526.480\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.671\n",
            "Epoch: 46 | Time: 0m 1s\n",
            "\tTrain Loss: 6.265 | Train PPL: 525.983\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.604\n",
            "Epoch: 47 | Time: 0m 1s\n",
            "\tTrain Loss: 6.265 | Train PPL: 525.592\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.528\n",
            "Epoch: 48 | Time: 0m 1s\n",
            "\tTrain Loss: 6.263 | Train PPL: 524.972\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.456\n",
            "Epoch: 49 | Time: 0m 1s\n",
            "\tTrain Loss: 6.263 | Train PPL: 524.581\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.383\n",
            "Epoch: 50 | Time: 0m 1s\n",
            "\tTrain Loss: 6.262 | Train PPL: 524.120\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.322\n",
            "Epoch: 51 | Time: 0m 1s\n",
            "\tTrain Loss: 6.261 | Train PPL: 523.659\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.260\n",
            "Epoch: 52 | Time: 0m 1s\n",
            "\tTrain Loss: 6.260 | Train PPL: 523.136\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.198\n",
            "Epoch: 53 | Time: 0m 1s\n",
            "\tTrain Loss: 6.259 | Train PPL: 522.788\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.131\n",
            "Epoch: 54 | Time: 0m 1s\n",
            "\tTrain Loss: 6.258 | Train PPL: 522.344\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.086\n",
            "Epoch: 55 | Time: 0m 1s\n",
            "\tTrain Loss: 6.258 | Train PPL: 521.924\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.034\n",
            "Epoch: 56 | Time: 0m 1s\n",
            "\tTrain Loss: 6.256 | Train PPL: 521.359\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1114.987\n",
            "Epoch: 57 | Time: 0m 1s\n",
            "\tTrain Loss: 6.255 | Train PPL: 520.863\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1114.949\n",
            "Epoch: 58 | Time: 0m 1s\n",
            "\tTrain Loss: 6.255 | Train PPL: 520.530\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1114.909\n",
            "Epoch: 59 | Time: 0m 1s\n",
            "\tTrain Loss: 6.254 | Train PPL: 519.998\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.874\n",
            "Epoch: 60 | Time: 0m 1s\n",
            "\tTrain Loss: 6.253 | Train PPL: 519.562\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.831\n",
            "Epoch: 61 | Time: 0m 1s\n",
            "\tTrain Loss: 6.252 | Train PPL: 519.084\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.798\n",
            "Epoch: 62 | Time: 0m 1s\n",
            "\tTrain Loss: 6.251 | Train PPL: 518.715\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.768\n",
            "Epoch: 63 | Time: 0m 1s\n",
            "\tTrain Loss: 6.251 | Train PPL: 518.289\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.736\n",
            "Epoch: 64 | Time: 0m 1s\n",
            "\tTrain Loss: 6.250 | Train PPL: 517.833\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.714\n",
            "Epoch: 65 | Time: 0m 1s\n",
            "\tTrain Loss: 6.249 | Train PPL: 517.367\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.697\n",
            "Epoch: 66 | Time: 0m 1s\n",
            "\tTrain Loss: 6.248 | Train PPL: 516.963\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.680\n",
            "Epoch: 67 | Time: 0m 1s\n",
            "\tTrain Loss: 6.247 | Train PPL: 516.514\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.662\n",
            "Epoch: 68 | Time: 0m 1s\n",
            "\tTrain Loss: 6.246 | Train PPL: 516.153\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.658\n",
            "Epoch: 69 | Time: 0m 1s\n",
            "\tTrain Loss: 6.246 | Train PPL: 515.689\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.644\n",
            "Epoch: 70 | Time: 0m 1s\n",
            "\tTrain Loss: 6.245 | Train PPL: 515.294\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.641\n",
            "Epoch: 71 | Time: 0m 1s\n",
            "\tTrain Loss: 6.244 | Train PPL: 514.857\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.645\n",
            "Epoch: 72 | Time: 0m 1s\n",
            "\tTrain Loss: 6.243 | Train PPL: 514.406\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.632\n",
            "Epoch: 73 | Time: 0m 1s\n",
            "\tTrain Loss: 6.242 | Train PPL: 513.935\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.643\n",
            "Epoch: 74 | Time: 0m 1s\n",
            "\tTrain Loss: 6.241 | Train PPL: 513.549\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.649\n",
            "Epoch: 75 | Time: 0m 1s\n",
            "\tTrain Loss: 6.241 | Train PPL: 513.288\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.657\n",
            "Epoch: 76 | Time: 0m 1s\n",
            "\tTrain Loss: 6.240 | Train PPL: 512.789\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.670\n",
            "Epoch: 77 | Time: 0m 1s\n",
            "\tTrain Loss: 6.239 | Train PPL: 512.362\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.827\n",
            "Epoch: 78 | Time: 0m 1s\n",
            "\tTrain Loss: 6.238 | Train PPL: 511.986\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.853\n",
            "Epoch: 79 | Time: 0m 1s\n",
            "\tTrain Loss: 6.238 | Train PPL: 511.596\n",
            "\t Val. Loss: 7.016 |  Val. PPL: 1114.876\n",
            "Epoch: 80 | Time: 0m 1s\n",
            "\tTrain Loss: 6.237 | Train PPL: 511.182\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1114.908\n",
            "Epoch: 81 | Time: 0m 1s\n",
            "\tTrain Loss: 6.236 | Train PPL: 510.719\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1114.934\n",
            "Epoch: 82 | Time: 0m 1s\n",
            "\tTrain Loss: 6.235 | Train PPL: 510.287\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1114.968\n",
            "Epoch: 83 | Time: 0m 1s\n",
            "\tTrain Loss: 6.234 | Train PPL: 509.881\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.008\n",
            "Epoch: 84 | Time: 0m 1s\n",
            "\tTrain Loss: 6.234 | Train PPL: 509.550\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.050\n",
            "Epoch: 85 | Time: 0m 1s\n",
            "\tTrain Loss: 6.233 | Train PPL: 509.078\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.074\n",
            "Epoch: 86 | Time: 0m 1s\n",
            "\tTrain Loss: 6.232 | Train PPL: 508.732\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.113\n",
            "Epoch: 87 | Time: 0m 1s\n",
            "\tTrain Loss: 6.231 | Train PPL: 508.294\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.154\n",
            "Epoch: 88 | Time: 0m 1s\n",
            "\tTrain Loss: 6.230 | Train PPL: 507.890\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.182\n",
            "Epoch: 89 | Time: 0m 1s\n",
            "\tTrain Loss: 6.230 | Train PPL: 507.547\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.219\n",
            "Epoch: 90 | Time: 0m 1s\n",
            "\tTrain Loss: 6.229 | Train PPL: 507.066\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.255\n",
            "Epoch: 91 | Time: 0m 1s\n",
            "\tTrain Loss: 6.228 | Train PPL: 506.716\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.300\n",
            "Epoch: 92 | Time: 0m 1s\n",
            "\tTrain Loss: 6.227 | Train PPL: 506.404\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.339\n",
            "Epoch: 93 | Time: 0m 1s\n",
            "\tTrain Loss: 6.226 | Train PPL: 505.956\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.380\n",
            "Epoch: 94 | Time: 0m 1s\n",
            "\tTrain Loss: 6.226 | Train PPL: 505.709\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.433\n",
            "Epoch: 95 | Time: 0m 1s\n",
            "\tTrain Loss: 6.225 | Train PPL: 505.134\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.476\n",
            "Epoch: 96 | Time: 0m 1s\n",
            "\tTrain Loss: 6.224 | Train PPL: 504.802\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.516\n",
            "Epoch: 97 | Time: 0m 1s\n",
            "\tTrain Loss: 6.223 | Train PPL: 504.456\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.578\n",
            "Epoch: 98 | Time: 0m 1s\n",
            "\tTrain Loss: 6.223 | Train PPL: 504.084\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.619\n",
            "Epoch: 99 | Time: 0m 1s\n",
            "\tTrain Loss: 6.222 | Train PPL: 503.642\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.675\n",
            "Epoch: 100 | Time: 0m 1s\n",
            "\tTrain Loss: 6.221 | Train PPL: 503.381\n",
            "\t Val. Loss: 7.017 |  Val. PPL: 1115.724\n",
            "7.016279315948486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n05HAXXiJbby"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-8, weight_decay=0.05)\r\n",
        "# scheduler = OneCycleLR(optimizer, \r\n",
        "#                         0.000001, \r\n",
        "#                         epochs=100, \r\n",
        "#                         cycle_momentum=False, \r\n",
        "#                         steps_per_epoch=len(train_iterator), \r\n",
        "#                         #base_momentum=config.momentum,\r\n",
        "#                         #max_momentum=0.95, \r\n",
        "#                         pct_start=0.5,\r\n",
        "#                         # anneal_strategy=config.anneal_strategy,\r\n",
        "#                         div_factor=1000,\r\n",
        "#                         # final_div_factor=config.final_div_factor\r\n",
        "#                         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKb02hGfr9_c",
        "outputId": "bf6adca7-4da6-4e02-dcfe-62e1f0ad7a6c"
      },
      "source": [
        "optimizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    eps: 1e-08\n",
              "    lr: 1e-09\n",
              "    weight_decay: 5e-05\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq3-W2A5rVLn",
        "outputId": "4038e1c5-1f25-46ff-c4c0-4c1620cbbfef"
      },
      "source": [
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.83048825263977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uGl0qfzjKcs",
        "outputId": "e883556d-59e4-4f6b-a25b-a297e975b509"
      },
      "source": [
        "N_EPOCHS = 50\r\n",
        "CLIP = 1\r\n",
        "print(best_valid_loss)\r\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\r\n",
        "\r\n",
        "best_valid_loss = 6.830108547210694 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.830108547210694\n",
            "Epoch: 01 | Time: 0m 1s\n",
            "\tTrain Loss: 6.400 | Train PPL: 602.021\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.291\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 6.400 | Train PPL: 602.059\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.291\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 6.400 | Train PPL: 602.069\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.291\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.150\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.290\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.179\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.290\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.210\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.289\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.225\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.289\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.268\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.288\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.376\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.288\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.375\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.288\n",
            "Epoch: 11 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.436\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.288\n",
            "Epoch: 12 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.423\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 13 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.490\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 14 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.502\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 15 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.559\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 16 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.600\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 17 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.656\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 18 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.686\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 19 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.720\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 20 | Time: 0m 1s\n",
            "\tTrain Loss: 6.401 | Train PPL: 602.721\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 21 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 602.774\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 22 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 602.826\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 23 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 602.913\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 24 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 602.918\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 25 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 602.975\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 26 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 602.977\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.287\n",
            "Epoch: 27 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 603.008\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.288\n",
            "Epoch: 28 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 603.088\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.288\n",
            "Epoch: 29 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 603.088\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.288\n",
            "Epoch: 30 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 603.116\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.288\n",
            "Epoch: 31 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 603.204\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.288\n",
            "Epoch: 32 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 603.233\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.289\n",
            "Epoch: 33 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 603.215\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.289\n",
            "Epoch: 34 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 603.247\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.289\n",
            "Epoch: 35 | Time: 0m 1s\n",
            "\tTrain Loss: 6.402 | Train PPL: 603.322\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.290\n",
            "Epoch: 36 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.367\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.290\n",
            "Epoch: 37 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.381\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.290\n",
            "Epoch: 38 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.478\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.291\n",
            "Epoch: 39 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.428\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.291\n",
            "Epoch: 40 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.532\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.291\n",
            "Epoch: 41 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.584\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.292\n",
            "Epoch: 42 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.586\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.293\n",
            "Epoch: 43 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.642\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.293\n",
            "Epoch: 44 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.663\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.294\n",
            "Epoch: 45 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.665\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.294\n",
            "Epoch: 46 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.741\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.295\n",
            "Epoch: 47 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.810\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.295\n",
            "Epoch: 48 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.847\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.296\n",
            "Epoch: 49 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.868\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.296\n",
            "Epoch: 50 | Time: 0m 1s\n",
            "\tTrain Loss: 6.403 | Train PPL: 603.944\n",
            "\t Val. Loss: 6.830 |  Val. PPL: 925.297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJbL1n2XMgob",
        "outputId": "e6de558b-83ed-4ee0-d655-7da9f116cc3d"
      },
      "source": [
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.830108547210694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB85z8dKM8cS"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.000001, weight_decay=0.1)\r\n",
        "scheduler = OneCycleLR(optimizer, \r\n",
        "                        config.ocp_max_lr, \r\n",
        "                        epochs=config.epochs, \r\n",
        "                        cycle_momentum=cycle_momentum, \r\n",
        "                        steps_per_epoch=len(trainloader), \r\n",
        "                        base_momentum=config.momentum,\r\n",
        "                        max_momentum=0.95, \r\n",
        "                        pct_start=0.208,\r\n",
        "                        anneal_strategy=config.anneal_strategy,\r\n",
        "                        div_factor=config.div_factor,\r\n",
        "                        final_div_factor=config.final_div_factor\r\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDT6kRLijzEZ",
        "outputId": "5dfcb35e-87e6-41fb-b2cd-d316e23eed63"
      },
      "source": [
        "N_EPOCHS = 50\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = 5.484 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 2.445 | Train PPL:  11.535\n",
            "\t Val. Loss: 5.488 |  Val. PPL: 241.844\n",
            "Epoch: 02 | Time: 0m 2s\n",
            "\tTrain Loss: 2.425 | Train PPL:  11.305\n",
            "\t Val. Loss: 5.480 |  Val. PPL: 239.949\n",
            "Epoch: 03 | Time: 0m 2s\n",
            "\tTrain Loss: 2.403 | Train PPL:  11.058\n",
            "\t Val. Loss: 5.477 |  Val. PPL: 239.027\n",
            "Epoch: 04 | Time: 0m 2s\n",
            "\tTrain Loss: 2.374 | Train PPL:  10.738\n",
            "\t Val. Loss: 5.471 |  Val. PPL: 237.608\n",
            "Epoch: 05 | Time: 0m 2s\n",
            "\tTrain Loss: 2.345 | Train PPL:  10.437\n",
            "\t Val. Loss: 5.465 |  Val. PPL: 236.272\n",
            "Epoch: 06 | Time: 0m 2s\n",
            "\tTrain Loss: 2.324 | Train PPL:  10.217\n",
            "\t Val. Loss: 5.460 |  Val. PPL: 235.171\n",
            "Epoch: 07 | Time: 0m 2s\n",
            "\tTrain Loss: 2.302 | Train PPL:   9.992\n",
            "\t Val. Loss: 5.450 |  Val. PPL: 232.854\n",
            "Epoch: 08 | Time: 0m 2s\n",
            "\tTrain Loss: 2.273 | Train PPL:   9.711\n",
            "\t Val. Loss: 5.454 |  Val. PPL: 233.587\n",
            "Epoch: 09 | Time: 0m 2s\n",
            "\tTrain Loss: 2.253 | Train PPL:   9.516\n",
            "\t Val. Loss: 5.447 |  Val. PPL: 231.988\n",
            "Epoch: 10 | Time: 0m 2s\n",
            "\tTrain Loss: 2.221 | Train PPL:   9.218\n",
            "\t Val. Loss: 5.449 |  Val. PPL: 232.481\n",
            "Epoch: 11 | Time: 0m 2s\n",
            "\tTrain Loss: 2.196 | Train PPL:   8.987\n",
            "\t Val. Loss: 5.445 |  Val. PPL: 231.562\n",
            "Epoch: 12 | Time: 0m 2s\n",
            "\tTrain Loss: 2.175 | Train PPL:   8.802\n",
            "\t Val. Loss: 5.441 |  Val. PPL: 230.705\n",
            "Epoch: 13 | Time: 0m 2s\n",
            "\tTrain Loss: 2.155 | Train PPL:   8.632\n",
            "\t Val. Loss: 5.441 |  Val. PPL: 230.752\n",
            "Epoch: 14 | Time: 0m 2s\n",
            "\tTrain Loss: 2.132 | Train PPL:   8.430\n",
            "\t Val. Loss: 5.440 |  Val. PPL: 230.392\n",
            "Epoch: 15 | Time: 0m 2s\n",
            "\tTrain Loss: 2.110 | Train PPL:   8.250\n",
            "\t Val. Loss: 5.438 |  Val. PPL: 230.004\n",
            "Epoch: 16 | Time: 0m 2s\n",
            "\tTrain Loss: 2.086 | Train PPL:   8.051\n",
            "\t Val. Loss: 5.439 |  Val. PPL: 230.203\n",
            "Epoch: 17 | Time: 0m 2s\n",
            "\tTrain Loss: 2.062 | Train PPL:   7.860\n",
            "\t Val. Loss: 5.439 |  Val. PPL: 230.242\n",
            "Epoch: 18 | Time: 0m 2s\n",
            "\tTrain Loss: 2.044 | Train PPL:   7.722\n",
            "\t Val. Loss: 5.428 |  Val. PPL: 227.606\n",
            "Epoch: 19 | Time: 0m 2s\n",
            "\tTrain Loss: 2.025 | Train PPL:   7.574\n",
            "\t Val. Loss: 5.433 |  Val. PPL: 228.945\n",
            "Epoch: 20 | Time: 0m 2s\n",
            "\tTrain Loss: 2.008 | Train PPL:   7.447\n",
            "\t Val. Loss: 5.428 |  Val. PPL: 227.593\n",
            "Epoch: 21 | Time: 0m 2s\n",
            "\tTrain Loss: 1.995 | Train PPL:   7.353\n",
            "\t Val. Loss: 5.425 |  Val. PPL: 226.936\n",
            "Epoch: 22 | Time: 0m 2s\n",
            "\tTrain Loss: 1.974 | Train PPL:   7.200\n",
            "\t Val. Loss: 5.431 |  Val. PPL: 228.364\n",
            "Epoch: 23 | Time: 0m 2s\n",
            "\tTrain Loss: 1.951 | Train PPL:   7.037\n",
            "\t Val. Loss: 5.425 |  Val. PPL: 226.909\n",
            "Epoch: 24 | Time: 0m 2s\n",
            "\tTrain Loss: 1.943 | Train PPL:   6.977\n",
            "\t Val. Loss: 5.433 |  Val. PPL: 228.762\n",
            "Epoch: 25 | Time: 0m 2s\n",
            "\tTrain Loss: 1.920 | Train PPL:   6.824\n",
            "\t Val. Loss: 5.427 |  Val. PPL: 227.432\n",
            "Epoch: 26 | Time: 0m 2s\n",
            "\tTrain Loss: 1.905 | Train PPL:   6.719\n",
            "\t Val. Loss: 5.427 |  Val. PPL: 227.423\n",
            "Epoch: 27 | Time: 0m 2s\n",
            "\tTrain Loss: 1.889 | Train PPL:   6.615\n",
            "\t Val. Loss: 5.429 |  Val. PPL: 227.880\n",
            "Epoch: 28 | Time: 0m 2s\n",
            "\tTrain Loss: 1.873 | Train PPL:   6.508\n",
            "\t Val. Loss: 5.434 |  Val. PPL: 229.069\n",
            "Epoch: 29 | Time: 0m 2s\n",
            "\tTrain Loss: 1.850 | Train PPL:   6.362\n",
            "\t Val. Loss: 5.430 |  Val. PPL: 228.038\n",
            "Epoch: 30 | Time: 0m 2s\n",
            "\tTrain Loss: 1.842 | Train PPL:   6.307\n",
            "\t Val. Loss: 5.427 |  Val. PPL: 227.426\n",
            "Epoch: 31 | Time: 0m 2s\n",
            "\tTrain Loss: 1.825 | Train PPL:   6.203\n",
            "\t Val. Loss: 5.432 |  Val. PPL: 228.666\n",
            "Epoch: 32 | Time: 0m 2s\n",
            "\tTrain Loss: 1.806 | Train PPL:   6.085\n",
            "\t Val. Loss: 5.435 |  Val. PPL: 229.192\n",
            "Epoch: 33 | Time: 0m 2s\n",
            "\tTrain Loss: 1.794 | Train PPL:   6.012\n",
            "\t Val. Loss: 5.428 |  Val. PPL: 227.698\n",
            "Epoch: 34 | Time: 0m 2s\n",
            "\tTrain Loss: 1.773 | Train PPL:   5.891\n",
            "\t Val. Loss: 5.423 |  Val. PPL: 226.517\n",
            "Epoch: 35 | Time: 0m 2s\n",
            "\tTrain Loss: 1.762 | Train PPL:   5.827\n",
            "\t Val. Loss: 5.429 |  Val. PPL: 227.812\n",
            "Epoch: 36 | Time: 0m 2s\n",
            "\tTrain Loss: 1.753 | Train PPL:   5.774\n",
            "\t Val. Loss: 5.418 |  Val. PPL: 225.335\n",
            "Epoch: 37 | Time: 0m 2s\n",
            "\tTrain Loss: 1.735 | Train PPL:   5.667\n",
            "\t Val. Loss: 5.421 |  Val. PPL: 226.186\n",
            "Epoch: 38 | Time: 0m 2s\n",
            "\tTrain Loss: 1.723 | Train PPL:   5.602\n",
            "\t Val. Loss: 5.424 |  Val. PPL: 226.781\n",
            "Epoch: 39 | Time: 0m 2s\n",
            "\tTrain Loss: 1.714 | Train PPL:   5.552\n",
            "\t Val. Loss: 5.423 |  Val. PPL: 226.561\n",
            "Epoch: 40 | Time: 0m 2s\n",
            "\tTrain Loss: 1.692 | Train PPL:   5.433\n",
            "\t Val. Loss: 5.425 |  Val. PPL: 226.960\n",
            "Epoch: 41 | Time: 0m 2s\n",
            "\tTrain Loss: 1.684 | Train PPL:   5.387\n",
            "\t Val. Loss: 5.423 |  Val. PPL: 226.638\n",
            "Epoch: 42 | Time: 0m 2s\n",
            "\tTrain Loss: 1.672 | Train PPL:   5.323\n",
            "\t Val. Loss: 5.429 |  Val. PPL: 227.955\n",
            "Epoch: 43 | Time: 0m 2s\n",
            "\tTrain Loss: 1.657 | Train PPL:   5.242\n",
            "\t Val. Loss: 5.423 |  Val. PPL: 226.658\n",
            "Epoch: 44 | Time: 0m 2s\n",
            "\tTrain Loss: 1.643 | Train PPL:   5.172\n",
            "\t Val. Loss: 5.428 |  Val. PPL: 227.703\n",
            "Epoch: 45 | Time: 0m 2s\n",
            "\tTrain Loss: 1.641 | Train PPL:   5.159\n",
            "\t Val. Loss: 5.421 |  Val. PPL: 226.170\n",
            "Epoch: 46 | Time: 0m 2s\n",
            "\tTrain Loss: 1.617 | Train PPL:   5.038\n",
            "\t Val. Loss: 5.429 |  Val. PPL: 227.848\n",
            "Epoch: 47 | Time: 0m 2s\n",
            "\tTrain Loss: 1.604 | Train PPL:   4.971\n",
            "\t Val. Loss: 5.426 |  Val. PPL: 227.173\n",
            "Epoch: 48 | Time: 0m 2s\n",
            "\tTrain Loss: 1.599 | Train PPL:   4.949\n",
            "\t Val. Loss: 5.433 |  Val. PPL: 228.753\n",
            "Epoch: 49 | Time: 0m 2s\n",
            "\tTrain Loss: 1.592 | Train PPL:   4.911\n",
            "\t Val. Loss: 5.433 |  Val. PPL: 228.923\n",
            "Epoch: 50 | Time: 0m 2s\n",
            "\tTrain Loss: 1.570 | Train PPL:   4.807\n",
            "\t Val. Loss: 5.435 |  Val. PPL: 229.236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxSXWHMakX9S"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=5e-6, weight_decay=0.0005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElnQtHVqlkSf",
        "outputId": "19685ae8-dc80-47af-d6b1-e48e37ad43b9"
      },
      "source": [
        "best_valid_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.96515295902888"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGCUoX7gkfQF",
        "outputId": "28947cb6-e493-47be-dc55-eb85a4311375"
      },
      "source": [
        "N_EPOCHS = 50\r\n",
        "CLIP = 1\r\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\r\n",
        "best_valid_loss = 4.9634 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 2.041 | Train PPL:   7.702\n",
            "\t Val. Loss: 4.956 |  Val. PPL: 141.985\n",
            "Epoch: 02 | Time: 0m 2s\n",
            "\tTrain Loss: 2.037 | Train PPL:   7.665\n",
            "\t Val. Loss: 4.958 |  Val. PPL: 142.298\n",
            "Epoch: 03 | Time: 0m 2s\n",
            "\tTrain Loss: 2.030 | Train PPL:   7.611\n",
            "\t Val. Loss: 4.958 |  Val. PPL: 142.280\n",
            "Epoch: 04 | Time: 0m 2s\n",
            "\tTrain Loss: 2.035 | Train PPL:   7.649\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.447\n",
            "Epoch: 05 | Time: 0m 2s\n",
            "\tTrain Loss: 2.033 | Train PPL:   7.634\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.554\n",
            "Epoch: 06 | Time: 0m 2s\n",
            "\tTrain Loss: 2.027 | Train PPL:   7.589\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.524\n",
            "Epoch: 07 | Time: 0m 2s\n",
            "\tTrain Loss: 2.032 | Train PPL:   7.628\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.545\n",
            "Epoch: 08 | Time: 0m 2s\n",
            "\tTrain Loss: 2.031 | Train PPL:   7.622\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.499\n",
            "Epoch: 09 | Time: 0m 2s\n",
            "\tTrain Loss: 2.027 | Train PPL:   7.590\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.523\n",
            "Epoch: 10 | Time: 0m 2s\n",
            "\tTrain Loss: 2.027 | Train PPL:   7.595\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.436\n",
            "Epoch: 11 | Time: 0m 2s\n",
            "\tTrain Loss: 2.026 | Train PPL:   7.580\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.548\n",
            "Epoch: 12 | Time: 0m 2s\n",
            "\tTrain Loss: 2.020 | Train PPL:   7.539\n",
            "\t Val. Loss: 4.958 |  Val. PPL: 142.334\n",
            "Epoch: 13 | Time: 0m 2s\n",
            "\tTrain Loss: 2.014 | Train PPL:   7.496\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.540\n",
            "Epoch: 14 | Time: 0m 2s\n",
            "\tTrain Loss: 2.018 | Train PPL:   7.520\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.471\n",
            "Epoch: 15 | Time: 0m 2s\n",
            "\tTrain Loss: 2.016 | Train PPL:   7.507\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.414\n",
            "Epoch: 16 | Time: 0m 2s\n",
            "\tTrain Loss: 2.016 | Train PPL:   7.512\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.499\n",
            "Epoch: 17 | Time: 0m 2s\n",
            "\tTrain Loss: 2.011 | Train PPL:   7.474\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.553\n",
            "Epoch: 18 | Time: 0m 2s\n",
            "\tTrain Loss: 2.015 | Train PPL:   7.500\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.520\n",
            "Epoch: 19 | Time: 0m 2s\n",
            "\tTrain Loss: 2.014 | Train PPL:   7.491\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.642\n",
            "Epoch: 20 | Time: 0m 2s\n",
            "\tTrain Loss: 2.011 | Train PPL:   7.471\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.690\n",
            "Epoch: 21 | Time: 0m 2s\n",
            "\tTrain Loss: 2.012 | Train PPL:   7.475\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.573\n",
            "Epoch: 22 | Time: 0m 2s\n",
            "\tTrain Loss: 2.014 | Train PPL:   7.491\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.519\n",
            "Epoch: 23 | Time: 0m 2s\n",
            "\tTrain Loss: 2.012 | Train PPL:   7.482\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.492\n",
            "Epoch: 24 | Time: 0m 2s\n",
            "\tTrain Loss: 2.002 | Train PPL:   7.405\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.566\n",
            "Epoch: 25 | Time: 0m 2s\n",
            "\tTrain Loss: 2.011 | Train PPL:   7.470\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.438\n",
            "Epoch: 26 | Time: 0m 2s\n",
            "\tTrain Loss: 2.005 | Train PPL:   7.430\n",
            "\t Val. Loss: 4.959 |  Val. PPL: 142.510\n",
            "Epoch: 27 | Time: 0m 2s\n",
            "\tTrain Loss: 2.008 | Train PPL:   7.445\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.534\n",
            "Epoch: 28 | Time: 0m 2s\n",
            "\tTrain Loss: 1.999 | Train PPL:   7.380\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.556\n",
            "Epoch: 29 | Time: 0m 2s\n",
            "\tTrain Loss: 1.999 | Train PPL:   7.380\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.615\n",
            "Epoch: 30 | Time: 0m 2s\n",
            "\tTrain Loss: 2.002 | Train PPL:   7.400\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.585\n",
            "Epoch: 31 | Time: 0m 2s\n",
            "\tTrain Loss: 2.002 | Train PPL:   7.403\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.583\n",
            "Epoch: 32 | Time: 0m 2s\n",
            "\tTrain Loss: 2.003 | Train PPL:   7.413\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.580\n",
            "Epoch: 33 | Time: 0m 2s\n",
            "\tTrain Loss: 2.001 | Train PPL:   7.400\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.585\n",
            "Epoch: 34 | Time: 0m 2s\n",
            "\tTrain Loss: 1.997 | Train PPL:   7.367\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.534\n",
            "Epoch: 35 | Time: 0m 2s\n",
            "\tTrain Loss: 1.995 | Train PPL:   7.355\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.585\n",
            "Epoch: 36 | Time: 0m 2s\n",
            "\tTrain Loss: 1.996 | Train PPL:   7.361\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.666\n",
            "Epoch: 37 | Time: 0m 2s\n",
            "\tTrain Loss: 1.996 | Train PPL:   7.356\n",
            "\t Val. Loss: 4.960 |  Val. PPL: 142.630\n",
            "Epoch: 38 | Time: 0m 2s\n",
            "\tTrain Loss: 1.996 | Train PPL:   7.362\n",
            "\t Val. Loss: 4.962 |  Val. PPL: 142.864\n",
            "Epoch: 39 | Time: 0m 2s\n",
            "\tTrain Loss: 1.992 | Train PPL:   7.327\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.756\n",
            "Epoch: 40 | Time: 0m 2s\n",
            "\tTrain Loss: 1.998 | Train PPL:   7.377\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.738\n",
            "Epoch: 41 | Time: 0m 2s\n",
            "\tTrain Loss: 1.992 | Train PPL:   7.333\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.794\n",
            "Epoch: 42 | Time: 0m 2s\n",
            "\tTrain Loss: 1.995 | Train PPL:   7.351\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.758\n",
            "Epoch: 43 | Time: 0m 2s\n",
            "\tTrain Loss: 1.995 | Train PPL:   7.352\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.701\n",
            "Epoch: 44 | Time: 0m 2s\n",
            "\tTrain Loss: 1.985 | Train PPL:   7.280\n",
            "\t Val. Loss: 4.962 |  Val. PPL: 142.863\n",
            "Epoch: 45 | Time: 0m 2s\n",
            "\tTrain Loss: 1.990 | Train PPL:   7.315\n",
            "\t Val. Loss: 4.962 |  Val. PPL: 142.815\n",
            "Epoch: 46 | Time: 0m 2s\n",
            "\tTrain Loss: 1.988 | Train PPL:   7.304\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.743\n",
            "Epoch: 47 | Time: 0m 2s\n",
            "\tTrain Loss: 1.990 | Train PPL:   7.318\n",
            "\t Val. Loss: 4.962 |  Val. PPL: 142.864\n",
            "Epoch: 48 | Time: 0m 2s\n",
            "\tTrain Loss: 1.982 | Train PPL:   7.254\n",
            "\t Val. Loss: 4.962 |  Val. PPL: 142.848\n",
            "Epoch: 49 | Time: 0m 2s\n",
            "\tTrain Loss: 1.983 | Train PPL:   7.262\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.738\n",
            "Epoch: 50 | Time: 0m 2s\n",
            "\tTrain Loss: 1.988 | Train PPL:   7.303\n",
            "\t Val. Loss: 4.961 |  Val. PPL: 142.791\n",
            "4.955717941125234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6WKCHQNrw1i"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=5e-8, weight_decay=0.005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNivIH5ir1HI",
        "outputId": "c1a6fa86-985a-49de-87ce-47eb4f6f730b"
      },
      "source": [
        "N_EPOCHS = 20\r\n",
        "CLIP = 1\r\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\r\n",
        "best_valid_loss = 4.890922844409943 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 2.241 | Train PPL:   9.402\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.906\n",
            "Epoch: 02 | Time: 0m 2s\n",
            "\tTrain Loss: 2.248 | Train PPL:   9.473\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.938\n",
            "Epoch: 03 | Time: 0m 2s\n",
            "\tTrain Loss: 2.255 | Train PPL:   9.537\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.961\n",
            "Epoch: 04 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.477\n",
            "\t Val. Loss: 4.891 |  Val. PPL: 133.118\n",
            "Epoch: 05 | Time: 0m 2s\n",
            "\tTrain Loss: 2.259 | Train PPL:   9.572\n",
            "\t Val. Loss: 4.891 |  Val. PPL: 133.125\n",
            "Epoch: 06 | Time: 0m 2s\n",
            "\tTrain Loss: 2.255 | Train PPL:   9.532\n",
            "\t Val. Loss: 4.891 |  Val. PPL: 133.151\n",
            "Epoch: 07 | Time: 0m 2s\n",
            "\tTrain Loss: 2.262 | Train PPL:   9.598\n",
            "\t Val. Loss: 4.892 |  Val. PPL: 133.192\n",
            "Epoch: 08 | Time: 0m 2s\n",
            "\tTrain Loss: 2.263 | Train PPL:   9.611\n",
            "\t Val. Loss: 4.892 |  Val. PPL: 133.223\n",
            "Epoch: 09 | Time: 0m 2s\n",
            "\tTrain Loss: 2.267 | Train PPL:   9.647\n",
            "\t Val. Loss: 4.892 |  Val. PPL: 133.251\n",
            "Epoch: 10 | Time: 0m 2s\n",
            "\tTrain Loss: 2.265 | Train PPL:   9.628\n",
            "\t Val. Loss: 4.893 |  Val. PPL: 133.313\n",
            "Epoch: 11 | Time: 0m 2s\n",
            "\tTrain Loss: 2.269 | Train PPL:   9.669\n",
            "\t Val. Loss: 4.893 |  Val. PPL: 133.346\n",
            "Epoch: 12 | Time: 0m 2s\n",
            "\tTrain Loss: 2.273 | Train PPL:   9.708\n",
            "\t Val. Loss: 4.893 |  Val. PPL: 133.371\n",
            "Epoch: 13 | Time: 0m 2s\n",
            "\tTrain Loss: 2.276 | Train PPL:   9.737\n",
            "\t Val. Loss: 4.893 |  Val. PPL: 133.404\n",
            "Epoch: 14 | Time: 0m 2s\n",
            "\tTrain Loss: 2.279 | Train PPL:   9.770\n",
            "\t Val. Loss: 4.894 |  Val. PPL: 133.433\n",
            "Epoch: 15 | Time: 0m 2s\n",
            "\tTrain Loss: 2.284 | Train PPL:   9.814\n",
            "\t Val. Loss: 4.894 |  Val. PPL: 133.461\n",
            "Epoch: 16 | Time: 0m 2s\n",
            "\tTrain Loss: 2.285 | Train PPL:   9.825\n",
            "\t Val. Loss: 4.894 |  Val. PPL: 133.499\n",
            "Epoch: 17 | Time: 0m 2s\n",
            "\tTrain Loss: 2.290 | Train PPL:   9.875\n",
            "\t Val. Loss: 4.894 |  Val. PPL: 133.540\n",
            "Epoch: 18 | Time: 0m 2s\n",
            "\tTrain Loss: 2.292 | Train PPL:   9.892\n",
            "\t Val. Loss: 4.895 |  Val. PPL: 133.597\n",
            "Epoch: 19 | Time: 0m 2s\n",
            "\tTrain Loss: 2.295 | Train PPL:   9.920\n",
            "\t Val. Loss: 4.895 |  Val. PPL: 133.626\n",
            "Epoch: 20 | Time: 0m 2s\n",
            "\tTrain Loss: 2.301 | Train PPL:   9.981\n",
            "\t Val. Loss: 4.895 |  Val. PPL: 133.678\n",
            "4.88964198033015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcsr26pardUo",
        "outputId": "7dd7f8bc-b25e-4b0f-d69a-cd7e9276a6b8"
      },
      "source": [
        "N_EPOCHS = 20\r\n",
        "CLIP = 1\r\n",
        "model.load_state_dict(torch.load('/content/CommonQA.pt'))\r\n",
        "best_valid_loss = 4.890922844409943 #float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "print(best_valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 2.241 | Train PPL:   9.402\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.904\n",
            "Epoch: 02 | Time: 0m 2s\n",
            "\tTrain Loss: 2.242 | Train PPL:   9.415\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.907\n",
            "Epoch: 03 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.474\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.911\n",
            "Epoch: 04 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.475\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.913\n",
            "Epoch: 05 | Time: 0m 2s\n",
            "\tTrain Loss: 2.248 | Train PPL:   9.470\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.915\n",
            "Epoch: 06 | Time: 0m 2s\n",
            "\tTrain Loss: 2.247 | Train PPL:   9.463\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.918\n",
            "Epoch: 07 | Time: 0m 2s\n",
            "\tTrain Loss: 2.245 | Train PPL:   9.439\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.920\n",
            "Epoch: 08 | Time: 0m 2s\n",
            "\tTrain Loss: 2.248 | Train PPL:   9.465\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.924\n",
            "Epoch: 09 | Time: 0m 2s\n",
            "\tTrain Loss: 2.242 | Train PPL:   9.415\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.927\n",
            "Epoch: 10 | Time: 0m 2s\n",
            "\tTrain Loss: 2.252 | Train PPL:   9.511\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.932\n",
            "Epoch: 11 | Time: 0m 2s\n",
            "\tTrain Loss: 2.247 | Train PPL:   9.455\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.936\n",
            "Epoch: 12 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.476\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.938\n",
            "Epoch: 13 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.479\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.942\n",
            "Epoch: 14 | Time: 0m 2s\n",
            "\tTrain Loss: 2.252 | Train PPL:   9.506\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.945\n",
            "Epoch: 15 | Time: 0m 2s\n",
            "\tTrain Loss: 2.250 | Train PPL:   9.490\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.948\n",
            "Epoch: 16 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.480\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.947\n",
            "Epoch: 17 | Time: 0m 2s\n",
            "\tTrain Loss: 2.250 | Train PPL:   9.485\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.949\n",
            "Epoch: 18 | Time: 0m 2s\n",
            "\tTrain Loss: 2.246 | Train PPL:   9.447\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.953\n",
            "Epoch: 19 | Time: 0m 2s\n",
            "\tTrain Loss: 2.249 | Train PPL:   9.479\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.955\n",
            "Epoch: 20 | Time: 0m 2s\n",
            "\tTrain Loss: 2.252 | Train PPL:   9.511\n",
            "\t Val. Loss: 4.890 |  Val. PPL: 132.959\n",
            "4.889623761177063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-T7eGpdQk7J"
      },
      "source": [
        "#### Final Perplexity = \r\n",
        "Epoch: 01 | Time: 0m 2s\r\n",
        "\tTrain Loss: 2.241 | Train PPL:   9.402\r\n",
        "\t Val. Loss: 4.890 |  Val. PPL: 132.904"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztR5mNm8EzFn"
      },
      "source": [
        "Finally, we test the model on the test set using these \"best\" parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaJo3X9aEzFn"
      },
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}